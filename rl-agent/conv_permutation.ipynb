{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv_permutation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q5g2SJTGxYy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "c2b3c5e2-f471-439d-e986-d3407579b221"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po1QMEWOQOs4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f091b93d-3de9-49a3-c225-b0b538332844"
      },
      "source": [
        "!pip install -r drive/My\\ Drive/IVGS/RL/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting absl-py==0.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 3.4MB/s \n",
            "\u001b[?25hCollecting astor==0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
            "Collecting attrs==19.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/23/96/d828354fa2dbdf216eaa7b7de0db692f12c234f7ef888cc14980ef40d1d2/attrs-19.1.0-py2.py3-none-any.whl\n",
            "Collecting backcall==0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/84/71/c8ca4f5bb1e08401b916c68003acf0a0655df935d74d93bf3f3364b310e0/backcall-0.1.0.tar.gz\n",
            "Collecting bleach==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/05/27e1466475e816d3001efb6e0a85a819be17411420494a1e602c36f8299d/bleach-3.1.0-py2.py3-none-any.whl (157kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 14.8MB/s \n",
            "\u001b[?25hCollecting certifi==2019.6.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 15.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 8)) (0.10.0)\n",
            "Collecting decorator==4.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/88/0075e461560a1e750a0dcbf77f1d9de775028c37a19a346a6c565a257399/decorator-4.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 10)) (0.6.0)\n",
            "Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 11)) (0.3)\n",
            "Collecting future==0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 14.8MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting google-pasta==0.1.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n",
            "\u001b[?25hCollecting grpcio==1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/83/18f374294bf34128a448ee2fae37651f943b0b5fa473b5b3aff262c15bf8/grpcio-1.21.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 27.1MB/s \n",
            "\u001b[?25hCollecting gym==0.12.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/c4/307107c687f75267d645415d57db8c0a6e29e20ac30d8f4a10e8030b6737/gym-0.12.5.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 43.7MB/s \n",
            "\u001b[?25hCollecting h5py==2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 40.2MB/s \n",
            "\u001b[?25hCollecting idna==2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hCollecting ipykernel==5.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/35/dd97fbb48d4e6b5ae97307497e31e46691adc2feedb6279d29fc1c8ad9c1/ipykernel-5.1.1-py3-none-any.whl (114kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 47.4MB/s \n",
            "\u001b[?25hCollecting ipython==7.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/2e/41dce4ed129057e05a555a7f9629aa2d5f81fdcd4d16568bc24b75a1d2c9/ipython-7.5.0-py3-none-any.whl (770kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 21)) (0.2.0)\n",
            "Collecting ipywidgets==7.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/c2/20a3514f87fc063b4853673966e85c091843de659374d6e1dd046319815a/ipywidgets-7.5.0-py2.py3-none-any.whl (121kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 47.8MB/s \n",
            "\u001b[?25hCollecting jedi==0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/42/6309f3871b2f8361764ac5b2fe6719f9c6e6561d9307d8cecda319cf5843/jedi-0.14.0-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 44.1MB/s \n",
            "\u001b[?25hCollecting Jinja2==2.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/e7/fd8b501e7a6dfe492a433deb7b9d833d39ca74916fa8bc63dd1a4947a671/Jinja2-2.10.1-py2.py3-none-any.whl (124kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 36.2MB/s \n",
            "\u001b[?25hCollecting jsonschema==3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/69/df679dfbdd051568b53c38ec8152a3ab6bc533434fc7ed11ab034bf5e82f/jsonschema-3.0.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 26)) (1.0.0)\n",
            "Collecting jupyter-client==5.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/c3/3043fe9ffd140d03c9d091a056794ccdc427c56ec19b8eea74f9ea0a498f/jupyter_client-5.2.4-py2.py3-none-any.whl (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.7MB/s \n",
            "\u001b[?25hCollecting jupyter-console==6.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/ee/6374ae8c21b7d0847f9c3722dcdfac986b8e54fa9ad9ea66e1eb6320d2b8/jupyter_console-6.0.0-py2.py3-none-any.whl\n",
            "Collecting jupyter-core==4.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/25/6ffb0f6e57fa6ef5d2f814377133b361b42a6dd39105f4885a4f1666c2c3/jupyter_core-4.5.0-py2.py3-none-any.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.8MB/s \n",
            "\u001b[?25hCollecting Keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 31.9MB/s \n",
            "\u001b[?25hCollecting Keras-Applications==1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hCollecting Keras-Preprocessing==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hCollecting kiwisolver==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.9MB/s \n",
            "\u001b[?25hCollecting Markdown==3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 35)) (1.1.1)\n",
            "Collecting matplotlib==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/83/d989ee20c78117c737ab40e0318ea221f1aed4e3f5a40b4f93541b369b93/matplotlib-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 37)) (0.8.4)\n",
            "Collecting nbconvert==5.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/e7/f46c9d65f149271e47fca6ab084ef5c6e4cb1870f4c5cce6690feac55231/nbconvert-5.5.0-py2.py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 44.1MB/s \n",
            "\u001b[?25hCollecting nbformat==4.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/27/9a654d2b6cc1eaa517d1c5a4405166c7f6d72f04f6e7eea41855fe808a46/nbformat-4.4.0-py2.py3-none-any.whl (155kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 41.8MB/s \n",
            "\u001b[?25hCollecting notebook==5.7.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/36/89ebfffc9dd8c8dbd81c1ffb53e3d4233ee666414c143959477cb07cc5f5/notebook-5.7.8-py2.py3-none-any.whl (9.0MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0MB 16.5MB/s \n",
            "\u001b[?25hCollecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 237kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandocfilters==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 42)) (1.4.2)\n",
            "Collecting parso==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/59/482f5a00fe3da7f0aaeedf61c2a25c445b68c9124437195f6e8b2beddbc0/parso-0.5.0-py2.py3-none-any.whl (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.0MB/s \n",
            "\u001b[?25hCollecting pexpect==4.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3e/377007e3f36ec42f1b84ec322ee12141a9e10d808312e5738f52f80a232c/pexpect-4.7.0-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 45)) (0.7.5)\n",
            "Collecting prometheus-client==0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/23/41a5a24b502d35a4ad50a5bb7202a5e1d9a0364d0c12f56db3dbf7aca76d/prometheus_client-0.7.1.tar.gz\n",
            "Collecting prompt-toolkit==2.0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 43.2MB/s \n",
            "\u001b[?25hCollecting protobuf==3.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/fb/29de8d08967f0cce1bb10b39846d836b0f3bf6776ddc36aed7c73498ca7e/protobuf-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: ptyprocess==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 49)) (0.6.0)\n",
            "Collecting pyglet==1.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 42.3MB/s \n",
            "\u001b[?25hCollecting Pygments==2.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/73/1dfa428150e3ccb0fa3e68db406e5be48698f2a979ccbcec795f28f44048/Pygments-2.4.2-py2.py3-none-any.whl (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.0MB/s \n",
            "\u001b[?25hCollecting pyparsing==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d9/3ec19e966301a6e25769976999bd7bbe552016f0d32b577dc9d63d2e0c49/pyparsing-2.4.0-py2.py3-none-any.whl (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hCollecting pyrsistent==0.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/0b/f514e76b4e074386b60cfc6c8c2d75ca615b81e415417ccf3fac80ae0bf6/pyrsistent-0.15.2.tar.gz (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 45.0MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 46.8MB/s \n",
            "\u001b[?25hCollecting PyYAML==5.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 42.8MB/s \n",
            "\u001b[?25hCollecting pyzmq==18.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/92/ea8f20560d5f1d0c6eb3c7c67ca72abfb97307f4e6494fc05cc7c37904cf/pyzmq-18.0.2-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.3MB/s \n",
            "\u001b[?25hCollecting qtconsole==4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/0b/efb5a694b6922bb85c35e4f1db6197daae23c764dd384023fc9517d79e26/qtconsole-4.5.1-py2.py3-none-any.whl (118kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 50.8MB/s \n",
            "\u001b[?25hCollecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.9MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 60)) (1.5.0)\n",
            "Collecting six==1.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting tensorboard==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.7MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 50kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 65)) (1.1.0)\n",
            "Collecting terminado==0.8.2\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/56/80ea7fa66565fa75ae21ce0c16bc90067530e5d15e48854afcc86585a391/terminado-0.8.2-py2.py3-none-any.whl\n",
            "Collecting testpath==0.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/a4/162f9ebb6489421fe46dcca2ae420369edfee4b563c668d93cb4605d12ba/testpath-0.4.2-py2.py3-none-any.whl (163kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 44.1MB/s \n",
            "\u001b[?25hCollecting tornado==6.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/78/2d2823598496127b21423baffaa186b668f73cd91887fcef78b6eade136b/tornado-6.0.3.tar.gz (482kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 43.0MB/s \n",
            "\u001b[?25hCollecting traitlets==4.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/d6/abcb22de61d78e2fc3959c964628a5771e47e7cc60d53e9342e21ed6cc9a/traitlets-4.3.2-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 48.1MB/s \n",
            "\u001b[?25hCollecting wcwidth==0.1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/9f/526a6947247599b084ee5232e4f9190a38f398d7300d866af3ab571a5bfe/wcwidth-0.1.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/IVGS/RL/requirements.txt (line 72)) (0.5.1)\n",
            "Collecting Werkzeug==0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/57/92a497e38161ce40606c27a86759c6b92dd34fcdb33f64171ec559257c02/Werkzeug-0.15.4-py2.py3-none-any.whl (327kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 47.4MB/s \n",
            "\u001b[?25hCollecting widgetsnbextension==3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/2b/32408aa2aaf5230450903b148dae888734add9e2fc190a817811546d2f93/widgetsnbextension-3.5.0-py2.py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 39.7MB/s \n",
            "\u001b[?25hCollecting wrapt==1.11.2\n",
            "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython==7.5.0->-r drive/My Drive/IVGS/RL/requirements.txt (line 20)) (49.6.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==1.14.0->-r drive/My Drive/IVGS/RL/requirements.txt (line 62)) (0.35.1)\n",
            "Building wheels for collected packages: absl-py, backcall, future, gast, gym, prometheus-client, pyrsistent, PyYAML, tornado, wrapt\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.7.1-cp36-none-any.whl size=117848 sha256=32f8d1d748d31936cda0ab71ba39aa722c576ead1acd0c21a60920c27c5389cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
            "  Building wheel for backcall (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for backcall: filename=backcall-0.1.0-cp36-none-any.whl size=10413 sha256=2c4a09aa9b4e143deddde43850d9691ecdc6b696642e2b8b132d701aff007564\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/b0/dd/29e28ff615af3dda4c67cab719dd51357597eabff926976b45\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.17.1-cp36-none-any.whl size=488730 sha256=9cb3a5dc3a21317d295d5f46e95c898214d21129541f76a1e29cb6f5926494cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=24567cbd6701595d27117774841b3a32b637e83953cfa5c8420b75714b438197\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.12.5-cp36-none-any.whl size=1613802 sha256=a5a8afd9cc141792bc4e36b41fc95d0e3ccf4bc784ad8e4ff401e2d6c1c5c096\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/a5/c9/87967963aa32540d543e51bcf0d0fc19c5d68b8f49598d3b98\n",
            "  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-cp36-none-any.whl size=41403 sha256=46f3ff6e00d5d39736953c2887ec9573aaf11420529900cfad1978c7c7853d31\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/54/34/fd47cd9b308826cc4292b54449c1899a30251ef3b506bc91ea\n",
            "  Building wheel for pyrsistent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrsistent: filename=pyrsistent-0.15.2-cp36-cp36m-linux_x86_64.whl size=97504 sha256=f247833b8ec0804ed742883fe7608be689fc87208ed61defcdc7a9c7de9d151d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/b9/15/c8c6a1e095a370e8c3273e65a5c982e5cf355dde16d77502f5\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.1.1-cp36-cp36m-linux_x86_64.whl size=44100 sha256=e075645fca1e83aa701a94e7dfe18b40d15c8733b44332334bdf2a0eb58931eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-6.0.3-cp36-cp36m-linux_x86_64.whl size=423186 sha256=8e9d913a1f702316b432f311553ca1e6d46c87c364059e1f336ac61f95fae0c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/bf/40/2f6ef700f48401ca40e5e3dd7d0e3c0a90e064897b7fe5fc08\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.2-cp36-cp36m-linux_x86_64.whl size=67525 sha256=51d53a6d71972467297c6495f2cba6d472980ac0c9c09f1fd1ec2a2d1f0b0045\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
            "Successfully built absl-py backcall future gast gym prometheus-client pyrsistent PyYAML tornado wrapt\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.5.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement notebook~=5.3.0; python_version >= \"3.0\", but you'll have notebook 5.7.8 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, absl-py, astor, attrs, backcall, bleach, certifi, decorator, future, gast, google-pasta, grpcio, numpy, scipy, pyglet, gym, h5py, idna, traitlets, tornado, wcwidth, prompt-toolkit, parso, jedi, pexpect, Pygments, ipython, python-dateutil, pyzmq, jupyter-core, jupyter-client, ipykernel, pyrsistent, jsonschema, nbformat, prometheus-client, terminado, Jinja2, testpath, nbconvert, notebook, widgetsnbextension, ipywidgets, jupyter-console, Keras-Preprocessing, Keras-Applications, PyYAML, Keras, kiwisolver, Markdown, pyparsing, matplotlib, protobuf, qtconsole, urllib3, requests, Werkzeug, tensorboard, wrapt, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: absl-py 0.8.1\n",
            "    Uninstalling absl-py-0.8.1:\n",
            "      Successfully uninstalled absl-py-0.8.1\n",
            "  Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Found existing installation: attrs 20.1.0\n",
            "    Uninstalling attrs-20.1.0:\n",
            "      Successfully uninstalled attrs-20.1.0\n",
            "  Found existing installation: backcall 0.2.0\n",
            "    Uninstalling backcall-0.2.0:\n",
            "      Successfully uninstalled backcall-0.2.0\n",
            "  Found existing installation: bleach 3.1.5\n",
            "    Uninstalling bleach-3.1.5:\n",
            "      Successfully uninstalled bleach-3.1.5\n",
            "  Found existing installation: certifi 2020.6.20\n",
            "    Uninstalling certifi-2020.6.20:\n",
            "      Successfully uninstalled certifi-2020.6.20\n",
            "  Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Found existing installation: grpcio 1.31.0\n",
            "    Uninstalling grpcio-1.31.0:\n",
            "      Successfully uninstalled grpcio-1.31.0\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: gym 0.17.2\n",
            "    Uninstalling gym-0.17.2:\n",
            "      Successfully uninstalled gym-0.17.2\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: traitlets 4.3.3\n",
            "    Uninstalling traitlets-4.3.3:\n",
            "      Successfully uninstalled traitlets-4.3.3\n",
            "  Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Found existing installation: wcwidth 0.2.5\n",
            "    Uninstalling wcwidth-0.2.5:\n",
            "      Successfully uninstalled wcwidth-0.2.5\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: parso 0.7.1\n",
            "    Uninstalling parso-0.7.1:\n",
            "      Successfully uninstalled parso-0.7.1\n",
            "  Found existing installation: jedi 0.17.2\n",
            "    Uninstalling jedi-0.17.2:\n",
            "      Successfully uninstalled jedi-0.17.2\n",
            "  Found existing installation: pexpect 4.8.0\n",
            "    Uninstalling pexpect-4.8.0:\n",
            "      Successfully uninstalled pexpect-4.8.0\n",
            "  Found existing installation: Pygments 2.1.3\n",
            "    Uninstalling Pygments-2.1.3:\n",
            "      Successfully uninstalled Pygments-2.1.3\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Found existing installation: python-dateutil 2.8.1\n",
            "    Uninstalling python-dateutil-2.8.1:\n",
            "      Successfully uninstalled python-dateutil-2.8.1\n",
            "  Found existing installation: pyzmq 19.0.2\n",
            "    Uninstalling pyzmq-19.0.2:\n",
            "      Successfully uninstalled pyzmq-19.0.2\n",
            "  Found existing installation: jupyter-core 4.6.3\n",
            "    Uninstalling jupyter-core-4.6.3:\n",
            "      Successfully uninstalled jupyter-core-4.6.3\n",
            "  Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "  Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Found existing installation: pyrsistent 0.16.0\n",
            "    Uninstalling pyrsistent-0.16.0:\n",
            "      Successfully uninstalled pyrsistent-0.16.0\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Found existing installation: nbformat 5.0.7\n",
            "    Uninstalling nbformat-5.0.7:\n",
            "      Successfully uninstalled nbformat-5.0.7\n",
            "  Found existing installation: prometheus-client 0.8.0\n",
            "    Uninstalling prometheus-client-0.8.0:\n",
            "      Successfully uninstalled prometheus-client-0.8.0\n",
            "  Found existing installation: terminado 0.8.3\n",
            "    Uninstalling terminado-0.8.3:\n",
            "      Successfully uninstalled terminado-0.8.3\n",
            "  Found existing installation: Jinja2 2.11.2\n",
            "    Uninstalling Jinja2-2.11.2:\n",
            "      Successfully uninstalled Jinja2-2.11.2\n",
            "  Found existing installation: testpath 0.4.4\n",
            "    Uninstalling testpath-0.4.4:\n",
            "      Successfully uninstalled testpath-0.4.4\n",
            "  Found existing installation: nbconvert 5.6.1\n",
            "    Uninstalling nbconvert-5.6.1:\n",
            "      Successfully uninstalled nbconvert-5.6.1\n",
            "  Found existing installation: notebook 5.3.1\n",
            "    Uninstalling notebook-5.3.1:\n",
            "      Successfully uninstalled notebook-5.3.1\n",
            "  Found existing installation: widgetsnbextension 3.5.1\n",
            "    Uninstalling widgetsnbextension-3.5.1:\n",
            "      Successfully uninstalled widgetsnbextension-3.5.1\n",
            "  Found existing installation: ipywidgets 7.5.1\n",
            "    Uninstalling ipywidgets-7.5.1:\n",
            "      Successfully uninstalled ipywidgets-7.5.1\n",
            "  Found existing installation: jupyter-console 5.2.0\n",
            "    Uninstalling jupyter-console-5.2.0:\n",
            "      Successfully uninstalled jupyter-console-5.2.0\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "  Found existing installation: kiwisolver 1.2.0\n",
            "    Uninstalling kiwisolver-1.2.0:\n",
            "      Successfully uninstalled kiwisolver-1.2.0\n",
            "  Found existing installation: Markdown 3.2.2\n",
            "    Uninstalling Markdown-3.2.2:\n",
            "      Successfully uninstalled Markdown-3.2.2\n",
            "  Found existing installation: pyparsing 2.4.7\n",
            "    Uninstalling pyparsing-2.4.7:\n",
            "      Successfully uninstalled pyparsing-2.4.7\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "  Found existing installation: qtconsole 4.7.6\n",
            "    Uninstalling qtconsole-4.7.6:\n",
            "      Successfully uninstalled qtconsole-4.7.6\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed Jinja2-2.10.1 Keras-2.2.4 Keras-Applications-1.0.8 Keras-Preprocessing-1.1.0 Markdown-3.1.1 PyYAML-5.1.1 Pygments-2.4.2 Werkzeug-0.15.4 absl-py-0.7.1 astor-0.8.0 attrs-19.1.0 backcall-0.1.0 bleach-3.1.0 certifi-2019.6.16 decorator-4.4.0 future-0.17.1 gast-0.2.2 google-pasta-0.1.7 grpcio-1.21.1 gym-0.12.5 h5py-2.9.0 idna-2.8 ipykernel-5.1.1 ipython-7.5.0 ipywidgets-7.5.0 jedi-0.14.0 jsonschema-3.0.1 jupyter-client-5.2.4 jupyter-console-6.0.0 jupyter-core-4.5.0 kiwisolver-1.1.0 matplotlib-3.1.0 nbconvert-5.5.0 nbformat-4.4.0 notebook-5.7.8 numpy-1.16.4 parso-0.5.0 pexpect-4.7.0 prometheus-client-0.7.1 prompt-toolkit-2.0.9 protobuf-3.8.0 pyglet-1.3.2 pyparsing-2.4.0 pyrsistent-0.15.2 python-dateutil-2.8.0 pyzmq-18.0.2 qtconsole-4.5.1 requests-2.22.0 scipy-1.3.0 six-1.12.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 terminado-0.8.2 testpath-0.4.2 tornado-6.0.3 traitlets-4.3.2 urllib3-1.25.3 wcwidth-0.1.7 widgetsnbextension-3.5.0 wrapt-1.11.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "dateutil",
                  "decorator",
                  "google",
                  "ipykernel",
                  "ipywidgets",
                  "jupyter_client",
                  "jupyter_core",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pexpect",
                  "prompt_toolkit",
                  "pygments",
                  "pyparsing",
                  "six",
                  "tornado",
                  "traitlets",
                  "wcwidth",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66DxQx0oQgGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "ff8af885-8f56-4c1b-a787-a8d37609cfbe"
      },
      "source": [
        "!unzip drive/My\\ Drive/IVGS/RL/lib.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/IVGS/RL/lib.zip\n",
            "   creating: bella/\n",
            "  inflating: bella/config.yml        \n",
            " extracting: bella/__init__.py       \n",
            "   creating: bella/test/\n",
            " extracting: bella/test/__init__.py  \n",
            "  inflating: bella/test/api.py       \n",
            "  inflating: bella/ciao.py           \n",
            "  inflating: bella/config.py         \n",
            "  inflating: bella/api.py            \n",
            " extracting: bella/.gitignore        \n",
            "   creating: gemel/\n",
            " extracting: gemel/__init__.py       \n",
            "  inflating: gemel/config.py         \n",
            "   creating: gemel/vnet/\n",
            " extracting: gemel/vnet/__init__.py  \n",
            "  inflating: gemel/vnet/vtn.py       \n",
            "   creating: gemel/utils/\n",
            "  inflating: gemel/utils/log.py      \n",
            " extracting: gemel/utils/__init__.py  \n",
            "  inflating: gemel/utils/rest.py     \n",
            "  inflating: gemel/utils/ssh.py      \n",
            "  inflating: gemel/utils/shell.py    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3CzB7cFRPu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bella.ciao import GemelEnv\n",
        "\n",
        "import time\n",
        "import os\n",
        "import threading\n",
        "import gym\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "from queue import Queue\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import enum\n",
        "from itertools import permutations\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.backend import tensorflow_backend as K\n",
        "\n",
        "from keras.layers import Dense, Flatten, Input, Add, Lambda, Embedding, Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras.layers.core import Activation, Reshape\n",
        "\n",
        "from IPython.core.display import display, HTML, clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeklJ_nBRmV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPSILON = 0.1\n",
        "EXPLORATION_DECAY = 0.99\n",
        "GAMMA = 0.99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qO-Qmr2R1sP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█'):\n",
        "    \"\"\"\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        iteration   - Required  : current iteration (Int)\n",
        "        total       - Required  : total iterations (Int)\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "    \"\"\"\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - filledLength)\n",
        "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total:\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj_mR-LnR2-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    class StateModel(enum.Enum):\n",
        "        VN_ONLY = 1\n",
        "        IDS = 2\n",
        "\n",
        "    def __init__(self, env, max_eps, period=10,\n",
        "                 state_mode=StateModel.IDS, model=None,\n",
        "                 gamma=GAMMA, max_epsilon=EPSILON,\n",
        "                 epsilon_decay=EXPLORATION_DECAY):\n",
        "\n",
        "        self.env = env\n",
        "        self.max_episodes = max_eps\n",
        "        self.epsilon = max_epsilon\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.epsilon_dacay = epsilon_decay\n",
        "        self.period = period\n",
        "        self.state_mode = state_mode\n",
        "        self.gamma = gamma\n",
        "        self.model = model or self._create_model()\n",
        "\n",
        "    def _create_model(self):\n",
        "        \"\"\"\n",
        "        Builds a neural net model to digest the state\n",
        "        \"\"\"\n",
        "        model = Sequential()\n",
        "        model.add(Dense(\n",
        "            5,\n",
        "            # input_shape=,\n",
        "            input_shape=(len(self.env._hosts_sorted_by_id),) \\\n",
        "                if self.state_mode == DQNAgent.StateModel.VN_ONLY \\\n",
        "                else self.env.observation_shape(),\n",
        "            activation=\"relu\"\n",
        "        ))\n",
        "        # model.add(Dense(20, activation=\"relu\"))\n",
        "        model.add(Dense(self.env.action_space.n, activation=\"linear\"))\n",
        "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def _to_feature_vector(self, state, ep):\n",
        "        if self.state_mode == DQNAgent.StateModel.VN_ONLY:\n",
        "            return state[0]\n",
        "        elif self.state_mode == DQNAgent.StateModel.IDS:\n",
        "            state_1 = np.array(list(permutations(state[1]))[100*ep])\n",
        "            return np.concatenate((state[0], state_1.flatten()))\n",
        "        else:\n",
        "            raise Exception(f\"state model {self.state_mode} unknown\")\n",
        "\n",
        "    def actionReverse(self, action, ep):\n",
        "        idx = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
        "        perm_idx = list(permutations(idx))[100*ep]\n",
        "        host = action // 2\n",
        "        security = action % 2\n",
        "        action = (2*perm_idx[host] + security) if host < len(perm_idx) else action\n",
        "        return action\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        histories = []\n",
        "\n",
        "        # train for max_eps episodes\n",
        "        for episode in range(1, self.max_episodes + 1):\n",
        "\n",
        "            printProgressBar(episode, self.max_episodes)\n",
        "\n",
        "            # start at random position\n",
        "            _, terminal, step = self.env.reset(), False, 0\n",
        "\n",
        "            time.sleep(self.period)\n",
        "\n",
        "            state = self.env.state()\n",
        "\n",
        "            # flatten state\n",
        "            state = self._to_feature_vector(state, episode)\n",
        "\n",
        "            history = []\n",
        "\n",
        "            # iterate step-by-step\n",
        "            while not terminal:\n",
        "\n",
        "                step += 1\n",
        "\n",
        "                # pick action based on policy\n",
        "                action_raw, is_random = self.policy(state)\n",
        "                action = self.actionReverse(action_raw, episode)\n",
        "\n",
        "                print()\n",
        "                print(f\"Taking action {action} from {action_raw}\")\n",
        "\n",
        "                # run action and get reward\n",
        "                state_next_raw, reward, terminal = self.env.step(action)\n",
        "\n",
        "                # instead of using the immediate next state, wait for it to simmer\n",
        "                if self.period > 0:\n",
        "                    time.sleep(self.period)\n",
        "                    state_next_raw = self.env.state()\n",
        "\n",
        "                # flatten state\n",
        "                state_next = self._to_feature_vector(state_next_raw, episode)\n",
        "\n",
        "                print()\n",
        "                print(f\"Step {step} reward={reward} new_state={state_next_raw[0]}\")\n",
        "\n",
        "                # # this makes sense in an episodic environement\n",
        "                # # where a terminal state means \"losing\"\n",
        "                # if terminal:\n",
        "                #    reward *= -1\n",
        "\n",
        "                preds = self.model.predict([[state_next]])\n",
        "                next_scores_prediction = preds[0]\n",
        "\n",
        "                print(f\"Predicted scores for each action in next step: {next_scores_prediction}\")\n",
        "\n",
        "                # compute target Q\n",
        "                q_target = (reward + self.gamma * np.amax(next_scores_prediction)) if not terminal else reward\n",
        "\n",
        "                # update model\n",
        "                q_updated = self.model.predict([[state]])[0]\n",
        "                q_updated[action_raw] = q_target\n",
        "                self.model.fit([[state]], [[q_updated]], verbose=0)\n",
        "\n",
        "                # update current state\n",
        "                state = state_next\n",
        "\n",
        "                # update history\n",
        "                history.append({\n",
        "                    \"time\": step,\n",
        "                    \"action\": action_raw,\n",
        "                    \"reward\": reward,\n",
        "                    \"state\": state_next_raw[0].tolist(),\n",
        "                    \"random\": is_random,\n",
        "                    \"prediction\": preds,\n",
        "                })\n",
        "\n",
        "            histories.append(history)\n",
        "\n",
        "            # apply exploration decay\n",
        "            self.epsilon *= self.epsilon_dacay\n",
        "            print(f\"Epsilon reduced to {self.epsilon}\")\n",
        "\n",
        "        return histories\n",
        "\n",
        "    def policy(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            print(\"PERFORMING RANDOM ACTION\")\n",
        "            return np.random.randint(self.env.action_space.n), True\n",
        "        else:\n",
        "            expected_rewards = self.model.predict([[state]])[0]\n",
        "            return np.argmax(expected_rewards), False\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        state, done = self.env.reset(), False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            state = self._to_feature_vector(state, 0)\n",
        "            exp_rew = self.model.predict([[state]])[0]\n",
        "            action = np.argmax(exp_rew)\n",
        "            new_state, reward, done = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            self.env.render()\n",
        "            time.sleep(0.05)\n",
        "            state = new_state\n",
        "\n",
        "        # self.env.close()\n",
        "        print(f\"Total reward: {total_reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3oO8OPTSH99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_conv_15(env):\n",
        "    input_shape = env.observation_shape()\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape + (1, ), input_shape=input_shape))\n",
        "    model.add(Conv1D(3, kernel_size=3, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(env.action_space.n))\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMy49FBaSUcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "afbd3e4b-053e-4da7-d5c1-42b5b42cee73"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=50, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=4, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_15(env), max_epsilon=0.2, epsilon_decay=0.8)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_10 (Reshape)         (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 187, 3)            12        \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 561)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 19)                10678     \n",
            "=================================================================\n",
            "Total params: 10,690\n",
            "Trainable params: 10,690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |█████████████████████████---------------------------------------------------------------------------| 25.0% \r\n",
            "Taking action 10\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.01859224 -0.02113799 -0.07403752  0.17268398 -0.31429738 -0.19203438\n",
            "  0.21484974 -0.37861913  0.27126408  0.37427974 -0.18797019  0.07995581\n",
            " -0.07618207 -0.05447214  0.32823795  0.0628845  -0.3131653  -0.04188405\n",
            "  0.12176893]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 2 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05853037  0.19199516 -0.08604752 -0.11965869 -0.27117437 -0.20004056\n",
            " -0.06564917 -0.13095252 -0.01258255  0.12110567  0.25834706 -0.01097299\n",
            "  0.3828965   0.04222479  0.03184314  0.3987868  -0.18942362  0.08102379\n",
            "  0.22541477]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03858925  0.14891128 -0.20487818 -0.14161028 -0.23473871 -0.03181647\n",
            "  0.10216586 -0.3148276  -0.00209893 -0.16154984  0.18594378 -0.05719914\n",
            "  0.3037342   0.14491983 -0.17993441  0.322645   -0.29894835  0.17340206\n",
            " -0.12789214]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.06789868 -0.10641503 -0.03004004 -0.05719858  0.08004916 -0.15068348\n",
            "  0.17388137  0.06371991 -0.04487799  0.23019907  0.18688683  0.00602128\n",
            "  0.13870442  0.15147118 -0.06866429  0.1328042  -0.09507132 -0.01382701\n",
            "  0.00347617]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.5628241e-01 -1.7201106e-04  1.2163554e-02  1.2052413e-02\n",
            "  1.0383648e-01 -2.1951978e-01 -8.5284948e-02  1.0481467e-01\n",
            " -2.6539598e-02  3.2343380e-02  1.4968364e-01 -2.6512427e-02\n",
            "  3.8094785e-02  1.5841570e-02 -1.4385271e-01  1.5203418e-01\n",
            " -1.3065428e-01  5.7371184e-02  3.2014795e-02]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.23513153 -0.06192651  0.27148628 -0.07767773 -0.20729409 -0.1679037\n",
            "  0.10475051 -0.15312345 -0.03821582 -0.04580741  0.02741719 -0.08004169\n",
            "  0.28518942  0.25988647  0.05269609  0.4323355  -0.41241243  0.00525808\n",
            " -0.25863987]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 7 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.07637414 -0.21503237 -0.12719928 -0.33762008 -0.36564153 -0.6099556\n",
            " -0.03492078  0.18924762  0.05713368  0.0849966   0.18354128 -0.14601122\n",
            "  0.34317765  0.03911392  0.01856752  0.258988   -0.20717512  0.22469862\n",
            "  0.12587681]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 8 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.07681233 -0.33605018  0.00437304  0.04882247  0.04371464 -0.04507995\n",
            "  0.05587975  0.08424737 -0.07308748  0.01975473 -0.00880211 -0.00834755\n",
            "  0.0286346   0.00768658 -0.00847698  0.02283904 -0.01632828  0.01958672\n",
            "  0.03163627]\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 9 reward=-3 new_state=[0 1 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.04614768 -0.00931484 -0.17634395 -0.4191358  -0.4593408  -0.31719053\n",
            "  0.24628913 -0.07104006 -0.04345647 -0.15292579  0.2224731  -0.00134004\n",
            "  0.32510817  0.08455192 -0.08411302  0.0326864  -0.25003663  0.10374626\n",
            "  0.04803483]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 10 reward=-3 new_state=[0 1 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.03930647 -0.18257876 -0.13692632 -0.37239277 -0.46204543  0.0694182\n",
            "  0.10645149 -0.12338241 -0.23412539 -0.2509533   0.22260787 -0.03600493\n",
            "  0.41089788 -0.01311309  0.1037425   0.4393386  -0.30738035 -0.18570453\n",
            " -0.11338106]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 11 reward=-3 new_state=[0 1 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.04307763 -0.05088362  0.1449917  -0.2417554  -0.11329569  0.20397873\n",
            "  0.12859519 -0.1780333  -0.07638621 -0.14371185  0.01389978 -0.12042766\n",
            "  0.2888362  -0.2204924   0.08439947  0.28780156 -0.15586153 -0.15193626\n",
            " -0.03785194]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 12 reward=-3 new_state=[0 1 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.15805462  0.08031793  0.12949361 -0.23688647 -0.24686763 -0.01688456\n",
            "  0.3776701   0.09152626 -0.24885806 -0.13604327  0.08405036 -0.06121287\n",
            "  0.21857508 -0.13290426 -0.03789388  0.11832098 -0.19814211 -0.15458564\n",
            "  0.16199261]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 13 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4895743  -0.10800119 -0.07490975 -0.23687045  0.05320562 -0.06688307\n",
            "  0.40056217  0.03515952 -0.3526123   0.23634434  0.02764181 -0.23114415\n",
            "  0.11752583  0.2059626   0.04270044  0.23250988 -0.06205532 -0.10120144\n",
            "  0.20511124]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 14 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.00097355  0.17357741 -0.39781964 -0.25959557 -0.251291   -0.2530887\n",
            "  0.45854035 -0.09088257 -0.03200003  0.1649321   0.2101004  -0.09814116\n",
            " -0.06483182  0.15111245 -0.03780361 -0.15093656  0.01226324  0.07167596\n",
            "  0.1374165 ]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 15 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.07712352 -0.02401653  0.07941988 -0.11630172 -0.02996105  0.05859552\n",
            "  0.10457238 -0.1596238  -0.04934224 -0.17899688  0.04303898  0.01928648\n",
            "  0.1287458   0.15525033 -0.14006189  0.3091493  -0.40464616 -0.13387056\n",
            " -0.15651494]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 16 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.22674829 -0.24736132 -0.10754432 -0.26596814 -0.23482466 -0.19668134\n",
            "  0.08006819 -0.20655999 -0.08693229  0.05976619  0.21637495 -0.0330656\n",
            "  0.32974625  0.31871325 -0.01549046  0.40120238 -0.4448351  -0.17249525\n",
            " -0.12418336]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 17 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.00790495 -0.03242678 -0.42375073 -0.33044174 -0.17080632 -0.29454803\n",
            " -0.05070017  0.05225483 -0.11454368 -0.06323341  0.342227   -0.04129241\n",
            "  0.45544466  0.00113327 -0.20858064  0.257587   -0.13779306 -0.32919547\n",
            " -0.06463932]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 18 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.19331717 -0.3971809   0.00102783 -0.07046936  0.03156624 -0.11615417\n",
            "  0.0348424  -0.09719416 -0.19338879 -0.06287988 -0.13584961 -0.19909738\n",
            " -0.02949163  0.05605115 -0.02151058  0.34043953 -0.24026905 -0.05325758\n",
            " -0.00519568]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 19 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.42862025 -0.02679193 -0.10872539 -0.30082917 -0.17352785 -0.25513566\n",
            "  0.12666927 -0.2653802  -0.1765757  -0.08449275  0.30462724 -0.13625039\n",
            "  0.25883982  0.3008341  -0.18092346  0.49579242 -0.3622378  -0.12377592\n",
            "  0.05682356]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 20 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.07698856  0.23656407 -0.20636378 -0.27710232 -0.5829707  -0.15165877\n",
            "  0.08436304 -0.14939392 -0.04512931 -0.07419     0.40682912 -0.04524294\n",
            "  0.25651202  0.38152716 -0.21846199 -0.19568983 -0.16529907 -0.04841273\n",
            "  0.13564014]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 21 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4851583  -0.13748515 -0.00667179 -0.28652915  0.24650767 -0.17055808\n",
            " -0.04949025 -0.02991595  0.02583788  0.21106198  0.05248942 -0.19388144\n",
            "  0.12900604 -0.06354915  0.17668359  0.16667554 -0.20072462 -0.2921448\n",
            "  0.06690941]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 22 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.33845446 -0.06690855 -0.02429432 -0.2193998  -0.10365771 -0.06261563\n",
            "  0.10383581 -0.1522056  -0.14142741 -0.2717708   0.09433261 -0.10413362\n",
            "  0.14842041  0.18175237 -0.21602179  0.39057022 -0.28878316 -0.13547269\n",
            " -0.15718834]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 23 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02055176  0.01337336 -0.09259607 -0.37204206 -0.42056817 -0.2341689\n",
            "  0.01839482 -0.32340378 -0.12458472 -0.09832345  0.26263323 -0.07261593\n",
            "  0.16078451  0.39370167 -0.07292562 -0.08927403 -0.23450622 -0.11586128\n",
            "  0.15176795]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-4.4500303e-01 -1.7719932e-01 -6.4005345e-02 -2.6934421e-01\n",
            "  3.4348419e-01 -6.3140318e-02 -9.7064152e-02  1.7933109e-01\n",
            "  1.2259585e-02 -2.0646717e-02  9.3450144e-02 -5.0060745e-02\n",
            " -3.8843043e-04  2.3077136e-02  1.6406102e-01  2.8799695e-01\n",
            " -2.6459634e-01 -3.6254454e-01  2.5753731e-02]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5038127e-01 -2.1816444e-01  8.4132314e-02 -4.9566790e-01\n",
            " -2.5225085e-01 -4.3497750e-01 -6.1782800e-02 -2.6326177e-01\n",
            " -1.2331452e-01 -4.9291318e-04  2.4919380e-01  9.3299463e-02\n",
            "  2.3766927e-01  3.6566716e-01  7.6162949e-02  2.8395268e-01\n",
            " -2.7909932e-01 -2.5010195e-01 -1.3906856e-01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.13330156  0.09891715 -0.2931371  -0.34456143 -0.3095924  -0.23227604\n",
            "  0.26448876 -0.22781695 -0.08338692 -0.01838689  0.2719493   0.05774157\n",
            " -0.11783984  0.41087335 -0.11726773  0.28547615 -0.20273137 -0.23976299\n",
            "  0.08239642]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18224071 -0.22937414  0.11757451 -0.2367304  -0.19407956 -0.14988148\n",
            " -0.21837184  0.07368733 -0.16301525 -0.20154127  0.13659725  0.00770675\n",
            " -0.02255134  0.15221201 -0.12051232  0.09957866 -0.04962575 -0.18018398\n",
            " -0.05581952]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10408228 -0.2054957  -0.15032576 -0.6164509  -0.12506235 -0.41037947\n",
            " -0.0974222  -0.2469068  -0.06161188 -0.02045501  0.28010345 -0.18258558\n",
            "  0.17812726  0.05613865 -0.03983501  0.45357662 -0.25733605 -0.44251618\n",
            " -0.09086788]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 29 reward=-1 new_state=[0 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-9.6783005e-02  1.3254154e-01 -1.6939510e-01 -5.7830125e-01\n",
            " -4.5176533e-01 -4.4824806e-01 -4.5691028e-02 -3.0925080e-01\n",
            "  2.3155224e-02 -1.1696804e-01  2.8205720e-01  8.1282616e-02\n",
            "  9.7870916e-02  4.8468947e-02  7.0972927e-02  1.5077329e-01\n",
            " -2.9265055e-01 -3.2304415e-01  4.8905611e-04]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 30 reward=-1 new_state=[0 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.4134305  -0.0432505   0.01313304 -0.23036051  0.08124328 -0.26966944\n",
            "  0.03027989 -0.18602292  0.04798111  0.16000365  0.15923364 -0.02294679\n",
            " -0.1266187   0.34126613  0.1671811   0.19961648 -0.22078063 -0.29920006\n",
            "  0.20704116]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 31 reward=-1 new_state=[0 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.01905873 -0.31652912  0.01364168 -0.4835134  -0.08223647 -0.47111738\n",
            " -0.28930777 -0.35421598 -0.03511869 -0.38627604  0.2910249   0.02131486\n",
            "  0.30399546 -0.04026216  0.02747742  0.2873752  -0.21938223 -0.6755672\n",
            " -0.12233864]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 32 reward=-1 new_state=[0 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.11225727 -0.22098953 -0.09599388 -0.6031051  -0.40717307 -0.40497974\n",
            " -0.08722408 -0.22356856 -0.23738511 -0.34601298  0.17670166 -0.0663605\n",
            "  0.06032016  0.05280658 -0.12446792  0.15075292 -0.072258   -0.3108221\n",
            " -0.05957571]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 33 reward=-1 new_state=[0 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.37405193  0.03405523  0.0826144  -0.41975477 -0.10630288 -0.22984774\n",
            "  0.04014634 -0.34401384  0.05905125 -0.06219092 -0.09494006 -0.228366\n",
            " -0.3145952  -0.1630785   0.24818961  0.07624628 -0.0821779  -0.1515429\n",
            "  0.1001345 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 34 reward=-2 new_state=[0 1 0 0 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.07068823 -0.24244381 -0.17909062 -0.57398254 -0.04568867 -0.5389978\n",
            " -0.07584897 -0.22384138 -0.00662274  0.01414898  0.14430285 -0.00718973\n",
            "  0.35761973  0.08970746 -0.13155918  0.166221   -0.29943848 -0.5701898\n",
            " -0.22369942]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 35 reward=-3 new_state=[0 1 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.41739482  0.0090961  -0.17460623 -0.35561606 -0.26551908 -0.24276526\n",
            "  0.01378916 -0.00362593 -0.14839537  0.1294361   0.11549154 -0.01495469\n",
            "  0.09446853 -0.03532859 -0.1130799   0.19328818 -0.26036248 -0.1655833\n",
            "  0.13756353]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.22054294  0.19695719 -0.00767811 -0.35041192 -0.24117997 -0.01933094\n",
            "  0.1672359  -0.2785302   0.09336743 -0.1058856   0.0852787  -0.02094992\n",
            " -0.2610183   0.14122412  0.01768159  0.27353963 -0.3519315  -0.24553633\n",
            " -0.00183972]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 37 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.02350921  0.05828056 -0.18585959 -0.42509454 -0.29485145 -0.3654486\n",
            " -0.15084563 -0.41852498 -0.02929371 -0.10630947 -0.15050517  0.07482923\n",
            " -0.07092819  0.0598627  -0.22400096  0.40147778 -0.572763   -0.6095633\n",
            " -0.27839762]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 38 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.30312243 -0.04570249 -0.00915598 -0.45235392 -0.16962017 -0.16650614\n",
            " -0.1358416   0.1754881  -0.13294289 -0.06163085  0.11404917 -0.00821117\n",
            "  0.07277575 -0.2212908  -0.00650033  0.290913   -0.13905714 -0.51620704\n",
            " -0.03976442]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 39 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.15793611 -0.24345464  0.05286936 -0.3734234  -0.16960642 -0.07043074\n",
            " -0.10963701  0.00092922 -0.12255941 -0.06146059  0.16222484  0.00710946\n",
            "  0.02267134  0.01033489  0.07921907 -0.07756109 -0.25349307 -0.45632634\n",
            "  0.07784645]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 40 reward=-2 new_state=[0 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.04104193  0.23704062 -0.17799586 -0.44450137 -0.3327714  -0.3253915\n",
            " -0.02709511 -0.16490646 -0.01128477 -0.14566663 -0.04503068  0.01304082\n",
            "  0.06962405 -0.23434807 -0.3201521   0.07871918 -0.3720937  -0.4605471\n",
            " -0.26217   ]\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 41 reward=-3 new_state=[1 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5824344   0.07223586  0.01692978 -0.084431   -0.07746971  0.13234816\n",
            " -0.03595479 -0.10346305  0.04747326 -0.06481338 -0.2703893  -0.10452526\n",
            " -0.34914538 -0.13347064  0.05895715  0.29283664 -0.44052467 -0.34184027\n",
            " -0.14246033]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 42 reward=-3 new_state=[1 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.19838539  0.17273515  0.06830846 -0.19048171 -0.32434687 -0.09617484\n",
            " -0.15281987 -0.09256282  0.04746978  0.10092968 -0.11904173 -0.13014185\n",
            " -0.17555022 -0.09398678  0.10365919 -0.14883643 -0.01117477 -0.21350887\n",
            "  0.21546389]\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 43 reward=-3 new_state=[1 1 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.14597048 -0.28788838 -0.00438999 -0.18464854 -0.01607191  0.10111687\n",
            " -0.25772902 -0.04538095 -0.11353213 -0.0547947  -0.07304674  0.08921714\n",
            " -0.01323711 -0.29441416  0.05552808 -0.05629304 -0.01023235 -0.60503757\n",
            " -0.07208391]\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 44 reward=-4 new_state=[1 1 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.00919913  0.4105239  -0.09183811 -0.54537576 -0.13640629 -0.19928008\n",
            "  0.05854583 -0.41121814  0.02498652  0.05721983 -0.10086816  0.05043045\n",
            " -0.09277284  0.2116783  -0.36018145  0.26847482 -0.3713761  -0.5464405\n",
            " -0.1201555 ]\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 45 reward=-4 new_state=[1 1 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4337552   0.29670882 -0.03951451 -0.36580268 -0.27073032 -0.15216891\n",
            " -0.08143145  0.10492703 -0.11486043  0.02147961  0.02487044 -0.18826158\n",
            " -0.03381165  0.2093939  -0.17647263  0.20000565 -0.31554872 -0.3758868\n",
            "  0.1278674 ]\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 46 reward=-4 new_state=[1 1 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.28400883 -0.01243522 -0.03434356 -0.22843939  0.13677624  0.06246943\n",
            " -0.13656911 -0.01531464 -0.0463884  -0.02360743 -0.16346975 -0.27597213\n",
            " -0.27173355 -0.2583794   0.17403886 -0.02096736 -0.09376627 -0.422509\n",
            "  0.05879305]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 47 reward=-5 new_state=[1 1 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10674133 -0.09064054  0.06964929 -0.4882196  -0.12751473  0.04362794\n",
            " -0.09598649 -0.1007238  -0.02815042 -0.13021481 -0.50856954 -0.05979798\n",
            "  0.0675043  -0.40110466  0.05199906  0.348883   -0.22623424 -0.5071559\n",
            " -0.09596515]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 48 reward=-5 new_state=[1 1 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.15140122 -0.16505519  0.17146127 -0.06042212 -0.06144137  0.12308727\n",
            " -0.23940364 -0.01178558 -0.09673928 -0.21111298 -0.25540134 -0.00355465\n",
            " -0.14011331 -0.03308275 -0.07443725  0.09660263 -0.19430359 -0.467925\n",
            " -0.03469833]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 49 reward=-4 new_state=[1 0 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.03565309 -0.18119358  0.20130196 -0.46195087 -0.4416831  -0.18689007\n",
            " -0.29035684 -0.28159928 -0.03408238 -0.15840007 -0.3643294  -0.2456137\n",
            " -0.00880902 -0.10121636 -0.16993733  0.2660022  -0.16224788 -0.5501357\n",
            " -0.13039623]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 50 reward=-3 new_state=[1 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.20206167 -0.19130103  0.23108336 -0.11894775 -0.10189867  0.03402908\n",
            " -0.15958339 -0.16658615  0.1514996  -0.07324362 -0.39948487 -0.1667545\n",
            " -0.271913   -0.19304465  0.16817172  0.21910964 -0.35430077 -0.67737675\n",
            " -0.0910366 ]\n",
            "Epsilon reduced to 0.16000000000000003\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.14478557 -0.03746638 -0.01015422 -0.38480186  0.13186334 -0.10926235\n",
            " -0.10074709 -0.10584138  0.27689582 -0.0572433  -0.20302361 -0.13301197\n",
            " -0.35581642  0.00812289 -0.01451782  0.01477696 -0.29248998 -0.68611324\n",
            "  0.12159219]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.18886527  0.03495207 -0.05972657 -0.45041907 -0.31658027 -0.40931454\n",
            " -0.08626388 -0.23484764  0.18057384  0.06402839 -0.12903176 -0.2735533\n",
            " -0.37275827  0.0360295   0.08985028 -0.0943781  -0.4986506  -0.45944172\n",
            " -0.02168238]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.13191208 -0.29491264 -0.20399924 -0.3291671  -0.12539741 -0.39359257\n",
            " -0.00269388 -0.13577886  0.03896388  0.00867559 -0.03777879 -0.28098834\n",
            " -0.3379907  -0.26599833  0.20365708  0.10124243 -0.4524236  -0.43755615\n",
            " -0.39179024]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.25707254 -0.26629877 -0.27063188 -0.42276278 -0.29636005 -0.2748986\n",
            " -0.11840528 -0.26318973  0.26607674 -0.00289226 -0.17662407 -0.01835825\n",
            " -0.32572478  0.04593399  0.1829864   0.18100175 -0.32221118 -0.595282\n",
            " -0.16160443]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.03443379 -0.34545583 -0.22202247 -0.18158682 -0.25737095 -0.3746341\n",
            " -0.25827497  0.20185253 -0.02473293  0.35220787 -0.36301327  0.00134247\n",
            " -0.19649097 -0.21470001  0.21282075  0.1573189  -0.34251934 -0.69525826\n",
            "  0.11158603]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 6 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.23501074 -0.19890086 -0.13750812 -0.4066887  -0.25407892 -0.36536708\n",
            " -0.09297711 -0.3822438  -0.0358608   0.25822937 -0.00216852 -0.10801656\n",
            " -0.35147953  0.1831819   0.02926078  0.31044543 -0.4918801  -0.56352586\n",
            "  0.0270736 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8561959e-01 -1.0845998e-01 -1.9809708e-01 -4.7502065e-01\n",
            " -2.9935780e-01 -2.3810157e-01 -1.8537979e-01 -4.1658378e-01\n",
            " -1.3642973e-01 -1.4470057e-01 -2.6944977e-01  6.0021505e-04\n",
            " -3.3603582e-01  3.1325206e-02 -1.8719430e-01  4.3332970e-01\n",
            " -6.8460572e-01 -8.2565004e-01 -3.3367902e-01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 0 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.1909192  -0.25427243  0.08010407 -0.46247116 -0.32960814 -0.2813919\n",
            " -0.2918563  -0.14879118 -0.04357683 -0.26635343 -0.06067757  0.12326641\n",
            " -0.23700768 -0.09984553 -0.07861879  0.19815159 -0.51852274 -0.6753999\n",
            " -0.15468423]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 0 0 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-9.18179154e-02 -3.66394699e-01 -5.09783141e-02 -2.49760404e-01\n",
            " -6.27560243e-02 -2.71445394e-01 -2.83793718e-01 -1.18113615e-01\n",
            " -1.51422113e-01  3.55782844e-02 -1.05006792e-01  1.27011076e-01\n",
            " -2.60923594e-01 -1.11126758e-01  8.15564953e-03  1.68970786e-04\n",
            " -1.94767475e-01 -4.94783252e-01  1.44915329e-02]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 10 reward=-2 new_state=[0 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.13019665 -0.18127796 -0.2540367  -0.59340984 -0.46207783 -0.28909957\n",
            " -0.10438708 -0.31720874 -0.23276804 -0.29983795 -0.2742406  -0.07191497\n",
            " -0.24236989 -0.3143883  -0.10854828  0.3889231  -0.341597   -0.7950702\n",
            " -0.27382824]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.07590966 -0.19665393  0.01190757 -0.46789533 -0.32710746 -0.21950169\n",
            " -0.01301464 -0.29698417 -0.24220757 -0.11555776 -0.25242722  0.07920448\n",
            " -0.15721357 -0.13474454 -0.09509932  0.50678277 -0.47815818 -0.81149584\n",
            " -0.33181024]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.07089575 -0.43276888  0.05617871 -0.3162151  -0.2079481  -0.09748565\n",
            " -0.09454348 -0.13598454 -0.29529706  0.00621361 -0.23619275  0.11192533\n",
            " -0.3125427  -0.24465929  0.08600592 -0.05714909 -0.11251104 -0.42849204\n",
            " -0.12111793]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.06463174 -0.30553326 -0.17559643 -0.4066299  -0.19802259 -0.21194874\n",
            "  0.03626774 -0.12560311 -0.33776712  0.16585094 -0.12349289  0.10035128\n",
            " -0.13313138 -0.17539278  0.004697    0.13181081 -0.15988001 -0.5538544\n",
            "  0.02578289]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.02853872 -0.25836855 -0.23305614 -0.46799886 -0.2255608  -0.43946427\n",
            " -0.08199526 -0.05964117 -0.2939037   0.06825364 -0.2731718  -0.06195749\n",
            " -0.18816528 -0.35110253 -0.04582603 -0.03219679 -0.17455837 -0.61137646\n",
            "  0.04422683]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.18414663 -0.30202645  0.1167679  -0.39906195 -0.19312236 -0.15963629\n",
            " -0.1039946  -0.09889754 -0.15618835 -0.07401254 -0.20142625  0.03536301\n",
            " -0.30710784 -0.38282546  0.05017374  0.03309831 -0.47133124 -0.6372211\n",
            " -0.01658404]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.30196488 -0.42241922 -0.06849909 -0.544755   -0.38159415 -0.14785354\n",
            " -0.04092816 -0.2943886  -0.3249276   0.01823465 -0.39343262 -0.12083863\n",
            " -0.31868252 -0.36539054  0.16109721  0.3755877  -0.33475754 -0.75030357\n",
            " -0.15256323]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.12088563 -0.45796937 -0.18569188 -0.51177126 -0.35448587 -0.26585472\n",
            " -0.07343061 -0.20888695 -0.26251748 -0.02004459 -0.19483973 -0.05482885\n",
            " -0.25235707 -0.36919576  0.03317355 -0.03549709 -0.17329969 -0.6131103\n",
            "  0.01393293]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.18326272 -0.08225363 -0.14826314 -0.31323624  0.01127722 -0.01642999\n",
            " -0.13967516 -0.21328455 -0.06589472  0.01906406 -0.18827116  0.08543523\n",
            " -0.23851582 -0.45525783  0.08957978  0.17156918 -0.39073464 -0.66850394\n",
            "  0.11057997]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.38655952 -0.12287457 -0.43545848 -0.6674998  -0.33180478 -0.00878796\n",
            " -0.02549851 -0.21334969 -0.2808874   0.06436219 -0.19247502 -0.05213195\n",
            " -0.31750733 -0.44702575  0.35761353  0.3995192  -0.48401657 -0.8207524\n",
            "  0.07473841]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.03510321 -0.21484391 -0.5141697  -0.5757761  -0.17276025 -0.35596678\n",
            " -0.01161983 -0.1814053  -0.3138444   0.12972128 -0.20818263 -0.21851216\n",
            " -0.09752855 -0.45629802  0.0817352   0.06340041 -0.23267782 -0.6631479\n",
            "  0.12210216]\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.17931563 -0.37317502 -0.04110894 -0.5042534  -0.31846067 -0.22441359\n",
            " -0.0309918  -0.03805875 -0.2774178  -0.01795188 -0.25806597 -0.08125078\n",
            " -0.258244   -0.56915146  0.07577421  0.13579871 -0.5038659  -0.6413772\n",
            "  0.05341862]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.33292055 -0.55310965 -0.42930833 -0.47997087 -0.12111499  0.07604004\n",
            " -0.04540185 -0.12365043 -0.31475413 -0.11785959 -0.36845458 -0.1766673\n",
            " -0.36349696 -0.4720462   0.12600026  0.53324753 -0.5145628  -0.8686584\n",
            " -0.16492487]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 0 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.02167227 -0.02656607 -0.5598096  -0.58366054 -0.08382079 -0.20799471\n",
            " -0.16048057 -0.07897991 -0.28290203 -0.04471556 -0.20523131 -0.03691653\n",
            " -0.08136643 -0.6298037   0.12741132  0.16919449 -0.28343025 -0.77224225\n",
            "  0.02003257]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 1 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.28755644 -0.48656934 -0.14310934 -0.23777466 -0.04552668 -0.34026915\n",
            " -0.20760329 -0.32396317 -0.2935903   0.00332814 -0.13012938 -0.12860171\n",
            " -0.39413154 -0.61706537 -0.18645954  0.21649551 -0.31224594 -0.77225\n",
            " -0.38804597]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 1 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.43727842 -0.28148583 -0.18783316 -0.52204204 -0.07652232 -0.26537114\n",
            " -0.2974333  -0.2801118  -0.30634084 -0.3600561  -0.0431418  -0.09680773\n",
            " -0.19775328 -0.7572645   0.01190854  0.24712318 -0.28131443 -0.95118576\n",
            " -0.06783684]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 1 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2287238  -0.30696854  0.02819982 -0.3421829  -0.04805192 -0.24011935\n",
            " -0.37448692 -0.00598518 -0.38018292  0.07002474 -0.3784266  -0.11132921\n",
            " -0.0103786  -0.6443907   0.30984163 -0.04073258 -0.31827274 -0.86979157\n",
            "  0.05962763]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 0 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39214596 -0.30002305 -0.10847984 -0.23756081 -0.18276748  0.07237808\n",
            " -0.19512051 -0.42295554 -0.33501458 -0.14346747 -0.17669773 -0.26365775\n",
            " -0.39973342 -0.75529057  0.02919486  0.1455803  -0.5403847  -0.67620766\n",
            " -0.2904012 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 1 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6647608  -0.14043397 -0.04492064 -0.49112675 -0.15099536 -0.29858577\n",
            " -0.30821258 -0.15289304 -0.1371882  -0.22393632 -0.08191552 -0.00647036\n",
            " -0.2103865  -0.66757464  0.04749759  0.10918304 -0.48765117 -0.7333921\n",
            " -0.16469117]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 1 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.33328363 -0.19092752 -0.22830231 -0.37594807  0.06993566 -0.06809354\n",
            " -0.33018997 -0.02167597 -0.19224502 -0.2593365  -0.21598837 -0.12479821\n",
            " -0.10489122 -0.8189222   0.10987813  0.10443424 -0.2433062  -0.8676181\n",
            " -0.29101628]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 30 reward=-1 new_state=[0 0 0 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.43484357 -0.6281748   0.01499799 -0.09388413 -0.12543702  0.17718686\n",
            " -0.27921957 -0.37018737 -0.20013162 -0.11899976 -0.40296197 -0.04876436\n",
            " -0.42816198 -0.5004244  -0.06105634  0.31156978 -0.47675332 -0.7042711\n",
            " -0.40775797]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 1 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.31587273 -0.12200197  0.01468973 -0.62517124 -0.24366751 -0.24974628\n",
            " -0.04749804 -0.32027602 -0.33488676 -0.22168884 -0.02878911  0.01413471\n",
            " -0.33129728 -0.40794635  0.05518757  0.04979029 -0.2006974  -0.66122735\n",
            " -0.09829816]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 32 reward=-1 new_state=[0 0 0 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.52068996 -0.25187576 -0.23690477 -0.38545293 -0.03957978 -0.03397432\n",
            " -0.30547562 -0.25873062 -0.1749945  -0.21239513 -0.14714809 -0.22080618\n",
            " -0.23668347 -0.78041965  0.12928793  0.07656763 -0.31131354 -0.905109\n",
            " -0.12400241]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10775293 -0.03905239 -0.22604889 -0.38894212 -0.0429164  -0.14701873\n",
            " -0.1005002  -0.3054099  -0.15022409 -0.00209486 -0.08502997 -0.17812882\n",
            " -0.44322357 -0.55739933  0.17272209  0.05631468 -0.58000016 -0.7576423\n",
            "  0.00651045]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.35544065  0.05605196 -0.03616072 -0.44542572 -0.02368518 -0.04827532\n",
            " -0.1207045  -0.1357004  -0.19144832 -0.17723906 -0.29163018 -0.11297534\n",
            " -0.25418103 -0.20050345 -0.01852263  0.11128209 -0.49665684 -0.6745817\n",
            "  0.08984999]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.32332313 -0.35362646 -0.27226427 -0.42280626 -0.20916048 -0.18199652\n",
            " -0.14030339 -0.39254647 -0.21719548 -0.30948704 -0.28007123 -0.0360251\n",
            " -0.43958396 -0.615239    0.03786215  0.25068218 -0.42180336 -0.94129276\n",
            " -0.10416597]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.20215607 -0.28964266 -0.03746777 -0.45231995  0.02949109 -0.15727665\n",
            " -0.21195365 -0.2596976  -0.07299544 -0.38318795 -0.18433118 -0.03673811\n",
            " -0.1968887  -0.42352027 -0.05052163 -0.08529932 -0.46334985 -0.68018883\n",
            " -0.19172002]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.31356207 -0.32557327 -0.17549793 -0.39024204 -0.27182466 -0.06799503\n",
            " -0.26595747 -0.36265805 -0.26795805 -0.46974373 -0.35373852  0.06440884\n",
            " -0.3914548  -0.66838974  0.05465071  0.45358464 -0.5148021  -0.90086085\n",
            " -0.44685546]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.29830596 -0.12500522 -0.07899898 -0.3602323  -0.00910623 -0.13672349\n",
            " -0.12763405 -0.18794893  0.04206047 -0.17190759 -0.12841254  0.02598369\n",
            " -0.26828274 -0.6675827  -0.01869187 -0.00695613 -0.50454074 -0.5776784\n",
            " -0.05568058]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11297605 -0.21069759 -0.07204836 -0.19392551  0.24717446 -0.16971707\n",
            " -0.20079502 -0.34970897 -0.17464966 -0.23714638 -0.28605026 -0.07363839\n",
            " -0.27476418 -0.42034692 -0.04432412 -0.00094298 -0.23057653 -0.68705285\n",
            " -0.10760792]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.3434978  -0.37009376 -0.23205459 -0.4896398  -0.44070426 -0.38963538\n",
            " -0.04518922 -0.17039295 -0.2993456  -0.40682864 -0.2933822  -0.16711327\n",
            " -0.33537316 -0.45826772 -0.0599805   0.49968663 -0.43022746 -0.85180986\n",
            " -0.28491187]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.21125326 -0.28752005 -0.00453607 -0.52914476 -0.20219378 -0.2598012\n",
            " -0.17752685 -0.21071137  0.02778378 -0.45607314 -0.18672657 -0.01144918\n",
            " -0.01994403 -0.4248805   0.1084414   0.20410845 -0.68891877 -0.77438575\n",
            " -0.1559904 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2568132  -0.22578135 -0.14082026 -0.2770234   0.18021269  0.05117396\n",
            " -0.13279343 -0.23181964 -0.13761257 -0.20449397 -0.41458285 -0.13212195\n",
            " -0.34827536 -0.45085213  0.04296172  0.18863486 -0.31729156 -0.74768347\n",
            " -0.18660371]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.15394032 -0.10062505 -0.07566886 -0.2898992   0.10220245 -0.09796873\n",
            " -0.01727043 -0.1495835   0.0615148  -0.1972199  -0.25846303 -0.04527075\n",
            " -0.16130638 -0.58703434 -0.00167343 -0.00646074 -0.3851861  -0.598274\n",
            " -0.11937711]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.3687041  -0.11320373 -0.03276922 -0.36212167  0.20072353 -0.25334546\n",
            " -0.2147564  -0.42169762 -0.10224848 -0.24585442 -0.14338581 -0.01884348\n",
            " -0.38605428 -0.18785125  0.06768099  0.09812136 -0.26976088 -0.63692784\n",
            " -0.17567219]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 45 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5035259  -0.3241554  -0.22094785 -0.61363137 -0.2231914  -0.18392384\n",
            " -0.17100435 -0.29126522 -0.00427872 -0.36452028 -0.19374391 -0.14179638\n",
            " -0.39493257 -0.1956304   0.02861177  0.4424171  -0.60306215 -0.87285286\n",
            " -0.18181661]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.22120078 -0.29891273 -0.08299752 -0.28280932  0.25948596 -0.14974388\n",
            " -0.01722792 -0.23960938  0.03664769 -0.05352997 -0.15971266 -0.08964664\n",
            " -0.11345245 -0.18359871  0.07121874 -0.00989098 -0.39987063 -0.562781\n",
            " -0.18743071]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.19187428 -0.13803448 -0.16084005 -0.27169067  0.32864553  0.01847777\n",
            " -0.1369036  -0.10135946 -0.06696051 -0.09029546 -0.39622658 -0.05594174\n",
            " -0.2524588  -0.27718437  0.08368777  0.17109898 -0.4276968  -0.64176625\n",
            " -0.1239029 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.4130345  -0.12291468 -0.03583043 -0.5422556   0.00689829 -0.22886938\n",
            " -0.08800104 -0.09474277  0.04531422 -0.19396023 -0.05282384  0.04876367\n",
            " -0.3309944  -0.35656786 -0.03026847  0.00090434 -0.57146895 -0.59771574\n",
            " -0.08538194]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.13459545 -0.21869263 -0.24679318 -0.51425946  0.07328802 -0.2669756\n",
            " -0.13434172 -0.58777404 -0.04168713 -0.41098323 -0.19747937 -0.0988914\n",
            " -0.32358682 -0.22850877 -0.00300994  0.18447147 -0.30036524 -0.8201671\n",
            " -0.30852348]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.7079579  -0.30339712 -0.13920745 -0.49050635 -0.06598557 -0.14531392\n",
            "  0.13666306 -0.17561954 -0.11565296  0.00642344 -0.30272558 -0.16934107\n",
            " -0.37131718 -0.11740235  0.0590044   0.36078987 -0.56224567 -0.7410362\n",
            " -0.27434742]\n",
            "Epsilon reduced to 0.12800000000000003\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.2862972  -0.38397354 -0.23741235 -0.21904051 -0.03391936 -0.32980102\n",
            " -0.5625569  -0.22014466 -0.2102212  -0.37461677 -0.29474688 -0.03450685\n",
            " -0.25735247 -0.03891089  0.02160926  0.28187305 -0.55587393 -0.7550774\n",
            " -0.2633936 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.1808469  -0.5195821  -0.20944323 -0.34686196  0.18994671 -0.4220298\n",
            " -0.2963532  -0.1138165  -0.01486224 -0.27453592 -0.05223522 -0.16427806\n",
            " -0.2580294   0.13076408  0.1360432  -0.1997303  -0.5232307  -0.6878971\n",
            " -0.4050385 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3959379  -0.32656252 -0.34988695 -0.2827831   0.09427351 -0.3776548\n",
            " -0.03434236 -0.34949923  0.10379432 -0.31360918 -0.15012707 -0.45614126\n",
            " -0.16851786 -0.20153275 -0.03739101  0.18974523 -0.53713334 -0.65817696\n",
            " -0.21108319]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.14657743 -0.29125816 -0.16875866 -0.2696757  -0.06597706 -0.5071851\n",
            " -0.24708627 -0.17683475  0.1995197  -0.2628394  -0.17962232 -0.20115842\n",
            " -0.33139887 -0.12566286  0.23416802 -0.20704365 -0.35492045 -0.7723125\n",
            " -0.33328456]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.42933878 -0.33959934 -0.31758124 -0.39698493  0.16901597 -0.42461848\n",
            " -0.26448107 -0.22741763  0.18824297 -0.16491432 -0.01058817 -0.33783606\n",
            " -0.46758258 -0.08557909  0.13174537  0.19542399 -0.8405983  -0.63911104\n",
            " -0.33117956]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.14262605 -0.44875717 -0.33732444 -0.31904513  0.04671526 -0.3389991\n",
            " -0.35929683 -0.05465436  0.01613742 -0.15884288 -0.35366446  0.17090708\n",
            " -0.25104928  0.21356831  0.35766265 -0.13646373 -0.7654437  -0.88275766\n",
            " -0.1389704 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.44205144 -0.2773095  -0.32204983 -0.30856746  0.05640181 -0.38708702\n",
            " -0.15701535 -0.11868535  0.22946009 -0.16244765  0.05593434 -0.3185276\n",
            " -0.30060595 -0.08982622  0.12935857  0.12035955 -0.7435759  -0.52722764\n",
            " -0.3036204 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 8 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.57760966 -0.15378183 -0.14104663 -0.24747342  0.10041708 -0.38863954\n",
            " -0.5693658  -0.08540519  0.10692069  0.22743104 -0.40504903 -0.11699646\n",
            " -0.33900052  0.19242938  0.13921872  0.08066683 -0.5556614  -0.67400426\n",
            " -0.07061981]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 9 reward=-3 new_state=[0 0 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.28104603 -0.13285923 -0.27572685 -0.4964941  -0.13586234 -0.29773474\n",
            " -0.21058854 -0.490063    0.14305037 -0.3048904  -0.14390485 -0.2679175\n",
            " -0.3756342   0.08186452 -0.14250894  0.2290701  -0.6540377  -0.6213394\n",
            " -0.24569431]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 10 reward=-3 new_state=[0 0 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5298377  -0.0583208  -0.22673488 -0.5482084  -0.00186052 -0.18677326\n",
            " -0.08933527 -0.37797168 -0.00569099 -0.3260872  -0.13122927 -0.19404076\n",
            " -0.4258066   0.25620437  0.08394809  0.42243695 -0.44682357 -0.51681364\n",
            " -0.15046951]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39694074 -0.26774278 -0.13960455 -0.38906693 -0.01452111 -0.40920648\n",
            " -0.39684364 -0.37378836 -0.01886336 -0.63215494 -0.16873683 -0.23544228\n",
            " -0.15171076  0.04508228 -0.2411103   0.3941306  -0.5264511  -0.83014536\n",
            " -0.24757485]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40708372 -0.1322923   0.04622018 -0.18995744  0.2013044  -0.2002006\n",
            " -0.30747366 -0.12652437  0.16379482 -0.3003136  -0.16223206 -0.1475927\n",
            " -0.21707568 -0.07882408 -0.05934419 -0.00764735 -0.3095942  -0.48400316\n",
            " -0.20611311]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2601241  -0.27692997  0.13630018 -0.25787386  0.09908893 -0.3371327\n",
            " -0.46268424 -0.08624467 -0.00452016 -0.36255386 -0.23021336 -0.16414376\n",
            " -0.22805174 -0.19518043  0.02466179 -0.09308409 -0.1885996  -0.3408331\n",
            " -0.2681908 ]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.27373248 -0.4182885  -0.37896156 -0.47497302 -0.02356004 -0.5937896\n",
            " -0.38059616 -0.3774085   0.11595514 -0.5492214  -0.27797598 -0.27498066\n",
            " -0.11274892 -0.02848935 -0.23566152  0.33295682 -0.56844306 -0.91028756\n",
            " -0.5271408 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.42640564 -0.27870628 -0.17215699 -0.38639927  0.20852311 -0.27634427\n",
            " -0.18411337 -0.128234   -0.01071721 -0.32250142 -0.26726317 -0.23450287\n",
            " -0.21146658  0.01813481  0.00190091  0.27707663 -0.5177836  -0.7223335\n",
            " -0.26777944]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18284976 -0.30041137 -0.10999945 -0.11442219  0.33317155 -0.10543074\n",
            " -0.17617823 -0.12479179 -0.03374001 -0.14907378 -0.24027753 -0.09781805\n",
            " -0.25477353 -0.02476499 -0.11200523 -0.03948045 -0.2893579  -0.49954832\n",
            " -0.15877782]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2059558  -0.44585618 -0.18475991 -0.08918961  0.08593197 -0.36397845\n",
            " -0.22330247 -0.22726418  0.06296568 -0.3482358  -0.32662433 -0.10495429\n",
            " -0.22675472 -0.04126148 -0.20137773  0.10892169 -0.35622478 -0.66635054\n",
            " -0.16231294]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3275319  -0.4246574  -0.364246   -0.340588   -0.00787154 -0.81885993\n",
            " -0.23420016 -0.17545022  0.26232344 -0.2466312  -0.18610215 -0.08481863\n",
            " -0.34568626  0.24772331  0.19413052 -0.01798336 -0.5681551  -0.8440386\n",
            " -0.18787815]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.21328335 -0.29761124 -0.35102707 -0.2842215   0.20502505 -0.7069696\n",
            " -0.237086   -0.27991724  0.12310664 -0.5024543  -0.29965523 -0.3859789\n",
            " -0.30444    -0.01337313 -0.03170168  0.05472266 -0.8225852  -0.64362156\n",
            " -0.38003758]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31590295 -0.6076504  -0.35286653 -0.3419702   0.0716325  -0.8179779\n",
            " -0.23261924  0.03465872  0.0656575  -0.29072753 -0.2377903  -0.3212872\n",
            " -0.3342521   0.26315433  0.02417452  0.1101232  -0.43405038 -0.59027433\n",
            " -0.37310168]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.19350712 -0.48243096 -0.34953877 -0.03900857 -0.00654757 -0.21748032\n",
            " -0.08769503 -0.08808896  0.00282814 -0.27836373 -0.42709732 -0.04444142\n",
            " -0.24338636  0.06753533 -0.08084884 -0.10785752 -0.45705885 -0.6556115\n",
            " -0.09111813]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.19855686 -0.22375208 -0.25492084 -0.19791155  0.10697556 -0.49277624\n",
            " -0.21122889  0.06378689  0.08465794 -0.17733514 -0.20741685 -0.2372081\n",
            " -0.24338461  0.03475095 -0.04877914 -0.05066655 -0.4006131  -0.6291333\n",
            " -0.19578387]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.03649501 -0.41912234 -0.26038286  0.01034556 -0.03138551 -0.1658963\n",
            " -0.04339055 -0.17352165 -0.01479908 -0.30657437 -0.34857333 -0.01987841\n",
            " -0.19783588 -0.05052513 -0.09430008 -0.16101767 -0.24414453 -0.61419773\n",
            " -0.13566613]\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 24 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.08676098 -0.29652652 -0.24656637 -0.09443285  0.01148769 -0.20518717\n",
            " -0.2231879  -0.20425674  0.10423946 -0.5369004  -0.3671221  -0.23817366\n",
            " -0.07104545  0.06665671 -0.22838713 -0.04293264 -0.37195322 -0.4136664\n",
            " -0.20012553]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 25 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.2975658  -0.3917177  -0.21294783 -0.30709904  0.0139458  -0.1648332\n",
            " -0.08642102 -0.11359422 -0.16430585 -0.37501752 -0.30153885 -0.12240715\n",
            "  0.04392529  0.14410627 -0.143307    0.10052998 -0.4454504  -0.5162332\n",
            " -0.372139  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 26 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10131297 -0.6059151  -0.56148756 -0.31656948 -0.22157583 -0.274559\n",
            " -0.01051362 -0.14546233 -0.00462087 -0.49687925 -0.3319135  -0.00155355\n",
            "  0.03455753  0.09229687 -0.18896928 -0.01069599 -0.5565986  -0.54401684\n",
            " -0.2752499 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 27 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10511263 -0.53117776 -0.451716   -0.32101429 -0.03334726 -0.5068521\n",
            " -0.11832975 -0.06634608 -0.07003336 -0.19040579 -0.16435704 -0.18939197\n",
            " -0.22880375  0.20365946  0.06035472  0.03360364 -0.77603924 -0.47185308\n",
            " -0.4094627 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4148298  -0.30978233 -0.29875553 -0.3274904   0.04201957 -0.29345992\n",
            " -0.10979266 -0.08255387 -0.07556445 -0.21201189 -0.0918739  -0.09119263\n",
            " -0.21516459  0.24304262 -0.0271577   0.21703365 -0.5946545  -0.41056982\n",
            " -0.02860641]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 29 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.27345833 -0.14125647 -0.275469   -0.20824558 -0.08852372 -0.24233522\n",
            "  0.03205252 -0.17241842 -0.21165267 -0.53363854 -0.35772055 -0.17006378\n",
            " -0.24986686  0.07872277 -0.07476543  0.12978922 -0.37332252 -0.62665445\n",
            " -0.19763573]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 30 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0116418  -0.30693835 -0.3604483  -0.5337119  -0.06860967 -0.5495533\n",
            " -0.16388313 -0.20710091 -0.25591204 -0.42895472 -0.31665072 -0.30934152\n",
            " -0.18417437  0.10197002 -0.01935862  0.2558278  -0.42753226 -0.52715117\n",
            " -0.25119922]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 31 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-3.1886044e-01 -5.0170732e-01 -2.5678274e-01 -2.5252467e-01\n",
            "  1.7696264e-01 -4.0452090e-01 -2.4864887e-01 -2.2472438e-01\n",
            " -2.4540588e-01 -4.0471599e-01 -2.3201612e-01 -9.4350152e-02\n",
            "  2.3316815e-02 -1.4508462e-03 -1.3622316e-02  3.3952855e-04\n",
            " -5.3213340e-01 -5.5119103e-01 -1.0092877e-01]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 32 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.18555157 -0.60257447 -0.46296293 -0.12193134  0.11137979 -0.3024117\n",
            " -0.17183076 -0.21510834 -0.12841207 -0.5706508  -0.47624633 -0.1714217\n",
            " -0.1800018  -0.09389898  0.02037517  0.1737425  -0.714963   -0.7521052\n",
            " -0.23841697]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 33 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.39457458 -0.22342195 -0.17035024 -0.21688332  0.04094718 -0.57028127\n",
            " -0.12400309 -0.14567755 -0.02021719 -0.26290202 -0.14089496 -0.20879784\n",
            " -0.13025638  0.17613201 -0.16614625  0.26783827 -0.771687   -0.59413725\n",
            " -0.30915514]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 34 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.21162938 -0.46574396 -0.37849578 -0.2450724   0.0531009  -0.42109585\n",
            " -0.23286954 -0.13186981 -0.1540322  -0.41710645 -0.1561781  -0.16905479\n",
            " -0.02816427 -0.04324898 -0.093437    0.17603947 -0.5635527  -0.5025286\n",
            " -0.21775627]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 35 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.19441168 -0.4362094  -0.3944722  -0.27849683 -0.03240365 -0.50093\n",
            " -0.17031923 -0.10240772 -0.15570043 -0.11328988 -0.17567086 -0.14054064\n",
            " -0.14315696  0.2557949  -0.06234883  0.24684733 -0.6634846  -0.45802167\n",
            " -0.18875203]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 36 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.32892472 -0.5041859  -0.41969943 -0.2935371   0.18813233 -0.4890697\n",
            " -0.19438475 -0.21236986 -0.06376038 -0.540187   -0.37300098 -0.34119755\n",
            " -0.05931752 -0.02999323  0.06975744  0.33876106 -0.68920475 -0.57323676\n",
            " -0.23552634]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 37 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10218128 -0.34631306 -0.41986257 -0.18711716  0.07455504 -0.65230787\n",
            " -0.17450681  0.10044158 -0.11079653 -0.38642833 -0.1797716  -0.34871694\n",
            " -0.30187204  0.0218482   0.03981417  0.22755563 -0.43811345 -0.46321043\n",
            " -0.19478858]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 38 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.18583383 -0.40392622 -0.3159963  -0.18967268  0.01059311 -0.52846044\n",
            " -0.08959676 -0.06520752 -0.03356879 -0.13593511 -0.13410473 -0.2187079\n",
            " -0.04555582  0.16479382 -0.10960025  0.08881902 -0.33046243 -0.46134713\n",
            " -0.3725277 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 39 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.15238804 -0.421828   -0.39050192 -0.2400116   0.12564424 -0.26562938\n",
            " -0.13508108 -0.00774573 -0.1104738  -0.2755365  -0.21028925 -0.1327482\n",
            " -0.13975833 -0.05371205  0.0388137   0.05025184 -0.5780193  -0.32379776\n",
            " -0.22652158]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 40 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.21142119 -0.30633956 -0.2780947  -0.18805234  0.10132572 -0.61585385\n",
            " -0.2792523  -0.34644035  0.10318987 -0.5570521  -0.3981755  -0.36788535\n",
            " -0.19869544  0.14053838 -0.10302033  0.3983256  -0.7620178  -0.46644798\n",
            " -0.21172592]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 41 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.27496323 -0.43250656 -0.31825784 -0.22319874  0.21156181 -0.3721779\n",
            " -0.11325454 -0.1228556  -0.1543967  -0.35876486 -0.27759936 -0.09546342\n",
            " -0.06510351  0.04242471 -0.03910397  0.04900315 -0.5810168  -0.45953175\n",
            " -0.21691117]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 42 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.45081794 -0.2696681  -0.4365344  -0.1884583  -0.01378836 -0.6824706\n",
            " -0.07833381 -0.11093064 -0.0544166  -0.24379225 -0.10061289 -0.3266753\n",
            " -0.36698443  0.22052619  0.11337866  0.16310973 -0.5576737  -0.30796933\n",
            " -0.03125093]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 43 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.58912104 -0.4687305  -0.484411   -0.3089472   0.08363134 -0.5286379\n",
            " -0.03543928 -0.09610291 -0.03060111  0.05877998 -0.06212476 -0.1624944\n",
            " -0.17454223  0.35699573 -0.07650959  0.13034093 -0.7813088  -0.3776732\n",
            " -0.36478394]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 44 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.35744742 -0.26531428 -0.50769734 -0.18174477  0.13129625 -0.4715511\n",
            " -0.13517976  0.03704929  0.20966576 -0.3766219  -0.31400672 -0.25843918\n",
            " -0.37035552  0.04302807  0.05119522  0.4881118  -0.5466954  -0.39190355\n",
            " -0.09098669]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 45 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [ 0.06852798 -0.54167265 -0.30452582 -0.01897675 -0.09921788 -0.34827778\n",
            " -0.050649   -0.24794452 -0.08372785 -0.3131571  -0.42382997  0.01360944\n",
            " -0.14690532  0.06652731 -0.09399093  0.3149818  -0.43705675 -0.3246203\n",
            " -0.08095894]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 46 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.12718633 -0.45326355 -0.43832704 -0.15593337  0.10183975 -0.3520333\n",
            "  0.02133418 -0.16459136 -0.0096218  -0.34983012 -0.16749819  0.01742268\n",
            " -0.03039256  0.01551083 -0.11357185 -0.05270008 -0.4872137  -0.31464747\n",
            " -0.2931804 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 47 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.18289524 -0.5196339  -0.5294329  -0.08714249  0.06365369 -0.2864653\n",
            "  0.12591559 -0.11178327 -0.16221255 -0.39585    -0.2516892  -0.104063\n",
            " -0.25570422  0.0520953   0.18422624 -0.03257643 -0.6146097  -0.28870505\n",
            " -0.05612396]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.62235284 -0.40496314 -0.43541214 -0.19311924  0.2106434  -0.5089431\n",
            " -0.1016298  -0.30272946  0.10966942 -0.2430002  -0.37134138 -0.20459273\n",
            " -0.2591608   0.17962229 -0.03314627  0.5284479  -1.0013295  -0.3159743\n",
            " -0.24477209]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 49 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.15542106 -0.27713862 -0.3820733  -0.07301609  0.08971489 -0.5287762\n",
            " -0.08773813  0.07309494 -0.04793875 -0.28910363 -0.19371912 -0.27206737\n",
            " -0.1838014  -0.01862    -0.03056271  0.09488574 -0.29937795 -0.20106041\n",
            " -0.23817842]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 50 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.49663538 -0.5638579  -0.14876273 -0.26908678  0.1069833  -0.38848847\n",
            " -0.12101323 -0.11086191 -0.14744884 -0.10055275 -0.16563253 -0.2371116\n",
            " -0.15791652  0.2658206  -0.04711723  0.08054608 -0.591342   -0.26639298\n",
            " -0.2805373 ]\n",
            "Epsilon reduced to 0.10240000000000003\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.25486845 -0.26053163 -0.30519935 -0.42975903  0.22414017 -0.4818233\n",
            " -0.20368612 -0.18170561 -0.03867843 -0.34757784 -0.09331933 -0.22906204\n",
            " -0.32466927  0.18036623 -0.06489957  0.4944498  -0.5177743  -0.28723732\n",
            " -0.3182632 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.25217676 -0.35496312 -0.3997713  -0.30493578  0.07889413 -0.8080816\n",
            " -0.16083917 -0.25951105  0.18946782 -0.23461019 -0.16417685 -0.12183879\n",
            " -0.34898317  0.10736609  0.14849196  0.3542356  -0.5635729  -0.3631737\n",
            " -0.22587927]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.14824435 -0.28223258 -0.35102618 -0.2329168   0.04402148 -0.6576502\n",
            " -0.24162273 -0.22893123 -0.02790948 -0.27787027 -0.2246018  -0.21547623\n",
            " -0.19230327  0.07942036  0.04771546  0.5406988  -0.44128755 -0.15011956\n",
            " -0.14823343]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.19396141 -0.5464199  -0.38176984 -0.38665697  0.14040199 -0.7477551\n",
            " -0.1291416  -0.18760507  0.04623559 -0.3602779  -0.10793911 -0.18969993\n",
            " -0.27030826  0.27023706  0.04812452  0.38203397 -0.5719003  -0.08722588\n",
            " -0.4712695 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.05806495 -0.2736892  -0.49626008 -0.14547743  0.07579692 -0.5532534\n",
            " -0.11536707 -0.3390408   0.25306508 -0.56946874 -0.4076922  -0.0260309\n",
            " -0.29428318 -0.08924152 -0.28827307  0.4968372  -0.5224755  -0.3275848\n",
            " -0.01545324]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.0198891  -0.38180062 -0.2843278  -0.18253505  0.06650552 -0.7237979\n",
            " -0.03865039 -0.15315323 -0.02004275 -0.5462987  -0.16151491  0.01652194\n",
            " -0.3342902   0.26388046  0.33501688  0.12905054 -0.73049206 -0.22205277\n",
            " -0.21412414]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4406393  -0.47778064 -0.36956775 -0.42224976  0.08549283 -0.5976302\n",
            " -0.0791849  -0.21643753 -0.00414061 -0.24284238 -0.12830223 -0.43195516\n",
            " -0.0605014  -0.00480859  0.11162372  0.545768   -0.60596883 -0.2435972\n",
            " -0.19762354]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.20048647 -0.23381482 -0.37474778 -0.08634537  0.1632527  -0.4208003\n",
            " -0.21255478 -0.02731998  0.11690564 -0.5006912  -0.33183894 -0.0456001\n",
            " -0.43946424  0.22259434  0.35805744  0.46399456 -0.41487166 -0.273915\n",
            " -0.21977264]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.14610888 -0.47896796 -0.29084915 -0.18867432  0.25836334 -0.77900344\n",
            " -0.13113585 -0.20678985 -0.02250783 -0.39030877 -0.180197   -0.23171642\n",
            " -0.23803532  0.29288682  0.06403044  0.3598266  -0.7185806  -0.05179912\n",
            " -0.36107594]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.15493028 -0.42219073 -0.20783848 -0.14738661  0.20479697 -0.54080343\n",
            " -0.22604363 -0.04692856  0.0111335  -0.5206983  -0.40258503 -0.11819788\n",
            " -0.39278835  0.38154304  0.0430011   0.63536    -0.47843271 -0.17938635\n",
            " -0.22822075]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.07601006 -0.5805325  -0.3274728   0.01976225  0.2801576  -0.5154255\n",
            "  0.03578547 -0.39260086  0.07787691 -0.3483642  -0.3322785   0.06637508\n",
            " -0.23548332  0.19900227 -0.08762807  0.38457054 -0.5170008  -0.1819918\n",
            " -0.27547008]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.28053156 -0.573802   -0.4066948  -0.29962856  0.2345931  -0.7505531\n",
            " -0.17925817  0.05016522  0.12482604 -0.08854851 -0.40041396 -0.14011773\n",
            " -0.43830466  0.49255326  0.20171008  0.6454898  -0.5616737  -0.09965043\n",
            " -0.37376574]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.15556672 -0.36714166 -0.3378941  -0.2441593   0.13150601 -0.6038525\n",
            " -0.07196088 -0.30606753  0.06076203 -0.32285818 -0.32233456 -0.10617638\n",
            " -0.37963367  0.20506756  0.19490737  0.24349818 -0.91712797 -0.14132299\n",
            " -0.32405177]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.2022153  -0.34253052 -0.23901159 -0.17101188  0.1492123  -0.439343\n",
            " -0.20318997 -0.0394311   0.06787769 -0.5213365  -0.32830074 -0.09524733\n",
            " -0.3644338   0.35101035  0.10787232  0.42074805 -0.30443406 -0.09469188\n",
            " -0.23283897]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.17974629 -0.4415254  -0.35706413 -0.22828826  0.20084299 -0.7557103\n",
            " -0.13988897 -0.22333142  0.08218763 -0.29449323 -0.14213684 -0.17180881\n",
            " -0.2555082   0.35616416  0.17241043  0.38103122 -0.7147884  -0.02884216\n",
            " -0.36762786]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09317892 -0.43552485 -0.24416868 -0.21300149  0.16292708 -0.3822186\n",
            " -0.20259278 -0.1642396  -0.05553393 -0.57882935 -0.23155516 -0.02747534\n",
            " -0.32780674  0.36434484  0.10699016  0.2563494  -0.51376766 -0.10270032\n",
            " -0.29286888]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.21034616 -0.42990068 -0.29948428 -0.15601197  0.10399614 -0.6855747\n",
            "  0.01592829 -0.09939826 -0.02737793 -0.38015383 -0.35827896 -0.3200564\n",
            " -0.37206268  0.37916705 -0.01600334  0.5272964  -0.5892076   0.03900719\n",
            " -0.3869346 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.06774697 -0.5498767  -0.16937485 -0.14245903  0.3083709  -0.47409606\n",
            " -0.20239942 -0.11493187 -0.1678208  -0.37561888 -0.3475135  -0.12048578\n",
            " -0.37448892  0.3558438   0.08299255  0.44454193 -0.6504318  -0.03278071\n",
            " -0.31569904]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.26833457 -0.3096896  -0.2594078  -0.35156542  0.1126456  -0.7528201\n",
            " -0.07798862 -0.3492459   0.06360011 -0.43652087 -0.1591533  -0.3433167\n",
            " -0.35692078  0.12658162 -0.04458714  0.6159243  -0.68392223 -0.15579277\n",
            " -0.37734845]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.23364443 -0.35794288 -0.25950503 -0.21255073  0.24896607 -0.7112823\n",
            " -0.14683746  0.01101348  0.15240192 -0.25673634 -0.35376334 -0.11501564\n",
            " -0.47074634  0.40831158  0.06799825  0.6484401  -0.5043966  -0.03733092\n",
            " -0.25655794]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.19419566 -0.50426877 -0.28495175 -0.23291978  0.2557847  -0.71644217\n",
            " -0.08670085 -0.20963484  0.03710984 -0.27064902 -0.22692114 -0.2047261\n",
            " -0.36016074  0.42749876  0.06191181  0.41307402 -0.7831516  -0.0327954\n",
            " -0.40400484]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.20177905 -0.34932894 -0.36061782 -0.33652505  0.24527682 -0.47993216\n",
            " -0.21685165 -0.13394327  0.06583366 -0.64679456 -0.22314255 -0.14287497\n",
            " -0.3199709   0.17986201  0.12543428  0.4740476  -0.30670404 -0.26607758\n",
            " -0.42422128]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11664452 -0.33132908 -0.2528701  -0.2011824   0.18460076 -0.5960306\n",
            " -0.15737246 -0.03709205  0.00605208 -0.2883643  -0.18101381 -0.1581454\n",
            " -0.48149964  0.3080967   0.03162858  0.31614786 -0.40258327 -0.14386566\n",
            " -0.3034271 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.05174893 -0.33336854 -0.20336126 -0.16677995  0.19808711 -0.31037024\n",
            " -0.14632413 -0.20489042 -0.0481762  -0.6142472  -0.0826899  -0.08937106\n",
            " -0.31435943  0.15462035  0.03917437  0.13583615 -0.31488746 -0.26693165\n",
            " -0.36766487]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.09095154 -0.34338546 -0.25361073 -0.26246762  0.07278419 -0.6182202\n",
            " -0.08372463 -0.03809802 -0.00856418 -0.31396055 -0.13430576 -0.2140134\n",
            " -0.44986802  0.3343384   0.15822887  0.32269996 -0.40403146 -0.0844292\n",
            " -0.42692503]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.7685257e-04 -2.8620097e-01 -1.5962876e-01 -1.8481933e-01\n",
            "  2.6100475e-01 -3.5011202e-01 -1.2913395e-01 -2.5274706e-01\n",
            "  1.4539100e-01 -5.3191048e-01 -1.7947860e-01 -7.5039841e-02\n",
            " -2.6929274e-01  1.9785985e-01 -5.2369505e-02  2.9121244e-01\n",
            " -4.6216911e-01 -2.8324726e-01 -4.3007025e-01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.0011862  -0.5463268  -0.20734398  0.0013398   0.16433041 -0.4312343\n",
            " -0.06479416 -0.15443064  0.04173398 -0.35794008 -0.33592945 -0.05030802\n",
            " -0.36109644  0.266444    0.06558625  0.40017244 -0.12993783 -0.17917937\n",
            " -0.25424564]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02815219 -0.32837078 -0.241034   -0.2183874   0.17177783 -0.33462012\n",
            " -0.15396746 -0.07033547  0.01582417 -0.61715156 -0.26858526 -0.11799232\n",
            " -0.3147283   0.14109884  0.00739069  0.32539883 -0.11071663 -0.22652689\n",
            " -0.32886946]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.06890453 -0.3319515  -0.20264608 -0.17850591  0.10997716 -0.5504246\n",
            " -0.12268826 -0.08333305  0.07126703 -0.34202775 -0.22632813 -0.24984643\n",
            " -0.42184153  0.30374435  0.08287483  0.34413582 -0.30855536  0.02660998\n",
            " -0.35156268]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.07475464 -0.19425394 -0.1653388  -0.09293777  0.2752797  -0.3807396\n",
            " -0.19580878 -0.09778337  0.13587855 -0.51414996 -0.23404142 -0.07196967\n",
            " -0.36868536  0.23700964 -0.03149868  0.41568357 -0.20129652 -0.27691635\n",
            " -0.23492466]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18108729 -0.4887076  -0.34535155 -0.26564172  0.21219768 -0.6206963\n",
            " -0.16119017 -0.21078013  0.04331764 -0.22519231 -0.14105223 -0.04279321\n",
            " -0.36636674  0.4808194   0.08306177  0.25922546 -0.7138456   0.00153192\n",
            " -0.32548407]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10651737 -0.2841036  -0.22851433 -0.13492338  0.2956157  -0.28195333\n",
            " -0.1929124  -0.0317536   0.12548855 -0.41393507 -0.30332926 -0.07095122\n",
            " -0.40055838  0.31141117  0.12075289  0.39267582 -0.2701624  -0.2258516\n",
            " -0.26365292]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.00332183 -0.55078787 -0.27823943 -0.25978732  0.25232434 -0.55216545\n",
            " -0.04315083 -0.18667662 -0.01456084 -0.29707024 -0.23643206 -0.19972698\n",
            " -0.39323354  0.46406206  0.1360003   0.26068246 -0.6133297   0.05250536\n",
            " -0.46949914]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0608201  -0.19662638 -0.14582713 -0.08489422  0.32990035 -0.3860379\n",
            " -0.17893729 -0.08610774  0.16466331 -0.4725186  -0.27920175 -0.08278733\n",
            " -0.3281306   0.34468228  0.00897618  0.3796699  -0.22139528 -0.20350067\n",
            " -0.30529466]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02864364 -0.41102213 -0.23295164  0.07195542  0.21266013 -0.30333588\n",
            " -0.00282317 -0.24747764  0.11761614 -0.37090614 -0.35017362  0.04594911\n",
            " -0.31382367  0.32580847 -0.08994794  0.27768362 -0.07257594 -0.11061838\n",
            " -0.20681298]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.01153083 -0.31156144 -0.25230688 -0.1552006   0.35795745 -0.33937722\n",
            " -0.15341556 -0.10772605  0.04187183 -0.5013274  -0.0971968  -0.05203385\n",
            " -0.36621433  0.41231441  0.07763869  0.05802848 -0.3196624  -0.10411117\n",
            " -0.21931466]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0713148  -0.37884912 -0.17114925  0.02434401  0.15224968 -0.2936852\n",
            "  0.0010233  -0.29778913  0.10589161 -0.27071577 -0.3224792   0.05143171\n",
            " -0.26913005  0.49076316 -0.10364136  0.37382424 -0.23801987 -0.17238455\n",
            " -0.21715109]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18509333 -0.32713553 -0.25997624 -0.20491615  0.2135833  -0.40004793\n",
            " -0.16209319  0.08411592 -0.01682639 -0.32954696 -0.19705851 -0.13700166\n",
            " -0.28463313  0.50158757 -0.0136319   0.29312453 -0.31056613 -0.01088161\n",
            " -0.11088673]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.11603173 -0.26938277 -0.23610498  0.03054371  0.12157778 -0.36552152\n",
            " -0.1584416  -0.1481009   0.09512043 -0.290928   -0.29547215 -0.0748733\n",
            " -0.32719824  0.47579947  0.21033211  0.33027512 -0.11389063 -0.02938569\n",
            " -0.14066723]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2459993  -0.26239073 -0.30150408 -0.30270422  0.22575627 -0.4240762\n",
            " -0.36920062  0.07189615 -0.0613465  -0.43260568 -0.3837902  -0.11065352\n",
            " -0.12214564  0.5922908  -0.04693107  0.50149304 -0.47294778 -0.20704769\n",
            " -0.09898005]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.15121388 -0.3134503  -0.22472288 -0.03138622  0.1674818  -0.04269433\n",
            " -0.0109265  -0.21100996  0.04754814 -0.4080845  -0.45130467 -0.02489699\n",
            " -0.22597542  0.5051657  -0.16457334  0.22245823 -0.06925169 -0.02928238\n",
            " -0.07978424]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.24165082 -0.07378057 -0.24052374 -0.30592635  0.1845952  -0.42266738\n",
            " -0.3079577  -0.09337176 -0.06206686 -0.47805342 -0.42945346 -0.18309648\n",
            " -0.20901641  0.3323932   0.05612215  0.38908657 -0.26160556  0.07368747\n",
            " -0.13051578]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.09444294 -0.27312654 -0.20901667 -0.1430767   0.25108472 -0.57473874\n",
            " -0.15419768 -0.09039439  0.06455438 -0.34666485 -0.13369186 -0.18498628\n",
            " -0.36077434  0.62937146  0.05317896  0.27426124 -0.29083088  0.20868732\n",
            " -0.20111957]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.20427571 -0.2841754  -0.17119521 -0.04560629  0.29469863 -0.4372766\n",
            " -0.19962661 -0.22465202  0.07522389 -0.41081387 -0.2878764  -0.12568691\n",
            " -0.2735349   0.4429168  -0.10401095  0.4087623  -0.18001607 -0.03781535\n",
            "  0.03625992]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 45 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.03912822 -0.5242822  -0.29651284 -0.02036199  0.2353299  -0.47956\n",
            " -0.07532015 -0.16946122  0.13883321 -0.23482376 -0.2397413   0.05654133\n",
            " -0.34801173  0.7255661   0.11316204  0.46197933 -0.18067831  0.08249169\n",
            " -0.05816007]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.16477254 -0.4070265  -0.2091476  -0.10032405  0.2680844  -0.3590322\n",
            " -0.20891964 -0.20238127  0.04178332 -0.59775203 -0.3611665  -0.09443785\n",
            " -0.29766682  0.61243147 -0.05431454  0.3348433  -0.28159586  0.00907158\n",
            "  0.00717221]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.03486684 -0.4655623  -0.35757992 -0.30996308  0.39741814 -0.6286168\n",
            " -0.1452093  -0.09338468  0.04665826 -0.3533067  -0.06457608 -0.14036012\n",
            " -0.3754813   0.79773515  0.08585627  0.26473567 -0.6738176   0.32479447\n",
            " -0.29420087]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02033089 -0.25293678 -0.3521373  -0.12017468  0.35649097 -0.5641641\n",
            " -0.31088644 -0.17950271  0.0839329  -0.56916535 -0.28237247 -0.12428131\n",
            " -0.22394678  0.453913   -0.07446669  0.33111364 -0.29348534  0.13868974\n",
            " -0.00268416]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.08711674 -0.5655897  -0.3379724  -0.14564344  0.36115804 -0.50172234\n",
            " -0.04394241 -0.28205043  0.17841227 -0.327369   -0.16116846  0.16747548\n",
            " -0.22002243  0.6328478   0.12084168  0.2595438  -0.31552804  0.13517714\n",
            " -0.25675714]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.25925454 -0.2648698  -0.24012463 -0.30521354  0.28067413 -0.56788\n",
            " -0.22433306 -0.1813272   0.01100054 -0.39481676 -0.16017154 -0.17797026\n",
            " -0.24297695  0.55990916 -0.10916404  0.2883898  -0.22542559  0.20912932\n",
            " -0.12865293]\n",
            "Epsilon reduced to 0.08192000000000003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-de56fea70a47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_hist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-6674072dd4f2>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_feature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mexp_rew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_rew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: _to_feature_vector() missing 1 required positional argument: 'ep'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg0VxCjTsK_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_conv_20(env):\n",
        "    input_shape = env.observation_shape()\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape + (1, ), input_shape=input_shape))\n",
        "    model.add(Conv1D(50, kernel_size=env.action_space.n, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(env.action_space.n))\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H-O2Rht2hpR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9958a47a-2520-4944-c9ea-2ed7f5462ea6"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=100, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=6, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_20(env), max_epsilon=0.2, epsilon_decay=0.8)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_12 (Reshape)         (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_12 (Conv1D)           (None, 187, 32)           128       \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 5984)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 19)                113715    \n",
            "=================================================================\n",
            "Total params: 113,843\n",
            "Trainable params: 113,843\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |████████████████------------------------------------------------------------------------------------| 16.7% \r\n",
            "Taking action 6\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.08758324 -0.00368383  0.1580424  -0.0982112   0.17350891  0.03125656\n",
            " -0.02040735 -0.07391182 -0.00616802 -0.00430446  0.10880446  0.02583558\n",
            " -0.19541138 -0.02213373  0.13884024  0.11600134 -0.00621178 -0.14402075\n",
            " -0.164229  ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.04307149  0.1198447   0.10482989 -0.1387328   0.13215826  0.08653601\n",
            "  0.07674137 -0.05158659  0.0020056  -0.03404711  0.01449664 -0.09152757\n",
            " -0.06559613 -0.02567809  0.05313151 -0.06247407  0.03659762 -0.03463651\n",
            "  0.00141892]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.07350972 -0.05190618  0.09698847 -0.1142343  -0.00700202  0.00373025\n",
            " -0.2558236  -0.0011192  -0.04790245 -0.00645135  0.07264134 -0.03921263\n",
            " -0.11398835  0.07955238  0.10845166  0.00600738 -0.01907661  0.00062327\n",
            " -0.04729512]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 4 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.13942839  0.00768759  0.08474591 -0.10556296  0.00999142  0.10574058\n",
            " -0.1739984  -0.1446658  -0.03556607  0.04874321  0.01884435 -0.00848978\n",
            " -0.11357711 -0.03205891 -0.00845119  0.03016469  0.00787438 -0.07329752\n",
            "  0.02563762]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.03185778  0.03944999  0.11126973 -0.05125238 -0.06305293  0.04447642\n",
            "  0.0457008  -0.04247128 -0.00938127  0.02815899  0.09385386  0.02349252\n",
            " -0.02092257  0.02140042  0.1024873   0.01270692 -0.03327579 -0.03557365\n",
            " -0.10731702]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1158245  -0.18135971  0.21892945 -0.0206481  -0.2231206   0.05741188\n",
            " -0.27835593 -0.0230425   0.00412517  0.02743961 -0.00828878 -0.05484089\n",
            " -0.21837172 -0.08701608  0.00739584  0.14854862 -0.04657057 -0.10534176\n",
            " -0.00891847]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.13188975 -0.05790868  0.14263928 -0.15100177 -0.19142097 -0.04841639\n",
            " -0.08645847 -0.19196329  0.01583895  0.0118697   0.04958672 -0.05920819\n",
            " -0.11847068  0.00309916  0.09348373  0.00772993  0.04141882 -0.08480014\n",
            " -0.07974316]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.14303604 -0.17786823  0.07413176 -0.10867748 -0.31970492  0.10147694\n",
            " -0.17009053 -0.12200741  0.0509216   0.06448722 -0.03091429 -0.05652987\n",
            " -0.11310235 -0.08687103 -0.05296568 -0.01045878 -0.06795483 -0.11420398\n",
            " -0.01930116]\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 9 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.17409505 -0.27437696 -0.08028379 -0.04223173 -0.22670275  0.00393973\n",
            " -0.24554497 -0.18444607 -0.0265847  -0.00145603  0.0255499  -0.04855682\n",
            " -0.10559665 -0.00056918  0.01644124  0.0144554  -0.02588    -0.16598028\n",
            "  0.02622771]\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 10 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1966031  -0.24834199 -0.00570336 -0.06453596 -0.3295954  -0.02280011\n",
            " -0.20446675 -0.10392374 -0.0156233   0.02659205  0.0607825  -0.01823008\n",
            " -0.08940606  0.02204769 -0.02338335  0.00249961 -0.08109204 -0.02918128\n",
            "  0.06290062]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.2587446  -0.31474447 -0.18908308 -0.16540813 -0.5176821  -0.09005889\n",
            " -0.41618353 -0.07238698 -0.043979   -0.01965587  0.07119089 -0.0556681\n",
            " -0.02564786 -0.04282681  0.01861777 -0.15949656 -0.07650512 -0.04129502\n",
            "  0.08006967]\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.24185245 -0.33752283 -0.22517698 -0.08913966 -0.36450374 -0.09660152\n",
            " -0.24154954 -0.17212093  0.01664913 -0.0258723  -0.01606788  0.02171706\n",
            " -0.03392637  0.08150183  0.00567165 -0.00725552 -0.08948705 -0.03525893\n",
            " -0.02826604]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 13 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1186069e-01 -2.5533220e-01 -9.1238797e-02 -4.2763375e-02\n",
            " -3.0632061e-01 -4.1878220e-02 -2.1867771e-01 -6.9656909e-02\n",
            "  3.3871275e-03 -2.0777164e-03 -5.8283232e-02 -1.7431885e-04\n",
            " -1.1494698e-01  4.2044912e-03  2.6342275e-02  2.5782567e-03\n",
            " -1.1704815e-01 -6.1522376e-02 -5.8849730e-02]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 14 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.2685149  -0.3859107  -0.20132886 -0.12482379 -0.40490913 -0.18521087\n",
            " -0.3266025  -0.09968119 -0.08001216 -0.02607074 -0.13529477  0.00340682\n",
            " -0.06720671 -0.05957915  0.0794618   0.00836878 -0.16540958 -0.05081614\n",
            " -0.1432921 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 15 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4230793  -0.3972318  -0.34127775 -0.04522965 -0.54490924 -0.14139292\n",
            " -0.334911    0.02236371 -0.04039739  0.03741083 -0.11848079 -0.00276537\n",
            " -0.00073695 -0.02641392 -0.07735036 -0.0196096  -0.35154176  0.00908437\n",
            " -0.35931113]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 16 reward=-3 new_state=[0 0 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.1939622e-01 -3.6683962e-01 -3.2374820e-01  2.6835204e-04\n",
            " -4.4093493e-01 -1.7962509e-01 -4.7174019e-01 -9.7303919e-02\n",
            " -1.1989758e-01 -1.9813120e-02 -9.7260848e-02 -5.1204726e-02\n",
            " -1.8260700e-01  2.7062500e-02  4.8544731e-02 -1.4529118e-02\n",
            " -4.0455240e-01 -9.7986527e-02 -2.2772689e-01]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 17 reward=-3 new_state=[0 0 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.24988978 -0.28924534 -0.367553   -0.04883657 -0.53424805 -0.2603993\n",
            " -0.34682962 -0.14674893 -0.15636642 -0.00947782 -0.20939286 -0.04195398\n",
            " -0.06560566 -0.02687199 -0.01552678 -0.0019242  -0.477809   -0.03691911\n",
            " -0.43564785]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 18 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.23008534 -0.30733323 -0.34442848 -0.10140325 -0.476758   -0.27987754\n",
            " -0.4470603  -0.10376782 -0.22202295 -0.13850352 -0.23303117 -0.12193596\n",
            " -0.02812981 -0.05065281  0.0669484   0.00806573 -0.47507495 -0.06830333\n",
            " -0.32050788]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 19 reward=-3 new_state=[0 0 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.25925    -0.22119145 -0.28349972 -0.05916254 -0.32505423 -0.21541929\n",
            " -0.29128978 -0.07503977 -0.09604465 -0.13386597 -0.197931   -0.06526235\n",
            " -0.06378038  0.03607072  0.01618739  0.03261858 -0.39952168 -0.11788417\n",
            " -0.28767186]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 20 reward=-3 new_state=[0 0 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.19341263 -0.19500042 -0.34522226 -0.0257676  -0.2851116  -0.14056297\n",
            " -0.27088967 -0.09759808 -0.19559547 -0.1467664  -0.16304989 -0.01604006\n",
            " -0.03099552 -0.00411796  0.02305264 -0.02611466 -0.45382762 -0.11018616\n",
            " -0.28838158]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 21 reward=-3 new_state=[0 0 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.24959132 -0.29055086 -0.42484733 -0.07654279 -0.46349567 -0.34822932\n",
            " -0.3499671  -0.09102714 -0.27183676 -0.1664905  -0.28710538 -0.04211571\n",
            " -0.00624128 -0.0500221   0.09241488  0.01349878 -0.68439275 -0.17439583\n",
            " -0.40007386]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 22 reward=-3 new_state=[0 0 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.38628575 -0.3743023  -0.511828   -0.01744688 -0.6795415  -0.30512106\n",
            " -0.33728573 -0.08296521 -0.28482717 -0.23609096 -0.34940857 -0.03495571\n",
            " -0.03481172 -0.02450252 -0.03620568 -0.04517211 -0.89390206 -0.30863452\n",
            " -0.5633532 ]\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 23 reward=-4 new_state=[0 1 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.37011474 -0.3175426  -0.546816   -0.17035805 -0.4963662  -0.32296363\n",
            " -0.4718216  -0.19522987 -0.29210338 -0.24286288 -0.3814338  -0.07654818\n",
            " -0.07410039 -0.062539    0.04383634 -0.0761784  -0.9492207  -0.33579227\n",
            " -0.4819427 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 24 reward=-4 new_state=[0 1 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.2509547  -0.28644773 -0.48090428 -0.17990582 -0.296544   -0.25314873\n",
            " -0.36763403 -0.14683162 -0.2854411  -0.22346807 -0.21843761 -0.02127014\n",
            " -0.04944883 -0.00358886  0.02253564 -0.06290613 -0.82841563 -0.30598593\n",
            " -0.45545074]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 25 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3351949  -0.2869831  -0.43935063 -0.04048642 -0.40070796 -0.19129878\n",
            " -0.20489776 -0.13734943 -0.22950839 -0.23527662 -0.2722398  -0.00346733\n",
            " -0.14606002  0.02418679 -0.0372096  -0.08458605 -0.956109   -0.3521635\n",
            " -0.43571797]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 26 reward=-4 new_state=[0 1 1 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.23724991 -0.17748904 -0.3847951  -0.09285269 -0.3393514  -0.13287173\n",
            " -0.24250863 -0.17308682 -0.23340033 -0.2980753  -0.18882632 -0.00982871\n",
            " -0.01064016  0.02359593 -0.00848503 -0.14435649 -0.82942337 -0.29931906\n",
            " -0.4023397 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 27 reward=-4 new_state=[0 1 1 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.2219072  -0.14378639 -0.34817433 -0.111214   -0.34878987 -0.22238696\n",
            " -0.15938604 -0.15168396 -0.18885602 -0.29682449 -0.17880425  0.00850892\n",
            " -0.03119698 -0.00531306 -0.08449405 -0.18275511 -0.86195433 -0.25731003\n",
            " -0.38613197]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 28 reward=-3 new_state=[0 1 1 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.41941765 -0.31790632 -0.57321334 -0.33556038 -0.42880273 -0.35489455\n",
            " -0.32911977 -0.36599305 -0.3122318  -0.38720632 -0.23339897 -0.01275069\n",
            " -0.15558769 -0.18593682 -0.06508046 -0.06768858 -1.3230299  -0.37342688\n",
            " -0.6230749 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 29 reward=-3 new_state=[0 1 1 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.4443798  -0.3129987  -0.5767143  -0.29225463 -0.52566004 -0.29695404\n",
            " -0.31002578 -0.30286282 -0.23208702 -0.38407692 -0.42718077 -0.06223416\n",
            " -0.04565776 -0.20831865  0.04878399 -0.06541237 -1.281574   -0.4274344\n",
            " -0.5461613 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 30 reward=-3 new_state=[0 1 1 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.27605084 -0.20508438 -0.4297089  -0.28153613 -0.579771   -0.16808324\n",
            " -0.20175445 -0.2811287  -0.2238859  -0.37156296 -0.36257938 -0.15601273\n",
            "  0.0020882  -0.13359442  0.10139131 -0.05841388 -1.2962176  -0.43327636\n",
            " -0.5414888 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 31 reward=-3 new_state=[0 1 1 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.45570594 -0.45460358 -0.58158934 -0.51341444 -0.79053265 -0.4989101\n",
            " -0.5109179  -0.5672076  -0.44607174 -0.5166257  -0.5132887  -0.31239143\n",
            " -0.09616835 -0.26525438  0.08904013 -0.00274873 -1.8090836  -0.6053354\n",
            " -0.85280603]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 32 reward=-3 new_state=[0 1 1 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.4988189  -0.5838273  -0.60964125 -0.693389   -0.7323148  -0.4911379\n",
            " -0.5683141  -0.63825977 -0.534419   -0.62410927 -0.37900248 -0.4172965\n",
            " -0.11239988 -0.35099334 -0.06409571 -0.01496568 -2.151069   -0.57370573\n",
            " -0.8145876 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 33 reward=-2 new_state=[0 1 1 1 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5909675  -0.361195   -0.7330617  -0.62492913 -0.694638   -0.460381\n",
            " -0.43868878 -0.7992942  -0.42350957 -0.49970913 -0.54988945 -0.34891292\n",
            " -0.10982947 -0.4760286  -0.02288028 -0.11472801 -2.2308295  -0.5949852\n",
            " -0.8342266 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 34 reward=-3 new_state=[0 1 1 1 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5757983  -0.401664   -0.7028447  -0.66993046 -0.7992299  -0.3843464\n",
            " -0.36409932 -0.9073069  -0.5265757  -0.6283248  -0.5187056  -0.43388104\n",
            "  0.00708178 -0.46433914 -0.03575081 -0.13303408 -2.32803    -0.70123833\n",
            " -0.88256514]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 35 reward=-4 new_state=[0 1 1 1 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.49491444 -0.5748327  -0.650748   -0.6830657  -0.6936602  -0.4526327\n",
            " -0.35638362 -0.8201771  -0.53967667 -0.610722   -0.46719098 -0.46349382\n",
            " -0.02913415 -0.45417076 -0.06273212 -0.04955451 -2.4224412  -0.7993856\n",
            " -0.8972458 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=-3 new_state=[0 1 1 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6502559  -0.6219778  -0.58310175 -0.64682055 -0.7416729  -0.40763137\n",
            " -0.52994806 -0.86794347 -0.37560743 -0.66079694 -0.58308625 -0.65013784\n",
            " -0.1711293  -0.5157595  -0.03944317  0.02717622 -2.6079776  -0.86300415\n",
            " -0.8582084 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 37 reward=-3 new_state=[0 1 1 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4281134  -0.29024166 -0.6087125  -0.61076033 -0.6338584  -0.3039087\n",
            " -0.33913952 -0.8125389  -0.4368244  -0.55586964 -0.46927515 -0.5441934\n",
            " -0.1477565  -0.4637782  -0.08706462 -0.12510492 -2.2450552  -0.7699753\n",
            " -0.717997  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 38 reward=-4 new_state=[0 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.43520847 -0.380333   -0.6300257  -0.6816017  -0.8668188  -0.22760151\n",
            " -0.44408047 -0.95557183 -0.38917577 -0.627174   -0.5141281  -0.6196156\n",
            " -0.26482424 -0.59527236 -0.00410463 -0.19668071 -2.4655213  -0.9660236\n",
            " -0.64545274]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 39 reward=-4 new_state=[0 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4366467  -0.4307847  -0.6026162  -0.63269925 -0.84930944 -0.33842075\n",
            " -0.34749705 -0.94021034 -0.4218378  -0.5527812  -0.63468075 -0.58597136\n",
            " -0.3630706  -0.6120658  -0.02575568 -0.04488791 -2.655644   -1.1503056\n",
            " -0.8324075 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 40 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.36251333 -0.31333858 -0.57206833 -0.4846276  -0.4743262  -0.31402934\n",
            " -0.29041973 -0.70740485 -0.29700977 -0.47744715 -0.4697521  -0.49703512\n",
            " -0.30563462 -0.5009594  -0.08726099 -0.11660457 -2.0388112  -0.8762462\n",
            " -0.59359837]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 41 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3771984  -0.29827365 -0.59620285 -0.46370715 -0.56333613 -0.25603938\n",
            " -0.2467231  -0.84532696 -0.30160496 -0.488152   -0.5801853  -0.5408663\n",
            " -0.37557086 -0.5595441  -0.07197975 -0.1345985  -2.2435517  -1.0055859\n",
            " -0.5650608 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 42 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.44637486 -0.38950372 -0.6186962  -0.69337136 -0.7054493  -0.38969317\n",
            " -0.3356801  -1.1148924  -0.40790948 -0.5121441  -0.72872764 -0.6109211\n",
            " -0.45903644 -0.55526054  0.0173141  -0.09329312 -2.7222695  -1.3384852\n",
            " -0.72660464]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 43 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6222107  -0.6548741  -0.75407964 -0.92419577 -0.74691045 -0.52594715\n",
            " -0.43341932 -1.3520846  -0.47036567 -0.7307169  -0.8803721  -0.751333\n",
            " -0.5583939  -0.7027608   0.02982262 -0.13596833 -3.4443922  -1.7284923\n",
            " -0.85028714]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 44 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5684925  -0.5285861  -0.7594618  -0.85288644 -0.62700963 -0.45535854\n",
            " -0.4263418  -1.2651895  -0.47158712 -0.6810706  -0.93609375 -0.78927773\n",
            " -0.5747647  -0.7077563  -0.04977833 -0.16787277 -3.302843   -1.5861987\n",
            " -0.82571936]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 45 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.51275086 -0.466478   -0.6631384  -0.68819314 -0.562081   -0.43152383\n",
            " -0.2529496  -1.3478725  -0.38378417 -0.6828384  -0.9397762  -0.76601315\n",
            " -0.54292405 -0.6822646  -0.08677302 -0.07681849 -3.265845   -1.6048588\n",
            " -0.7798231 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 46 reward=-4 new_state=[0 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5394984  -0.53081185 -0.65893716 -1.0702864  -0.95298    -0.5609663\n",
            " -0.41908142 -1.7017715  -0.52992475 -0.7190264  -1.1766636  -0.9247729\n",
            " -0.765109   -0.77279496 -0.05784143 -0.11555883 -4.061984   -2.0022757\n",
            " -0.9056863 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 47 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6525095  -0.56583124 -0.86613035 -1.1343999  -1.0481433  -0.5732858\n",
            " -0.67046386 -2.056562   -0.6417763  -0.7520486  -1.403816   -1.0354524\n",
            " -1.0512623  -1.0903847  -0.05798974 -0.20424812 -4.6477923  -2.30036\n",
            " -0.98582137]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 48 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-6.1842602e-01 -6.2573951e-01 -8.8830042e-01 -1.3107567e+00\n",
            " -7.1164149e-01 -5.2132517e-01 -5.2961302e-01 -1.8778807e+00\n",
            " -6.3315856e-01 -7.1414888e-01 -1.2539240e+00 -9.7106743e-01\n",
            " -9.1151470e-01 -9.4623697e-01 -1.9632371e-03 -2.0530032e-01\n",
            " -4.7845054e+00 -2.3028767e+00 -1.0103959e+00]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 49 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6747196  -0.6712452  -0.8884298  -1.3829868  -0.74030715 -0.57885766\n",
            " -0.44017914 -1.9412328  -0.6736343  -0.7017016  -1.3192981  -0.96578974\n",
            " -1.0017655  -0.9984463  -0.04331272 -0.11703297 -4.9835424  -2.4095652\n",
            " -1.164295  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 50 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.57974327 -0.66169465 -0.91080403 -1.4809846  -0.8773643  -0.60143024\n",
            " -0.3751475  -2.0465019  -0.66220623 -0.76478404 -1.4153606  -1.0533556\n",
            " -0.99271166 -0.9691252  -0.0174764  -0.08617293 -5.1463785  -2.7721996\n",
            " -1.2672871 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 51 reward=-5 new_state=[0 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.63066614 -0.6139764  -0.8824742  -1.6098135  -0.93958384 -0.65687716\n",
            " -0.5100194  -2.1579385  -0.586635   -0.82965755 -1.4960606  -1.0987372\n",
            " -1.0191337  -0.97669864 -0.08281593 -0.15604068 -5.2350583  -2.7669036\n",
            " -1.4169343 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 52 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6563695  -0.56136644 -0.8975816  -1.8274052  -0.96355814 -0.5998127\n",
            " -0.6840922  -2.392424   -0.68727756 -0.8848047  -1.7178643  -1.2704253\n",
            " -1.2482828  -1.1177758  -0.12891334 -0.13515073 -5.7728515  -3.0217035\n",
            " -1.7445515 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 53 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6761768  -0.61755806 -0.81445843 -1.6402994  -0.8776087  -0.53454584\n",
            " -0.5275682  -2.1078098  -0.60183746 -0.8654371  -1.4514147  -1.1708221\n",
            " -1.1750841  -0.97508985 -0.0733775  -0.33940247 -5.3032045  -2.877398\n",
            " -1.5915931 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 54 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5716683  -0.47600803 -0.738308   -1.6378952  -0.8216176  -0.560045\n",
            " -0.46930107 -2.0802753  -0.59031385 -0.755235   -1.4404457  -1.0405416\n",
            " -1.2952359  -0.89630646 -0.11812483 -0.42347905 -5.21597    -2.7685587\n",
            " -1.7449697 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 55 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6123719  -0.57569426 -0.8341702  -1.7361585  -0.88827103 -0.5940396\n",
            " -0.48333967 -2.2228055  -0.58841777 -0.7852331  -1.6056005  -1.1634585\n",
            " -1.4739099  -0.9944427  -0.13900489 -0.5513899  -5.4152274  -3.044472\n",
            " -1.882847  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 56 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.80702066 -0.6994215  -0.91858816 -2.1974108  -1.0795773  -0.6982204\n",
            " -0.7305793  -2.639433   -0.7697403  -0.9496079  -1.8831326  -1.406437\n",
            " -1.7465806  -1.1793553  -0.16646895 -0.805124   -6.131289   -3.4422495\n",
            " -2.3006277 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 57 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.73604137 -0.5743515  -0.9208707  -2.031603   -0.95183545 -0.56056833\n",
            " -0.43426186 -2.3914533  -0.6331435  -0.8274261  -1.6600729  -1.2067538\n",
            " -1.7187101  -1.2023774  -0.03665105 -0.9043477  -5.6577353  -3.2874582\n",
            " -2.2195663 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 58 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.75361985 -0.64566755 -0.97316045 -2.1850042  -0.9996469  -0.68312824\n",
            " -0.42722693 -2.5149438  -0.74823296 -0.8525707  -1.7825481  -1.2470622\n",
            " -2.069861   -1.2098771  -0.09653288 -1.0631828  -5.853373   -3.4854517\n",
            " -2.605064  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 59 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.78844565 -0.81510633 -1.0652641  -2.1807392  -1.1966038  -0.7682497\n",
            " -0.671959   -2.650792   -0.84281266 -0.99238676 -1.983562   -1.3920642\n",
            " -2.0681808  -1.2211612  -0.08046895 -1.1893587  -5.9625635  -3.8256612\n",
            " -2.9078422 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 60 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7291965  -0.65987784 -0.94510794 -2.4143136  -1.1466544  -0.62229335\n",
            " -0.7020775  -2.7398524  -0.7250686  -0.90399677 -2.0743458  -1.3726208\n",
            " -2.205067   -1.2085023  -0.04012898 -1.2064544  -5.8265347  -3.7419128\n",
            " -2.8653157 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 61 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7030353  -0.6178128  -0.8306829  -2.134219   -0.8998846  -0.52640146\n",
            " -0.47355944 -2.3412137  -0.56769156 -0.8721813  -1.68792    -1.242864\n",
            " -1.988318   -1.129547   -0.00803549 -1.2561742  -4.8516755  -3.3344536\n",
            " -2.4028854 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 62 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.425106   -0.5116258  -0.7189631  -1.8421829  -0.844032   -0.59107137\n",
            " -0.41884986 -2.2533455  -0.575969   -0.7449606  -1.603265   -1.1548932\n",
            " -1.9077095  -0.9823634  -0.06127413 -1.0861585  -4.367378   -3.1897228\n",
            " -2.430761  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 63 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7322547  -0.6992152  -0.7988289  -2.203412   -1.114455   -0.796617\n",
            " -0.41611254 -2.5252504  -0.6272516  -0.87083715 -1.9179491  -1.2734641\n",
            " -2.1644957  -1.0955197  -0.16198057 -1.330987   -4.59017    -3.4887424\n",
            " -2.8333645 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 64 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7201492  -0.708649   -1.000071   -2.1732364  -0.9653048  -0.6289012\n",
            " -0.7088125  -2.6872435  -0.87265456 -0.8369859  -1.8364292  -1.3612207\n",
            " -2.275994   -1.2807978  -0.09812789 -1.4277003  -4.718597   -3.5748224\n",
            " -3.008492  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 65 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7029217  -0.6343151  -0.8705307  -2.4040987  -1.0670414  -0.8649116\n",
            " -0.47494876 -2.6566973  -0.7227545  -1.0619104  -1.9388531  -1.3531449\n",
            " -2.2921417  -1.2341789  -0.05143109 -1.6549187  -4.5387983  -3.7960851\n",
            " -3.2728786 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 66 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.61131614 -0.6556428  -0.88814557 -2.0738096  -0.9158332  -0.6086125\n",
            " -0.4945649  -2.1771696  -0.74193734 -0.7547055  -1.5643259  -1.1033225\n",
            " -1.9648616  -0.9071027  -0.02467009 -1.3522222  -3.7231004  -3.3163795\n",
            " -2.8057148 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 67 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.70510817 -0.5643386  -0.9555327  -2.228234   -1.1137383  -0.65830183\n",
            " -0.5312246  -2.5433474  -0.6753547  -0.8696234  -1.8543884  -1.1522377\n",
            " -2.1907237  -1.069238   -0.02624746 -1.6184722  -4.0016522  -3.4945133\n",
            " -3.1866052 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 68 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.75168794 -0.8178991  -1.0824873  -2.6534655  -1.2209412  -0.8215557\n",
            " -0.5109877  -3.2488642  -0.906569   -1.0827595  -2.3046978  -1.6131961\n",
            " -3.0171168  -1.4810851  -0.17990054 -1.9540695  -4.735813   -4.3267183\n",
            " -3.93504   ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 69 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.60308135 -0.4810181  -0.851229   -2.1808274  -1.0127853  -0.48301718\n",
            " -0.44915962 -2.4521494  -0.6581182  -0.8107359  -1.7298177  -1.1941102\n",
            " -2.14199    -1.1490805   0.05898033 -1.5379325  -3.8290417  -3.4063087\n",
            " -3.0681708 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 70 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.71157026 -0.61021423 -0.93435335 -2.0610933  -0.9741312  -0.6177912\n",
            " -0.44294092 -2.3712993  -0.5727607  -0.8198686  -1.738959   -1.0694106\n",
            " -2.2854652  -1.0469373  -0.04828241 -1.5866098  -3.3599792  -3.517174\n",
            " -3.0976057 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 71 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6003036  -0.506683   -0.87821573 -1.9042608  -0.9846662  -0.57606685\n",
            " -0.44402033 -2.3927486  -0.68984604 -0.76537085 -1.6578107  -1.1617565\n",
            " -2.1690981  -1.0407637  -0.06868395 -1.4914428  -3.3780856  -3.3638859\n",
            " -3.058778  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 72 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7486421  -0.78721726 -1.071897   -2.4104517  -1.0417658  -0.716029\n",
            " -0.62811613 -2.8121367  -0.8700931  -0.90063447 -1.9165732  -1.3524725\n",
            " -2.7034442  -1.3489802  -0.06484095 -1.809476   -3.9300508  -3.881032\n",
            " -3.7319522 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 73 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.75498396 -0.71805656 -1.0572942  -2.4494545  -1.0489206  -0.6620008\n",
            " -0.55491984 -2.5110223  -0.73445606 -0.9224731  -1.8380059  -1.2387872\n",
            " -2.542724   -1.1805712   0.08422948 -1.9808875  -3.6843069  -3.8320699\n",
            " -3.7126446 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 74 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.53966796 -0.53753525 -0.7591297  -1.8761638  -0.74228173 -0.5591501\n",
            " -0.45709324 -1.9492638  -0.55876493 -0.7338548  -1.4707059  -1.0239185\n",
            " -1.8649933  -0.8853096  -0.05490865 -1.4506072  -2.8143368  -2.960212\n",
            " -2.7487614 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 75 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.9022477  -0.67419255 -0.9151383  -2.2868643  -1.0512859  -0.7047684\n",
            " -0.5679217  -2.5344784  -0.7172418  -0.9181232  -1.8782911  -1.1704497\n",
            " -2.3300903  -1.0845968  -0.04635245 -1.837341   -3.3849254  -3.645982\n",
            " -3.472477  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 76 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.145427   -0.6423473  -1.006672   -2.3549547  -1.1957552  -0.62303394\n",
            " -0.5167259  -2.9872282  -0.7864642  -0.9227649  -1.9875135  -1.305029\n",
            " -2.7726226  -1.345962   -0.03672388 -1.9034753  -3.7854996  -3.927174\n",
            " -3.905533  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 77 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.1218519  -0.6537341  -0.9687105  -2.2588336  -0.8798384  -0.6471418\n",
            " -0.5833895  -2.308662   -0.7142219  -0.84524894 -1.565204   -1.1496853\n",
            " -2.2125936  -0.9911775   0.00905448 -1.6776541  -3.3618338  -3.4493783\n",
            " -3.1989892 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 78 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.2862353  -0.73855203 -0.95212317 -2.1462533  -0.92732537 -0.6703006\n",
            " -0.55926895 -2.2024992  -0.7554636  -0.8217952  -1.6553977  -1.0393649\n",
            " -2.2034445  -0.9278709   0.02125556 -1.7415359  -3.1513557  -3.4103851\n",
            " -3.3041382 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 79 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.0992569  -0.3848082  -0.7077235  -1.8525691  -0.90689445 -0.39831525\n",
            " -0.39925492 -2.222874   -0.49747604 -0.63184386 -1.658532   -1.0342842\n",
            " -2.0541813  -0.99482965 -0.04432586 -1.4636941  -3.002481   -3.0435958\n",
            " -2.9532285 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 80 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.8807981  -0.9919981  -1.2322007  -2.8188279  -1.1573944  -0.8880323\n",
            " -0.7814694  -3.126512   -0.9619466  -1.0651207  -2.0673757  -1.4766272\n",
            " -3.029567   -1.4673715  -0.13601209 -2.179256   -4.154301   -4.381176\n",
            " -4.2805157 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 81 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.6772562  -0.6544757  -1.1055326  -2.3216584  -1.0678824  -0.63018763\n",
            " -0.4033347  -2.6163778  -0.7632092  -0.8685203  -1.8317423  -1.2114114\n",
            " -2.5092518  -1.2270268   0.05723298 -1.9337299  -3.8366654  -3.7757869\n",
            " -3.7919235 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 82 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.4107132  -0.5432078  -0.75949895 -1.8943022  -0.8385555  -0.5871341\n",
            " -0.3489793  -2.0316622  -0.53658164 -0.7583076  -1.5105915  -1.0016536\n",
            " -2.066244   -0.8965626  -0.04049225 -1.40067    -3.0418677  -3.092737\n",
            " -2.8999681 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 83 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.7314936  -0.6906159  -0.9345685  -2.1029062  -1.0142173  -0.6510344\n",
            " -0.56439203 -2.346237   -0.7191827  -0.84811187 -1.7069063  -1.1480387\n",
            " -2.329307   -1.0250522  -0.08360685 -1.7242419  -3.4812846  -3.5411265\n",
            " -3.4332585 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 84 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.0419118  -0.6742576  -0.98622406 -2.4486806  -0.962551   -0.70182544\n",
            " -0.6780715  -2.7955856  -0.87833446 -0.7782859  -2.1342428  -1.3804829\n",
            " -2.7015817  -1.3112532  -0.11776597 -1.8419038  -3.9765985  -3.8302205\n",
            " -3.8154345 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 85 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3720677  -0.70114076 -1.0650452  -2.456491   -1.0586735  -0.6059768\n",
            " -0.6223163  -2.5664248  -0.86321    -0.80794996 -2.166481   -1.2705536\n",
            " -2.608779   -1.207468    0.05151971 -2.0108583  -4.0568604  -3.8392103\n",
            " -3.930726  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 86 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1473687e+00 -6.2339795e-01 -9.6636307e-01 -2.2065434e+00\n",
            " -9.6935302e-01 -7.0219457e-01 -5.8125246e-01 -2.3523598e+00\n",
            " -7.4210972e-01 -7.8759688e-01 -2.0603578e+00 -1.1267934e+00\n",
            " -2.3261948e+00 -9.5169771e-01  2.7642073e-03 -1.8160551e+00\n",
            " -3.6406689e+00 -3.5391486e+00 -3.5247281e+00]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 87 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1547158  -0.5255913  -0.9044761  -1.9342031  -0.8776686  -0.5134896\n",
            " -0.44214293 -2.2824411  -0.6006533  -0.78247136 -2.0619752  -1.1064692\n",
            " -2.2615244  -0.9586257  -0.04163014 -1.6325729  -3.5351398  -3.3829868\n",
            " -3.17771   ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 88 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.8747034  -0.72360444 -1.0731257  -2.6486928  -1.2611018  -0.81999695\n",
            " -0.5849715  -3.1272078  -0.84360045 -0.96084166 -2.8118634  -1.4630616\n",
            " -3.0276008  -1.4283714  -0.1346263  -2.2161412  -4.283255   -4.202752\n",
            " -4.2458653 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 89 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.536422   -0.73715985 -0.94157565 -2.3601897  -0.91746765 -0.5890922\n",
            " -0.49768686 -2.4934964  -0.7822273  -0.7905239  -2.3271718  -1.2195196\n",
            " -2.5133185  -1.1969204   0.00951307 -1.9722213  -4.1112857  -3.6574621\n",
            " -3.6251323 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 90 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7645314  -0.57821333 -0.95263875 -2.2111793  -1.0576141  -0.58895135\n",
            " -0.52828115 -2.5697467  -0.6855632  -0.78974086 -2.6093712  -1.1967988\n",
            " -2.5857153  -1.1406124  -0.04341258 -2.0974865  -4.141702   -3.8448062\n",
            " -3.7713702 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 91 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.0405033  -0.7676976  -1.0081766  -2.4690495  -1.1661494  -0.62560683\n",
            " -0.5128985  -2.5520623  -0.76240635 -0.90868294 -2.7475998  -1.2225302\n",
            " -2.6013763  -1.1203389  -0.04912772 -2.3401825  -4.150979   -3.8730059\n",
            " -3.8584957 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 92 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.1427536  -0.6459738  -1.0317928  -2.6041503  -1.1241705  -0.54686445\n",
            " -0.674616   -2.9165223  -0.84490323 -0.89128333 -2.8249562  -1.363569\n",
            " -2.7684143  -1.3295231  -0.04890312 -2.4859364  -4.3113728  -4.0102572\n",
            " -4.12348   ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 93 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.229112   -0.85199    -1.080733   -2.5180042  -1.1277322  -0.6800355\n",
            " -0.6480864  -2.7654839  -0.90673685 -0.96873236 -2.7407968  -1.3016295\n",
            " -2.7506952  -1.2784245   0.04260841 -2.6042113  -4.3869014  -4.0378346\n",
            " -4.0521655 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 94 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.5606618  -0.631396   -0.8298791  -2.1222398  -0.8526534  -0.62383467\n",
            " -0.4606997  -2.206831   -0.6061131  -0.69975346 -2.5425162  -1.1270082\n",
            " -2.2475228  -0.98600924 -0.02372759 -2.0096104  -3.6524644  -3.4501212\n",
            " -3.2504776 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 95 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.2181888  -0.81150734 -0.96285224 -2.3669808  -1.1131865  -0.6724276\n",
            " -0.525436   -2.6807854  -0.7788255  -0.91900444 -2.9538896  -1.3301134\n",
            " -2.7434638  -1.1753261  -0.09571569 -2.7320745  -4.427227   -4.0690265\n",
            " -3.9585843 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 96 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.8476658e+00 -8.6505735e-01 -1.1093220e+00 -2.9631371e+00\n",
            " -1.2857206e+00 -7.6728463e-01 -6.7026448e-01 -3.2922673e+00\n",
            " -9.7560942e-01 -1.0760031e+00 -3.3477693e+00 -1.5187559e+00\n",
            " -3.0888703e+00 -1.4657562e+00  7.9251686e-04 -3.1891477e+00\n",
            " -4.7271113e+00 -4.5638866e+00 -4.6952062e+00]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 97 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.2942286  -0.69128424 -1.0190806  -2.5039957  -0.95001036 -0.5962196\n",
            " -0.5335566  -2.6861725  -0.7962866  -0.8602449  -2.8834176  -1.2873487\n",
            " -2.7800698  -1.2618895  -0.01544268 -2.8068106  -4.2960224  -3.9154518\n",
            " -3.9508712 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 98 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.204647   -0.7257026  -1.0615597  -2.3262484  -0.9704527  -0.6898244\n",
            " -0.5649726  -2.448483   -0.77289754 -0.88425183 -2.9095895  -1.2113615\n",
            " -2.574881   -1.0651288  -0.04701724 -2.7494135  -4.045637   -3.8615108\n",
            " -3.7973728 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 99 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.025917   -0.6811174  -0.88554716 -2.3452692  -0.9880835  -0.619902\n",
            " -0.4726379  -2.4666495  -0.5471143  -0.8664783  -2.9015622  -1.1486892\n",
            " -2.523722   -1.1152226  -0.13365036 -2.59119    -3.8869483  -3.656705\n",
            " -3.5546148 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 100 reward=-4 new_state=[0 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.8924496  -0.9969991  -1.0978603  -2.8307006  -1.2681979  -0.77435446\n",
            " -0.6932153  -3.312026   -0.9904559  -1.1427226  -3.3782065  -1.5424595\n",
            " -3.1348567  -1.4648467  -0.06011061 -3.3063745  -4.597143   -4.622652\n",
            " -4.567793  ]\n",
            "Epsilon reduced to 0.16000000000000003\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.248335   -0.7531975  -1.0613723  -2.5395203  -1.0942612  -0.9521616\n",
            " -0.5732619  -2.6909444  -0.80364555 -0.9311833  -3.1971495  -1.3605016\n",
            " -2.4571533  -1.1167446   0.03715477 -2.4944696  -3.8123991  -3.7630699\n",
            " -3.6800387 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.8837657  -0.9295239  -1.1545532  -2.9687474  -1.4252884  -1.3177952\n",
            " -0.634531   -3.45608    -0.9319954  -1.11462    -3.9202225  -1.6140475\n",
            " -3.3474884  -1.4667027  -0.05253023 -3.2809362  -4.1988435  -4.570381\n",
            " -4.441454  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5906796  -0.831946   -1.1138654  -2.6823988  -1.1552762  -1.0628262\n",
            " -0.52779317 -3.0417035  -0.8896851  -1.0077859  -3.4467583  -1.3895246\n",
            " -2.6727536  -1.2811809  -0.028412   -2.8688335  -4.128568   -4.1388726\n",
            " -4.104686  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.0539360e+00 -1.0256366e+00 -1.2089459e+00 -2.8390756e+00\n",
            " -1.4146950e+00 -1.2024816e+00 -8.1227970e-01 -3.3085814e+00\n",
            " -1.0174607e+00 -1.1438373e+00 -3.6437726e+00 -1.4722276e+00\n",
            " -2.7533240e+00 -1.3665538e+00  2.1690959e-03 -3.2914870e+00\n",
            " -4.4789052e+00 -4.4770131e+00 -4.4678540e+00]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 5 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.3009636  -0.8224882  -1.0126427  -2.5276573  -1.0943499  -0.8152053\n",
            " -0.7143905  -2.8031929  -0.80902195 -0.86318165 -3.1121895  -1.2501086\n",
            " -2.2046804  -1.2022953   0.00601564 -2.7565386  -4.026518   -3.8785994\n",
            " -3.766916  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 6 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6017554  -0.75411594 -1.1270413  -2.6373794  -1.2177533  -0.87870884\n",
            " -0.6691144  -2.8768399  -0.85430026 -1.0948751  -3.2246468  -1.3514503\n",
            " -2.180971   -1.2249476  -0.01442497 -3.0545523  -4.154358   -3.9990408\n",
            " -4.073519  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 7 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.96634    -0.6882301  -0.9633673  -2.364273   -0.89131093 -0.70522684\n",
            " -0.5833346  -2.3650744  -0.809662   -0.9781163  -2.6621354  -1.160464\n",
            " -1.5737542  -0.9851971   0.0778436  -2.5027452  -3.6244993  -3.4198434\n",
            " -3.4265697 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 8 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.4564734  -0.5269746  -0.74794203 -1.8524336  -0.8001233  -0.56824183\n",
            " -0.4187589  -2.010252   -0.5465612  -1.061322   -2.4793327  -1.0132368\n",
            " -1.1490825  -0.8219222  -0.08062434 -2.0831854  -2.8932028  -2.8819475\n",
            " -2.7972932 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 9 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5593762  -0.7325539  -0.95243967 -2.4862015  -1.1674907  -0.74972767\n",
            " -0.48341295 -2.9461322  -0.83240503 -1.38066    -3.0715175  -1.2108426\n",
            " -1.6856778  -1.2669026  -0.01141322 -2.8791244  -3.895842   -3.7722132\n",
            " -3.9038386 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 10 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.0386326  -0.6324437  -0.96246606 -2.4210463  -0.9514785  -0.6946856\n",
            " -0.57975036 -2.5959878  -0.77363616 -1.2870164  -2.8604112  -1.1181452\n",
            " -1.3887911  -1.0085793   0.10719219 -2.5928288  -3.764185   -3.51542\n",
            " -3.4571755 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 11 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.3427222  -0.42443302 -0.6767761  -1.8130897  -0.7682403  -0.49746734\n",
            " -0.4977245  -2.0395553  -0.48413405 -1.1400328  -2.35317    -0.92212284\n",
            " -0.93290836 -0.7417149  -0.01071723 -1.9497536  -2.8225     -2.7571313\n",
            " -2.7136388 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 12 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.383405   -0.6379428  -0.71800816 -1.7848703  -0.69468224 -0.4475163\n",
            " -0.5737312  -1.9379746  -0.4855366  -1.2018917  -2.3831913  -0.89762855\n",
            " -0.90290225 -0.7844849   0.03445148 -1.9756099  -2.6576865  -2.7417614\n",
            " -2.644994  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 13 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.2350814  -0.71213764 -0.6201621  -1.759549   -0.69704986 -0.4195148\n",
            " -0.6903079  -2.0322502  -0.44688898 -1.2701774  -2.3193707  -0.87379193\n",
            " -0.8546043  -0.74915904  0.03320696 -1.9139175  -2.6286693  -2.684462\n",
            " -2.572797  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 14 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.1750426  -0.79869604 -0.60532486 -1.6717962  -0.71329594 -0.4687318\n",
            " -0.77207196 -2.005639   -0.44745877 -1.219361   -2.2193274  -0.82571995\n",
            " -0.92585427 -0.68657935  0.00272116 -1.7905519  -2.5194001  -2.5690646\n",
            " -2.491351  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 15 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.6776385  -1.0763584  -0.7768122  -2.0047164  -0.9408735  -0.6186763\n",
            " -1.2413354  -2.559869   -0.60159814 -1.483991   -2.6188877  -1.0887607\n",
            " -1.2551965  -0.93874544 -0.00662289 -2.2744417  -3.0586708  -3.1137075\n",
            " -3.0807023 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 16 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.659095   -1.6587048  -1.0670575  -2.6140099  -1.2498107  -0.8328006\n",
            " -1.7016307  -3.3973382  -0.85554826 -1.968242   -3.3486102  -1.3173681\n",
            " -1.782203   -1.2196363  -0.02809343 -3.1346726  -3.9763575  -3.9686842\n",
            " -4.0574064 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 17 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1045382  -1.6150321  -0.9589722  -2.3990314  -0.91885805 -0.6893395\n",
            " -1.551072   -2.672627   -0.75609636 -1.7567023  -2.8806634  -1.1586541\n",
            " -1.6363167  -0.99110216  0.08394121 -2.5259387  -3.5205555  -3.5219548\n",
            " -3.4285502 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 18 reward=-2 new_state=[1 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-3.0777457  -1.5906186  -0.8875436  -2.2180536  -0.96134436 -0.619263\n",
            " -1.5236106  -2.8211617  -0.6544265  -1.7251061  -2.9782248  -1.1301016\n",
            " -1.8715363  -1.0000097  -0.07780547 -2.4908066  -3.297364   -3.4177957\n",
            " -3.3229306 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 19 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.537752   -2.031674   -1.0259458  -2.7200065  -1.181127   -0.87815773\n",
            " -2.0703368  -3.2710755  -0.8730533  -2.1217284  -3.3442266  -1.4029528\n",
            " -2.2569392  -1.2127349   0.0622501  -2.9518402  -4.018337   -3.8343303\n",
            " -3.8941617 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 20 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1626923  -1.8013572  -0.89243484 -2.1958177  -1.0457581  -0.6846194\n",
            " -1.7106439  -2.7887967  -0.64076436 -1.8966318  -2.861672   -1.0219816\n",
            " -2.0482526  -1.0099615   0.05298692 -2.6262994  -3.4292288  -3.3099914\n",
            " -3.4257143 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 21 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.4715245  -2.156709   -1.0580355  -2.490229   -0.98177123 -0.67307097\n",
            " -2.0403438  -3.1630661  -0.7585235  -2.0580187  -3.1507022  -1.2458123\n",
            " -2.4171987  -1.2112579  -0.01059798 -2.8971725  -3.6594212  -3.5162148\n",
            " -3.7465515 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 22 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.3325267e+00 -2.1916265e+00 -9.5174694e-01 -2.5932772e+00\n",
            " -1.0264148e+00 -7.9524201e-01 -2.0689828e+00 -3.1534848e+00\n",
            " -8.2203728e-01 -2.1012890e+00 -3.1569507e+00 -1.2621769e+00\n",
            " -2.5153935e+00 -1.1438497e+00 -1.6361452e-04 -2.7852976e+00\n",
            " -3.6023731e+00 -3.3403049e+00 -3.6810920e+00]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 23 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.8971617  -1.807472   -0.835036   -2.0986297  -0.85732514 -0.59326667\n",
            " -1.6991928  -2.5126073  -0.5647005  -1.8817241  -2.6689913  -1.0859345\n",
            " -2.3123229  -0.8823827  -0.02376631 -2.3720198  -3.0174434  -2.9053345\n",
            " -3.1121452 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 24 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.7155669  -2.4165928  -1.0438678  -2.5164669  -1.2424382  -0.7940563\n",
            " -2.4315312  -3.417228   -0.8424424  -2.2711325  -3.2551398  -1.3229965\n",
            " -3.1866634  -1.2117627   0.01307251 -3.0136173  -3.846997   -3.5364568\n",
            " -4.0069485 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 25 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6365678  -2.5725422  -1.111988   -2.757013   -1.074854   -0.91664404\n",
            " -2.4986446  -3.4238813  -0.8149264  -2.3297029  -3.3837602  -1.3749732\n",
            " -3.4346673  -1.2466902   0.04076476 -3.1260262  -4.0033174  -3.3972654\n",
            " -3.962988  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 26 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1393948  -2.097616   -0.90793234 -2.2720098  -0.8773245  -0.62403935\n",
            " -2.0056093  -2.8471038  -0.64045775 -2.103203   -2.927707   -1.2405039\n",
            " -3.023728   -0.94918114 -0.0273548  -2.6083927  -3.1870198  -2.9670157\n",
            " -3.432828  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 27 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.238762   -2.251463   -0.9209827  -2.2695591  -0.925145   -0.69811225\n",
            " -2.1255581  -2.906625   -0.6501824  -2.0350482  -3.042358   -1.0835888\n",
            " -3.3306224  -1.0886455  -0.01840678 -2.6331441  -3.2778635  -2.7823243\n",
            " -3.3844538 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 28 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.800652   -3.0243948  -1.1230205  -2.812319   -1.2517424  -0.92665696\n",
            " -2.7474723  -3.6820223  -0.87589854 -2.5135882  -3.5540354  -1.366444\n",
            " -3.96724    -1.384814    0.06114066 -3.2290926  -4.127496   -3.341955\n",
            " -4.1719217 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 29 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.4030995  -2.6538632  -0.9615858  -2.4132392  -1.0641242  -0.6910347\n",
            " -2.3580763  -3.1124303  -0.72079027 -2.1515     -3.1227276  -1.1665907\n",
            " -3.6351945  -1.1360829   0.06333032 -2.8132288  -3.7166324  -3.0309126\n",
            " -3.6200378 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 30 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.2318323  -2.5506432  -0.9367737  -2.231187   -0.8859232  -0.65492976\n",
            " -2.1957967  -2.9895089  -0.6796616  -2.0809324  -3.0187297  -1.1060542\n",
            " -3.4517605  -1.0828546  -0.09103906 -2.6488593  -3.1703582  -2.8003476\n",
            " -3.4081    ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 31 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6209328  -3.1348352  -1.0177287  -2.730885   -1.2215047  -0.9010321\n",
            " -2.7484412  -3.5663579  -0.8777827  -2.4383926  -3.4020894  -1.3780115\n",
            " -3.941253   -1.2362819   0.05526106 -3.0035603  -4.002572   -3.1626031\n",
            " -3.9945884 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 32 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.2221994  -2.6439552  -0.88096225 -2.2905605  -1.0208575  -0.6833677\n",
            " -2.1805303  -2.9475586  -0.6577612  -2.1731536  -3.00155    -1.1446823\n",
            " -3.2575352  -1.0448608   0.06225248 -2.6249182  -3.5045145  -2.8909166\n",
            " -3.4711795 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 33 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.186318   -2.62383    -0.9404699  -2.162841   -0.863094   -0.6495955\n",
            " -2.2433488  -2.936626   -0.6759268  -2.071177   -2.9084542  -1.0971843\n",
            " -3.3285072  -1.0538574  -0.05833317 -2.5882337  -3.1569984  -2.7126977\n",
            " -3.3762105 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 34 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.61868    -3.2929986  -1.0712183  -2.6970828  -1.1963717  -0.89771307\n",
            " -2.6968327  -3.5650275  -0.9115548  -2.4047565  -3.3919528  -1.3548377\n",
            " -3.4732747  -1.2543807   0.05538655 -3.037881   -3.9213774  -3.0757365\n",
            " -3.9657247 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 35 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.2926633  -2.8784597  -0.9149092  -2.3940642  -1.1211796  -0.6368822\n",
            " -2.542291   -3.1495626  -0.7591673  -2.2439933  -3.055144   -1.255077\n",
            " -3.4474547  -1.0717897   0.10923005 -2.7711492  -3.7118497  -2.8154104\n",
            " -3.6303198 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 36 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.2879772  -2.7970593  -0.93201345 -2.2926974  -0.9769984  -0.65286374\n",
            " -2.508432   -3.2120466  -0.7372533  -2.2156177  -3.0416818  -1.2106477\n",
            " -3.3285863  -1.0608494  -0.02672741 -2.7346287  -3.3965526  -2.7750642\n",
            " -3.5708613 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 37 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.583539   -3.5123289  -1.0360532  -2.6704783  -1.1417072  -0.7713367\n",
            " -2.8711977  -3.6499407  -0.85023475 -2.5248754  -3.435623   -1.3446419\n",
            " -3.5532703  -1.2383275   0.04112427 -3.0735202  -3.920098   -2.9828298\n",
            " -4.01273   ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 38 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1321323  -2.8345187  -0.8496047  -2.303506   -1.0310615  -0.6658783\n",
            " -2.357629   -2.9200869  -0.6987546  -2.077658   -2.9237452  -1.1306429\n",
            " -3.0012789  -0.99220324  0.08695458 -2.5013072  -3.5029044  -2.5463834\n",
            " -3.3885705 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 39 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.9441733  -2.6033216  -0.8715329  -2.036515   -0.83422035 -0.62719035\n",
            " -2.140924   -2.7670214  -0.6274408  -1.9185162  -2.7323327  -1.0264883\n",
            " -2.6238415  -0.980509   -0.05941102 -2.3774843  -2.8883286  -2.414753\n",
            " -3.081447  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 40 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.444383   -3.3560183  -1.0303007  -2.5446386  -1.1053306  -0.8217181\n",
            " -2.687402   -3.323118   -0.8339371  -2.3625154  -3.173907   -1.3070945\n",
            " -2.9457753  -1.1416118  -0.00397882 -2.885446   -3.778523   -2.8570325\n",
            " -3.747346  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 41 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1691384  -2.9253561  -0.87448215 -2.2909946  -0.99860424 -0.6774598\n",
            " -2.3396327  -2.8612642  -0.68002355 -2.1450274  -2.916758   -1.1559284\n",
            " -2.642028   -0.99349844  0.06988098 -2.6351395  -3.4781542  -2.611054\n",
            " -3.373665  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 42 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.134025   -2.8646936  -0.9334815  -2.1525595  -0.8415327  -0.6382701\n",
            " -2.2898967  -2.9196036  -0.66823685 -2.0784223  -2.8377674  -1.0717107\n",
            " -2.606073   -1.0567502  -0.05691367 -2.5388706  -3.0610576  -2.543275\n",
            " -3.3034449 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 43 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5047987  -3.447294   -1.0522774  -2.563891   -1.1851472  -0.767769\n",
            " -2.8008497  -3.4352453  -0.8731155  -2.3314354  -3.2152872  -1.299182\n",
            " -2.7190785  -1.1863557   0.01167338 -2.9508774  -3.7126908  -2.8353064\n",
            " -3.8075802 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 44 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.0840552  -2.8898504  -0.9273197  -2.2985404  -1.0107446  -0.68328744\n",
            " -2.371979   -2.9016068  -0.723255   -2.0530026  -2.7899218  -1.1400583\n",
            " -2.4495816  -0.9968794   0.0941199  -2.536475   -3.390048   -2.5272179\n",
            " -3.3683636 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 45 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.9949565 -2.7454784 -0.9225961 -2.1356502 -0.8261092 -0.6040223\n",
            " -2.273124  -2.806659  -0.6514872 -2.0245595 -2.810491  -1.1105198\n",
            " -2.7398367 -1.0285485 -0.0327909 -2.5614545 -3.0639067 -2.4693074\n",
            " -3.2739418]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 46 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5940094 -3.6138716 -1.0292339 -2.5551336 -1.1015989 -0.7779045\n",
            " -2.925478  -3.4890208 -0.7840776 -2.4165208 -3.2357285 -1.2815826\n",
            " -3.000146  -1.2221713  0.0256837 -3.0340745 -3.8102307 -2.730931\n",
            " -3.852084 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 47 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1746616  -2.946406   -0.89404565 -2.3203807  -1.067701   -0.63433826\n",
            " -2.4586003  -2.9797354  -0.71445245 -2.1000757  -2.9469535  -1.1071078\n",
            " -2.767115   -1.0139815   0.13427962 -2.634454   -3.527958   -2.5330658\n",
            " -3.42998   ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 48 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.2164009  -2.9347696  -0.94709027 -2.2426395  -0.8728189  -0.6336816\n",
            " -2.4981985  -3.0885081  -0.65804094 -2.1031067  -2.849753   -1.1280264\n",
            " -2.7828944  -1.0463524  -0.07166839 -2.6327467  -3.1023893  -2.5396001\n",
            " -3.4167283 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 49 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5833147  -3.5010607  -1.0676304  -2.5902002  -1.1502746  -0.9015868\n",
            " -2.874562   -3.5653923  -0.9056846  -2.4595277  -3.220664   -1.3236228\n",
            " -2.7445936  -1.2517067   0.02449198 -3.0904837  -3.7827728  -2.8001163\n",
            " -3.8745883 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 50 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.0484312  -2.8540573  -0.86137265 -2.0978644  -0.8824418  -0.5857291\n",
            " -2.2811573  -2.9319093  -0.52228355 -2.0910766  -2.802561   -1.0152268\n",
            " -2.7252748  -1.0331541   0.04075201 -2.5795803  -3.2031279  -2.4563208\n",
            " -3.2110283 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 51 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.8353312  -2.55872    -0.8107286  -1.9589988  -0.78359073 -0.57921356\n",
            " -2.1432595  -2.8639734  -0.53872013 -1.8989804  -2.7142563  -1.0262914\n",
            " -2.6177049  -0.9127638  -0.08220582 -2.3336368  -2.7570012  -2.284021\n",
            " -2.9172335 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 52 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.5455868  -2.3631437  -0.7121104  -1.8865129  -1.0024025  -0.5015305\n",
            " -2.0349686  -2.7601037  -0.48047614 -1.8852632  -2.5602996  -1.0057471\n",
            " -2.7685235  -0.81525654 -0.07019536 -2.1678035  -2.6742227  -2.089778\n",
            " -2.8189495 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 53 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.0380924  -2.873512   -0.91645485 -2.1458666  -1.2138742  -0.60940194\n",
            " -2.3043666  -3.139454   -0.6692303  -2.0319626  -2.712127   -1.0767692\n",
            " -2.9133778  -1.0243268   0.02446521 -2.5028336  -3.2130265  -2.3639576\n",
            " -3.2866378 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 54 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6214705  -3.472083   -1.083476   -2.5460544  -1.7106438  -0.79410577\n",
            " -2.7713962  -3.8119726  -0.79394746 -2.3830657  -3.2133212  -1.1368484\n",
            " -3.4358168  -1.277836    0.10634296 -3.0799859  -3.7322023  -2.724381\n",
            " -3.844596  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 55 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.9320338  -2.7899933  -0.84661806 -2.1078403  -1.6252235  -0.6165322\n",
            " -2.3523417  -3.2358234  -0.55427384 -2.13467    -2.890531   -1.177313\n",
            " -3.1782875  -0.9310679  -0.01378959 -2.4999537  -3.1651235  -2.366885\n",
            " -3.2086473 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 56 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1111865  -2.8985593  -0.8190692  -2.1783438  -1.8039912  -0.53698325\n",
            " -2.5344791  -3.5605888  -0.5566812  -2.1743581  -2.9783344  -1.1185375\n",
            " -3.706844   -1.0454242  -0.0497721  -2.6615973  -3.007569   -2.4454377\n",
            " -3.3013833 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 57 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.175759   -3.150812   -0.87979203 -2.2082915  -2.1121528  -0.62300295\n",
            " -2.4745781  -3.7194152  -0.5915276  -2.3026593  -3.0805416  -1.102933\n",
            " -3.842095   -1.0805632   0.07041915 -2.7989724  -3.406258   -2.6022782\n",
            " -3.5241265 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 58 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.8445868  -3.9173975  -1.1109983  -2.767784   -2.5089154  -0.7417728\n",
            " -2.98851    -4.4662285  -0.8600625  -2.5493357  -3.5045269  -1.2595506\n",
            " -4.28489    -1.3024986   0.11517378 -3.2489169  -4.026527   -2.8311377\n",
            " -4.091749  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 59 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.585716   -3.380215   -1.015588   -2.5387154  -2.3013625  -0.6735785\n",
            " -2.8223975  -3.863145   -0.7516477  -2.3734474  -3.3164845  -1.3229947\n",
            " -4.252974   -1.2149076  -0.06671634 -2.9338923  -3.394113   -2.7158108\n",
            " -3.7233746 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 60 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.9727194  -2.7900014  -0.85723144 -2.1113596  -2.204881   -0.5701825\n",
            " -2.3431966  -3.4030044  -0.5572989  -2.1371853  -3.0558836  -1.1537395\n",
            " -3.9827054  -0.9625827  -0.04497346 -2.5367486  -3.0047169  -2.4107108\n",
            " -3.1772819 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 61 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.3176959  -3.2148702  -0.89964503 -2.3468943  -2.5802445  -0.6961221\n",
            " -2.5160558  -4.149399   -0.77573866 -2.338273   -3.3428762  -1.118748\n",
            " -4.3243985  -1.0613511   0.06362543 -2.7608998  -3.6073544  -2.688821\n",
            " -3.69984   ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 62 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.8982785  -3.9534419  -1.176915   -2.92037    -3.0022917  -0.8360491\n",
            " -3.1729598  -4.474963   -0.9639168  -2.7473826  -4.0091233  -1.5576965\n",
            " -4.787672   -1.3940793   0.04850565 -3.2916055  -3.966347   -3.1112366\n",
            " -4.258462  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 63 reward=-3 new_state=[1 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-3.3570492  -3.2188663  -0.93204266 -2.3023617  -2.6614745  -0.6196896\n",
            " -2.6272764  -3.9571455  -0.64525115 -2.3293855  -3.597916   -1.2279884\n",
            " -4.462181   -1.1074234  -0.04896012 -2.749846   -3.1996846  -2.7200725\n",
            " -3.5702298 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 64 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.3014905  -3.3338928  -0.9287764  -2.3639042  -2.8754194  -0.6716124\n",
            " -2.5997381  -4.2683463  -0.753163   -2.2775965  -3.5962338  -1.0770015\n",
            " -4.6212287  -1.3445581   0.04945596 -2.8306403  -3.5642235  -2.6952858\n",
            " -3.6988945 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 65 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.932791   -3.9100902  -1.1688303  -2.9913757  -3.269623   -0.8151063\n",
            " -3.1643608  -4.5503507  -0.91841364 -2.649428   -4.0649276  -1.4427152\n",
            " -4.885477   -1.83942     0.13310936 -3.4135582  -4.1287017  -3.1073213\n",
            " -4.2727265 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 66 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.080031   -3.876401   -1.1094624  -2.8099096  -3.1771297  -0.7189548\n",
            " -3.0932446  -4.595166   -0.8654312  -2.6102579  -4.010815   -1.3106085\n",
            " -4.9914713  -2.055597   -0.02718607 -3.2898698  -3.6175787  -2.9575944\n",
            " -4.2360415 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 67 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.3918767  -3.3126028  -0.9672725  -2.4105868  -3.0130293  -0.70682365\n",
            " -2.501932   -4.1209874  -0.6838727  -2.4007833  -3.8840427  -1.1712649\n",
            " -4.505107   -1.8502237   0.04754638 -2.9102485  -3.6856601  -2.76859\n",
            " -3.694217  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 68 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.537794   -3.4361668  -0.9488474  -2.5496428  -3.1950562  -0.7083241\n",
            " -2.6999764  -4.28787    -0.6963706  -2.4761238  -3.9195433  -1.2948341\n",
            " -4.328965   -2.0519128   0.04999506 -2.9470375  -3.656527   -2.8809834\n",
            " -3.7557473 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 69 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.483912   -3.3852715  -1.0455728  -2.5730085  -3.0505788  -0.6967536\n",
            " -2.8107233  -4.116802   -0.75895417 -2.4744363  -3.903572   -1.323514\n",
            " -4.226251   -2.2104807  -0.04121758 -2.918959   -3.376549   -2.7937005\n",
            " -3.6881604 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 70 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.4125075  -3.232519   -0.99540126 -2.387308   -3.0172691  -0.6025185\n",
            " -2.7709982  -4.137259   -0.7131293  -2.3293142  -3.6818461  -1.2384121\n",
            " -4.4971094  -2.2881567  -0.06963161 -2.9004471  -3.4552126  -2.6402702\n",
            " -3.7705636 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 71 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.4571006  -3.4785154  -0.9882949  -2.3307323  -3.1470165  -0.70755196\n",
            " -2.5140054  -4.2753077  -0.7156048  -2.3529663  -3.8716319  -1.1132714\n",
            " -4.216555   -2.4368541   0.01801348 -2.9057255  -3.7582226  -2.7654111\n",
            " -3.7390769 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 72 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.4962213  -3.6247084  -0.9567143  -2.7614849  -3.442471   -0.7848847\n",
            " -2.9398434  -4.4656262  -0.7854098  -2.5679963  -4.1949987  -1.3984265\n",
            " -4.1737723  -2.7303681   0.06666485 -2.9986365  -3.8369238  -2.9432518\n",
            " -3.873838  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 73 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6208456  -3.4121766  -0.99942577 -2.5169961  -3.2550764  -0.74637836\n",
            " -2.8137724  -4.2737117  -0.6531926  -2.483222   -3.8592958  -1.2771497\n",
            " -4.071696   -2.6570055  -0.07315834 -3.0025024  -3.3865898  -2.871747\n",
            " -3.789533  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 74 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6162755e+00 -3.6337597e+00 -1.0220942e+00 -2.5633149e+00\n",
            " -3.4792583e+00 -7.0321900e-01 -2.8807621e+00 -4.5326929e+00\n",
            " -7.2365218e-01 -2.4606459e+00 -4.0601091e+00 -1.2614388e+00\n",
            " -4.2961431e+00 -2.8168988e+00 -1.6821322e-03 -3.1232455e+00\n",
            " -3.9054344e+00 -2.7861769e+00 -4.0064287e+00]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 75 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.9467263  -3.5685716  -1.1512758  -2.6818614  -3.731702   -0.71145505\n",
            " -2.9821877  -4.6298428  -0.7825806  -2.5850856  -4.166293   -1.2726201\n",
            " -4.186847   -3.074536    0.07399669 -3.3328679  -4.084701   -2.9882343\n",
            " -4.2652273 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 76 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5110097  -3.4815967  -1.046478   -2.5890796  -3.3910084  -0.66132843\n",
            " -2.82445    -4.106289   -0.6609037  -2.4871619  -4.0759597  -1.2776719\n",
            " -3.753347   -2.9146605   0.02237672 -3.0090568  -3.5822763  -2.810854\n",
            " -3.7456136 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 77 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1527283 -3.104604  -0.9404756 -2.2777596 -3.0520113 -0.6052486\n",
            " -2.6438801 -4.0199776 -0.7775587 -2.3017023 -3.6957805 -1.2388119\n",
            " -3.7448173 -2.6813066 -0.0409275 -2.6893985 -3.3374615 -2.610617\n",
            " -3.5392647]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 78 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.491263   -3.3988519  -0.9851839  -2.4298275  -3.3864934  -0.64005893\n",
            " -2.6120656  -4.415      -0.738979   -2.373432   -4.0766234  -1.1376932\n",
            " -3.8193681  -2.9012356   0.06543199 -2.878862   -3.9907207  -2.7609766\n",
            " -3.8519363 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 79 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.3268285  -3.4632192  -0.98105055 -2.601416   -3.511619   -0.6589321\n",
            " -2.7726486  -4.342056   -0.7025685  -2.4823666  -4.1440735  -1.1954256\n",
            " -3.5128758  -2.9434001   0.11194916 -2.9444258  -3.7739823  -2.7264514\n",
            " -3.6987646 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 80 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5529773  -3.4017727  -0.96835333 -2.5689485  -3.3024945  -0.7451783\n",
            " -2.819768   -4.161343   -0.73148036 -2.4054523  -3.937241   -1.3719121\n",
            " -3.5329792  -3.0768466  -0.07431106 -2.8971157  -3.4712484  -2.7768285\n",
            " -3.7039921 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 81 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1915917  -3.3536363  -0.9380534  -2.3126354  -3.1837926  -0.75483817\n",
            " -2.5037034  -4.1688185  -0.7733091  -2.2350042  -3.7306316  -1.201981\n",
            " -3.4573734  -2.889372   -0.01580839 -2.7297215  -3.6890159  -2.6866283\n",
            " -3.5600936 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 82 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6164908  -3.3775985  -1.0093584  -2.5413334  -3.5714574  -0.6741861\n",
            " -2.8548663  -4.494191   -0.7526402  -2.4032154  -3.87959    -1.1761531\n",
            " -3.7993398  -3.1854055   0.05913672 -3.0365     -3.9921293  -2.7295797\n",
            " -3.980994  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 83 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.454311   -3.4495637  -1.0612005  -2.679278   -3.4012501  -0.75817025\n",
            " -2.842112   -4.049509   -0.7774819  -2.509575   -4.014036   -1.40437\n",
            " -3.5816789  -3.2716973   0.03708286 -2.9602115  -3.5764985  -2.816722\n",
            " -3.6918674 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 84 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5527935  -3.2959323  -0.97255486 -2.4690104  -3.29163    -0.62720394\n",
            " -2.810506   -4.2877083  -0.73146343 -2.416822   -3.9684927  -1.3095114\n",
            " -3.9349892  -3.2062774  -0.06770533 -2.9197273  -3.6214733  -2.7722533\n",
            " -3.8803785 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 85 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5281596  -3.5659933  -0.9939802  -2.378136   -3.4585974  -0.7251552\n",
            " -2.6432033  -4.433514   -0.75054604 -2.4139724  -4.0112076  -1.235479\n",
            " -3.886305   -3.2376108   0.02990027 -2.9616601  -4.093364   -2.8757713\n",
            " -3.8544922 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 86 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.593234   -3.545611   -1.0287933  -2.6218755  -3.7168138  -0.72793263\n",
            " -2.8917336  -4.4401813  -0.717853   -2.4586587  -4.199386   -1.2107586\n",
            " -3.6637385  -3.3625968   0.0901546  -3.081357   -3.9068246  -2.7683141\n",
            " -3.936058  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 87 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6331894  -3.443982   -1.0014648  -2.6259475  -3.3724601  -0.70803815\n",
            " -2.826762   -4.2872734  -0.75348955 -2.4401357  -4.075292   -1.3170526\n",
            " -3.8186471  -3.2919345  -0.03106458 -2.9644198  -3.5629005  -2.7546182\n",
            " -3.7398722 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 88 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.3975537e+00 -3.4860895e+00 -9.4860154e-01 -2.4799657e+00\n",
            " -3.4117367e+00 -6.9933921e-01 -2.6947217e+00 -4.4478488e+00\n",
            " -7.4910295e-01 -2.4641263e+00 -4.1653075e+00 -1.2877752e+00\n",
            " -4.1232038e+00 -3.2258871e+00  9.2534604e-04 -2.9420035e+00\n",
            " -3.9702868e+00 -2.7956300e+00 -3.8137729e+00]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 89 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6962476  -3.4220662  -1.0966138  -2.5128832  -3.5754464  -0.72389275\n",
            " -2.705122   -4.435576   -0.7435488  -2.404487   -4.040425   -1.1582628\n",
            " -3.972663   -3.2865975   0.04102162 -3.0881324  -4.0549345  -2.8353493\n",
            " -3.9576554 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 90 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.363437   -3.398098   -0.97957355 -2.6706991  -3.477827   -0.7553682\n",
            " -2.8388066  -4.2085695  -0.739871   -2.5050542  -4.079724   -1.4038768\n",
            " -3.952999   -3.4796154   0.02213589 -2.8609812  -3.6383061  -2.7228234\n",
            " -3.7168193 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 91 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5223098  -3.4299836  -1.0348971  -2.4488115  -3.384589   -0.68905735\n",
            " -2.7591567  -4.21099    -0.7368521  -2.442561   -4.0076942  -1.2946831\n",
            " -4.018008   -3.3649573  -0.03033431 -2.9303718  -3.562524   -2.794087\n",
            " -3.7556434 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 92 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.4964836  -3.6171494  -0.9961083  -2.4197161  -3.502206   -0.7805337\n",
            " -2.6905162  -4.50148    -0.77815795 -2.4228091  -4.069715   -1.2755075\n",
            " -4.089688   -3.371911   -0.0116185  -2.961824   -4.08487    -2.822177\n",
            " -3.8253934 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 93 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5111384  -3.5266325  -0.96559405 -2.694012   -3.678068   -0.7401927\n",
            " -2.7980142  -4.5718045  -0.82102126 -2.61671    -4.2248893  -1.3169999\n",
            " -4.1029778  -3.4260316   0.10227949 -3.0713818  -3.9615786  -2.9287584\n",
            " -3.9396727 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 94 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6510067  -3.3878999  -1.0127168  -2.7099645  -3.4841256  -0.65251726\n",
            " -2.875336   -4.224878   -0.694136   -2.5028589  -4.1082234  -1.3921216\n",
            " -4.2164607  -3.5103438  -0.00640922 -3.0059526  -3.6501253  -2.8185701\n",
            " -3.9110425 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 95 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5433657  -3.549014   -1.0587488  -2.5326676  -3.4541137  -0.7336924\n",
            " -2.8338149  -4.355922   -0.87210494 -2.4692137  -4.001451   -1.3473686\n",
            " -4.223919   -3.4811742  -0.0101876  -3.0274878  -3.8030746  -2.7993462\n",
            " -3.8791444 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 96 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.791643   -3.5281713  -1.0307856  -2.6155522  -3.7412524  -0.7497545\n",
            " -2.8894498  -4.6957355  -0.74209076 -2.502425   -4.2135506  -1.189597\n",
            " -4.201708   -3.4938262   0.08958291 -3.1576254  -4.13472    -2.7396076\n",
            " -4.092738  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 97 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.3979902  -3.4382873  -0.9677496  -2.7017124  -3.5641859  -0.8165545\n",
            " -2.8805892  -4.324069   -0.7511386  -2.5105028  -4.060301   -1.3672633\n",
            " -4.0743356  -3.4625094   0.08854261 -2.8921678  -3.8506322  -2.7408576\n",
            " -3.7383723 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 98 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5060258  -3.2351208  -0.9740524  -2.3543081  -3.3437622  -0.6462396\n",
            " -2.6351042  -4.0828953  -0.6205774  -2.292654   -3.9772172  -1.1626856\n",
            " -3.8403115  -3.2551403  -0.01331463 -2.810957   -3.5175376  -2.6107383\n",
            " -3.6695495 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 99 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.2907674  -3.4263258  -0.95814335 -2.3149688  -3.3285925  -0.7669132\n",
            " -2.4654121  -4.272421   -0.7538905  -2.3038757  -3.8047984  -1.202693\n",
            " -3.8258936  -3.2172298  -0.01831024 -2.792006   -3.8377755  -2.720116\n",
            " -3.6141503 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 100 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5734372  -3.4445658  -0.9630033  -2.6804626  -3.6939108  -0.70581806\n",
            " -2.8156202  -4.4556513  -0.7057263  -2.5016727  -4.184701   -1.2301825\n",
            " -3.9890938  -3.4422023   0.11631007 -3.0401127  -4.0497575  -2.8408556\n",
            " -3.880881  ]\n",
            "Epsilon reduced to 0.12800000000000003\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.7523944  -3.6597455  -1.0088267  -2.7900004  -3.7575366  -0.91747546\n",
            " -2.8513746  -4.6551805  -0.98055714 -2.6930373  -4.3465476  -1.4947073\n",
            " -3.9441237  -3.5196235  -0.04547019 -3.387469   -3.841636   -2.87184\n",
            " -4.0062113 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.6418247  -3.9399574  -1.2716811  -3.1708074  -4.04295    -0.83504313\n",
            " -3.2198584  -5.47546    -0.9417194  -2.9841218  -4.7580013  -1.5218371\n",
            " -4.754121   -4.1994123   0.07258812 -3.7273362  -4.472542   -3.5739954\n",
            " -4.8224316 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.0626307e+00 -3.8384924e+00 -1.1769103e+00 -2.7819085e+00\n",
            " -3.8916249e+00 -1.0332652e+00 -2.9732642e+00 -5.0588460e+00\n",
            " -9.7245651e-01 -2.9370778e+00 -4.3990026e+00 -1.4931002e+00\n",
            " -3.6031468e+00 -3.8120632e+00 -7.6048018e-04 -3.3436244e+00\n",
            " -3.8472981e+00 -3.1797929e+00 -4.2448769e+00]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.09725    -4.113907   -1.0515113  -2.9155803  -4.1941752  -1.2119105\n",
            " -2.9763155  -5.554964   -1.0397514  -2.986632   -4.744098   -1.5122794\n",
            " -3.3877916  -4.0621185  -0.08551456 -3.641138   -3.1376324  -3.1845617\n",
            " -4.4559593 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.103028   -3.8494716  -1.1272789  -2.9188695  -3.9553227  -0.92785335\n",
            " -3.1454957  -5.041734   -1.0693852  -2.7353628  -4.5553126  -1.501904\n",
            " -3.987046   -3.7020621  -0.00783713 -3.575419   -2.9732032  -3.1082518\n",
            " -4.0973735 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.2229075e+00 -3.9451401e+00 -1.1683267e+00 -2.9518235e+00\n",
            " -4.0343561e+00 -1.1336181e+00 -3.0428932e+00 -5.3413248e+00\n",
            " -1.0520369e+00 -3.0889819e+00 -4.5542135e+00 -1.3712014e+00\n",
            " -3.6269777e+00 -4.0557475e+00  4.3618693e-03 -3.6394043e+00\n",
            " -2.6468947e+00 -3.1929879e+00 -4.4533658e+00]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 7 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1429563  -3.160027   -0.88450724 -2.4582748  -3.175232   -0.70694906\n",
            " -2.6187346  -3.9913204  -0.8239322  -2.1176639  -3.7068791  -1.1718454\n",
            " -3.4423752  -3.1189537   0.04182404 -2.8297508  -1.75502    -2.424622\n",
            " -3.3284116 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 8 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.57303    -2.3231738  -0.69740933 -1.9334422  -2.515453   -0.56977755\n",
            " -2.0138044  -3.214926   -0.5876167  -1.815544   -2.9280076  -1.0056075\n",
            " -2.9117982  -2.5193367  -0.01294254 -2.1433845  -1.4251816  -2.007051\n",
            " -2.848564  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 9 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.6722801e+00 -2.8446455e+00 -1.0761685e+00 -2.4777524e+00\n",
            " -3.4730582e+00 -9.2115444e-01 -2.6693442e+00 -4.4912877e+00\n",
            " -8.2620478e-01 -2.6572275e+00 -3.8134029e+00 -1.2789854e+00\n",
            " -3.2345164e+00 -3.4296973e+00  4.1557141e-03 -2.9966543e+00\n",
            " -1.5540334e+00 -2.7857974e+00 -3.8684752e+00]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 10 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.232549  -3.090562  -1.2088758 -2.7325122 -3.927581  -1.001146\n",
            " -2.9800184 -5.1406903 -0.9474318 -2.9596376 -4.5107346 -1.3213493\n",
            " -3.4332213 -3.9288645  0.0566836 -3.5392396 -1.0633134 -3.0406947\n",
            " -4.1858253]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 11 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.5276287  -1.8833878  -0.7275757  -1.7564086  -2.480939   -0.524198\n",
            " -1.8438154  -2.9930801  -0.51934236 -1.7237171  -2.8272076  -0.91768366\n",
            " -2.785759   -2.3986464   0.04272706 -2.031204   -1.0155227  -1.9478223\n",
            " -2.7345266 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 12 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.626492   -1.889175   -0.79464406 -1.8802339  -2.5004454  -0.6278218\n",
            " -1.9219788  -3.1060722  -0.48956168 -1.878974   -3.0925581  -0.90976405\n",
            " -2.5581672  -2.648205    0.01853525 -2.1075027  -0.7960348  -2.032084\n",
            " -2.7553177 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 13 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.8307867  -2.351744   -0.9466093  -2.550331   -3.4692492  -0.7850225\n",
            " -2.9195025  -4.5380473  -0.7736866  -2.6255736  -4.010137   -1.2347423\n",
            " -3.876714   -3.5258183   0.05836446 -2.9965699  -0.7053132  -2.9898868\n",
            " -4.0222216 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 14 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.659752   -2.1676233  -1.182801   -2.5451233  -3.378824   -0.81748474\n",
            " -2.4595134  -4.4372783  -0.72019315 -2.3709714  -3.9305897  -1.1825984\n",
            " -3.2297237  -3.4573672   0.08811262 -3.0485425  -0.75392324 -2.6558192\n",
            " -3.7849395 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 15 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.055909   -2.0070975  -0.8158141  -2.2684717  -3.0265641  -0.60681295\n",
            " -2.4967303  -3.4905953  -0.73032826 -2.093742   -3.4839237  -1.1753979\n",
            " -3.339572   -2.8498442  -0.01193512 -2.6090913  -0.60650295 -2.3983302\n",
            " -3.0845773 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 16 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.3342445  -2.0739892  -0.8227948  -2.194228   -3.4161787  -0.8600199\n",
            " -2.3855326  -4.367294   -0.6981403  -2.323751   -4.040427   -1.1620783\n",
            " -2.9869483  -3.3449762  -0.04307556 -2.8375995  -0.044668   -2.5601532\n",
            " -3.5967948 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 17 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.7720473  -1.7368544  -0.90402114 -2.0193863  -2.6713905  -0.66016895\n",
            " -2.0486443  -3.2553008  -0.57177013 -1.9849173  -3.2890413  -0.9230502\n",
            " -2.6796765  -2.7655702   0.09185144 -2.2488244  -1.100672   -2.105244\n",
            " -2.8852255 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 18 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1570492  -2.014438   -0.8878016  -2.1383975  -3.0859654  -0.64818233\n",
            " -2.422881   -3.869882   -0.7118899  -2.1108863  -3.3560495  -1.0776569\n",
            " -3.3173807  -2.9558005  -0.00756505 -2.494913   -1.0084385  -2.4747505\n",
            " -3.347226  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 19 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.8339818  -1.9522744  -0.84512794 -2.11386    -2.9819794  -0.6477431\n",
            " -2.3941438  -3.711016   -0.6524059  -2.1538374  -3.176053   -1.0372599\n",
            " -3.2227354  -2.8555043  -0.01146022 -2.4097211  -1.3693813  -2.238747\n",
            " -3.2078154 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 20 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.3569942  -1.3984971  -0.7271678  -1.8642206  -2.376172   -0.5143743\n",
            " -1.9591614  -2.862335   -0.53886557 -1.9288913  -2.6668348  -0.92905664\n",
            " -2.729847   -2.4002252   0.02348955 -2.0040016  -1.1148769  -1.8585823\n",
            " -2.6064196 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 21 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1958401  -1.1909659  -0.64501446 -1.5355436  -2.204835   -0.4269902\n",
            " -1.6071101  -2.5698984  -0.4481221  -1.8639338  -2.576663   -0.8327798\n",
            " -2.6817515  -2.09995    -0.05733192 -1.8623904  -1.0733814  -1.818356\n",
            " -2.3811867 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 22 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.9830847e+00 -1.7654124e+00 -8.3082694e-01 -1.9796562e+00\n",
            " -3.0112722e+00 -5.9959537e-01 -2.3150527e+00 -3.7151062e+00\n",
            " -6.9200492e-01 -2.3858914e+00 -3.3208158e+00 -9.5423466e-01\n",
            " -3.1161625e+00 -2.9028172e+00  2.7796121e-03 -2.4258776e+00\n",
            " -1.4527640e+00 -2.3134031e+00 -3.1908274e+00]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 23 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6040978  -1.5036818  -0.79223955 -1.8122481  -2.7037458  -0.58180606\n",
            " -2.0514672  -3.2441764  -0.6370184  -2.2388241  -2.997359   -0.9750578\n",
            " -3.0148246  -2.5950305  -0.02058053 -2.2265213  -1.6531639  -2.1589336\n",
            " -2.9418883 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 24 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1322598 -1.1601995 -0.6004143 -1.5508144 -2.1798337 -0.4001093\n",
            " -1.6243899 -2.6412606 -0.4129621 -1.9327072 -2.581155  -0.7282235\n",
            " -2.6845999 -2.057584  -0.0146691 -1.8117957 -1.4542735 -1.7180136\n",
            " -2.357192 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 25 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.871933   -1.4575101  -0.7796453  -1.8852345  -2.949355   -0.59404254\n",
            " -2.199202   -3.4800434  -0.593064   -2.4430866  -3.1921737  -0.92987525\n",
            " -3.1651495  -2.6213303  -0.02395672 -2.383509   -1.787974   -2.2628465\n",
            " -2.9743109 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 26 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.5682933  -1.4603844  -0.79161143 -1.8344265  -2.6989188  -0.58338845\n",
            " -2.0705645  -3.3052857  -0.62818277 -2.3181255  -3.016065   -0.9398688\n",
            " -3.1165445  -2.4697742  -0.02458655 -2.2565386  -2.054943   -2.1330554\n",
            " -2.9126034 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 27 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6548145  -1.4585702  -0.8210008  -1.8430643  -2.533026   -0.50337005\n",
            " -1.8569981  -3.0662665  -0.59456706 -2.397297   -2.9684246  -0.93800485\n",
            " -2.9758027  -2.535326   -0.04588078 -2.2944694  -1.9586661  -2.1231272\n",
            " -2.8707955 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 28 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.907023   -1.4241095  -0.83597296 -2.180768   -3.036809   -0.5752753\n",
            " -2.3929107  -3.672219   -0.6829478  -2.833177   -3.5276828  -1.1502107\n",
            " -3.4676197  -2.8592618  -0.02193734 -2.5478642  -2.2664466  -2.4489243\n",
            " -3.300765  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 29 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.795743   -1.5807921  -0.86404645 -2.0073867  -2.9537618  -0.56577367\n",
            " -2.3823998  -3.580952   -0.6814968  -2.8333578  -3.258744   -1.0105892\n",
            " -3.2319937  -2.7650046   0.04194234 -2.5146859  -2.516782   -2.2288158\n",
            " -3.1388261 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 30 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7658288  -1.4784148  -0.8345605  -1.9877195  -2.5660775  -0.57712454\n",
            " -2.1057658  -3.182817   -0.6225351  -2.5760713  -2.9314375  -1.0515623\n",
            " -2.9852731  -2.770198   -0.05505451 -2.3715587  -2.288029   -2.1133928\n",
            " -3.001839  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 31 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.961983   -1.4051538  -0.8565598  -2.1147866  -3.047487   -0.6433731\n",
            " -2.3414245  -3.626018   -0.67797256 -2.742393   -3.1956522  -1.1206259\n",
            " -3.152134   -2.8175542  -0.02819118 -2.5481148  -2.2840648  -2.4298117\n",
            " -3.1768591 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 32 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.8451805  -1.4696407  -0.9002664  -2.025766   -2.9786015  -0.6028915\n",
            " -2.3766866  -3.7158473  -0.7399959  -2.7737238  -3.3116686  -1.0091208\n",
            " -3.3191419  -2.7822342  -0.04476378 -2.5196266  -2.5053437  -2.318444\n",
            " -3.2680345 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 33 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6035357  -1.3344908  -0.7812536  -1.8119576  -2.4250805  -0.5616507\n",
            " -1.8703866  -3.0424228  -0.5977891  -2.4656606  -2.7091668  -0.95039296\n",
            " -2.8173282  -2.506877   -0.05290739 -2.187725   -2.0717256  -2.0555763\n",
            " -2.8107293 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 34 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.1102374  -1.4985232  -0.84876025 -2.2247796  -3.1480873  -0.6427879\n",
            " -2.4861202  -3.736155   -0.68571264 -2.8837357  -3.3024204  -1.1536162\n",
            " -3.210737   -2.9847734   0.01384431 -2.605846   -2.4388773  -2.4762702\n",
            " -3.2741618 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 35 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6484454  -1.2752708  -0.85192055 -1.9985685  -2.8793867  -0.58111185\n",
            " -2.2749343  -3.4608827  -0.6980407  -2.8090358  -3.1697085  -1.0641309\n",
            " -3.2211695  -2.6767101  -0.02841074 -2.4193707  -2.4641144  -2.2326722\n",
            " -3.1343946 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 36 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6031585  -1.2533469  -0.77731615 -1.8591294  -2.4735456  -0.5109501\n",
            " -1.9335629  -3.1765814  -0.6183881  -2.6303794  -2.8983698  -0.9351058\n",
            " -3.0410173  -2.500425   -0.04790298 -2.2419817  -2.2431216  -2.0386412\n",
            " -2.904311  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 37 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.1350598  -1.6000957  -0.8499297  -2.2631545  -3.143681   -0.662521\n",
            " -2.4392138  -3.7362711  -0.685138   -2.9600165  -3.4776506  -1.0953376\n",
            " -3.1617818  -2.951158    0.02516787 -2.6431992  -2.577724   -2.3811114\n",
            " -3.2735999 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 38 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.969642   -1.4666473  -0.88035136 -2.0891187  -3.0729816  -0.5616534\n",
            " -2.4914625  -3.6445854  -0.7409947  -2.9152951  -3.3406086  -1.0722696\n",
            " -3.2227883  -2.9175487   0.04706292 -2.5815384  -2.6613202  -2.329224\n",
            " -3.2930481 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 39 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.470499   -1.1421571  -0.82774776 -1.8227999  -2.375702   -0.5722172\n",
            " -1.9068916  -3.1055949  -0.6639767  -2.5376313  -2.7811499  -0.9455481\n",
            " -2.8184993  -2.4675593  -0.07716857 -2.2306623  -1.8635482  -1.9692118\n",
            " -2.844996  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 40 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.739673   -1.232741   -0.7402568  -1.8972     -2.860398   -0.5977393\n",
            " -2.291768   -3.4069173  -0.63486063 -2.7238874  -2.984009   -1.0575423\n",
            " -3.145581   -2.5632353   0.00442878 -2.318793   -2.288529   -2.2359695\n",
            " -3.0120587 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 41 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.961347  -1.5440319 -0.9437874 -2.1088412 -3.0266635 -0.6300909\n",
            " -2.4936507 -3.6150951 -0.791183  -2.8533022 -3.2440934 -1.0222417\n",
            " -3.081241  -2.9032612  0.0316918 -2.5797079 -2.4615324 -2.3102858\n",
            " -3.2153306]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 42 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6187992  -1.2161176  -0.8567571  -1.8941404  -2.4349563  -0.6235082\n",
            " -1.9710264  -3.1048036  -0.6129415  -2.6574776  -2.9357185  -1.0022748\n",
            " -2.7476523  -2.6078386  -0.09903537 -2.3360362  -2.1285114  -2.0505\n",
            " -2.9008403 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 43 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6196632  -1.1290922  -0.7284202  -1.8118607  -2.6535337  -0.55890167\n",
            " -2.0850346  -3.1726477  -0.56913215 -2.6496756  -2.982686   -0.98955226\n",
            " -3.046125   -2.4268694  -0.02101354 -2.1947477  -2.0134685  -2.2035923\n",
            " -2.8109596 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 44 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7953641  -1.515366   -0.863444   -2.0639265  -2.9055777  -0.55513287\n",
            " -2.364557   -3.4666753  -0.6700061  -2.782631   -3.1022537  -0.9973341\n",
            " -3.1227858  -2.7797964   0.01310142 -2.45109    -2.3872583  -2.2308986\n",
            " -3.1052735 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 45 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7339704  -1.2759726  -0.8442414  -1.9494684  -2.619429   -0.5332871\n",
            " -2.110044   -3.1645076  -0.59917974 -2.7525609  -2.9221241  -1.0020245\n",
            " -2.834565   -2.7713242  -0.01893943 -2.4254322  -2.1010823  -2.113226\n",
            " -2.981179  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 46 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7596490e+00 -1.2391942e+00 -8.4578437e-01 -1.9269537e+00\n",
            " -2.9577432e+00 -5.7519972e-01 -2.2627556e+00 -3.4518447e+00\n",
            " -6.4200580e-01 -2.8884158e+00 -3.2968514e+00 -9.7530186e-01\n",
            " -2.9642830e+00 -2.6382742e+00  4.7768990e-04 -2.4315741e+00\n",
            " -1.9785422e+00 -2.2322249e+00 -3.0585287e+00]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 47 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.9321585  -1.4367824  -0.8772881  -2.029225   -3.0115101  -0.5422419\n",
            " -2.3902159  -3.6081023  -0.7171566  -2.9412687  -3.2400703  -1.0539644\n",
            " -3.2667356  -2.7256596   0.05990298 -2.5326383  -2.2768571  -2.4209905\n",
            " -3.1934745 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 48 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6002324  -1.312166   -0.81090486 -1.9559999  -2.4989285  -0.5634776\n",
            " -2.0609105  -3.222093   -0.64187133 -2.6054916  -2.8422868  -0.9594424\n",
            " -2.9086952  -2.5531836   0.00718409 -2.2903912  -1.9281871  -2.0271957\n",
            " -2.872265  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 49 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7455664  -1.1915923  -0.8265755  -1.9466735  -2.810253   -0.62416035\n",
            " -2.2324252  -3.3915615  -0.66649574 -2.7589765  -3.1664429  -0.9607233\n",
            " -2.9876087  -2.5987284   0.00875784 -2.3871915  -1.8736117  -2.1844566\n",
            " -3.006952  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 50 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.709942   -1.225397   -0.8882815  -1.8932415  -2.7802217  -0.60354924\n",
            " -2.2115283  -3.3637974  -0.7196698  -2.7810483  -3.0464268  -1.0283027\n",
            " -3.1349983  -2.531087   -0.04239745 -2.3674939  -1.9687417  -2.292508\n",
            " -3.0325491 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 51 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4595845  -1.2311693  -0.7560294  -1.7359172  -2.346078   -0.540112\n",
            " -1.8990147  -3.0061698  -0.6427452  -2.410005   -2.4962897  -0.9422086\n",
            " -2.5983212  -2.4808905  -0.05193295 -2.170377   -1.6482892  -1.8851457\n",
            " -2.6952791 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 52 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7320063  -1.266733   -0.7891943  -1.9040091  -2.7975152  -0.585218\n",
            " -2.2587264  -3.3462586  -0.7158546  -2.6133988  -2.7790494  -1.0050273\n",
            " -2.920662   -2.5758188   0.06231827 -2.2835515  -1.8177012  -2.135141\n",
            " -2.9492993 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 53 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7029047  -1.2905505  -0.88874626 -1.9460039  -2.8461435  -0.5523292\n",
            " -2.2769985  -3.5243633  -0.7156297  -2.7893238  -2.8129478  -0.94528395\n",
            " -3.0377932  -2.6024554  -0.02953405 -2.434525   -1.7654663  -2.183228\n",
            " -3.0774624 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 54 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3737411  -1.0765201  -0.7061068  -1.5717068  -2.205666   -0.48006862\n",
            " -1.6568881  -2.7027495  -0.5376298  -2.4008727  -2.305636   -0.84030247\n",
            " -2.5850453  -2.2723844  -0.02076244 -1.9961401  -1.6428065  -1.8701786\n",
            " -2.5766735 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 55 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.678973   -1.266416   -0.77439576 -1.906647   -2.7483296  -0.5695039\n",
            " -2.1934566  -3.0828447  -0.5963251  -2.7165954  -2.5966551  -1.0398715\n",
            " -2.887455   -2.5263162   0.01463429 -2.302225   -1.8981994  -2.113465\n",
            " -2.8207507 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 56 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.5583878  -1.104967   -0.769202   -1.8699963  -2.7906756  -0.49023664\n",
            " -2.1503909  -3.232219   -0.63667226 -2.8135576  -2.5735567  -1.0267692\n",
            " -3.0734444  -2.491778    0.02271371 -2.2996202  -1.8215035  -2.1777694\n",
            " -2.9286015 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 57 reward=-2 new_state=[1 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.345502   -1.167312   -0.77426803 -1.6915058  -2.288076   -0.50406396\n",
            " -1.760127   -2.8581707  -0.53415823 -2.5307975  -2.2772262  -0.8384443\n",
            " -2.640913   -2.3001978  -0.09432483 -2.1193817  -1.7086351  -1.8451159\n",
            " -2.6305494 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 58 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.582043   -1.2960286  -0.7155259  -1.913151   -2.7563725  -0.53678024\n",
            " -2.0485406  -3.193043   -0.5756826  -2.672043   -2.482559   -0.8592135\n",
            " -2.6509843  -2.505209    0.07398377 -2.22025    -1.9429541  -1.9838817\n",
            " -2.7881653 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 59 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.5763648  -1.2729872  -0.8071612  -1.8313717  -2.6741223  -0.5137373\n",
            " -1.9795545  -3.2010708  -0.63926303 -2.695421   -2.45882    -1.0165637\n",
            " -2.880896   -2.5049272   0.04385975 -2.2541754  -2.115712   -2.0892868\n",
            " -2.927502  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 60 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.2825837  -1.0991223  -0.79131013 -1.6570084  -2.2314956  -0.5060189\n",
            " -1.743896   -2.867747   -0.5810863  -2.4581144  -2.0684636  -0.99637115\n",
            " -2.5621626  -2.2801323  -0.07124788 -2.0528657  -1.6333025  -1.82515\n",
            " -2.5940948 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 61 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3984993  -1.0645708  -0.67788595 -1.682628   -2.5087967  -0.51142555\n",
            " -1.8330748  -2.9000556  -0.54664403 -2.5452366  -2.2076588  -1.0697708\n",
            " -2.658896   -2.2950869   0.06161417 -2.0701802  -1.8816985  -1.9959012\n",
            " -2.6698027 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 62 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.8233876  -1.3647542  -0.8684953  -1.9519932  -2.8787189  -0.56521535\n",
            " -2.1544619  -3.552779   -0.7190999  -2.8605998  -2.4231575  -1.2890004\n",
            " -2.85167    -2.7202961   0.018601   -2.3538218  -2.0022633  -2.2786174\n",
            " -3.0720432 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 63 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4809713  -1.0831853  -0.8435406  -1.8098085  -2.3836868  -0.50218016\n",
            " -1.8782446  -3.0155146  -0.6315537  -2.7542946  -2.2365837  -1.3725835\n",
            " -2.8338058  -2.4796364  -0.08415672 -2.2323394  -1.6698635  -2.015685\n",
            " -2.8433337 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 64 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3392134  -0.965205   -0.6810961  -1.6253967  -2.4578454  -0.50354856\n",
            " -1.8721915  -2.9344795  -0.56199956 -2.5146115  -2.1084142  -1.2079108\n",
            " -2.6611066  -2.182572    0.02224247 -1.9457909  -1.4611452  -2.0032234\n",
            " -2.5859277 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 65 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7147884  -1.2434716  -0.7529954  -1.9345218  -2.7637682  -0.5162984\n",
            " -2.190552   -3.3327672  -0.64732474 -2.7326539  -2.2971513  -1.3925714\n",
            " -2.863558   -2.6614087   0.06263193 -2.295447   -1.828537   -2.159616\n",
            " -3.0279899 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 66 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.5299456  -1.155445   -0.7476539  -1.7182983  -2.4667175  -0.48374122\n",
            " -1.8726581  -3.1638756  -0.6113757  -2.6639893  -2.0839028  -1.4116408\n",
            " -2.7605128  -2.365536   -0.07762197 -2.062941   -1.480555   -1.9640441\n",
            " -2.72641   ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 67 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4270265  -0.9700253  -0.7580256  -1.7655339  -2.6473455  -0.52044266\n",
            " -1.9526803  -3.0417612  -0.6049686  -2.663648   -2.1274118  -1.4231765\n",
            " -2.6763477  -2.4418747  -0.00612071 -2.1544976  -1.2509886  -2.0412116\n",
            " -2.7983458 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 68 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.5661426  -1.2597045  -0.7428918  -1.7667812  -2.732968   -0.49782217\n",
            " -2.1503043  -3.2828846  -0.63435745 -2.8788803  -2.291912   -1.4795545\n",
            " -2.8228889  -2.4589622   0.03378347 -2.204263   -1.4331905  -2.1012862\n",
            " -2.8719923 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 69 reward=-1 new_state=[1 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6390648  -1.2456734  -0.82165194 -1.8689755  -2.5010939  -0.51566863\n",
            " -1.9256557  -3.1861458  -0.66226053 -2.721017   -2.1762855  -1.6038195\n",
            " -2.717412   -2.5506475  -0.04982767 -2.2066877  -1.2498345  -2.0516586\n",
            " -2.864977  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 70 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4021394  -0.6622882  -0.7635657  -1.7306828  -2.4123588  -0.5395471\n",
            " -1.8307405  -2.9619148  -0.63498676 -2.4804535  -1.9392666  -1.3712374\n",
            " -2.711521   -2.2013066   0.02733588 -2.1455598  -0.9095668  -1.9545795\n",
            " -2.7160044 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 71 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.755569   -0.90239257 -0.7939381  -1.9451897  -2.7813804  -0.69441557\n",
            " -2.1785212  -3.4825602  -0.5237986  -3.0120056  -2.3196743  -1.7130133\n",
            " -2.9994779  -2.6149354  -0.09596339 -2.5418394  -0.71985817 -2.219512\n",
            " -3.0423427 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 72 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4320042  -1.0477867  -0.7521368  -1.7881432  -2.5682333  -0.60606354\n",
            " -1.969154   -3.0552363  -0.27898842 -2.6345067  -2.2025225  -1.5198215\n",
            " -2.4193254  -2.4979036  -0.03878466 -2.2069027  -0.73244387 -2.0010645\n",
            " -2.8018286 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 73 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.099587   -0.9655556  -0.53971434 -1.4672836  -2.202324   -0.57628155\n",
            " -1.5162619  -2.647785    0.00985099 -2.2865694  -1.9010335  -1.223343\n",
            " -2.00079    -2.0217083  -0.026277   -1.8537847  -0.37488535 -1.662263\n",
            " -2.3119452 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 74 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.0616486  -0.9008674  -0.70915556 -1.4969151  -1.9355041  -0.3302746\n",
            " -1.4077226  -2.4379883   0.01249766 -2.016263   -1.8388426  -1.2863978\n",
            " -2.1974287  -1.9519236  -0.01716755 -1.8153732  -0.76215214 -1.6033103\n",
            " -2.2176409 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 75 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3031917  -0.626729   -0.70130414 -1.4907813  -2.172984   -0.43635648\n",
            " -1.733732   -2.6297007  -0.02392424 -2.3333125  -1.8872752  -1.1977249\n",
            " -2.3096838  -2.1288      0.0292884  -2.0327585  -0.8120093  -1.8236018\n",
            " -2.5370474 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 76 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4000006  -0.6302199  -0.6778819  -1.4834689  -2.3450873  -0.5426026\n",
            " -1.8159282  -2.8418972  -0.0259216  -2.4714599  -1.9007171  -1.3758172\n",
            " -2.2118657  -2.1239443  -0.02154828 -2.025671   -0.5065327  -1.8694062\n",
            " -2.538156  ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 77 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3299553  -0.9515445  -0.6833339  -1.7548747  -2.4088202  -0.5087735\n",
            " -1.9070767  -2.8025198   0.1431743  -2.4981508  -2.0510523  -1.5438471\n",
            " -2.0001187  -2.466975   -0.02283154 -2.0964353  -0.48613933 -1.9331278\n",
            " -2.566089  ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 78 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.2988298  -0.7171334  -0.68784547 -1.5015048  -2.1760767  -0.44612736\n",
            " -1.6237136  -2.5301864   0.1956641  -2.1889546  -1.7278602  -1.1997894\n",
            " -1.9396538  -2.0788124   0.04983934 -1.9086133  -0.694758   -1.7610708\n",
            " -2.4145029 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 79 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.0232584  -1.0235537  -0.7003652  -1.4706229  -1.962873   -0.3100383\n",
            " -1.4581509  -2.4485536   0.22960663 -2.0332255  -1.834037   -1.3441317\n",
            " -1.8833746  -1.9307052  -0.0057035  -1.8018407  -0.66967183 -1.5417181\n",
            " -2.1875334 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 80 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3593204  -0.7299182  -0.6935306  -1.469605   -2.2414176  -0.53536254\n",
            " -1.7089444  -2.7451708   0.15405773 -2.3189065  -1.8095523  -1.3952966\n",
            " -1.9781411  -2.0819209  -0.05821713 -1.9518269  -0.4016196  -1.8828057\n",
            " -2.5126815 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 81 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.523272   -0.80153185 -0.7765366  -1.7555625  -2.4804475  -0.5715034\n",
            " -2.034813   -2.9903119   0.26056328 -2.5411892  -1.8901888  -1.5351468\n",
            " -2.0158834  -2.413393   -0.0258649  -2.2404737  -0.43836504 -1.934322\n",
            " -2.7706807 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 82 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1752684  -0.8460779  -0.6321375  -1.5544803  -2.2258573  -0.5116219\n",
            " -1.6899835  -2.6387067   0.2191434  -2.3596761  -1.806138   -1.3873894\n",
            " -1.5634147  -2.1088643   0.05016128 -1.9339768  -0.2860641  -1.7027605\n",
            " -2.329769  ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 83 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.036466   -0.5524704  -0.62900954 -1.3384984  -1.9150945  -0.4704991\n",
            " -1.4171213  -2.273118    0.13765968 -1.9194857  -1.6046805  -1.0733011\n",
            " -1.7896699  -1.7567675   0.04345915 -1.7604408  -0.44438213 -1.5654813\n",
            " -2.2250202 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 84 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1891236  -0.9325271  -0.7298891  -1.5369616  -2.131634   -0.36054912\n",
            " -1.5790244  -2.628483   -0.02152019 -2.1160195  -1.8163443  -1.4409461\n",
            " -1.8531787  -1.966445   -0.02809068 -1.8817991  -0.3336704  -1.7529106\n",
            " -2.3358178 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 85 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4499774  -0.7558631  -0.7574792  -1.6589153  -2.2729268  -0.5093964\n",
            " -1.8974636  -2.7604985   0.0595897  -2.3560627  -1.8504174  -1.5001224\n",
            " -1.8364999  -2.343898   -0.03985846 -2.1164036  -0.3011469  -1.878824\n",
            " -2.677713  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 86 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.0186808  -0.62535304 -0.5869119  -1.3362088  -1.9886773  -0.404769\n",
            " -1.4729118  -2.2484665   0.11371876 -2.0073519  -1.6123196  -1.1379156\n",
            " -1.6122046  -1.7824699   0.07010324 -1.7219768  -0.40106133 -1.5910149\n",
            " -2.172392  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 87 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.844358   -0.4983036  -0.52620745 -1.193632   -1.7999474  -0.37470126\n",
            " -1.3174404  -2.0862243  -0.04672037 -1.8110507  -1.5371777  -0.9905149\n",
            " -1.5352334  -1.6176285   0.01712807 -1.5964599  -0.3314432  -1.3473657\n",
            " -1.9974836 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 88 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.5062627  -0.53318226 -0.4628908  -1.0900432  -1.6040535  -0.3690294\n",
            " -1.0891747  -1.8516186  -0.127525   -1.5912892  -1.2897675  -0.8798168\n",
            " -1.2339422  -1.3591483   0.04919296 -1.3136536  -0.1837932  -1.1303656\n",
            " -1.7359523 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 89 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.7545428  -0.8218729  -0.5466568  -1.3668317  -2.0153608  -0.4869283\n",
            " -1.4720436  -2.4726565  -0.24904509 -2.063674   -1.8236994  -1.2871475\n",
            " -1.3942883  -1.8462645   0.01323337 -1.8337898   0.3232456  -1.2744654\n",
            " -2.10353   ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 90 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.5979922  -0.63634133 -0.60072374 -1.2897042  -1.8836617  -0.3998398\n",
            " -1.3632214  -2.3219821  -0.18502927 -1.8193858  -1.541939   -1.1810532\n",
            " -1.3030969  -1.6836816   0.0097451  -1.661759   -0.12664495 -1.1310058\n",
            " -2.0646057 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 91 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.605241   -0.51027775 -0.57168996 -1.3130646  -1.9396608  -0.4975473\n",
            " -1.5108312  -2.449177   -0.397865   -1.9960216  -1.6049925  -1.1866493\n",
            " -1.2731073  -1.8535286  -0.02168529 -1.8374115   0.14132185 -1.0404273\n",
            " -2.1970062 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 92 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.2986959  -0.5383585  -0.4750461  -1.1139917  -1.634562   -0.4025095\n",
            " -1.1787968  -2.0142415  -0.22161439 -1.6881808  -1.2856553  -0.98875505\n",
            " -1.0638665  -1.4720569   0.01054749 -1.4178652   0.14475557 -0.87512815\n",
            " -1.8620508 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 93 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.3486071  -0.45984823 -0.5286126  -1.1807375  -1.781699   -0.46710366\n",
            " -1.3369839  -2.2170527  -0.38948607 -1.7995867  -1.493399   -1.0909107\n",
            " -0.942027   -1.6833761  -0.01358897 -1.6495723   0.25513858 -0.8231238\n",
            " -1.997858  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 94 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.1293052e+00 -4.8645672e-01 -4.7553200e-01 -1.0259482e+00\n",
            " -1.5621307e+00 -4.2138517e-01 -1.1287662e+00 -1.9455934e+00\n",
            " -1.8449339e-01 -1.5906249e+00 -1.2076653e+00 -9.6107924e-01\n",
            " -7.6654226e-01 -1.3828506e+00  1.1391244e-03 -1.3627532e+00\n",
            "  2.0924009e-01 -7.1436572e-01 -1.7645183e+00]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 95 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.0873473  -0.6548596  -0.40521234 -1.079019   -1.7580645  -0.5177887\n",
            " -1.2133307  -2.1086931  -0.426552   -1.8183638  -1.5371876  -1.0945013\n",
            " -0.42264947 -1.6149414  -0.03536318 -1.5139647   0.7118438  -0.7772457\n",
            " -1.8099612 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 96 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.0354016  -0.4502386  -0.4816479  -1.019879   -1.595211   -0.35744426\n",
            " -1.130163   -1.9133241  -0.27485088 -1.575224   -1.262337   -0.955359\n",
            " -0.39872    -1.4096937   0.0354064  -1.3594645   0.27125406 -0.65360045\n",
            " -1.762744  ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 97 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.0654064e+00 -4.0289181e-01 -4.8642153e-01 -1.0640254e+00\n",
            " -1.6386473e+00 -4.0317687e-01 -1.2519844e+00 -2.0014133e+00\n",
            " -4.7865665e-01 -1.6367209e+00 -1.3557764e+00 -9.9320787e-01\n",
            " -2.6777264e-01 -1.5247335e+00 -5.0377112e-04 -1.4314439e+00\n",
            "  3.7734866e-01 -5.4242587e-01 -1.8397800e+00]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 98 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8562861  -0.3618642  -0.4455217  -0.88976306 -1.4133134  -0.37897566\n",
            " -1.0101284  -1.7371756  -0.2664976  -1.3727034  -1.1342639  -0.8745169\n",
            " -0.03490731 -1.2414396   0.01456134 -1.0908117   0.33645266 -0.46066588\n",
            " -1.5983793 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 99 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.78170174 -0.53944045 -0.38040134 -0.9030666  -1.49003    -0.44714797\n",
            " -1.0115944  -1.7590714  -0.44452995 -1.4712029  -1.2683656  -0.9136398\n",
            "  0.22806928 -1.3417627  -0.0090156  -1.0179404   0.69998676 -0.54833436\n",
            " -1.5170746 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 100 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7970813  -0.36535716 -0.45618927 -0.8573377  -1.3536239  -0.36154243\n",
            " -0.9899673  -1.6480392  -0.24340509 -1.278664   -1.1010039  -0.8538376\n",
            "  0.22526424 -1.1714275   0.0157657  -0.96370184  0.3061326  -0.39981472\n",
            " -1.5269305 ]\n",
            "Epsilon reduced to 0.10240000000000003\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1272674  -0.60746217 -0.5003298  -1.2423694  -1.6189909  -0.53012687\n",
            " -1.4547536  -2.0162141  -0.2721711  -1.6367224  -1.3690758  -1.2573481\n",
            "  0.04866456 -1.662181    0.00578632 -1.1308516   0.2326342  -0.73467714\n",
            " -1.9201025 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.3830498  -0.4403381  -0.69052774 -1.4201666  -1.9902675  -0.52363867\n",
            " -1.5338897  -2.5833127  -0.53428906 -2.025376   -1.7249148  -1.3731241\n",
            "  0.46519527 -2.0405903   0.01029104 -1.4556478   0.16412632 -0.6118266\n",
            " -2.250758  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1556549  -0.53128225 -0.68954986 -1.2632784  -1.6463134  -0.45467514\n",
            " -1.2134364  -2.1701608  -0.4285518  -1.59891    -1.3406633  -1.231971\n",
            "  0.25222963 -1.6864109  -0.07309671 -1.2625191   0.04781808 -0.58198017\n",
            " -1.9218273 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1286049  -0.36840418 -0.6454342  -1.3040313  -1.7856042  -0.5612925\n",
            " -1.3031268  -2.269239   -0.44802773 -1.7811747  -1.4611872  -1.2211882\n",
            "  0.7224125  -1.7860628  -0.2821104  -1.1608919   0.16569988 -0.5148207\n",
            " -2.0287693 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9996306  -0.4533437  -0.5540586  -1.2175603  -1.7605407  -0.563564\n",
            " -1.3448027  -2.2098503  -0.47803578 -1.7134044  -1.3932874  -1.2078314\n",
            "  0.82571036 -1.6541458  -0.39582455 -1.2032118   0.45592758 -0.45939812\n",
            " -1.8242314 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0181683  -0.49956396 -0.6078898  -1.2821825  -1.7761462  -0.52306145\n",
            " -1.3511313  -2.211522   -0.5428414  -1.767557   -1.4274942  -1.2342391\n",
            "  0.7934605  -1.6316369  -0.5139264  -1.191121    0.44580048 -0.40966046\n",
            " -1.8470286 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9845194  -0.6118285  -0.51286554 -1.2560302  -1.8047512  -0.49936602\n",
            " -1.6113104  -2.2151115  -0.39281633 -1.7562242  -1.3953718  -1.2417402\n",
            "  0.6228248  -1.7254901  -0.6813684  -1.1384768   0.4222914  -0.55146545\n",
            " -1.9105884 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0743827  -0.52149355 -0.6001245  -1.3396838  -1.8125083  -0.4875144\n",
            " -1.4557428  -2.312104   -0.6400618  -1.8722528  -1.4975226  -1.2608331\n",
            "  0.85715634 -1.7395151  -0.6959002  -1.2136382   0.3977673  -0.4138921\n",
            " -1.9577847 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.93003076 -0.56684417 -0.6033207  -1.1509308  -1.551404   -0.44733673\n",
            " -1.1953957  -1.9669167  -0.48491883 -1.4172902  -1.2049934  -1.1607038\n",
            "  0.6100169  -1.4974774  -0.6144209  -1.1465569   0.10957498 -0.33130363\n",
            " -1.709445  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 10 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0591043  -0.5617737  -0.53334445 -1.2799469  -1.7924914  -0.5983979\n",
            " -1.3238206  -2.3262956  -0.6224715  -1.8382996  -1.5729179  -1.329011\n",
            "  1.0015956  -1.803099   -0.9508188  -0.86424404  0.15905231 -0.57943916\n",
            " -1.8532192 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 11 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8537722  -0.29794368 -0.50300586 -1.0165659  -1.4379961  -0.39279845\n",
            " -1.1118393  -1.7772853  -0.36052153 -1.3305756  -1.053162   -0.96328914\n",
            "  0.9407753  -1.3313801  -0.80099726 -0.7695066  -0.08416575 -0.20683199\n",
            " -1.6229068 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 12 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0464523  -0.44457942 -0.54113907 -1.2647841  -1.6459458  -0.45206827\n",
            " -1.4395695  -2.1945963  -0.44643736 -1.720785   -1.373547   -1.2491493\n",
            "  0.87108773 -1.7649509  -1.0344381  -0.8483288  -0.24011615 -0.5734104\n",
            " -1.9342692 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.86900014 -0.7382908  -0.44868895 -1.1372137  -1.6430736  -0.4400103\n",
            " -1.501393   -1.9811417  -0.39044484 -1.6321611  -1.3975449  -1.1877232\n",
            "  0.64799714 -1.6259782  -1.1357628  -0.7723484   0.2920783  -0.60719043\n",
            " -1.7254652 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.96644044 -0.31830212 -0.56194466 -1.1422051  -1.5222015  -0.47648406\n",
            " -1.1916159  -2.0705898  -0.4599231  -1.5615543  -1.2910093  -1.1493019\n",
            "  1.1404967  -1.550596   -1.0599065  -0.8783104  -0.37074572 -0.31091538\n",
            " -1.7397412 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0365062  -0.61143255 -0.5960282  -1.2153789  -1.5442996  -0.3961684\n",
            " -1.3982658  -2.0700846  -0.3451522  -1.5132087  -1.2849227  -1.2010343\n",
            "  0.47325495 -1.6881655  -1.188762   -1.0689589  -0.42022133 -0.53863525\n",
            " -1.9670446 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0089556  -0.5360058  -0.5180947  -1.25479    -1.672967   -0.5370346\n",
            " -1.2753999  -2.1989145  -0.6457023  -1.7616861  -1.463229   -1.2571391\n",
            "  1.0586588  -1.7101293  -1.2752404  -0.76377684 -0.26987284 -0.49791783\n",
            " -1.7965475 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.68949527 -0.32023796 -0.4743028  -0.89715934 -1.3471978  -0.35260266\n",
            " -1.115817   -1.6380167  -0.47439167 -1.2682801  -0.976321   -0.9179227\n",
            "  1.039394   -1.2150662  -0.9075018  -0.7184971  -0.2400756  -0.14305396\n",
            " -1.5135845 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9813013  -0.44417202 -0.51104546 -1.2345583  -1.5702686  -0.44143263\n",
            " -1.4109684  -2.1467812  -0.4471003  -1.6577779  -1.2735374  -1.2140487\n",
            "  0.97165126 -1.7038487  -1.3119328  -0.77359974 -0.51552075 -0.5489805\n",
            " -1.8513285 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.783636   -0.4179741  -0.5118431  -1.0647981  -1.521322   -0.41089532\n",
            " -1.3109643  -1.9271568  -0.48679352 -1.4214255  -1.1534759  -1.0738064\n",
            "  1.2476361  -1.4340057  -1.0787487  -0.8179178  -0.3130125  -0.20637208\n",
            " -1.5983905 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9743467  -0.56856865 -0.6433034  -1.2934301  -1.5980577  -0.38507703\n",
            " -1.2506799  -2.144108   -0.7608881  -1.5763726  -1.3493651  -1.2274394\n",
            "  0.8653317  -1.5993518  -0.98048615 -1.1485652  -0.40282622 -0.33762223\n",
            " -1.7472047 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.85522664 -0.49886298 -0.4979927  -1.0876808  -1.4598944  -0.3953186\n",
            " -1.3241128  -1.7920206  -0.31660536 -1.3081647  -1.096221   -1.096633\n",
            "  0.9833268  -1.4931853  -1.327881   -0.7654329  -0.5235811  -0.35122725\n",
            " -1.678155  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9101641  -0.680148   -0.50910765 -1.2966672  -1.8294294  -0.54415685\n",
            " -1.4140586  -2.2849824  -0.7408607  -1.8947049  -1.532601   -1.3229984\n",
            "  1.1611483  -1.7031628  -1.3012806  -0.8340007  -0.09897161 -0.51843464\n",
            " -1.741936  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9030574  -0.60232013 -0.53613234 -1.1112992  -1.4512594  -0.30799022\n",
            " -1.365935   -1.7462896  -0.40173492 -1.2442142  -1.035796   -1.030322\n",
            "  0.5148721  -1.4542806  -1.1183755  -0.9998964  -0.3881502  -0.32182842\n",
            " -1.6689242 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8572222  -0.784597   -0.5231053  -1.2497587  -1.4829593  -0.35135472\n",
            " -1.2770154  -1.9601693  -0.59516364 -1.5251629  -1.2653035  -1.1978749\n",
            "  0.3958114  -1.5763998  -1.1254432  -0.8315226  -0.10069978 -0.60608524\n",
            " -1.6243738 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.77115756 -0.3847579  -0.5323491  -1.002513   -1.3017209  -0.3146647\n",
            " -1.0502448  -1.6382446  -0.39793813 -1.1330818  -0.9115016  -0.90536654\n",
            "  0.80053246 -1.2657737  -1.0028362  -0.76995695 -0.4742218  -0.12840289\n",
            " -1.5470319 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8560558  -0.56815803 -0.56767946 -1.1700125  -1.4123077  -0.34535232\n",
            " -1.0718244  -1.9270167  -0.6189722  -1.42477    -1.1752709  -1.1492234\n",
            "  0.77198005 -1.3939778  -0.93784887 -0.97512794 -0.42037502 -0.3783627\n",
            " -1.5014653 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7979748  -0.4566495  -0.49783096 -1.0307558  -1.4713414  -0.37425393\n",
            " -1.25885    -1.8318864  -0.4293673  -1.2769245  -1.0305911  -1.0172007\n",
            "  1.1054317  -1.3711799  -1.1500031  -0.81473404 -0.47945753 -0.15604025\n",
            " -1.5929234 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.88151675 -0.5736459  -0.4398796  -1.1591815  -1.4844251  -0.43216696\n",
            " -1.2909663  -1.9677755  -0.4180222  -1.5674295  -1.3059152  -1.1828358\n",
            "  0.87729776 -1.5924797  -1.4994068  -0.48279196 -0.41270137 -0.60259265\n",
            " -1.6827539 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 29 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8881272  -0.45727703 -0.619482   -1.1092969  -1.4478652  -0.31954655\n",
            " -1.178458   -1.8444124  -0.36840266 -1.2718982  -1.0759512  -1.0178839\n",
            "  0.77306795 -1.4242529  -1.1323239  -0.9113947  -0.6181308  -0.2291589\n",
            " -1.6948638 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 30 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8910076  -0.5922139  -0.4579615  -1.1933308  -1.5636647  -0.415187\n",
            " -1.3436707  -2.0232008  -0.40523866 -1.6580862  -1.3826574  -1.2165993\n",
            "  0.8609173  -1.6802474  -1.52816    -0.5133642  -0.46333516 -0.6529125\n",
            " -1.7634069 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 31 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.81770086 -0.549589   -0.55568755 -1.1125321  -1.4398813  -0.29717445\n",
            " -1.3214123  -1.7402037  -0.46316582 -1.2844516  -1.0261736  -1.0154697\n",
            "  0.5959417  -1.4936217  -1.1258166  -0.9535464  -0.3826291  -0.27085558\n",
            " -1.6559374 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 32 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9909463  -0.62614685 -0.6060114  -1.2463907  -1.518219   -0.41285402\n",
            " -1.129094   -2.0416892  -0.56544966 -1.4946839  -1.3987176  -1.2285182\n",
            "  0.65085953 -1.5798632  -1.3411528  -0.8297355  -0.6322897  -0.4677721\n",
            " -1.7205479 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 33 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9211451  -0.5456044  -0.5707571  -1.2096349  -1.6035569  -0.47240835\n",
            " -1.3573713  -2.0609856  -0.4809392  -1.5035124  -1.3616091  -1.2194587\n",
            "  1.3646182  -1.6452286  -1.4743521  -0.8310717  -0.62412655 -0.21718286\n",
            " -1.7734171 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 34 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.875327   -0.59496385 -0.5105871  -1.1967978  -1.6062546  -0.46499455\n",
            " -1.2711067  -2.0112567  -0.58433986 -1.553836   -1.3057547  -1.1935616\n",
            "  1.0855651  -1.4300516  -1.2191478  -0.8055566  -0.4730305  -0.39959362\n",
            " -1.5318363 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 35 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6178087  -0.48454565 -0.35867125 -0.83272094 -1.2600763  -0.32893956\n",
            " -0.90978897 -1.4417763  -0.40547442 -1.0756536  -0.8918192  -0.8294842\n",
            "  0.6193733  -1.1332728  -1.0002326  -0.434788   -0.05966131 -0.24709563\n",
            " -1.3251312 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8268294  -0.653981   -0.5312136  -1.2163515  -1.4041569  -0.29393774\n",
            " -1.2534432  -1.9129509  -0.5325831  -1.4298291  -1.1285682  -1.142338\n",
            "  0.5565297  -1.4813273  -1.1215277  -0.8944876  -0.3202738  -0.51171315\n",
            " -1.5919573 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 37 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.64251846 -0.4142313  -0.45889273 -0.88409996 -1.3479459  -0.27007255\n",
            " -1.1742109  -1.5217732  -0.3660359  -1.1879683  -0.88640124 -0.8945141\n",
            "  0.8579263  -1.2577449  -1.1295851  -0.66009444 -0.35492146 -0.15525907\n",
            " -1.4959974 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 38 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8672516  -0.72564644 -0.5820008  -1.2613662  -1.6320324  -0.4358449\n",
            " -1.2145413  -2.1204011  -0.6780328  -1.6699756  -1.453769   -1.2712986\n",
            "  0.69539404 -1.6029341  -1.1710216  -0.93751574 -0.2104858  -0.5014477\n",
            " -1.6614362 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 39 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6339126  -0.5718044  -0.3770631  -0.96632755 -1.4189734  -0.39089173\n",
            " -1.155173   -1.6903107  -0.42501011 -1.2988214  -1.0946032  -1.0088438\n",
            "  0.9820417  -1.3462241  -1.2789181  -0.44529933 -0.06031122 -0.26791742\n",
            " -1.4791589 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 40 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8641489  -0.568762   -0.42988014 -1.168214   -1.5096893  -0.43848416\n",
            " -1.2844837  -2.0009089  -0.44098595 -1.5638399  -1.2556615  -1.1586597\n",
            "  0.92893755 -1.6258501  -1.5418353  -0.5021879  -0.3740384  -0.5666973\n",
            " -1.6468852 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 41 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.73556364 -0.47939667 -0.5137287  -1.0408881  -1.2705433  -0.31692263\n",
            " -1.2140734  -1.6639221  -0.42365825 -1.1998006  -0.9049816  -0.947068\n",
            "  0.6842532  -1.3354807  -1.2291651  -0.79900986 -0.29147252 -0.20521344\n",
            " -1.5730226 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 42 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9788703  -0.5664315  -0.58882725 -1.2797837  -1.5266342  -0.3435271\n",
            " -1.348953   -2.1410954  -0.51944655 -1.5571977  -1.280995   -1.2353895\n",
            "  0.55827284 -1.7456189  -1.35122    -0.89203596 -0.57777905 -0.5470047\n",
            " -1.8649772 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7258724  -0.542222   -0.48929238 -0.96812457 -1.3260081  -0.28802678\n",
            " -1.0818641  -1.5914514  -0.33106124 -1.0806905  -0.95404434 -0.9415402\n",
            "  0.7876091  -1.2643414  -1.0240692  -0.84466666 -0.29945475 -0.18745881\n",
            " -1.4780456 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8682843  -0.55998874 -0.5027136  -1.0831482  -1.2167475  -0.23148954\n",
            " -1.1257215  -1.6826798  -0.36147115 -1.2726603  -1.0903777  -1.0847318\n",
            "  0.3130641  -1.4334002  -1.2289602  -0.576005   -0.34445035 -0.5406807\n",
            " -1.5046983 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.58215797 -0.40151134 -0.3336553  -0.7840672  -1.1006181  -0.27496615\n",
            " -0.9078494  -1.2855804  -0.0929945  -0.9733067  -0.7594883  -0.8309146\n",
            "  0.71395487 -1.0298598  -1.0815076  -0.32974267 -0.26530677 -0.18654159\n",
            " -1.1925019 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7678104  -0.5861052  -0.41255045 -1.0715134  -1.3831468  -0.3007732\n",
            " -1.3612254  -1.7457448  -0.41620475 -1.540626   -1.1706564  -1.1127315\n",
            "  0.69791406 -1.4606041  -1.3052388  -0.46282095 -0.24956468 -0.55590814\n",
            " -1.4817828 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.72856563 -0.5435666  -0.44821092 -0.9105518  -1.2257638  -0.25577718\n",
            " -1.1114125  -1.4666438  -0.234045   -1.1421629  -0.9660074  -0.9377693\n",
            "  0.29884517 -1.2801925  -0.96133685 -0.6181639  -0.2966916  -0.3242247\n",
            " -1.3409616 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.78072894 -0.660333   -0.50822055 -1.1292589  -1.2888842  -0.20742323\n",
            " -1.2548374  -1.6974661  -0.48391187 -1.3881456  -1.1233052  -1.10308\n",
            "  0.24074954 -1.4678309  -1.0980322  -0.4863485  -0.18782525 -0.57617587\n",
            " -1.4948404 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5647714  -0.39701247 -0.3594645  -0.8551371  -1.1780479  -0.3054862\n",
            " -1.1388608  -1.4977984  -0.26645014 -1.1134071  -0.84667504 -0.8996423\n",
            "  1.0239196  -1.1226298  -1.282789   -0.1736968  -0.06037502 -0.2753852\n",
            " -1.3758844 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8774408  -0.49418554 -0.44119436 -1.0909032  -1.4678886  -0.3964532\n",
            " -1.2751548  -1.9397528  -0.38231996 -1.5001335  -1.2658061  -1.1039693\n",
            "  0.9624144  -1.5749927  -1.4181416   0.09491254 -0.23685692 -0.5314031\n",
            " -1.7359854 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 51 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5819798  -0.6506715  -0.4160058  -0.8893516  -1.2176311  -0.32683223\n",
            " -1.0457656  -1.4987715  -0.34442142 -1.073594   -0.875274   -0.9145476\n",
            "  0.49895287 -1.1764972  -0.912949   -0.1846845   0.1099312  -0.38767374\n",
            " -1.335689  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 52 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.77322376 -0.52032787 -0.47413376 -1.1816994  -1.5181503  -0.41006476\n",
            " -1.3794897  -1.9867133  -0.47206923 -1.5810875  -1.2042348  -1.1532016\n",
            "  1.1003834  -1.5033057  -1.0063691  -0.23227613 -0.16136262 -0.46943828\n",
            " -1.6227872 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 53 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8354597  -0.4419167  -0.4617145  -1.0530012  -1.3990064  -0.36478183\n",
            " -1.3416138  -1.805069   -0.28509754 -1.3483745  -1.152703   -1.0469744\n",
            "  1.1041917  -1.4786563  -1.2918363  -0.20686615 -0.30409366 -0.33063537\n",
            " -1.6571342 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 54 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.92384756 -0.513782   -0.5960095  -1.2183822  -1.422234   -0.35346812\n",
            " -1.2512736  -1.988988   -0.39157912 -1.4102645  -1.2371612  -1.1765378\n",
            "  0.64900446 -1.6016837  -0.8494596  -0.01430295 -0.36221823 -0.4956036\n",
            " -1.7694534 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 55 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6075857  -0.53866935 -0.35992253 -0.74510735 -1.1360426  -0.23829404\n",
            " -0.9871417  -1.314284   -0.23909266 -0.9268241  -0.6705854  -0.7222044\n",
            "  0.34913528 -0.99901056 -0.61535287 -0.25419232 -0.13824768 -0.16579975\n",
            " -1.2060316 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 56 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6688542  -0.6179203  -0.38637707 -0.89476836 -1.1855586  -0.33773932\n",
            " -1.074866   -1.3481296  -0.19973771 -1.0602503  -0.8487036  -0.8532343\n",
            "  0.25291052 -1.2117676  -0.6173547  -0.25875077 -0.32787222 -0.23358157\n",
            " -1.2599481 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 57 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5053935  -0.24851188 -0.36141706 -0.68332976 -1.0223367  -0.17182937\n",
            " -0.78921485 -1.1575693  -0.3028192  -0.9274145  -0.73041546 -0.6707447\n",
            "  0.43149838 -0.8188277  -0.529865   -0.21728559  0.02193015 -0.22672491\n",
            " -1.1677583 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 58 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.43828362 -0.2590158  -0.34286284 -0.66167146 -0.9247813  -0.13725235\n",
            " -0.7595898  -1.1089876  -0.22092523 -0.8892886  -0.64149714 -0.6730456\n",
            "  0.33338428 -0.75000346 -0.39130262 -0.13485707 -0.00647752 -0.23728341\n",
            " -1.0882901 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 59 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.654998   -0.57527554 -0.40054306 -0.8486398  -1.2370887  -0.31836715\n",
            " -1.0830657  -1.4638826  -0.10454848 -1.031467   -0.82319015 -0.8487128\n",
            "  0.5473001  -1.172637   -0.5588048  -0.16634183 -0.18724565 -0.20542148\n",
            " -1.3426647 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 60 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6352193  -0.548964   -0.39580333 -0.79444754 -1.1597676  -0.2791815\n",
            " -1.0840461  -1.248969   -0.20246483 -1.0019306  -0.82298625 -0.79597926\n",
            "  0.21193255 -1.0770465  -0.38533002 -0.36021677 -0.43654373 -0.21330549\n",
            " -1.2001742 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 61 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.570725   -0.25191113 -0.36857334 -0.75955427 -1.0214336  -0.17062283\n",
            " -0.75949067 -1.1456988  -0.2193341  -0.9322123  -0.766789   -0.68447053\n",
            "  0.3825233  -0.82239187 -0.5182624  -0.08869542 -0.08557737 -0.21379948\n",
            " -1.1567427 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 62 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5999273  -0.5171653  -0.4081997  -0.85783875 -1.1939709  -0.2263906\n",
            " -1.0710917  -1.2177665  -0.21305893 -1.0514714  -0.9676339  -0.7882358\n",
            "  0.04196205 -1.1222115  -0.31772062 -0.2432855  -0.5805957  -0.2885859\n",
            " -1.2223147 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 63 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7831204  -0.56985694 -0.5187846  -1.0426726  -1.5132952  -0.41500542\n",
            " -1.2630669  -1.7215918  -0.3716537  -1.2789398  -1.0728985  -1.0840656\n",
            "  0.7866094  -1.4728208  -0.71981287 -0.2010755  -0.48807353 -0.37062284\n",
            " -1.6792159 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 64 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.56947356 -0.40091842 -0.36630818 -0.81905043 -1.0819979  -0.25115582\n",
            " -0.97001565 -1.1398698  -0.19938639 -0.98842263 -0.866468   -0.7441952\n",
            "  0.5496732  -0.963383   -0.60832787 -0.15742178 -0.4400121  -0.20130911\n",
            " -1.2526728 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 65 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7101636  -0.5442231  -0.43131602 -0.93116015 -1.2811396  -0.33044878\n",
            " -1.0695697  -1.505494   -0.20722719 -1.159966   -0.9793003  -0.9121875\n",
            "  0.45094413 -1.2627048  -0.49102396 -0.00980892 -0.66311705 -0.29429057\n",
            " -1.4352556 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 66 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7521035  -0.564086   -0.49301547 -1.0631458  -1.4272833  -0.30356294\n",
            " -1.1657934  -1.6611999  -0.5100207  -1.3550453  -0.97855425 -0.97843295\n",
            "  0.60281336 -1.3622972  -0.42732537 -0.26549566 -0.4752555  -0.31535202\n",
            " -1.6457275 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 67 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8356924  -0.60568726 -0.42011303 -1.026039   -1.4838476  -0.30463508\n",
            " -1.3587421  -1.6561792  -0.2897042  -1.3637527  -1.2174011  -0.949031\n",
            "  0.3055834  -1.450569   -0.3550755  -0.1940283  -0.8724109  -0.34551015\n",
            " -1.5612183 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 68 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.75314915 -0.65309024 -0.45939717 -1.0882665  -1.6039476  -0.42562178\n",
            " -1.2393781  -1.8769804  -0.39308652 -1.4646393  -1.2373022  -1.1964915\n",
            "  1.0274241  -1.5233384  -0.6297913  -0.06449455 -0.7202024  -0.4398037\n",
            " -1.6202887 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 69 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8813632  -0.62292165 -0.5104892  -1.1109248  -1.392585   -0.25393024\n",
            " -1.1611187  -1.5005008  -0.38384363 -1.3567405  -1.2028179  -0.9836706\n",
            "  0.27078924 -1.3807999  -0.31438145 -0.14017937 -0.67600787 -0.39592054\n",
            " -1.6202626 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 70 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8774422  -0.7230817  -0.4234397  -1.0163616  -1.4693682  -0.2996478\n",
            " -1.2889131  -1.5771077  -0.26031798 -1.299027   -1.0912488  -1.022295\n",
            "  0.29929656 -1.4339554  -0.30777335 -0.22783634 -0.9162649  -0.47704226\n",
            " -1.6058033 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 71 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.67074597 -0.46267202 -0.45919684 -0.9329223  -1.2642798  -0.24543852\n",
            " -0.9797846  -1.4144293  -0.48389035 -1.2092211  -0.95732445 -0.9015277\n",
            "  0.61954063 -1.2270709  -0.29831755 -0.1229899  -0.66859233 -0.32217526\n",
            " -1.4906737 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 72 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.73992985 -0.5675708  -0.42030746 -1.0519849  -1.5177772  -0.38786379\n",
            " -1.3146546  -1.7429831  -0.36162925 -1.3340819  -1.1804291  -1.0262783\n",
            "  0.5012321  -1.418417   -0.36083132 -0.2098943  -1.1224192  -0.32557094\n",
            " -1.5374321 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 73 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7916101  -0.63035876 -0.44219    -1.0442305  -1.3815122  -0.36057472\n",
            " -1.1359419  -1.6531943  -0.50766325 -1.228791   -1.1064125  -1.0492157\n",
            "  0.8375911  -1.3721813  -0.5096203  -0.09169515 -1.0140015  -0.347757\n",
            " -1.5497593 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 74 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.75884587 -0.5127317  -0.44011822 -0.9359654  -1.2343187  -0.289067\n",
            " -1.0523238  -1.27215    -0.4699197  -1.1308968  -0.9906138  -0.8251569\n",
            "  0.38196823 -1.2112975  -0.40563333 -0.10358024 -1.108135   -0.26653022\n",
            " -1.4387494 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 75 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.89404124 -0.74029505 -0.42724156 -1.0172683  -1.5140915  -0.3617907\n",
            " -1.3019831  -1.6530927  -0.53341633 -1.3408821  -1.0899988  -1.0061315\n",
            "  0.3814048  -1.4547907  -0.37942618 -0.20881055 -1.215817   -0.36378446\n",
            " -1.6162381 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 76 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7510211  -0.4979837  -0.5042015  -1.0879456  -1.530844   -0.34923357\n",
            " -1.2454144  -1.7823526  -0.9004238  -1.4539487  -1.1082698  -1.1088804\n",
            "  0.912665   -1.3912231  -0.42865223 -0.1425471  -1.1363424  -0.36043325\n",
            " -1.6398567 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 77 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8861617  -0.5388197  -0.45832407 -1.072564   -1.5157049  -0.30525118\n",
            " -1.277902   -1.62745    -0.52084494 -1.3431152  -1.1891199  -0.9731895\n",
            "  0.43293872 -1.4231341  -0.30022794 -0.25365883 -1.4477038  -0.3393198\n",
            " -1.6186213 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 78 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.73752314 -0.6898656  -0.41959172 -1.0349913  -1.5746706  -0.38121283\n",
            " -1.2926733  -1.8006394  -0.6785877  -1.4537315  -1.2047015  -1.1704344\n",
            "  0.8663021  -1.4857252  -0.25875863 -0.0997486  -1.5627655  -0.4072673\n",
            " -1.6520839 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 79 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.73598504 -0.63902247 -0.4236627  -1.0287217  -1.3117414  -0.30787298\n",
            " -1.1267505  -1.4444393  -0.88253903 -1.2557172  -1.0706998  -0.97657514\n",
            "  0.4767998  -1.3021181  -0.33593434 -0.08188735 -1.2691438  -0.3321205\n",
            " -1.4967277 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 80 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8944469  -0.692091   -0.49196962 -1.0757169  -1.5736489  -0.4010748\n",
            " -1.2723953  -1.7770568  -0.62678856 -1.4419683  -1.2685114  -1.0696318\n",
            "  0.47875762 -1.5752215  -0.4115466  -0.13562402 -1.7983304  -0.41206354\n",
            " -1.7648195 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 81 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.77877563 -0.6580208  -0.53454775 -1.1183391  -1.4896834  -0.326609\n",
            " -1.097566   -1.6115582  -1.1123779  -1.4090452  -1.0954772  -1.0052501\n",
            "  0.6016911  -1.3955468  -0.38778698 -0.28581905 -1.348825   -0.3492794\n",
            " -1.6509523 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 82 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7607632  -0.592277   -0.4701908  -1.0424104  -1.3931422  -0.29995143\n",
            " -1.2559932  -1.5378693  -0.66175336 -1.3244408  -1.1585684  -0.95346004\n",
            "  0.3234803  -1.3896465  -0.4099944  -0.18234104 -1.7342287  -0.36360207\n",
            " -1.4672436 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 83 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.78829646 -0.60816866 -0.57213545 -1.1133285  -1.5672162  -0.40429258\n",
            " -1.3151567  -1.659135   -0.8334688  -1.3514539  -1.2147862  -1.1707433\n",
            "  0.7224056  -1.5683157  -0.38571873  0.01998042 -1.8301005  -0.431641\n",
            " -1.7430915 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 84 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.66642654 -0.5060449  -0.41529563 -0.8958133  -1.2981896  -0.3095625\n",
            " -1.110071   -1.3023518  -0.8039474  -1.1653538  -0.97372913 -0.8565767\n",
            "  0.6706727  -1.155382   -0.52476525 -0.19639452 -1.4591976  -0.23599724\n",
            " -1.4143959 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 85 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9399465  -0.7410844  -0.4947222  -1.0742648  -1.6228273  -0.3522301\n",
            " -1.3650961  -1.6509007  -0.84686327 -1.3458611  -1.124138   -1.0106982\n",
            "  0.32615378 -1.4829947  -0.2821259  -0.26771185 -1.7583435  -0.38411304\n",
            " -1.6262112 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 86 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8798626  -0.60084194 -0.58699036 -1.1823807  -1.6463435  -0.31229958\n",
            " -1.4136685  -1.7369198  -1.3387775  -1.5376916  -1.2122724  -1.0932021\n",
            "  0.6529781  -1.5684906  -0.2203106  -0.3028347  -1.7561047  -0.39359683\n",
            " -1.8218161 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 87 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8497314  -0.5682482  -0.45838538 -1.0865211  -1.5129993  -0.31896338\n",
            " -1.3362367  -1.5815763  -0.8322981  -1.4103711  -1.2084942  -0.9598219\n",
            "  0.34690642 -1.4839725  -0.31084365 -0.14534004 -1.8468863  -0.38971865\n",
            " -1.6906577 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 88 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.89960325 -0.74432564 -0.4677478  -1.1825055  -1.7789648  -0.39739823\n",
            " -1.4647849  -1.8561277  -0.9864511  -1.53361    -1.3447837  -1.2219085\n",
            "  0.6548824  -1.7711761  -0.24380565 -0.07800605 -1.8128116  -0.5520273\n",
            " -1.8415328 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 89 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.82758677 -0.66642326 -0.49840897 -1.1338456  -1.4433558  -0.31401974\n",
            " -1.2218466  -1.6319405  -1.1933951  -1.3617742  -1.069586   -1.0655631\n",
            "  0.5691607  -1.4249046  -0.27465826 -0.2941951  -1.5926046  -0.3571254\n",
            " -1.6374748 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 90 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.96682376 -0.6624821  -0.5386557  -1.0805691  -1.5517128  -0.39328143\n",
            " -1.2914934  -1.7164599  -0.7780384  -1.4439818  -1.2346988  -1.0213597\n",
            "  0.32493725 -1.5697783  -0.3818133  -0.20206787 -1.9539167  -0.4183636\n",
            " -1.7299958 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 91 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6840324  -0.70195085 -0.5000262  -1.0981503  -1.521719   -0.3293344\n",
            " -1.2264874  -1.6976547  -1.2758508  -1.4038216  -1.1453576  -1.0861511\n",
            "  0.811788   -1.4289541  -0.37370557 -0.17051475 -1.5246868  -0.43091893\n",
            " -1.6001724 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 92 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7148826  -0.5321482  -0.4590502  -0.99644744 -1.3213317  -0.25692073\n",
            " -1.1857588  -1.3723536  -0.7849723  -1.2103219  -1.0277603  -0.8614936\n",
            "  0.18984453 -1.230349   -0.32021815 -0.27365172 -1.6239135  -0.39452323\n",
            " -1.4461005 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 93 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0445048  -0.6906869  -0.5332703  -1.1771573  -1.6425445  -0.32780877\n",
            " -1.2643188  -1.7413169  -1.1453574  -1.417866   -1.3596791  -1.1620288\n",
            "  0.72541595 -1.5898036  -0.49230933 -0.06483087 -2.0529528  -0.4052149\n",
            " -1.8961219 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 94 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.81511754 -0.4976845  -0.4271056  -1.0294774  -1.4199245  -0.31912342\n",
            " -1.1690707  -1.4778023  -1.0165226  -1.352895   -1.0654589  -0.9347888\n",
            "  0.8236305  -1.3061236  -0.7349472  -0.24724711 -1.6228926  -0.25421065\n",
            " -1.5356466 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 95 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.90260583 -0.6651718  -0.52337676 -1.1180376  -1.5684834  -0.36044654\n",
            " -1.313187   -1.6492954  -0.8716268  -1.3609971  -1.1665537  -1.0170135\n",
            "  0.22728556 -1.499458   -0.62199014 -0.19941267 -1.8477604  -0.42100462\n",
            " -1.6599439 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 96 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6793633  -0.6353107  -0.48399004 -1.0835776  -1.5316217  -0.35644484\n",
            " -1.2696972  -1.6254604  -1.3872775  -1.3734894  -1.0436647  -1.1287906\n",
            "  0.8831701  -1.4573398  -0.91677886 -0.06949894 -1.5300261  -0.4035926\n",
            " -1.6407031 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 97 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.88557184 -0.6017554  -0.47819018 -1.1699694  -1.5547826  -0.27248564\n",
            " -1.3432226  -1.5528857  -0.9277073  -1.465037   -1.2401968  -1.0080194\n",
            "  0.23905967 -1.4844906  -0.717885   -0.13931218 -1.6389552  -0.50370723\n",
            " -1.7462069 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 98 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.96193457 -0.7145803  -0.49442574 -1.1308974  -1.6906779  -0.32978514\n",
            " -1.394804   -1.7226254  -1.0634944  -1.4141936  -1.305348   -1.1134588\n",
            "  0.67198527 -1.6580412  -0.81565464 -0.12012222 -1.8384793  -0.3723081\n",
            " -1.7494385 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 99 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.73719174 -0.57684535 -0.48499352 -1.0912952  -1.3743556  -0.3114963\n",
            " -1.1340518  -1.5188646  -1.2828903  -1.3225018  -1.0707127  -1.0007362\n",
            "  0.558461   -1.3190479  -0.9165675  -0.17778641 -1.6133506  -0.33229554\n",
            " -1.5722941 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 100 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.81113416 -0.57411873 -0.48551527 -1.071435   -1.5073895  -0.3861432\n",
            " -1.2166775  -1.6275762  -0.79766655 -1.3584313  -1.1573057  -1.0096803\n",
            "  0.40624437 -1.5133626  -1.0459925  -0.00852514 -1.6673518  -0.44236767\n",
            " -1.6681925 ]\n",
            "Epsilon reduced to 0.08192000000000003\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.83283633 -0.54296243 -0.47392556 -1.1459115  -1.614835   -0.5245249\n",
            " -1.2724507  -1.7937149  -1.2315525  -1.5394776  -1.3960754  -1.1650739\n",
            "  1.2778122  -1.5948738  -1.6268452   0.13886163 -1.540665   -0.40585968\n",
            " -1.674144  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9356776  -0.3988156  -0.54668176 -1.11014    -1.507873   -0.43863732\n",
            " -1.1530955  -1.7338278  -0.85131246 -1.5712153  -1.2406578  -1.1417127\n",
            "  0.9912238  -1.5250125  -1.2011015   0.02374529 -1.599881   -0.33137366\n",
            " -1.75964   ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6819363  -0.49272382 -0.42881584 -1.0034746  -1.4286175  -0.47563016\n",
            " -1.118974   -1.5466042  -1.2088976  -1.3436104  -1.1830852  -1.0159725\n",
            "  1.2324742  -1.4185779  -1.5922297   0.13603401 -1.3147198  -0.3141423\n",
            " -1.4553549 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9158828  -0.41961074 -0.55288607 -1.10148    -1.5482767  -0.49247205\n",
            " -1.1500661  -1.7349192  -0.8198187  -1.567754   -1.3475903  -1.1689955\n",
            "  1.015892   -1.4954224  -1.3662331  -0.09106501 -1.5076451  -0.3724439\n",
            " -1.7547979 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9375264  -0.46024367 -0.58687234 -1.2328427  -1.6124573  -0.46003738\n",
            " -1.3688407  -1.8626662  -1.3370831  -1.629336   -1.4104762  -1.2408036\n",
            "  1.3610336  -1.5938472  -1.5230032  -0.05323141 -1.4026653  -0.3123039\n",
            " -1.7581947 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7876194  -0.4040831  -0.48108715 -1.0745356  -1.5353789  -0.43477517\n",
            " -1.1848391  -1.646204   -0.9025947  -1.4991822  -1.2517858  -1.0953069\n",
            "  1.0697187  -1.407674   -0.9635767  -0.28330037 -1.2113528  -0.27064073\n",
            " -1.6274703 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8551923  -0.45939526 -0.5953125  -1.1083325  -1.3520768  -0.33570433\n",
            " -1.1303139  -1.4951011  -1.3611691  -1.3163074  -1.2822896  -1.1238519\n",
            "  0.8907342  -1.4643496  -1.0559359   0.09539691 -1.2813705  -0.23729953\n",
            " -1.6165371 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.83626705 -0.309482   -0.48364076 -1.0277888  -1.3956146  -0.4313621\n",
            " -1.0554883  -1.5494272  -0.78408927 -1.3894434  -1.2274104  -1.0604589\n",
            "  1.0737845  -1.3843423  -1.3495231  -0.00859438 -1.1117061  -0.30463856\n",
            " -1.6420013 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6867898  -0.51678425 -0.44346794 -1.0037842  -1.4531028  -0.48363364\n",
            " -1.1651679  -1.6112868  -1.2125074  -1.4183233  -1.2525051  -1.0525678\n",
            "  1.2351693  -1.4466271  -1.6869162   0.15654205 -0.9118539  -0.3490723\n",
            " -1.4606726 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 10 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.74507636 -0.34451935 -0.42057443 -0.93927896 -1.2438364  -0.3520184\n",
            " -1.1507939  -1.3641756  -0.72861874 -1.2980112  -1.1071937  -0.96708053\n",
            "  0.7860878  -1.3340034  -1.2520167   0.26368913 -0.94425035 -0.41657472\n",
            " -1.6057768 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 11 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8009561  -0.45138538 -0.601352   -1.0870626  -1.3278561  -0.30318832\n",
            " -1.1138247  -1.437467   -1.3267877  -1.2654599  -1.2617055  -1.1066484\n",
            "  0.78617024 -1.4330819  -1.0147402  -0.02690247 -0.9867774  -0.27466404\n",
            " -1.6084251 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 12 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.77899426 -0.4077019  -0.4650705  -1.0260881  -1.4519765  -0.40628064\n",
            " -1.1312087  -1.5504311  -0.8058875  -1.4227235  -1.233091   -1.0554167\n",
            "  0.97349447 -1.3464465  -0.9567404  -0.29225966 -0.6870436  -0.27900553\n",
            " -1.5767276 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.800204   -0.44518253 -0.55582595 -1.0517184  -1.2970397  -0.31163707\n",
            " -1.1069077  -1.4513721  -1.2914766  -1.2869631  -1.2392864  -1.0791107\n",
            "  0.7989322  -1.4118829  -0.8722518   0.07049667 -0.7936279  -0.23020218\n",
            " -1.5646627 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.76168406 -0.364665   -0.40060225 -0.9257564  -1.2509093  -0.3398951\n",
            " -1.1620197  -1.3587573  -0.7224558  -1.2996612  -1.1147225  -0.97831553\n",
            "  0.78566676 -1.3501835  -0.9780638   0.2247599  -0.65433276 -0.40872177\n",
            " -1.576849  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6611625  -0.44373006 -0.47048587 -0.98146796 -1.3780992  -0.36418638\n",
            " -1.2187122  -1.4871215  -1.1924082  -1.3582108  -1.1678253  -1.0264597\n",
            "  1.2439009  -1.290376   -0.911325   -0.24034317 -0.5366756  -0.17312068\n",
            " -1.3918165 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8264092  -0.5313081  -0.47490755 -1.058426   -1.4273694  -0.33263326\n",
            " -1.360355   -1.509154   -0.9472024  -1.3870279  -1.2644463  -1.09091\n",
            "  0.88365525 -1.415667   -0.7736515  -0.08097367 -0.5411694  -0.3631522\n",
            " -1.5909083 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.67443097 -0.48835146 -0.39152637 -0.98816425 -1.3970373  -0.4727457\n",
            " -1.0862633  -1.5909626  -1.1398757  -1.3543925  -1.2259202  -1.0470668\n",
            "  1.1430365  -1.3505199  -0.9792712   0.09126826 -0.32580924 -0.3425375\n",
            " -1.3791096 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.65484446 -0.3074124  -0.4415544  -0.9247469  -1.2886336  -0.31707737\n",
            " -0.99856246 -1.440902   -0.9160992  -1.2274488  -1.0919405  -0.92205435\n",
            "  1.2030067  -1.2314773  -0.79103684 -0.2075918  -0.23175476 -0.17675082\n",
            " -1.4146365 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8067499  -0.43433228 -0.5300524  -1.050705   -1.2506042  -0.34237352\n",
            " -1.0153013  -1.4014785  -1.1494468  -1.2012883  -1.2409852  -1.0547758\n",
            "  0.768733   -1.3737245  -0.59372264  0.05154855 -0.36124572 -0.25573277\n",
            " -1.5060945 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7024351  -0.346817   -0.39459145 -0.8823422  -1.142986   -0.3271648\n",
            " -1.077225   -1.2386374  -0.6118562  -1.1639776  -1.0274365  -0.9216464\n",
            "  0.6973205  -1.2191243  -0.7044715   0.20050022 -0.31694978 -0.3959876\n",
            " -1.4548441 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6342402  -0.29769745 -0.4421946  -0.85310614 -1.1840451  -0.37287098\n",
            " -0.977136   -1.3038508  -0.9288245  -1.1374398  -1.0294269  -0.92513704\n",
            "  1.1257306  -1.1621264  -0.8107188   0.07482422 -0.28784618 -0.16844381\n",
            " -1.2760012 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.67018694 -0.4447213  -0.4039383  -0.9074359  -1.2163589  -0.25484958\n",
            " -1.0765882  -1.2528182  -0.8888365  -1.184006   -1.0244114  -0.89820844\n",
            "  0.70822036 -1.2401447  -0.5296842  -0.03320381 -0.2891665  -0.35290104\n",
            " -1.4073828 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6594955  -0.50754184 -0.4235044  -0.9270271  -1.1817424  -0.33785838\n",
            " -0.7973313  -1.3208574  -1.0708256  -1.1804438  -1.1283503  -0.9389128\n",
            "  0.54830414 -1.2967423  -0.50253564  0.11081583 -0.2816245  -0.34459755\n",
            " -1.3640518 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7011942  -0.4773534  -0.4336707  -0.9338538  -1.252075   -0.25173014\n",
            " -0.9588882  -1.2821046  -0.9039176  -1.1746542  -1.0823182  -0.93173236\n",
            "  0.73220724 -1.2825272  -0.53334254 -0.06992027 -0.25150144 -0.36847946\n",
            " -1.4481068 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6197476  -0.28991082 -0.37944597 -0.880199   -1.2002249  -0.40190783\n",
            " -0.7352874  -1.3509716  -0.85763764 -1.162281   -1.0328621  -0.9109065\n",
            "  1.1803316  -1.213218   -0.67526126  0.1311941  -0.13214651 -0.19551274\n",
            " -1.289455  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.66684926 -0.4072336  -0.38666075 -0.88731617 -1.2144108  -0.26415932\n",
            " -0.8610394  -1.3048651  -0.8743854  -1.1798556  -1.0297136  -0.9317519\n",
            "  0.902241   -1.2453257  -0.55075985  0.00941717 -0.15554471 -0.31753978\n",
            " -1.4061298 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6748636  -0.47858608 -0.4623659  -0.99454755 -1.2110956  -0.28492647\n",
            " -0.81671035 -1.3501734  -1.2118285  -1.1717077  -1.1102644  -1.0116653\n",
            "  0.75196326 -1.2327065  -0.20367435 -0.20946638 -0.0964006  -0.20162807\n",
            " -1.3252629 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.709695   -0.31984198 -0.4111697  -0.91280264 -1.2704136  -0.3088943\n",
            " -0.71369505 -1.3731947  -0.8348705  -1.126314   -1.0832115  -0.90425885\n",
            "  1.0942479  -1.2296312  -0.6029111  -0.23391894  0.02885024 -0.17686\n",
            " -1.3471246 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 29 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.74606097 -0.5281441  -0.4942034  -1.0595734  -1.3059158  -0.3856847\n",
            " -0.6623487  -1.4521848  -1.1780728  -1.3214436  -1.3010287  -1.0513706\n",
            "  0.53287256 -1.4038177  -0.3949677   0.05430715  0.01411715 -0.4100814\n",
            " -1.4947727 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 30 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.63909024 -0.3602705  -0.34790453 -0.88988054 -1.247585   -0.35785538\n",
            " -0.5867476  -1.3989105  -0.93252116 -1.1955931  -1.1424638  -0.90955263\n",
            "  1.0677601  -1.2439286  -0.6753676   0.08066168  0.22852872 -0.29499096\n",
            " -1.2793018 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 31 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5952154  -0.27089012 -0.41887245 -0.89980465 -1.2035367  -0.37144047\n",
            " -0.5642507  -1.3699281  -0.9111512  -1.2154851  -1.0961876  -0.90480685\n",
            "  1.017869   -1.2378414  -0.53112876  0.11629545 -0.06932575 -0.24431059\n",
            " -1.3281218 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 32 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.61339206 -0.4572019  -0.37422398 -0.92111903 -1.3171766  -0.36418957\n",
            " -0.6971974  -1.4654865  -0.86099124 -1.2653468  -1.2363052  -0.96134716\n",
            "  0.9275368  -1.2732875  -0.625249   -0.13864154  0.11739017 -0.30134633\n",
            " -1.3245544 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 33 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4774714  -0.5008753  -0.37699425 -0.91040266 -1.2844739  -0.39410627\n",
            " -0.70594275 -1.4134247  -0.9362405  -1.293387   -1.1762103  -0.9505608\n",
            "  0.9595747  -1.1792198  -0.46511576 -0.11046025  0.12073298 -0.29501247\n",
            " -1.2102182 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 34 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5993956  -0.42034972 -0.33481497 -0.90194714 -1.3015494  -0.3618661\n",
            " -0.639328   -1.4190369  -0.9294826  -1.2003503  -1.198936   -0.9078657\n",
            "  0.9945149  -1.2820725  -0.563515   -0.06382848  0.14845411 -0.26613885\n",
            " -1.3013695 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 35 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6388228  -0.40965092 -0.4849154  -0.8872333  -1.1420951  -0.32676318\n",
            " -0.461339   -1.2902783  -0.90855163 -1.0707486  -1.0221797  -0.9121204\n",
            "  0.59058934 -1.1981063  -0.28016075  0.08029439 -0.19376823 -0.25803316\n",
            " -1.3130784 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6072094  -0.27145553 -0.3896676  -0.8882549  -1.224087   -0.320499\n",
            " -0.5187159  -1.3403121  -0.9660073  -1.1225055  -1.0079547  -0.8808621\n",
            "  1.1359689  -1.1826683  -0.6223221  -0.13299799  0.03159484 -0.1809159\n",
            " -1.3461705 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 37 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5020977  -0.39628142 -0.35829243 -0.84827584 -1.1439035  -0.37829942\n",
            " -0.52923244 -1.3773549  -0.8477493  -1.2000751  -1.0878822  -0.89961576\n",
            "  0.88000554 -1.159585   -0.5539925   0.12734485 -0.01851482 -0.28449476\n",
            " -1.2228199 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 38 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5233376  -0.22867933 -0.3510925  -0.81692237 -1.1829896  -0.28941724\n",
            " -0.47574002 -1.250025   -1.0126934  -1.0696483  -0.9361585  -0.8200431\n",
            "  1.1574118  -1.1248369  -0.57544035 -0.09842351  0.02945247 -0.17444305\n",
            " -1.2689664 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 39 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5146352  -0.3660391  -0.3531178  -0.8652455  -1.1652584  -0.33132237\n",
            " -0.43513718 -1.3596191  -0.8012875  -1.2005733  -1.1151302  -0.90952635\n",
            "  0.86694837 -1.1600641  -0.44970503  0.14317052  0.0208325  -0.33688277\n",
            " -1.2288471 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 40 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4546465  -0.4330191  -0.42082942 -0.8385214  -1.0804019  -0.15816252\n",
            " -0.48420623 -1.0822822  -1.0807985  -0.9254166  -0.94749093 -0.80624187\n",
            "  0.56741923 -1.1590714  -0.26534355  0.02447582 -0.2032102  -0.2535201\n",
            " -1.3123547 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 41 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.40687752 -0.5179089  -0.32490087 -0.8824295  -1.2138176  -0.30889988\n",
            " -0.5634595  -1.3549118  -0.81064564 -1.1934996  -1.0171392  -0.9054414\n",
            "  0.75949603 -1.0818558  -0.40466595 -0.12213517 -0.08277602 -0.2921102\n",
            " -1.1297846 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 42 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.58029836 -0.2965029  -0.3944715  -0.94805866 -1.2294173  -0.34006378\n",
            " -0.47597012 -1.3640825  -0.930098   -1.2262198  -1.1559237  -0.9412792\n",
            "  1.1730561  -1.2932154  -0.55932796 -0.04980271 -0.02358719 -0.21632995\n",
            " -1.3632063 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 43 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.35009292 -0.43525073 -0.3267155  -0.7423668  -1.0411389  -0.29156232\n",
            " -0.5869254  -1.1270392  -0.6384663  -1.0341796  -0.9415195  -0.75500757\n",
            "  0.43091652 -0.8864296  -0.274797   -0.17753457 -0.20072103 -0.2125096\n",
            " -1.0440437 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 44 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.48515046 -0.30026263 -0.46555153 -0.7666096  -1.0553315  -0.20268206\n",
            " -0.38759178 -1.0962124  -1.050942   -0.860768   -0.85248107 -0.7980862\n",
            "  0.79253894 -1.0126443  -0.282966   -0.05149684 -0.09132446 -0.11130004\n",
            " -1.1796514 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 45 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.42057252 -0.5450554  -0.47289637 -0.89853203 -1.1632688  -0.26960462\n",
            " -0.546995   -1.292745   -0.9609006  -1.1459324  -1.0490357  -0.9057803\n",
            "  0.4956592  -1.0985113  -0.03428674 -0.12651502 -0.11015408 -0.3233318\n",
            " -1.1805096 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 46 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.34081158 -0.4995645  -0.44295087 -0.8228926  -1.1531475  -0.29748553\n",
            " -0.5038512  -1.2010982  -0.8902961  -1.0683908  -1.0387095  -0.86627805\n",
            "  0.7613249  -1.1769559  -0.59202397  0.10157393 -0.20855199 -0.34290603\n",
            " -1.2304971 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 47 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.521215   -0.42544436 -0.5287661  -0.90711164 -1.1771586  -0.30794585\n",
            " -0.33139297 -1.3010486  -0.89546484 -1.1644571  -1.0792826  -0.89766395\n",
            "  0.5028682  -1.2047148  -0.22390467  0.09093931 -0.11244545 -0.33210376\n",
            " -1.2531705 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.525786   -0.25583977 -0.68587816 -0.9250421  -1.1709511  -0.3595312\n",
            " -0.4159334  -1.3441104  -1.0252758  -1.1388658  -1.0816351  -0.89138204\n",
            "  1.1797184  -1.2517     -0.49047288 -0.03261787 -0.15981974 -0.20589054\n",
            " -1.3420545 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.56326205 -0.37115172 -0.54802877 -0.8687565  -1.0929787  -0.2726072\n",
            " -0.33297428 -1.2446965  -0.8555016  -1.0865325  -0.9736563  -0.86307937\n",
            "  0.45115718 -1.1286403  -0.12015408  0.05199363 -0.2795565  -0.3186145\n",
            " -1.2311903 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4683578  -0.3060668  -0.63487905 -0.75819135 -1.0321238  -0.19923083\n",
            " -0.37808895 -1.080889   -1.0342399  -0.84959835 -0.8675124  -0.77445173\n",
            "  0.7730071  -0.99683625 -0.2633592  -0.06488293 -0.18029529 -0.12461415\n",
            " -1.1514074 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 51 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.32597205 -0.57474357 -0.5838249  -0.8927046  -1.140048   -0.30769816\n",
            " -0.53403974 -1.2604663  -1.0261903  -1.135936   -1.0483811  -0.9058643\n",
            "  0.5101489  -1.0991874  -0.20961094 -0.11313263 -0.20576593 -0.28863966\n",
            " -1.1808954 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 52 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.38816652 -0.36038223 -0.6905812  -0.8387394  -1.1337247  -0.25740916\n",
            " -0.47771257 -1.1825227  -0.91320914 -1.0588738  -0.94201964 -0.82568115\n",
            "  0.93415916 -1.1609548  -0.5997661   0.05356361 -0.3578245  -0.2255586\n",
            " -1.3160098 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 53 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.40440428 -0.43799293 -0.6047697  -0.8378009  -1.0279171  -0.25118524\n",
            " -0.46642965 -1.1075033  -0.76629865 -0.98867446 -0.8584963  -0.7764514\n",
            "  0.33464998 -0.8992352  -0.04541805 -0.2215098  -0.34878966 -0.23823875\n",
            " -1.0291443 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 54 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4839618  -0.2640318  -0.83732325 -0.8514159  -1.0988989  -0.3577191\n",
            " -0.38734654 -1.2858521  -0.9581392  -1.0377551  -1.0045718  -0.8764426\n",
            "  1.1923687  -1.1503037  -0.4976437  -0.00145169 -0.27347845 -0.15231\n",
            " -1.2458107 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 55 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.41501606 -0.556879   -0.5740521  -0.9219246  -1.0973828  -0.2285495\n",
            " -0.44592106 -1.1168854  -0.78095716 -1.0817392  -0.92039907 -0.89002734\n",
            "  0.16545844 -1.0726565  -0.05363541 -0.04889059 -0.47624412 -0.40553397\n",
            " -1.1980654 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 56 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.36542484 -0.32997036 -0.63738644 -0.71662927 -1.0741305  -0.20091595\n",
            " -0.2943832  -1.1492208  -0.73337215 -1.0338169  -0.93035257 -0.79822785\n",
            "  0.80282736 -1.0276291  -0.5401003   0.05589699 -0.29714137 -0.31933114\n",
            " -1.284801  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 57 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.38120323 -0.2964935  -0.58912796 -0.6339638  -0.84956455 -0.13748911\n",
            " -0.28956893 -0.86805654 -1.0153453  -0.7277453  -0.78593963 -0.6848161\n",
            "  0.6326521  -0.8015173  -0.36246648 -0.08339223 -0.26584852 -0.08635405\n",
            " -1.0149208 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 58 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.2700669e-01 -2.0458731e-01 -8.2407874e-01 -8.3396298e-01\n",
            " -1.1109393e+00 -3.3458808e-01 -2.4747702e-01 -1.2934471e+00\n",
            " -7.5987291e-01 -1.1477544e+00 -1.0279552e+00 -8.7431192e-01\n",
            "  9.4748521e-01 -1.2130036e+00 -4.0999359e-01  9.7925123e-04\n",
            " -2.1884379e-01 -1.5650053e-01 -1.3335154e+00]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 59 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.34578004 -0.3823556  -0.58950377 -0.7419452  -0.8617375  -0.13480133\n",
            " -0.4822136  -0.9776565  -0.9239133  -0.85735846 -0.84037167 -0.764311\n",
            "  0.23575509 -0.91941077 -0.09215971 -0.0306571  -0.39975673 -0.21428423\n",
            " -1.024839  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 60 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.3394755  -0.3573463  -0.6634076  -0.7390087  -0.95926565 -0.24092318\n",
            " -0.43211514 -1.0617058  -0.5243903  -1.0123852  -0.8937934  -0.76044446\n",
            "  0.4684876  -0.84468806 -0.0983502  -0.17987645 -0.17918402 -0.11537397\n",
            " -0.97060996]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 61 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.32313642 -0.3471686  -0.52313155 -0.6776817  -0.796334   -0.1311882\n",
            " -0.41831592 -0.93557084 -0.7864653  -0.7688613  -0.7659188  -0.6903979\n",
            "  0.26666653 -0.7769574  -0.13507636  0.02075714 -0.4365334  -0.13040356\n",
            " -0.97969115]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 62 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.33663347 -0.36095062 -0.6579892  -0.744959   -0.9038689  -0.2151021\n",
            " -0.5183938  -1.0241721  -0.56946504 -0.9773905  -0.8246162  -0.70345193\n",
            "  0.31345293 -0.7324231  -0.10586124 -0.23117714 -0.22414394 -0.06209065\n",
            " -0.9475532 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 63 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.29099408 -0.29794243 -0.68424    -0.7009351  -0.8649235  -0.23691727\n",
            " -0.47452545 -0.9786434  -0.7426756  -0.83845174 -0.7839842  -0.694915\n",
            "  0.65382147 -0.7640959  -0.33880585  0.03164747 -0.37431696 -0.1253562\n",
            " -0.92014956]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 64 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.31772107 -0.39464366 -0.68232614 -0.7593747  -0.9463047  -0.2491511\n",
            " -0.46949574 -1.023908   -0.52914643 -1.0309802  -0.8834552  -0.77391523\n",
            "  0.43404016 -0.72223765 -0.14613107 -0.10347985 -0.25901654 -0.03976421\n",
            " -0.9691675 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 65 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.41109088 -0.40595984 -0.601904   -0.79114985 -0.8797584  -0.17427294\n",
            " -0.37630358 -1.0059676  -0.82487243 -0.8714263  -0.8761532  -0.8206047\n",
            "  0.24387923 -0.8001608  -0.0501532   0.05509493 -0.3617361  -0.14544486\n",
            " -1.0379983 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 66 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.37348855 -0.38144183 -0.7074498  -0.7630794  -0.9843199  -0.24319412\n",
            " -0.4539989  -1.0887593  -0.52972883 -1.0288821  -0.917989   -0.787393\n",
            "  0.4180776  -0.64450216 -0.11135251 -0.2451984  -0.17524062 -0.01433593\n",
            " -1.002866  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 67 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.37259594 -0.3816944  -0.54958665 -0.72700214 -0.8320086  -0.1567806\n",
            " -0.39459127 -0.94295454 -0.82475555 -0.80645204 -0.7929346  -0.7639408\n",
            "  0.20496736 -0.6895673  -0.1497851  -0.01446912 -0.3318797  -0.13876235\n",
            " -1.0107193 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 68 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.32517642 -0.30381992 -0.535159   -0.6435418  -0.7722418  -0.18106666\n",
            " -0.22122689 -0.8887998  -0.45647606 -0.90346754 -0.78411996 -0.6944856\n",
            "  0.1335989  -0.6637656  -0.19321813  0.21968502 -0.25682124 -0.2530684\n",
            " -1.0375947 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 69 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.21776837 -0.30732048 -0.5117182  -0.54405946 -0.7532894  -0.16573232\n",
            " -0.40647385 -0.79809815 -0.4729108  -0.6813622  -0.6348823  -0.58849454\n",
            "  0.35818794 -0.58556265 -0.21647996 -0.06250267 -0.25926813 -0.048324\n",
            " -0.8487893 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 70 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.30186054 -0.31993696 -0.5928282  -0.662962   -0.8792205  -0.18419167\n",
            " -0.48412883 -0.8984558  -0.46898553 -0.912493   -0.82516414 -0.66544265\n",
            "  0.19104835 -0.5078833   0.02811205 -0.25898093 -0.03105132 -0.03056058\n",
            " -0.8777855 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 71 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.21971948 -0.1735948  -0.3876152  -0.45267943 -0.5686051  -0.14836328\n",
            " -0.28643265 -0.63807803 -0.36208594 -0.51893103 -0.52001715 -0.4441097\n",
            "  0.21500854 -0.32725486 -0.12760799 -0.00313601 -0.1759327  -0.05207649\n",
            " -0.67350346]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 72 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.39923903 -0.21670502 -0.5451274  -0.6082427  -0.81333834 -0.20292674\n",
            " -0.3109134  -0.91025066 -0.42066178 -0.8812823  -0.82124907 -0.65806794\n",
            "  0.23461084 -0.41356093 -0.0557201   0.07642969 -0.02211363 -0.13367787\n",
            " -0.9372107 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 73 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.21756876 -0.17124578 -0.3861062  -0.44916198 -0.5635817  -0.1469751\n",
            " -0.28327945 -0.633351   -0.35960677 -0.51410806 -0.51591307 -0.4412344\n",
            "  0.21519493 -0.30655318 -0.1257189   0.02310792 -0.12927972 -0.04830148\n",
            " -0.66785794]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 74 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.30859077 -0.22835594 -0.4133244  -0.58141047 -0.67603505 -0.14449759\n",
            " -0.19761491 -0.67353636 -0.30959332 -0.7529993  -0.68348277 -0.58311844\n",
            " -0.00182338 -0.45386738 -0.04571259  0.3714742  -0.06192095 -0.16252403\n",
            " -0.8602075 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 75 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.16696462 -0.28131282 -0.3914844  -0.50533265 -0.6908614  -0.12462356\n",
            " -0.44240212 -0.648526   -0.44712156 -0.6095731  -0.5621738  -0.5145408\n",
            "  0.05863974 -0.52757543 -0.07089508 -0.06279518 -0.12850448 -0.07130848\n",
            " -0.72277176]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 76 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.32429785 -0.33731428 -0.6051088  -0.691323   -0.87810963 -0.19134367\n",
            " -0.48539388 -0.90724003 -0.49395883 -0.8978696  -0.817101   -0.68572074\n",
            "  0.1546109  -0.44614378  0.02355153 -0.23947659  0.16254348  0.01778546\n",
            " -0.8835949 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 77 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.23940463 -0.35470304 -0.37024066 -0.5265982  -0.7537758  -0.15639934\n",
            " -0.42047742 -0.66974896 -0.3270861  -0.6898392  -0.673179   -0.57428026\n",
            "  0.00400634 -0.58261245 -0.06422582 -0.0202257  -0.11792321 -0.05818908\n",
            " -0.81610143]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 78 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3831028  -0.21303117 -0.5158762  -0.61017305 -0.7857248  -0.21751054\n",
            " -0.29163462 -0.86052245 -0.4010614  -0.86061376 -0.7950282  -0.63375795\n",
            "  0.22236913 -0.34841585 -0.10074361  0.20867689  0.1827013  -0.10348794\n",
            " -0.9122629 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 79 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.20044897 -0.37697726 -0.4045896  -0.6340793  -0.9137999  -0.24264756\n",
            " -0.35037395 -0.8985547  -0.61918336 -0.7976505  -0.77315134 -0.6446182\n",
            "  0.33549362 -0.5813459  -0.3841249   0.2098268   0.02843196 -0.1492272\n",
            " -0.97049   ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 80 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.24102871 -0.37697068 -0.5366626  -0.75974345 -0.9463982  -0.19152187\n",
            " -0.395271   -0.9264081  -0.5520669  -0.9871411  -0.7889972  -0.7089308\n",
            "  0.3100597  -0.5378906  -0.05805713  0.15890896  0.12582257 -0.00202657\n",
            " -1.024235  ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 81 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.28728282 -0.22137183 -0.5732723  -0.618803   -0.81679815 -0.18016095\n",
            " -0.33953288 -0.808018   -0.6105109  -0.7047158  -0.67205995 -0.5761261\n",
            "  0.4985137  -0.38592803 -0.36549744  0.0667593  -0.04483636  0.04518257\n",
            " -0.9715003 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 82 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.34312928 -0.27559522 -0.4843139  -0.71154267 -0.8875171  -0.20166723\n",
            " -0.18099828 -0.88901055 -0.47630006 -0.91600335 -0.79625154 -0.6981655\n",
            "  0.3498131  -0.43903205 -0.15808806  0.5470808   0.12637356 -0.10996969\n",
            " -1.0757599 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 83 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.19202274 -0.3604205  -0.50602126 -0.63348454 -0.97737515 -0.2659404\n",
            " -0.48232976 -1.0096083  -0.7077685  -0.8220985  -0.86073256 -0.6542106\n",
            "  0.48486188 -0.5637995  -0.20971273 -0.01817245  0.22928698 -0.06101664\n",
            " -0.91914016]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 84 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.34243113 -0.27497864 -0.48440444 -0.71082973 -0.88624036 -0.2013423\n",
            " -0.18004742 -0.88789445 -0.47567666 -0.9148572  -0.79538304 -0.6973549\n",
            "  0.3478908  -0.42879134 -0.15834975  0.57629013  0.16715752 -0.10679082\n",
            " -1.0741935 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 85 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.29550755 -0.13907161 -0.50693536 -0.54257554 -0.8437924  -0.15886907\n",
            " -0.33464867 -0.77802384 -0.65577644 -0.67954975 -0.6091822  -0.51907384\n",
            "  0.5943735  -0.27086058 -0.3451876  -0.0553759   0.03806597  0.05142228\n",
            " -0.93076426]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 86 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.40437642 -0.31650636 -0.5357191  -0.776451   -0.9926788  -0.2004812\n",
            " -0.25700858 -0.98705816 -0.5182909  -1.0264899  -0.901811   -0.7617243\n",
            "  0.28415743 -0.5368     -0.18214868  0.49560633  0.13088776 -0.14442737\n",
            " -1.1900411 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 87 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.12842661 -0.34534734 -0.40133768 -0.5635583  -0.868495   -0.21257015\n",
            " -0.41601637 -0.8316134  -0.68364066 -0.73398757 -0.6640576  -0.5807266\n",
            "  0.30874893 -0.5611746  -0.32183635  0.06858056  0.11059188 -0.09441127\n",
            " -0.85860115]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 88 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.32302466 -0.2941744  -0.67876935 -0.7532369  -1.0227964  -0.25127345\n",
            " -0.41460466 -1.0458264  -0.62380135 -1.0373238  -0.88007426 -0.73587316\n",
            "  0.5391278  -0.3018516  -0.13044842 -0.05439271  0.4016231   0.0791367\n",
            " -1.0782164 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 89 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.21499172 -0.14163372 -0.42406812 -0.47744215 -0.72148436 -0.14792724\n",
            " -0.26473576 -0.6505706  -0.5569607  -0.6151698  -0.5006947  -0.46163517\n",
            "  0.5206349  -0.21483025 -0.29079714 -0.07302874  0.05184916  0.06832404\n",
            " -0.7963775 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 90 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.42060396 -0.17872941 -0.63328457 -0.7088374  -0.96529543 -0.28239262\n",
            " -0.20907223 -1.0186608  -0.5541385  -0.96241045 -0.8870852  -0.7284407\n",
            "  0.5990094  -0.21955653 -0.18349348  0.27504805  0.41795042 -0.0154557\n",
            " -1.1121657 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 91 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.36022043 -0.17175263 -0.58489263 -0.6224161  -0.8114295  -0.21124023\n",
            " -0.30347002 -0.858154   -0.6166802  -0.69268507 -0.71116716 -0.57242185\n",
            "  0.5907627  -0.24285051 -0.36215395 -0.00985707  0.1464413   0.08127484\n",
            " -0.9769971 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 92 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.26985526 -0.33712614 -0.62161034 -0.7823106  -0.99420923 -0.22674143\n",
            " -0.39939517 -0.9576845  -0.6374756  -1.042452   -0.85184133 -0.7597224\n",
            "  0.35840952 -0.4968049  -0.11490772  0.17853785  0.31360778  0.05198795\n",
            " -1.1182464 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 93 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.10230217 -0.31800348 -0.3816404  -0.47780246 -0.8025569  -0.20921312\n",
            " -0.35533294 -0.7511393  -0.6150711  -0.65581787 -0.6024449  -0.53907716\n",
            "  0.3549752  -0.45662025 -0.29290953  0.04857979  0.1993716  -0.0430731\n",
            " -0.76739264]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 94 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.32807374 -0.28415948 -0.67291504 -0.7438281  -1.0224326  -0.27037534\n",
            " -0.40313455 -1.0224757  -0.59045863 -0.9913697  -0.87342364 -0.7398946\n",
            "  0.5463885  -0.29780576 -0.08535536 -0.13238464  0.54569775  0.15416722\n",
            " -1.0490727 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 95 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2197597  -0.09631581 -0.37873766 -0.44676697 -0.6852375  -0.17002268\n",
            " -0.21709602 -0.6567996  -0.57139194 -0.5825482  -0.5016264  -0.44593367\n",
            "  0.5542701  -0.16768992 -0.2688916   0.00155826  0.099427    0.04092465\n",
            " -0.80406463]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 96 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.31138918 -0.37559032 -0.62263346 -0.81970835 -1.0831087  -0.22286743\n",
            " -0.43168768 -1.0059284  -0.584679   -1.0993029  -0.9352216  -0.80883807\n",
            "  0.2759149  -0.5942504  -0.10365044  0.06251643  0.31334135  0.05681688\n",
            " -1.2040741 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 97 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3274626  -0.23010522 -0.53062665 -0.56645685 -0.8303305  -0.17787802\n",
            " -0.37627056 -0.82976186 -0.6521009  -0.7227687  -0.6461926  -0.5954733\n",
            "  0.42022187 -0.3431333  -0.35363978 -0.0469056   0.07456291  0.02750688\n",
            " -0.9773855 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 98 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.37255135 -0.25009453 -0.5573035  -0.7614616  -0.9925019  -0.24069397\n",
            " -0.25949514 -1.0061669  -0.55820763 -1.0652874  -0.9048045  -0.77245194\n",
            "  0.36428764 -0.4680022  -0.20902742  0.5092291   0.3035714  -0.08882368\n",
            " -1.2433202 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 99 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.35254326 -0.22000009 -0.59124386 -0.6445804  -0.8445131  -0.2299592\n",
            " -0.35215273 -0.8383922  -0.65772074 -0.7302079  -0.74548554 -0.60448664\n",
            "  0.53299165 -0.32354861 -0.30969638 -0.01937831  0.21607687  0.08530423\n",
            " -0.9690428 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 100 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.37806535 -0.29635122 -0.7465115  -0.79636127 -1.0464427  -0.2616042\n",
            " -0.48748556 -1.0837286  -0.7536613  -1.0312443  -0.90097296 -0.75969183\n",
            "  0.56059176 -0.30022648 -0.17326461 -0.18453711  0.5814002   0.17696737\n",
            " -1.1191993 ]\n",
            "Epsilon reduced to 0.06553600000000002\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.27950907 -0.2740011  -0.63025486 -0.69790006 -0.93739027 -0.25160617\n",
            " -0.2116565  -1.0136685  -0.6150658  -0.904789   -0.7659023  -0.73117936\n",
            "  0.73649204 -0.3494372  -0.4585281   0.34994975  0.16898093 -0.02912065\n",
            " -1.1324741 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5310848  -0.2373161  -0.95449054 -0.87024486 -1.2068787  -0.33837938\n",
            " -0.365686   -1.3822813  -0.9305987  -1.214616   -1.1068898  -0.9222209\n",
            "  1.1386415  -0.33089736 -0.5033844   0.34189683  0.41507414 -0.02054441\n",
            " -1.3655537 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.5504457e-01 -2.4708337e-01 -6.1357892e-01 -7.4713212e-01\n",
            " -1.0079790e+00 -3.1849465e-01 -2.0925522e-01 -1.1123705e+00\n",
            " -5.9596002e-01 -9.3915069e-01 -8.1650662e-01 -7.7756852e-01\n",
            "  8.5165596e-01 -2.9792753e-01 -4.0471649e-01  1.3569854e-01\n",
            "  2.4014270e-01 -7.6559302e-04 -1.1201975e+00]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4423924  -0.5016773  -0.569201   -0.92442256 -1.1857808  -0.25909126\n",
            " -0.3125768  -1.2812195  -1.0626856  -1.1915157  -1.0689111  -1.0140848\n",
            "  0.3613445  -0.7561222  -0.16340019  0.587041    0.42119578 -0.19807598\n",
            " -1.3612993 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.30260968 -0.29351464 -0.44794238 -0.6654966  -0.86582255 -0.17618611\n",
            " -0.3442904  -0.9779467  -0.62289023 -0.87144524 -0.59720224 -0.63552\n",
            "  0.5036325  -0.46285638 -0.3812925   0.24876265  0.27244002 -0.07496557\n",
            " -1.1275662 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3475678  -0.31875643 -0.39264873 -0.75294065 -1.0565457  -0.31702504\n",
            " -0.3519643  -1.166183   -0.69505334 -1.0929859  -0.88763493 -0.79314744\n",
            "  0.4898634  -0.43144333 -0.22527163  0.33213964  0.6661169  -0.13002068\n",
            " -1.1221504 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2912686  -0.2102736  -0.3350391  -0.5645301  -0.7772949  -0.19994725\n",
            " -0.29689062 -0.8783175  -0.48250014 -0.6937915  -0.43762958 -0.5600553\n",
            "  0.50768536 -0.19391997 -0.26936382 -0.02488165  0.43413305 -0.01099628\n",
            " -0.85843664]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.36306587 -0.27903658 -0.47146717 -0.73625314 -0.9912948  -0.21626168\n",
            " -0.4278589  -1.0152657  -0.693861   -0.9842411  -0.56983024 -0.71306425\n",
            "  0.5743317  -0.23164883 -0.17569155 -0.13735911  0.55038583  0.12711555\n",
            " -1.0640126 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.30823395 -0.21098259 -0.38880938 -0.6885324  -0.90516174 -0.30502334\n",
            " -0.25746354 -0.9341693  -0.5181336  -0.8718327  -0.43866092 -0.6809518\n",
            "  0.54969835 -0.28471345 -0.18914942 -0.07661743  0.4351606   0.04661022\n",
            " -1.0127879 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.37126502 -0.2751934  -0.43082553 -0.7504     -1.0015924  -0.23900023\n",
            " -0.44834995 -1.0176721  -0.68959856 -0.966279   -0.40956002 -0.7229546\n",
            "  0.56828296 -0.26339027 -0.16478507 -0.20694901  0.61459905  0.19331509\n",
            " -1.0586361 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.26508525 -0.2807673  -0.2672459  -0.574383   -0.78767806 -0.1619797\n",
            " -0.28699777 -0.8432473  -0.4494641  -0.75345623 -0.1672926  -0.5627832\n",
            "  0.3266722  -0.38433248 -0.23598878  0.17457907  0.28607354 -0.09348328\n",
            " -0.9553398 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3673741  -0.27296638 -0.3908959  -0.74298805 -0.99126023 -0.2361986\n",
            " -0.4434935  -1.0058442  -0.68266404 -0.9558191  -0.18065521 -0.7161629\n",
            "  0.56197536 -0.26042148 -0.16321513 -0.21022731  0.60515326  0.1916907\n",
            " -1.0481073 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.0657035e-01 -2.3024845e-01 -2.7427250e-01 -5.7871467e-01\n",
            " -7.9054618e-01 -2.0923799e-01 -2.7292028e-01 -8.4468132e-01\n",
            " -4.4394004e-01 -6.7512554e-01 -1.4136925e-02 -5.7251793e-01\n",
            "  4.6923244e-01 -2.0852321e-01 -2.3529589e-01 -9.4998628e-02\n",
            "  4.0992072e-01  1.9697053e-04 -8.3762151e-01]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3921161  -0.3314986  -0.23853306 -0.7282874  -1.0384569  -0.31377986\n",
            " -0.3554307  -1.1329224  -0.65678436 -1.0258366  -0.273389   -0.78746086\n",
            "  0.43284947 -0.4454275  -0.24689938  0.1692602   0.5851765  -0.08631097\n",
            " -1.1263044 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4249598e-01 -1.8139219e-01 -2.7806526e-01 -5.6152964e-01\n",
            " -8.2401216e-01 -2.1782577e-01 -2.8941655e-01 -8.3520788e-01\n",
            " -4.9361905e-01 -7.6659310e-01  1.0382178e-01 -6.0535258e-01\n",
            "  4.7429758e-01 -2.9353422e-01 -2.5728419e-01 -1.3442314e-01\n",
            "  2.3225735e-01 -5.3148635e-04 -9.5159489e-01]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2913446  -0.32080075 -0.32294983 -0.7681657  -0.94923043 -0.19358814\n",
            " -0.42507684 -0.9150315  -0.65889347 -0.97132236  0.21172753 -0.7206341\n",
            "  0.36177245 -0.451994   -0.18998158  0.0393834   0.41013125  0.13362654\n",
            " -1.0976504 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.24329299 -0.1899162  -0.17278281 -0.532017   -0.74941087 -0.18088588\n",
            " -0.1845997  -0.70093447 -0.43884078 -0.6474111   0.39832127 -0.55164325\n",
            "  0.39058965 -0.26282915 -0.30169952  0.11357702  0.23311327 -0.0329827\n",
            " -0.9016585 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.31734338 -0.3061878  -0.3594302  -0.6761589  -0.9517296  -0.23462498\n",
            " -0.4236854  -0.97247714 -0.5862958  -0.9138602   0.26884574 -0.7008582\n",
            "  0.4331798  -0.27845892 -0.19243923 -0.22888377  0.3422608   0.12462945\n",
            " -0.96557045]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.14929947 -0.25103724 -0.17983419 -0.37415555 -0.6386554  -0.10698414\n",
            " -0.3153416  -0.58427536 -0.35065144 -0.52066696  0.11579066 -0.40985346\n",
            "  0.14232355 -0.28523844 -0.2282058  -0.08493808  0.05379016 -0.01803049\n",
            " -0.59937197]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.22022393 -0.3291144  -0.21130614 -0.50462824 -0.7956137  -0.19003473\n",
            " -0.39207065 -0.77669805 -0.45275107 -0.67186254  0.14174642 -0.50591516\n",
            "  0.22681743 -0.45542467 -0.10948885 -0.01001999  0.10427444 -0.01365978\n",
            " -0.72954977]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.12419693 -0.24940383 -0.13271518 -0.34696484 -0.61346024 -0.09750979\n",
            " -0.279414   -0.5521444  -0.37916434 -0.47013757  0.02599444 -0.3399578\n",
            "  0.10403866 -0.31741002 -0.1703497  -0.04604618 -0.04173321 -0.08717638\n",
            " -0.5583222 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.15595639 -0.19016173 -0.12939215 -0.30225596 -0.47574383 -0.0933406\n",
            " -0.23331815 -0.46169364 -0.26163307 -0.4420259  -0.04381291 -0.3227041\n",
            "  0.00202399 -0.2608769  -0.146125   -0.05290752  0.13040487 -0.08861233\n",
            " -0.47590008]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.22018807 -0.29469126 -0.23247953 -0.5065334  -0.7169216  -0.09789182\n",
            " -0.3807671  -0.72123736 -0.45565644 -0.5995675  -0.00338669 -0.44039062\n",
            "  0.14855912 -0.4456343  -0.15856567 -0.091048   -0.06544014 -0.04898272\n",
            " -0.7244871 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2878623  -0.30276486 -0.2662722  -0.5748478  -0.9424792  -0.17467675\n",
            " -0.47517633 -0.9533596  -0.61876017 -0.7905541   0.05244443 -0.5526309\n",
            "  0.35791835 -0.5299297  -0.16846745 -0.01942793  0.03341581 -0.02279529\n",
            " -0.88379276]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.15941359 -0.22626099 -0.14511226 -0.33186004 -0.586897   -0.11307693\n",
            " -0.3425921  -0.5832005  -0.3744308  -0.5692573  -0.10435031 -0.37725723\n",
            "  0.04353321 -0.35667557 -0.12037653 -0.04582001 -0.00080276 -0.11115928\n",
            " -0.6006637 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.17877203 -0.2700503  -0.12373903 -0.3678177  -0.6344434  -0.12221254\n",
            " -0.25698012 -0.59412944 -0.42108166 -0.49939746 -0.10483087 -0.32760945\n",
            "  0.23805267 -0.28482816 -0.24481665 -0.10878491  0.0573349  -0.03969738\n",
            " -0.5980657 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.25954956 -0.34835577 -0.198523   -0.5923306  -0.87049323 -0.14063935\n",
            " -0.46518275 -0.9383892  -0.5036279  -0.7663331  -0.13755015 -0.54889196\n",
            "  0.18094513 -0.4693543  -0.22285971  0.02433351  0.05752591 -0.06602756\n",
            " -0.86244005]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.19802012 -0.26581478 -0.21578035 -0.50745076 -0.71518284 -0.1237629\n",
            " -0.3308374  -0.70554847 -0.5244935  -0.5817276  -0.14779732 -0.4410031\n",
            "  0.27168572 -0.37704828 -0.18452953 -0.01429266 -0.0183156  -0.00533634\n",
            " -0.7215306 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 29 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2256931  -0.2824433  -0.18889755 -0.43745625 -0.79149157 -0.16522111\n",
            " -0.36535498 -0.72663933 -0.43799862 -0.66178745 -0.16395755 -0.46531034\n",
            "  0.23710464 -0.3700719  -0.24029464 -0.08326706  0.07381815 -0.1019145\n",
            " -0.76990974]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 30 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.23683359 -0.34327602 -0.22221284 -0.5348134  -0.7061994  -0.19802727\n",
            " -0.37122488 -0.6446736  -0.39002723 -0.65050185 -0.2690075  -0.50702065\n",
            "  0.06337797 -0.47556534 -0.09202117 -0.03065654  0.03379322 -0.03053084\n",
            " -0.7134018 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 31 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.19304465 -0.25804135 -0.14485143 -0.36949053 -0.61285436 -0.0901725\n",
            " -0.27950844 -0.5884628  -0.3801314  -0.4987157  -0.28744155 -0.3379817\n",
            "  0.29970896 -0.29053837 -0.15744257 -0.01775428 -0.0415948  -0.07007138\n",
            " -0.6188836 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 32 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.28467208 -0.35029313 -0.22237578 -0.6417908  -0.9452998  -0.13397744\n",
            " -0.45969737 -0.9989861  -0.63664436 -0.7939547  -0.4214856  -0.55020446\n",
            "  0.27324125 -0.50573653 -0.26628992 -0.01553003  0.05188909 -0.08204495\n",
            " -0.91679275]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 33 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2511244  -0.34870067 -0.26284283 -0.5591314  -0.7436071  -0.15123829\n",
            " -0.37859586 -0.6695334  -0.4191159  -0.66388565 -0.45939243 -0.5042753\n",
            "  0.03382434 -0.47207063 -0.22262783 -0.15793689 -0.02631707 -0.10582445\n",
            " -0.7570032 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 34 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.26465598 -0.32653475 -0.22244108 -0.49709627 -0.89146227 -0.16130269\n",
            " -0.4419705  -0.8334942  -0.5160343  -0.78572893 -0.5665689  -0.5346245\n",
            "  0.2302779  -0.48904118 -0.25095883 -0.160897    0.01015472 -0.11207956\n",
            " -0.8731619 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 35 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.31638083 -0.38416263 -0.2711962  -0.6314414  -0.8498683  -0.19350554\n",
            " -0.47060782 -0.83738226 -0.47493213 -0.7796188  -0.64124525 -0.5680392\n",
            "  0.12443542 -0.5612164  -0.14429092 -0.07809695 -0.05597847 -0.07739125\n",
            " -0.8552763 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2528128  -0.30004436 -0.1548672  -0.47664893 -0.81807643 -0.09634525\n",
            " -0.41040802 -0.8446931  -0.5843792  -0.67028546 -0.6913313  -0.42701975\n",
            "  0.2595563  -0.40335715 -0.21761978 -0.08292938 -0.06199206 -0.0586511\n",
            " -0.78262717]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 37 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.24122739 -0.36593953 -0.19300912 -0.53075045 -0.8037548  -0.18102157\n",
            " -0.40069246 -0.79424816 -0.42385727 -0.6953251  -0.6180831  -0.51328915\n",
            "  0.14309435 -0.43655446 -0.20298387 -0.04072928  0.04784412 -0.08651695\n",
            " -0.78431666]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 38 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.15220776 -0.28840292 -0.19997466 -0.46967602 -0.6470888  -0.13451777\n",
            " -0.39175883 -0.6121282  -0.4286096  -0.6004479  -0.6084552  -0.434705\n",
            "  0.07215997 -0.40813246 -0.17892095 -0.05084879  0.06179723 -0.03303992\n",
            " -0.666561  ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 39 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.1799179  -0.21883392 -0.13339481 -0.433475   -0.74290186 -0.12828656\n",
            " -0.36016843 -0.7810166  -0.54031104 -0.6394906  -0.6877285  -0.40753925\n",
            "  0.2995654  -0.32509103 -0.24149299 -0.03997737  0.02516935 -0.03241388\n",
            " -0.7397436 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 40 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.32317966 -0.37533388 -0.27132204 -0.6508392  -0.86312807 -0.19499628\n",
            " -0.40681955 -0.8239475  -0.5228004  -0.73833734 -0.93781394 -0.55762047\n",
            "  0.18244976 -0.5239889  -0.1735348  -0.05184681 -0.05782843 -0.06332235\n",
            " -0.8414786 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 41 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.1981701  -0.2868009  -0.1438283  -0.3460862  -0.5903469  -0.11741969\n",
            " -0.24276982 -0.5275364  -0.27178618 -0.4591934  -0.6691048  -0.3425008\n",
            "  0.17564458 -0.27900818 -0.19610475 -0.08469062 -0.07326394 -0.08293092\n",
            " -0.5956592 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 42 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3154468  -0.4075329  -0.20976429 -0.583659   -0.85340095 -0.18348597\n",
            " -0.39822125 -0.8580937  -0.41730088 -0.7333122  -0.8872013  -0.5400716\n",
            "  0.17415467 -0.44964856 -0.28482234 -0.10985472  0.03769545 -0.08785539\n",
            " -0.8445418 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 43 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.20269135 -0.28221074 -0.20724194 -0.5637858  -0.75022876 -0.09038486\n",
            " -0.43949205 -0.73796064 -0.37630624 -0.71982765 -0.917965   -0.4974177\n",
            "  0.01506217 -0.4676713  -0.13626829 -0.09659515  0.07677402 -0.09533992\n",
            " -0.74613714]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 44 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.22718741 -0.24996689 -0.20237917 -0.47586462 -0.84339345 -0.10324126\n",
            " -0.38141537 -0.8189541  -0.6530845  -0.6954515  -0.9804649  -0.45571172\n",
            "  0.38933417 -0.4018701  -0.2542399  -0.04481997  0.01909598 -0.07189617\n",
            " -0.81050694]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 45 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.24929991 -0.39549813 -0.2196537  -0.5904651  -0.84886277 -0.20526235\n",
            " -0.45007905 -0.8354501  -0.47007158 -0.78223544 -1.02591    -0.5556549\n",
            "  0.07259554 -0.54039603 -0.14365518 -0.06853292 -0.03260334 -0.11680127\n",
            " -0.8196845 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 46 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.11731319 -0.25954723 -0.10618293 -0.3375063  -0.5519113  -0.10289883\n",
            " -0.27902952 -0.53929484 -0.29463586 -0.45998898 -0.75887287 -0.31748885\n",
            "  0.09840886 -0.2643778  -0.17433327 -0.03301747 -0.08746496 -0.0740139\n",
            " -0.5520833 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 47 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.29409733 -0.37995726 -0.22453457 -0.6178176  -0.89437354 -0.15828\n",
            " -0.44974086 -0.946632   -0.5580089  -0.7667935  -1.0867794  -0.55418295\n",
            "  0.27985486 -0.48026022 -0.28239837 -0.0432865   0.06024381 -0.05042942\n",
            " -0.8892182 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.21809559 -0.25277975 -0.2102493  -0.52170587 -0.7050202  -0.08944748\n",
            " -0.4051658  -0.71434945 -0.48745865 -0.639211   -0.9918836  -0.4425278\n",
            "  0.10528872 -0.42162535 -0.17332675 -0.03993692  0.07089756 -0.02498383\n",
            " -0.7122413 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.9665971e-01 -2.7620462e-01 -1.2168013e-01 -3.8464424e-01\n",
            " -6.8328738e-01 -1.4601851e-01 -2.9146326e-01 -6.6201019e-01\n",
            " -4.1233170e-01 -5.8748251e-01 -8.7528145e-01 -3.7937573e-01\n",
            "  2.4907979e-01 -2.9741067e-01 -2.3067886e-01 -1.0583850e-01\n",
            "  4.6300236e-04 -7.4529901e-02 -6.6450489e-01]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.5730205e-01 -3.8756803e-01 -1.8656367e-01 -6.2951905e-01\n",
            " -8.3395118e-01 -1.9647081e-01 -4.4041553e-01 -8.3224708e-01\n",
            " -3.7045449e-01 -8.0434304e-01 -1.1398063e+00 -5.7996172e-01\n",
            "  8.8917986e-03 -5.1315314e-01 -9.9390619e-02 -3.7415214e-02\n",
            " -9.0033468e-04 -8.5246108e-02 -7.9932725e-01]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 51 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.18942998 -0.27050582 -0.14682671 -0.3943585  -0.6637121  -0.09907812\n",
            " -0.30965096 -0.6042291  -0.36940974 -0.5328149  -0.9207191  -0.3642693\n",
            "  0.19225487 -0.31489232 -0.177726   -0.06240546 -0.06778253 -0.09806139\n",
            " -0.6340403 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 52 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.29369986 -0.32497665 -0.23457426 -0.63860595 -0.9591803  -0.144132\n",
            " -0.48334163 -0.99604917 -0.6629456  -0.81465936 -1.2434362  -0.55351716\n",
            "  0.28443107 -0.5238498  -0.27089518 -0.0258223  -0.02359817 -0.07562546\n",
            " -0.95593125]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 53 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.25060585 -0.3290994  -0.24246714 -0.5238521  -0.7460927  -0.1709524\n",
            " -0.43735263 -0.70235616 -0.3729907  -0.71983564 -1.0364903  -0.5078875\n",
            "  0.0209696  -0.52418804 -0.08306899 -0.11238365 -0.01060965 -0.10015001\n",
            " -0.7566888 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 54 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.18425076 -0.26204655 -0.15091747 -0.42975    -0.7704764  -0.13810222\n",
            " -0.42160675 -0.78840846 -0.46617675 -0.67436355 -1.0057075  -0.44280297\n",
            "  0.18510775 -0.38619295 -0.25503665 -0.10463415 -0.0266202  -0.06472968\n",
            " -0.7758988 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 55 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4614087e-01 -3.8996926e-01 -2.1872795e-01 -6.3559949e-01\n",
            " -9.1416681e-01 -1.9792520e-01 -5.2073854e-01 -9.7766787e-01\n",
            " -4.9311465e-01 -8.6850357e-01 -1.1782281e+00 -5.8450204e-01\n",
            "  7.9149388e-02 -5.5673987e-01 -1.7343818e-01  5.3584948e-04\n",
            " -5.1828519e-02 -9.5561467e-02 -9.0233469e-01]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 56 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2576353  -0.34308466 -0.23737006 -0.5157135  -0.81907016 -0.12203757\n",
            " -0.34431207 -0.7259664  -0.5142804  -0.626009   -1.147274   -0.45425358\n",
            "  0.25091788 -0.4025134  -0.26490703 -0.10592133 -0.12056339 -0.03776607\n",
            " -0.7415216 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 57 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3659615e-01 -3.1207597e-01 -1.9711459e-01 -4.5359406e-01\n",
            " -7.9136336e-01 -1.6412123e-01 -4.0147626e-01 -7.5310898e-01\n",
            " -4.4212145e-01 -6.5913248e-01 -9.8880094e-01 -4.7740015e-01\n",
            "  2.1112147e-01 -3.9186123e-01 -2.3812853e-01 -8.6038820e-02\n",
            " -6.2025338e-04 -6.7915447e-02 -7.6207846e-01]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 58 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.18628457 -0.30517122 -0.19258948 -0.48469073 -0.6613566  -0.13978061\n",
            " -0.40075606 -0.6321546  -0.44696358 -0.6216775  -0.886706   -0.44047958\n",
            "  0.0819558  -0.419809   -0.17494452 -0.0712414   0.0331628  -0.01904613\n",
            " -0.6745439 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 59 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.15409702 -0.1948154  -0.12363565 -0.3694861  -0.68662745 -0.13450584\n",
            " -0.3062321  -0.6967728  -0.4751388  -0.58029884 -0.84085834 -0.3785985\n",
            "  0.3639439  -0.28138554 -0.20606467 -0.02700033  0.01245943 -0.03744522\n",
            " -0.68403715]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 60 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.26347306 -0.38063252 -0.22748707 -0.6544299  -0.9231987  -0.18967682\n",
            " -0.44529542 -0.9512524  -0.579279   -0.8208853  -1.1190246  -0.56622255\n",
            "  0.1521959  -0.51995003 -0.19999197  0.03640782 -0.05414955 -0.06847584\n",
            " -0.88052815]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 61 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2386737  -0.3377504  -0.17725204 -0.4719088  -0.7359606  -0.13273764\n",
            " -0.3475266  -0.6941847  -0.42628518 -0.6052312  -0.96347195 -0.43870893\n",
            "  0.12853745 -0.42321023 -0.2054902  -0.1399487  -0.06741439 -0.09919687\n",
            " -0.7480164 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 62 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.25327972 -0.33184302 -0.19148013 -0.5683653  -0.867733   -0.14529203\n",
            " -0.44663548 -0.8672929  -0.41965324 -0.7869749  -1.0156814  -0.53806746\n",
            "  0.10627833 -0.490742   -0.2293384  -0.05554303 -0.01542968 -0.12457876\n",
            " -0.86401063]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 63 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.24785732 -0.30703735 -0.21567255 -0.5869879  -0.79137397 -0.14739183\n",
            " -0.46792468 -0.8032294  -0.32364997 -0.76580435 -0.9734584  -0.53371346\n",
            "  0.01895617 -0.49083325 -0.08409584 -0.06904658  0.0019289  -0.10181204\n",
            " -0.76831526]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 64 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2407965  -0.27040097 -0.13039751 -0.44489518 -0.79050267 -0.09953767\n",
            " -0.3407853  -0.8098547  -0.5515003  -0.63940185 -0.94124985 -0.3819993\n",
            "  0.32173607 -0.35394076 -0.23361047 -0.06212451 -0.04708584 -0.05568684\n",
            " -0.737745  ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 65 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.27824178 -0.40636396 -0.24840148 -0.63289267 -0.9333514  -0.16017537\n",
            " -0.49707463 -0.9239058  -0.42610574 -0.89278835 -1.072172   -0.59494776\n",
            "  0.01772191 -0.5878374  -0.208598   -0.11973314 -0.07101047 -0.14566118\n",
            " -0.9391449 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 66 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.22699462 -0.34042516 -0.19522208 -0.4860793  -0.69904685 -0.15938093\n",
            " -0.33186138 -0.65685153 -0.35631213 -0.6012301  -0.87540156 -0.4504048\n",
            "  0.08102781 -0.37895873 -0.24699207 -0.13345534 -0.09717315 -0.04612993\n",
            " -0.70461273]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 67 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2837048  -0.32809055 -0.19732942 -0.5706292  -0.88554645 -0.14356178\n",
            " -0.45902118 -0.8883537  -0.4263337  -0.76867366 -0.9875076  -0.5486604\n",
            "  0.1548813  -0.45625013 -0.2554989  -0.08955128 -0.03249435 -0.09120122\n",
            " -0.84946156]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 68 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.24880427 -0.29957762 -0.23713775 -0.58375823 -0.7878076  -0.15488054\n",
            " -0.41493705 -0.77168524 -0.5070552  -0.7103399  -0.88547367 -0.5008953\n",
            "  0.16861442 -0.46601573 -0.13948552 -0.0180504   0.00670324 -0.01944164\n",
            " -0.7538889 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 69 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.12753986 -0.23196891 -0.07887869 -0.3253798  -0.61727685 -0.09804308\n",
            " -0.25473237 -0.6072098  -0.39915666 -0.48733702 -0.7197094  -0.31696913\n",
            "  0.28626284 -0.25858474 -0.15847617 -0.03488242  0.00799655 -0.06304245\n",
            " -0.5997556 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 70 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.21309972 -0.35396585 -0.18966949 -0.5100246  -0.7491969  -0.17516805\n",
            " -0.38120767 -0.7324999  -0.38376555 -0.6724134  -0.79585904 -0.48685625\n",
            "  0.08262856 -0.41581032 -0.19224449 -0.04697447 -0.02056505 -0.08814131\n",
            " -0.7280889 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 71 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.16406332 -0.3020134  -0.14843392 -0.43587264 -0.65658677 -0.11332602\n",
            " -0.32075822 -0.626184   -0.35116825 -0.5399612  -0.76954114 -0.38986602\n",
            "  0.12871392 -0.3518525  -0.17134102 -0.05578253 -0.07100929 -0.06662915\n",
            " -0.66415703]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 72 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2553324  -0.2690573  -0.20326403 -0.5425379  -0.91129863 -0.1294792\n",
            " -0.4353597  -0.89887464 -0.64386284 -0.76123434 -0.9546761  -0.4978702\n",
            "  0.2988307  -0.44375917 -0.2544825  -0.0739004  -0.01687073 -0.09423423\n",
            " -0.87020993]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 73 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.25043982 -0.33625507 -0.21295108 -0.5611903  -0.7592857  -0.165949\n",
            " -0.43862963 -0.7065405  -0.41631502 -0.7462306  -0.87243694 -0.51052624\n",
            "  0.06925499 -0.5442917  -0.06534931 -0.04738892 -0.03804117 -0.09624282\n",
            " -0.783797  ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 74 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.168611   -0.2665565  -0.12257873 -0.3618705  -0.6589278  -0.09345732\n",
            " -0.3142336  -0.6447872  -0.34912378 -0.5414384  -0.74125886 -0.3515185\n",
            "  0.17454317 -0.31300735 -0.21187286 -0.0717295  -0.02558775 -0.08666637\n",
            " -0.64045906]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 75 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.24485734 -0.3664638  -0.1788006  -0.569059   -0.8354729  -0.17280278\n",
            " -0.46367976 -0.8857885  -0.52771735 -0.7749048  -0.78640026 -0.52658015\n",
            "  0.16569725 -0.47727785 -0.18071233 -0.03160815  0.0373969  -0.06248273\n",
            " -0.85050946]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 76 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.29146528 -0.3053917  -0.27391395 -0.6006316  -0.80341685 -0.11936395\n",
            " -0.40392196 -0.77207804 -0.47211254 -0.73933226 -0.9160022  -0.5199485\n",
            "  0.17057976 -0.49372956 -0.22370882 -0.13746732 -0.05351903 -0.03427051\n",
            " -0.8525674 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 77 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2808233  -0.28778842 -0.19236316 -0.5189786  -0.8363135  -0.14700232\n",
            " -0.43586373 -0.8289386  -0.553748   -0.7207138  -0.8666354  -0.48124835\n",
            "  0.2682283  -0.43423864 -0.26027837 -0.10322865 -0.00614834 -0.07922652\n",
            " -0.81525725]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 78 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.20673941 -0.2974605  -0.18803926 -0.47422844 -0.6335525  -0.18201406\n",
            " -0.33467066 -0.5940781  -0.3428499  -0.6087246  -0.67741996 -0.45584697\n",
            "  0.08079512 -0.42521688 -0.04792903 -0.00090586  0.0367533  -0.03049854\n",
            " -0.6475951 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 79 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.13541943 -0.2532101  -0.09797657 -0.3596329  -0.6373875  -0.09036308\n",
            " -0.28539297 -0.61928606 -0.39928743 -0.4970545  -0.7315542  -0.34110123\n",
            "  0.2760725  -0.28782558 -0.16679808 -0.04257014  0.00543131 -0.06679562\n",
            " -0.6212103 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 80 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.26207024 -0.35154423 -0.21334012 -0.6148121  -0.90119416 -0.15691286\n",
            " -0.4364127  -0.942335   -0.5860925  -0.7829908  -0.8567093  -0.53733706\n",
            "  0.19807331 -0.47110617 -0.23177077  0.01932991 -0.01401286 -0.06487074\n",
            " -0.8797249 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 81 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.15869421 -0.25632215 -0.16547859 -0.43034217 -0.62033916 -0.1006481\n",
            " -0.35151523 -0.5581497  -0.39699984 -0.5450464  -0.7448894  -0.3984614\n",
            "  0.0820713  -0.37825084 -0.14521806 -0.10452611 -0.04853676 -0.06545738\n",
            " -0.63738286]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 82 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.20746216 -0.26732028 -0.15528907 -0.455233   -0.77716744 -0.13685922\n",
            " -0.41099986 -0.76326215 -0.48414528 -0.6667931  -0.82411915 -0.44532502\n",
            "  0.14547285 -0.3970199  -0.23968689 -0.06722079 -0.01540167 -0.10219567\n",
            " -0.7542764 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 83 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.22540952 -0.33255598 -0.20567457 -0.55687547 -0.7466789  -0.17426413\n",
            " -0.4471334  -0.7219606  -0.4246639  -0.7385791  -0.8350727  -0.50477767\n",
            " -0.00461818 -0.5349917  -0.09436526 -0.03058697 -0.04696964 -0.07887229\n",
            " -0.7611176 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 84 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.21046631 -0.26932567 -0.1645339  -0.4388506  -0.7682825  -0.0926393\n",
            " -0.34224585 -0.7585394  -0.5436347  -0.61356556 -0.8590075  -0.38524914\n",
            "  0.36677176 -0.3647368  -0.24344476 -0.04072199 -0.03176408 -0.06707417\n",
            " -0.732694  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 85 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2439122  -0.34037295 -0.21401878 -0.5949381  -0.8862873  -0.15369077\n",
            " -0.48089045 -0.9253872  -0.53854126 -0.80033934 -0.868028   -0.5297182\n",
            "  0.26861885 -0.5138725  -0.22105923 -0.03551148  0.03506571 -0.09915048\n",
            " -0.87084424]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 86 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.11670641 -0.25814414 -0.15916394 -0.3646695  -0.5495772  -0.08999873\n",
            " -0.3235726  -0.5503634  -0.3452393  -0.44182554 -0.5996949  -0.34646222\n",
            "  0.10328232 -0.3039501  -0.12157784 -0.07936067  0.00541538 -0.03523444\n",
            " -0.5501175 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 87 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.2357938  -0.25958794 -0.16055398 -0.41984037 -0.6862945  -0.10317547\n",
            " -0.32479176 -0.6485475  -0.3498324  -0.5302916  -0.75363237 -0.38359383\n",
            "  0.24760978 -0.31234112 -0.18906493 -0.12298803 -0.02411721 -0.08269289\n",
            " -0.69880956]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 88 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.25614223 -0.27412236 -0.24411479 -0.5001729  -0.67012525 -0.11594107\n",
            " -0.31592935 -0.6365003  -0.37354976 -0.5512016  -0.6931086  -0.41791454\n",
            "  0.21914999 -0.3847422  -0.07072388 -0.08326623 -0.04133847 -0.02215899\n",
            " -0.67553335]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 89 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.12757717 -0.21459132 -0.10121072 -0.27356005 -0.52653736 -0.09436408\n",
            " -0.1976527  -0.5141667  -0.2956882  -0.4046129  -0.58576    -0.27584922\n",
            "  0.29123044 -0.16143619 -0.13874598 -0.02559711  0.01695354 -0.06365661\n",
            " -0.5184188 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 90 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.19460547 -0.3145396  -0.21293971 -0.45931834 -0.64051527 -0.12158757\n",
            " -0.29584095 -0.57532567 -0.29186204 -0.5173996  -0.6720263  -0.3912664\n",
            "  0.09625404 -0.31330985 -0.15300986 -0.12683521 -0.08267955 -0.08617863\n",
            " -0.6375526 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 91 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.16507602 -0.26072335 -0.15970573 -0.41467872 -0.54262096 -0.0939739\n",
            " -0.27481905 -0.53095764 -0.25852817 -0.44452593 -0.55991286 -0.34709007\n",
            "  0.15608543 -0.23034883 -0.10845889 -0.06156578 -0.04444043 -0.06286178\n",
            " -0.59350866]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 92 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.23720405 -0.25995654 -0.22334059 -0.506346   -0.83220816 -0.09452948\n",
            " -0.4099031  -0.83652514 -0.55044454 -0.6700624  -0.72074926 -0.4449914\n",
            "  0.33387628 -0.31698978 -0.20095451 -0.16830292 -0.0539446  -0.06853937\n",
            " -0.8177501 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 93 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.17349519 -0.27245167 -0.18514405 -0.4860484  -0.6170597  -0.10362035\n",
            " -0.32938164 -0.5631009  -0.28117988 -0.5623363  -0.52496773 -0.39465803\n",
            "  0.11348509 -0.3351152  -0.03731773 -0.07471152 -0.03566729 -0.08922927\n",
            " -0.64296365]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 94 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.18815397 -0.2505987  -0.11303774 -0.35050267 -0.57466537 -0.09942866\n",
            " -0.25309137 -0.5765839  -0.3266418  -0.46701396 -0.5358266  -0.31876817\n",
            "  0.12564373 -0.17858006 -0.1706593  -0.12334762 -0.06377118 -0.0749488\n",
            " -0.57398194]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 95 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.17962322 -0.26771492 -0.1689209  -0.45872438 -0.6728476  -0.10119864\n",
            " -0.3674497  -0.70522404 -0.35246846 -0.55583924 -0.47992662 -0.3914464\n",
            "  0.20547825 -0.23053336 -0.14265585 -0.08609717 -0.03541901 -0.0534087\n",
            " -0.6787879 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 96 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.20944737 -0.2543666  -0.20900148 -0.46244013 -0.64299923 -0.08952846\n",
            " -0.3396274  -0.6685087  -0.43671268 -0.54259646 -0.40269005 -0.37799639\n",
            "  0.25995827 -0.24220815 -0.14563207 -0.10853188 -0.07216112 -0.04098174\n",
            " -0.6772498 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 97 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.26406422 -0.2722281  -0.18727107 -0.4370824  -0.6811706  -0.13159855\n",
            " -0.29538965 -0.64535224 -0.3285161  -0.5626364  -0.41704434 -0.41085303\n",
            "  0.21367694 -0.24849522 -0.20679118 -0.22452694  0.00577207 -0.0930308\n",
            " -0.7295398 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 98 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.12862796 -0.2409735  -0.16313644 -0.3808361  -0.49124318 -0.12301371\n",
            " -0.24573958 -0.44140506 -0.25715703 -0.41415298 -0.20978269 -0.32680488\n",
            "  0.13936323 -0.22389527 -0.03694846 -0.02412775 -0.00776055 -0.03251746\n",
            " -0.5083831 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 99 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.19155096 -0.22603038 -0.1355308  -0.34355968 -0.5691131  -0.09185476\n",
            " -0.2240393  -0.5610353  -0.28629524 -0.44977275 -0.29537246 -0.3099124\n",
            "  0.28455874 -0.10666143 -0.17160116 -0.12662658 -0.01132562 -0.07566147\n",
            " -0.56892157]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 100 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.26215777 -0.2869859  -0.22813602 -0.5152295  -0.8066231  -0.08793162\n",
            " -0.35026848 -0.81056947 -0.47278294 -0.6888229  -0.27164486 -0.4745578\n",
            "  0.25287756 -0.27978382 -0.1586261  -0.12483736 -0.09084922 -0.10048559\n",
            " -0.8035556 ]\n",
            "Epsilon reduced to 0.052428800000000025\n",
            "Total reward: -35\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVekR_eC2njZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_conv_21(env):\n",
        "    input_shape = env.observation_shape()\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape + (1, ), input_shape=input_shape))\n",
        "    model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(env.action_space.n))\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrPtOzJjaMgK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c26ef0b4-ce89-490f-81f1-a814fee2010f"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=100, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=5, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_21(env), max_epsilon=0.2, epsilon_decay=0.8)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_13 (Reshape)         (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_13 (Conv1D)           (None, 187, 32)           128       \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 5984)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 19)                113715    \n",
            "=================================================================\n",
            "Total params: 113,843\n",
            "Trainable params: 113,843\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |████████████████████--------------------------------------------------------------------------------| 20.0% \r\n",
            "Taking action 1\n",
            "\n",
            "Step 1 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 1.13929898e-01  1.24528576e-02 -1.16965346e-01  9.76524130e-02\n",
            " -1.74010053e-01 -2.81695817e-02  1.80050179e-01  3.91077623e-02\n",
            " -1.26572981e-01  3.73640098e-04 -2.63966359e-02 -3.42231407e-03\n",
            "  2.11468693e-02 -8.59040301e-03 -1.27852894e-04  7.99732730e-02\n",
            " -7.72626698e-02 -1.14312321e-01 -1.22597404e-02]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 2 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.03297601 -0.01168639 -0.01470037  0.05308904 -0.120624    0.06729389\n",
            "  0.09968036  0.02762607 -0.00842179  0.06769449  0.05993289 -0.09142628\n",
            "  0.00467935 -0.01033627 -0.02501957  0.03486304 -0.04964964 -0.04709039\n",
            "  0.09484926]\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 3 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 4.2685278e-02 -1.3155188e-01  1.5484503e-01 -1.0248830e-01\n",
            "  1.2235870e-03  1.3744283e-01  1.1205559e-01  1.4994838e-02\n",
            "  5.6161240e-02  1.2847376e-01  7.0280172e-02 -1.5039523e-01\n",
            " -5.1417764e-02  6.1444011e-02 -1.2050588e-04  2.2144255e-03\n",
            "  2.1906228e-01 -1.8236730e-01  1.9521694e-01]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 4 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.01540388 -0.20382902  0.12787601 -0.13233608  0.12002074  0.02255378\n",
            "  0.06559192  0.1313623  -0.10583469  0.04363301 -0.03766648 -0.15713367\n",
            " -0.09931073  0.06709852 -0.12360513 -0.07749375 -0.02782329 -0.23239014\n",
            " -0.09706482]\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 5 reward=-3 new_state=[1 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.06441819 -0.10162789 -0.05933502  0.06440049 -0.13726057  0.03361113\n",
            "  0.0157429   0.01311535 -0.07037935  0.12092873 -0.0172119  -0.06838931\n",
            "  0.00656792  0.00987546 -0.09150311  0.06549229 -0.06779085  0.02167816\n",
            "  0.01537524]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 6 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.06647291 -0.11121134  0.13027836 -0.04798951  0.0314545  -0.01993151\n",
            "  0.02871695  0.0051963   0.00102276  0.04442006 -0.00418216  0.00597797\n",
            " -0.10312229 -0.06852628 -0.2503573   0.13972509 -0.07077803 -0.18821655\n",
            "  0.03897913]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 7 reward=-3 new_state=[1 0 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 1.9328148e-03 -3.5069379e-01  6.7110837e-02 -5.3590678e-02\n",
            "  7.1180314e-02 -9.5344894e-03 -1.3498011e-01  7.3451161e-02\n",
            " -1.8603620e-01  3.9318442e-02 -1.0624743e-01 -7.9901911e-02\n",
            " -1.6309604e-01  3.9300948e-02 -2.1859366e-01  5.3773370e-02\n",
            " -1.0526655e-01 -2.3784399e-01 -3.2649795e-04]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 8 reward=-3 new_state=[1 0 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.05145168 -0.10166691 -0.02242366  0.04910674  0.00597628 -0.11522787\n",
            " -0.07081167 -0.06208162 -0.10959613  0.07033172 -0.143605   -0.00484161\n",
            " -0.04227765 -0.02586536 -0.14984165  0.06420362 -0.17947404 -0.1173768\n",
            "  0.05236225]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 9 reward=-3 new_state=[1 0 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1231727  -0.09269681 -0.02009544  0.07670506  0.01361404 -0.11079661\n",
            " -0.07813232 -0.07639744  0.0345376  -0.00218744 -0.08258378 -0.00409936\n",
            " -0.00497578 -0.06829078 -0.08154456  0.07734163 -0.10982426 -0.08452019\n",
            " -0.02394181]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 10 reward=-2 new_state=[0 0 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.15267423 -0.16646516 -0.01975707  0.02577451 -0.12256561 -0.02418834\n",
            " -0.11776634 -0.03664386 -0.00704364 -0.02066459 -0.10131633 -0.053727\n",
            "  0.01546516 -0.08623661 -0.17425919  0.02890576 -0.23353438 -0.14500879\n",
            " -0.01460288]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.09329141 -0.19140023 -0.00619669 -0.03817086 -0.04301681 -0.07574891\n",
            " -0.12218372 -0.21419251  0.05652964 -0.11541481 -0.13851699 -0.02712131\n",
            " -0.03070986 -0.15010665 -0.42380562  0.14013815 -0.00354504 -0.26312613\n",
            " -0.06904706]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03820739 -0.39194238 -0.0854936  -0.09688258 -0.20698562 -0.05875263\n",
            " -0.23993044 -0.10333145 -0.18902265 -0.06570629 -0.16780487 -0.08495525\n",
            " -0.08937611 -0.0521762  -0.38681275  0.03400666 -0.11551625 -0.3668337\n",
            " -0.12468608]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 13 reward=-2 new_state=[0 0 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0147871  -0.28698424 -0.23105861  0.05127895 -0.19274023 -0.0986709\n",
            " -0.06476742 -0.12402294 -0.08318397 -0.1306601  -0.07202969 -0.053223\n",
            " -0.0170994  -0.10500517 -0.3027035   0.02971103 -0.21366869 -0.2606489\n",
            " -0.07875643]\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 14 reward=-3 new_state=[0 1 0 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.07359523 -0.15860069 -0.08944385  0.01221373 -0.20860334 -0.03201278\n",
            " -0.12133866 -0.33742216  0.06454238 -0.11924728 -0.07681374 -0.04156058\n",
            "  0.01412225 -0.12426328 -0.4911025   0.02601309  0.04939556 -0.51694655\n",
            " -0.1050358 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 15 reward=-2 new_state=[0 1 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10337801 -0.42515635 -0.22421248  0.00392811 -0.21753016 -0.00286829\n",
            " -0.1419895  -0.21527846  0.09370851 -0.26790354 -0.03582838 -0.11905072\n",
            " -0.03565005 -0.09024488 -0.32797015  0.09043168  0.06763926 -0.49597463\n",
            " -0.2874354 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 16 reward=-2 new_state=[0 1 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.14785749 -0.47757548 -0.27747738 -0.01182282 -0.2748863   0.01233461\n",
            " -0.16727713 -0.27702668 -0.1053468  -0.33342475 -0.11006572 -0.03961527\n",
            " -0.01360642  0.01441717 -0.36179316  0.1250153  -0.05517912 -0.61201966\n",
            " -0.31124488]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 17 reward=-2 new_state=[0 1 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2637561  -0.5202348  -0.27391785 -0.21069002 -0.38706687  0.0284036\n",
            " -0.27015838 -0.47485408 -0.22127499 -0.3382048  -0.16461825 -0.03239643\n",
            " -0.12408754 -0.00872469 -0.59632796  0.0375215  -0.05179191 -1.0489041\n",
            " -0.26855606]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 18 reward=-2 new_state=[0 1 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.21304154 -0.34202334 -0.28792912 -0.10261276 -0.3266833  -0.11999451\n",
            " -0.13381581 -0.14816654 -0.18390527 -0.24426399 -0.10968657  0.04938662\n",
            " -0.0141408   0.01036041 -0.29322448 -0.00361886 -0.03758757 -0.78159964\n",
            " -0.20196283]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 1 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.48011136 -0.33286425 -0.1711906  -0.2673899  -0.37788263  0.00625245\n",
            " -0.30938265 -0.45669794 -0.2771181  -0.29858708 -0.1441483  -0.09931163\n",
            " -0.0900491  -0.06688065 -0.6321148  -0.02654029  0.04177297 -0.9234156\n",
            " -0.32273233]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 20 reward=-2 new_state=[0 1 0 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.6152882e-01 -2.2701313e-01 -1.7135245e-01 -1.8614663e-01\n",
            " -4.0931919e-01 -6.2724680e-04 -2.5970516e-01 -2.4448778e-01\n",
            " -1.9976254e-01 -1.6222860e-01 -1.2421545e-01 -5.2232835e-02\n",
            "  4.9249932e-02 -1.2953088e-01 -3.6939162e-01  6.5414675e-02\n",
            " -1.1648690e-01 -7.5449938e-01 -2.1735512e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 21 reward=-2 new_state=[0 1 0 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39275047 -0.4312087  -0.23966384 -0.2979993  -0.5548395  -0.04689694\n",
            " -0.31562746 -0.6100631  -0.32277608 -0.41681015 -0.13721845 -0.19927673\n",
            "  0.01884143 -0.24226451 -0.6929016   0.04494572  0.14131658 -1.4265808\n",
            " -0.29672462]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 22 reward=-2 new_state=[0 1 0 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4647524  -0.56666565 -0.43316263 -0.39384028 -0.5102097   0.03720466\n",
            " -0.36586615 -0.414798   -0.42681643 -0.46811637 -0.09630755 -0.2952383\n",
            " -0.15764995 -0.17116886 -0.59392273 -0.06484625 -0.05389822 -1.2807955\n",
            " -0.4611338 ]\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 23 reward=-3 new_state=[0 1 1 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.30091587 -0.2712532  -0.27443093 -0.13297841 -0.4622757  -0.04166558\n",
            " -0.22121438 -0.46326262 -0.37704527 -0.40063795  0.02867293 -0.29272386\n",
            "  0.13766177 -0.14968413 -0.39231148 -0.09805792  0.05541066 -1.1409088\n",
            " -0.33041388]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 24 reward=-3 new_state=[0 1 1 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.34867197 -0.36099976 -0.33070922 -0.13078944 -0.41566443 -0.02551493\n",
            " -0.20825464 -0.37405634 -0.33895177 -0.47825652 -0.11930097 -0.42473662\n",
            "  0.10155074 -0.05758226 -0.5062313  -0.05642904 -0.00308433 -1.2565132\n",
            " -0.31426686]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 25 reward=-2 new_state=[0 0 1 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.343327   -0.24011384 -0.32818547 -0.5211352  -0.52112406 -0.145259\n",
            " -0.38881695 -0.39215264 -0.3190806  -0.46755534 -0.17453644 -0.36995834\n",
            " -0.05966815 -0.1628588  -0.8052803  -0.01810443  0.00754199 -1.2740666\n",
            " -0.22669604]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 26 reward=-2 new_state=[0 0 1 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3931737  -0.28547493 -0.4453022  -0.41120237 -0.506866   -0.2511128\n",
            " -0.35024926 -0.31359658 -0.299618   -0.516331   -0.17114908 -0.38618755\n",
            " -0.11498861 -0.15343669 -0.61281246  0.00810699 -0.08658197 -1.2260263\n",
            " -0.32166356]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 27 reward=-2 new_state=[0 0 1 1 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4310775  -0.49833047 -0.7110435  -0.28396317 -0.72232956 -0.31244674\n",
            " -0.4783939  -0.70753735 -0.5328324  -0.68186224 -0.03102485 -0.4711502\n",
            " -0.10533939 -0.06520022 -0.8487544  -0.10721269 -0.07755493 -1.666105\n",
            " -0.34851873]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 28 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3769054  -0.4078173  -0.6314254  -0.29034647 -0.6424227  -0.2783907\n",
            " -0.35853505 -0.46334365 -0.5011963  -0.5344943  -0.03480972 -0.44112232\n",
            " -0.28937852 -0.04264138 -0.76223594 -0.03217144 -0.19069147 -1.4764067\n",
            " -0.22225541]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 29 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3755046  -0.41444832 -0.7605333  -0.45074496 -0.68953687 -0.35643598\n",
            " -0.3547726  -0.37343627 -0.46048895 -0.47648203 -0.23784326 -0.4831885\n",
            " -0.21373352 -0.00373767 -0.8629901  -0.09268974 -0.13461028 -1.320617\n",
            " -0.3371942 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 30 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4710014  -0.18441565 -0.4270795  -0.32812354 -0.4612166  -0.21177915\n",
            " -0.24244349 -0.26880923 -0.39384744 -0.474547   -0.24485607 -0.39017695\n",
            " -0.15777656 -0.08167221 -0.74393535  0.00446814 -0.02820482 -1.0563613\n",
            " -0.22081129]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 31 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9225857  -0.7420486  -0.92533845 -0.7655309  -0.8002895  -0.6289879\n",
            " -0.59196776 -0.84960604 -0.6413425  -1.0906075  -0.37827554 -0.8445275\n",
            " -0.5402717  -0.19959491 -1.6945038  -0.15673171  0.04962997 -2.4211814\n",
            " -0.51410294]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 32 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.86419755 -0.46627355 -0.82721543 -0.437577   -0.55578005 -0.4819818\n",
            " -0.35183215 -0.6343219  -0.46347764 -0.7409642  -0.3044296  -0.55705273\n",
            " -0.4677752  -0.21393487 -1.0934162   0.06182923  0.01309636 -1.8175424\n",
            " -0.39516738]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 33 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7061132  -0.21695948 -0.7434495  -0.30561092 -0.59913474 -0.3807977\n",
            " -0.26918715 -0.37969288 -0.5254294  -0.71581334 -0.24840105 -0.55576944\n",
            " -0.31513745 -0.06791576 -1.1646796  -0.06176503 -0.12596722 -1.3575642\n",
            " -0.27883962]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 34 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2199951  -0.6733981  -1.1239281  -0.615966   -0.8007314  -0.7349055\n",
            " -0.58808625 -0.84687084 -0.63142455 -1.0038675  -0.65112674 -0.7889105\n",
            " -0.7012781  -0.13089347 -1.9753278  -0.11536123  0.18190113 -2.6735246\n",
            " -0.54311514]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 35 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.4677644  -0.7782726  -1.2078122  -0.8627584  -0.97272253 -0.8027412\n",
            " -0.5470178  -0.68042994 -0.80536497 -1.0702143  -0.8506263  -0.96340615\n",
            " -0.8724457  -0.05200014 -2.030556   -0.08696917  0.02566379 -3.1638327\n",
            " -0.47512093]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 36 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1420498  -0.44545338 -1.062662   -0.5601314  -0.7512634  -0.6520285\n",
            " -0.48238266 -0.5655942  -0.6260774  -0.8183594  -0.61431485 -0.6186697\n",
            " -0.60051876 -0.0448366  -1.4946154  -0.07581654 -0.10648828 -2.1371272\n",
            " -0.36464918]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 37 reward=-4 new_state=[0 0 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.397281   -0.6376434  -1.0780233  -0.5527107  -0.73968697 -0.7222196\n",
            " -0.5024398  -0.7399117  -0.69182646 -0.96157557 -0.712197   -0.7976345\n",
            " -0.62410027 -0.11795925 -1.8055493   0.01767158  0.03464706 -2.7111888\n",
            " -0.6165092 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 38 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.3943765  -0.49361435 -0.99380726 -0.76958627 -0.7874107  -0.7486293\n",
            " -0.5442475  -0.59770805 -0.61285    -1.2487465  -0.7867271  -0.83914626\n",
            " -0.6889431  -0.14334154 -2.1794534   0.01045552 -0.0423669  -2.8462896\n",
            " -0.44211763]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 39 reward=-3 new_state=[0 0 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2669582  -0.5112715  -1.0744468  -0.40938598 -0.5995383  -0.7613978\n",
            " -0.37744716 -0.61962223 -0.62298256 -0.97620577 -0.63121533 -0.6625022\n",
            " -0.7143595  -0.13676707 -1.6171309  -0.00889036 -0.2092518  -2.380176\n",
            " -0.58486223]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 40 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.683097   -0.7377705  -1.2792434  -0.7509911  -0.8756236  -0.9419056\n",
            " -0.5377312  -0.8879096  -0.8056717  -1.4268378  -0.89533347 -0.9014955\n",
            " -0.9520102  -0.22795488 -2.4553254  -0.01589637 -0.3150401  -3.2709494\n",
            " -0.5524636 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 41 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.0876694  -0.6516686  -1.5580842  -1.2809062  -1.0703928  -1.2142581\n",
            " -0.8037173  -0.98923355 -1.1340698  -2.021292   -1.1754789  -1.2274004\n",
            " -1.0228003  -0.09718458 -3.3497481  -0.07427363 -0.57866025 -4.167724\n",
            " -0.71640456]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 42 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7703112  -0.59072566 -1.2138228  -1.015854   -0.98680615 -0.97159386\n",
            " -0.88009304 -0.636177   -0.8379432  -1.6417742  -0.989367   -0.92770547\n",
            " -0.8646854  -0.09722054 -2.7029905  -0.0871342  -0.63263136 -3.399865\n",
            " -0.42998266]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 43 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7788789  -0.53337926 -1.1870131  -0.6421106  -0.9404388  -0.87084943\n",
            " -0.7802529  -0.7170021  -0.8031408  -1.528951   -0.844704   -1.0036762\n",
            " -0.81300926 -0.09012077 -2.4746904  -0.07225346 -0.7390049  -3.0027442\n",
            " -0.3785982 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 44 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.2082531  -0.80122143 -1.5130153  -1.0829413  -1.078976   -1.1919962\n",
            " -1.2245699  -1.0164689  -1.0747182  -2.0938816  -1.309405   -1.2162989\n",
            " -1.0357456  -0.04092811 -3.4888496  -0.00913306 -0.86890703 -3.7529757\n",
            " -0.5965135 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 45 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8047787  -0.56933504 -1.2425984  -1.0809361  -0.9678295  -0.96476185\n",
            " -1.0345206  -0.5722483  -0.81092024 -1.734764   -1.0544312  -0.96178186\n",
            " -0.8653318  -0.11095763 -2.821106   -0.05699912 -0.78717196 -2.9328375\n",
            " -0.39222348]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 46 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.4616567  -0.38053358 -1.0142299  -0.50842947 -0.78115183 -0.7395049\n",
            " -0.88789713 -0.48646522 -0.68556345 -1.3141054  -0.72771144 -0.83391833\n",
            " -0.72804606 -0.09654363 -2.1637166   0.07200197 -0.7896281  -2.192484\n",
            " -0.420424  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 47 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.4350672  -0.8205114  -1.5991552  -1.1861216  -1.135533   -1.2747635\n",
            " -1.4609545  -1.098689   -1.1481452  -2.423442   -1.3537925  -1.34519\n",
            " -1.2051984  -0.03644457 -3.7217562  -0.15965366 -1.1524888  -3.4438777\n",
            " -0.68973315]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.8070602  -0.51409274 -1.2095184  -0.93181527 -0.9235788  -0.8639162\n",
            " -1.1646786  -0.58002746 -0.8405611  -1.6567271  -0.9911724  -0.8931276\n",
            " -0.7839057  -0.1206829  -2.6482708  -0.05885124 -0.8883306  -2.3502097\n",
            " -0.44837102]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.527502   -0.39389133 -1.0078791  -0.7038329  -0.7392786  -0.7375648\n",
            " -1.0537457  -0.5550055  -0.7373909  -1.1821525  -0.85216486 -0.75365204\n",
            " -0.7056651  -0.15433483 -2.2763326   0.037038   -0.8200466  -1.8398412\n",
            " -0.34504628]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.1509697  -0.5559444  -1.5140631  -1.0423212  -0.94950074 -1.2459089\n",
            " -1.3989497  -1.0442417  -1.1485624  -2.1978889  -1.267233   -1.258767\n",
            " -1.0466993  -0.39520803 -3.3455398  -0.17095165 -1.3034343  -2.5394568\n",
            " -0.6161213 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 51 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7195294  -0.42154592 -1.0930111  -0.59066087 -0.7937472  -0.7885466\n",
            " -1.0523658  -0.6271069  -0.845006   -1.4131233  -0.91439867 -0.89072126\n",
            " -0.7235456  -0.40302023 -2.2170105  -0.04318547 -1.0354351  -1.7214196\n",
            " -0.3883555 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 52 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.4468578  -0.36019394 -0.93749094 -0.71913934 -0.72658753 -0.71099156\n",
            " -1.0737576  -0.4872756  -0.7282366  -1.2344512  -0.8518444  -0.75273246\n",
            " -0.6250777  -0.43813354 -2.2233944   0.03845482 -0.89591026 -1.3876324\n",
            " -0.32187492]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 53 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.3603015  -0.5539519  -1.6689641  -1.098889   -0.96755356 -1.4483222\n",
            " -1.6270896  -1.0950009  -1.1525542  -2.5401356  -1.3567519  -1.2519516\n",
            " -1.1369637  -0.8394068  -3.823966   -0.09365215 -1.3412174  -2.123973\n",
            " -0.69262785]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 54 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.116155   -0.56759477 -1.3958014  -0.9495363  -1.0507251  -1.1658357\n",
            " -1.3503151  -0.7299663  -0.90953946 -2.0213253  -1.1645031  -1.0904534\n",
            " -0.99720013 -0.8207683  -2.965388   -0.13189355 -1.1149142  -1.8022925\n",
            " -0.49313313]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 55 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6369376  -0.47639352 -1.0890025  -0.8219164  -0.7858214  -0.9119778\n",
            " -1.2747235  -0.66227764 -0.7701045  -1.4982004  -0.96307814 -0.7276863\n",
            " -0.7680856  -0.7498359  -2.4607897   0.02823279 -1.0316994  -1.3419948\n",
            " -0.45617533]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 56 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.0659716  -0.4132756  -1.5268064  -0.91273886 -0.89638734 -1.3173054\n",
            " -1.5066627  -0.9092913  -1.0660944  -2.0184782  -1.181091   -1.0905465\n",
            " -0.95257246 -0.97915804 -3.330277   -0.03183559 -1.3338739  -1.3105568\n",
            " -0.584134  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 57 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.9789798  -0.56601423 -1.2530926  -0.7885153  -0.9459285  -1.2040619\n",
            " -1.3339546  -0.6648446  -0.8892344  -1.8730584  -1.0489461  -1.0160168\n",
            " -0.9620228  -0.9308817  -2.8186834  -0.14528751 -1.2327918  -1.2329335\n",
            " -0.3961697 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 58 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.1925492  -0.61693823 -1.4183832  -0.9262671  -0.95161486 -1.3761343\n",
            " -1.6877017  -0.9033504  -0.91768074 -2.2720127  -1.3521798  -1.1061814\n",
            " -1.0589209  -1.2129928  -3.3061354  -0.02287148 -1.3137194  -1.3094405\n",
            " -0.4111326 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 59 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.9004357  -0.5619704  -1.3156891  -1.0360695  -0.8339301  -1.3833754\n",
            " -1.4221158  -0.82745945 -0.98779523 -1.9645241  -1.1774065  -0.94831526\n",
            " -0.9542924  -1.1990113  -3.0474358  -0.01514863 -1.329706   -0.9268846\n",
            " -0.63423145]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 60 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7000266  -0.44057968 -1.0581583  -0.68533635 -0.7871865  -1.133356\n",
            " -1.1745609  -0.46570957 -0.778692   -1.5430148  -0.9049591  -0.8405214\n",
            " -0.83910644 -0.9581964  -2.3770733   0.01024759 -1.1418865  -0.7380321\n",
            " -0.4126015 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 61 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.0568674  -0.58251166 -1.2876011  -0.9197767  -0.9131458  -1.4363121\n",
            " -1.6024886  -0.8023894  -0.88650495 -1.9745859  -1.1279771  -1.0571485\n",
            " -0.92635155 -1.2851068  -3.0310411  -0.08341125 -1.2149615  -0.7968006\n",
            " -0.5212747 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 62 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.21374    -0.55780214 -1.5241576  -1.1234767  -0.97336054 -1.5465438\n",
            " -1.7604401  -0.9049827  -0.98241955 -2.386252   -1.1888838  -1.1057081\n",
            " -1.088338   -1.5209702  -3.5808072  -0.06263518 -1.4935753  -0.5909977\n",
            " -0.5172958 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 63 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7619505  -0.42169055 -1.1409934  -0.69546485 -0.89268345 -1.2045785\n",
            " -1.2415944  -0.47309592 -0.79482454 -1.6138264  -0.90091443 -0.8492424\n",
            " -0.7625968  -1.0255673  -2.4974365  -0.06774807 -1.2356085  -0.4919401\n",
            " -0.29579672]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 64 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.0689363  -0.5622637  -1.1356249  -0.7353103  -0.90447384 -1.4032991\n",
            " -1.4407289  -0.77464247 -0.87657267 -1.9579298  -1.0220096  -1.0459809\n",
            " -0.8467559  -1.2860711  -2.9202793  -0.1666674  -1.325778   -0.41569924\n",
            " -0.38218495]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 65 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.1083074  -0.5359606  -1.322599   -0.9946323  -0.7440636  -1.4771695\n",
            " -1.6162294  -1.0657173  -0.7977267  -2.1960669  -1.1286823  -0.8911061\n",
            " -0.84870905 -1.3439733  -3.320791   -0.15481846 -1.2392739  -0.34096187\n",
            " -0.61835796]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 66 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.3660095  -0.40931758 -0.8930497  -0.63683665 -0.6655268  -0.9200325\n",
            " -1.058564   -0.45543808 -0.5168679  -1.2538863  -0.7155592  -0.75165725\n",
            " -0.5540187  -0.878237   -2.094644    0.0534558  -0.8506256  -0.3183642\n",
            " -0.331272  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 67 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.079787   -0.5293691  -1.2330061  -0.6605551  -0.7762108  -1.4295255\n",
            " -1.4673841  -0.95470965 -0.67650366 -2.0611584  -1.0989228  -0.9803082\n",
            " -0.8820158  -1.2344365  -2.9046981  -0.07593869 -1.3761591  -0.4009802\n",
            " -0.66524374]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 68 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.4292983  -0.37180528 -0.94161564 -0.7191738  -0.6873825  -1.0512128\n",
            " -1.1161246  -0.4476213  -0.4113445  -1.4035392  -0.73497164 -0.7317662\n",
            " -0.6516369  -1.0035421  -2.238484   -0.06847785 -0.9808738  -0.1781086\n",
            " -0.2268353 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 69 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.92068    -0.45043534 -1.1133949  -0.85058486 -0.73989177 -1.4903558\n",
            " -1.3871636  -0.7781615  -0.60860455 -1.8947223  -1.0333463  -0.89254546\n",
            " -0.8043982  -1.2786866  -3.0548499   0.02795743 -1.0777544  -0.14284724\n",
            " -0.59958494]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 70 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.8692666  -0.533976   -1.1342162  -0.9162011  -0.62921345 -1.3780844\n",
            " -1.4230599  -0.8582213  -0.49217433 -2.0262794  -1.1084465  -0.96200967\n",
            " -0.7685709  -1.2815094  -2.9735806  -0.14242308 -0.85648763 -0.18461864\n",
            " -0.47741035]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 71 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.671828   -0.3367967  -0.9286367  -0.62696874 -0.6101617  -1.2398311\n",
            " -1.0853152  -0.74038416 -0.3667354  -1.5027963  -0.8638514  -0.829251\n",
            " -0.72048473 -1.0315119  -2.4035919  -0.05025197 -0.95255446  0.01085287\n",
            " -0.47514138]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 72 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.3687723  -0.29666564 -0.8388663  -0.6101228  -0.5468693  -0.92733777\n",
            " -0.98413515 -0.52554315 -0.13391782 -1.216511   -0.74301076 -0.69683254\n",
            " -0.5786876  -0.8796396  -1.9805509  -0.04867797 -0.801789    0.00360235\n",
            " -0.3332147 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 73 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.1451508  -0.22964075 -0.8262407  -0.43373954 -0.39817354 -0.86701435\n",
            " -0.8103601  -0.4472453  -0.09314242 -1.1588571  -0.6891033  -0.6181659\n",
            " -0.4849262  -0.85164285 -1.6868386  -0.02964647 -0.7341361   0.11057388\n",
            " -0.3053155 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 74 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2161341  -0.33465463 -0.80111367 -0.4622708  -0.39467707 -0.87163633\n",
            " -0.8739127  -0.40627658 -0.06950162 -1.0524653  -0.7012128  -0.6057925\n",
            " -0.50219035 -0.7922965  -1.6572633   0.1663761  -0.7671382   0.13247971\n",
            " -0.34926683]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 75 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2252189  -0.29842916 -0.8652489  -0.43732747 -0.34858525 -0.9102126\n",
            " -0.8769294  -0.48032194 -0.07126355 -1.1937162  -0.72892207 -0.6306872\n",
            " -0.52497804 -0.88532346 -1.7643667   0.22536896 -0.77209145  0.2197173\n",
            " -0.34231824]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 76 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.1298612  -0.30778375 -0.7676762  -0.45418313 -0.27400625 -0.84317726\n",
            " -0.8345595  -0.41668916 -0.02799525 -1.0218242  -0.6856798  -0.5729005\n",
            " -0.49431473 -0.77751136 -1.5709968   0.3894486  -0.757797    0.2900584\n",
            " -0.3600765 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 77 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.123466   -0.36101195 -0.74412227 -0.5539143  -0.16819121 -0.96448684\n",
            " -0.9325548  -0.5324539  -0.10682434 -1.1669142  -0.6510258  -0.6103149\n",
            " -0.44198784 -0.7279744  -1.7526497   0.37185615 -0.70475996  0.46086985\n",
            " -0.377017  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 78 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.0199575  -0.3432392  -0.6565916  -0.4080551  -0.20788306 -0.7330947\n",
            " -0.76654875 -0.31572872 -0.01192959 -0.897978   -0.5363965  -0.4878725\n",
            " -0.4497255  -0.5699008  -1.3860887   0.46827954 -0.7262689   0.30193347\n",
            " -0.2928537 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 79 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.7046704  -0.5841326  -1.1829641  -0.48581657 -0.20594153 -1.304592\n",
            " -1.2905661  -0.79071563  0.00478109 -1.7763587  -1.0368742  -0.7166318\n",
            " -0.9176198  -0.94754976 -2.3132722   0.82862896 -0.8411452   0.43666047\n",
            " -0.4008185 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 80 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.3667303  -0.51980543 -0.93008137 -0.54393595 -0.0946332  -1.175392\n",
            " -1.0542712  -0.48910722  0.00951981 -1.3581767  -0.7168647  -0.70708483\n",
            " -0.76139635 -0.75306857 -1.9403439   0.7730956  -0.7729636   0.38368732\n",
            " -0.45143384]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 81 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2208676  -0.37517455 -0.8324881  -0.40782413 -0.10870823 -0.9474312\n",
            " -0.9662631  -0.50075376  0.01676185 -1.1903057  -0.7605743  -0.5464657\n",
            " -0.5692262  -0.49572888 -1.7508754   0.8326601  -0.6523605   0.80585194\n",
            " -0.32944143]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 82 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.0316063  -0.28075737 -0.6304338  -0.429018   -0.1023159  -0.7662233\n",
            " -0.76339805 -0.33260924  0.06821518 -0.8821332  -0.5224567  -0.52200437\n",
            " -0.42492747 -0.36087805 -1.4365468   0.66298234 -0.521879    0.70595604\n",
            " -0.21912265]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 83 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.0783991  -0.3410916  -0.7433184  -0.48677564  0.00335215 -0.91944534\n",
            " -0.9470732  -0.54251504 -0.01820659 -1.161409   -0.62808436 -0.51327914\n",
            " -0.49147612 -0.39379728 -1.7872286   0.7710594  -0.36778885  1.0571692\n",
            " -0.34069586]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 84 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2884047  -0.46168542 -0.8966568  -0.50646096 -0.10100452 -0.9706239\n",
            " -0.9874647  -0.41784358  0.10706647 -1.1981791  -0.68982893 -0.592826\n",
            " -0.59805495 -0.43573746 -1.8006124   1.0409099  -0.589095    1.0923165\n",
            " -0.29212224]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 85 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2444903  -0.34510538 -0.82522756 -0.4177177  -0.07481362 -0.986642\n",
            " -0.9974462  -0.46951777  0.08784879 -1.1871693  -0.7434152  -0.5554069\n",
            " -0.5045692  -0.27020466 -1.8037314   1.3377804  -0.47057724  1.4278389\n",
            " -0.30730897]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 86 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.0942482  -0.32829604 -0.6455234  -0.42342278  0.0079532  -0.854917\n",
            " -0.81083953 -0.31824774  0.14662571 -0.9166637  -0.5285068  -0.50793606\n",
            " -0.50133634 -0.19511375 -1.4388374   1.2235945  -0.420306    1.293224\n",
            " -0.22493541]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 87 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.1924193  -0.32491213 -0.8109601  -0.4204674  -0.00218248 -0.98155284\n",
            " -0.98847437 -0.48968267  0.10850189 -1.1902906  -0.7459895  -0.5371188\n",
            " -0.5086596  -0.18223628 -1.7643093   1.6617981  -0.4340027   1.6936496\n",
            " -0.32980338]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 88 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2246263  -0.41957283 -0.7572746  -0.5092264   0.06896156 -0.97388494\n",
            " -0.95372975 -0.3748145   0.15609536 -1.107826   -0.61015004 -0.54807913\n",
            " -0.5782236  -0.24769    -1.682204    1.6876843  -0.48823026  1.5518142\n",
            " -0.27900827]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 89 reward=1 new_state=[0 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.3087311  -0.3923871  -0.93676734 -0.49286455  0.07086644 -1.1392747\n",
            " -1.11175    -0.5103648   0.07487042 -1.3999449  -0.82033277 -0.6115002\n",
            " -0.58628035 -0.17609115 -1.985522    2.2591605  -0.49227098  2.004798\n",
            " -0.42224625]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 90 reward=1 new_state=[0 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.3404083  -0.5334931  -0.8280965  -0.29881746  0.06818264 -1.0828046\n",
            " -1.0856187  -0.41951022  0.1742708  -1.2580317  -0.64365655 -0.69065887\n",
            " -0.7332855  -0.38498706 -1.774353    2.116495   -0.3381964   1.3195208\n",
            " -0.44948003]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 91 reward=1 new_state=[0 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.5862445  -0.58936614 -1.0820736  -0.21887678  0.14427878 -1.2975191\n",
            " -1.2442216  -0.5721085   0.22852169 -1.6669946  -0.9264519  -0.78474873\n",
            " -0.8363693  -0.27019703 -2.1498544   2.791457   -0.4459414   2.1402636\n",
            " -0.55873525]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 92 reward=0 new_state=[0 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.8541152  -0.34603333 -0.92324793 -0.39410758  0.3591304  -1.4365984\n",
            " -1.3374741  -0.77044857  0.12373477 -1.8627497  -0.9414342  -0.8285725\n",
            " -0.7747118  -0.4503209  -2.8079557   2.9256935  -0.10377885  2.2050257\n",
            " -0.576171  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 93 reward=0 new_state=[0 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6573455  -0.5281539  -0.8881838  -0.41167456  0.30857876 -1.3668245\n",
            " -1.3541259  -0.67742383  0.13167718 -1.7702266  -0.8527751  -0.7780145\n",
            " -0.7554353  -0.47527215 -2.5229192   2.8506308  -0.0234173   2.1877337\n",
            " -0.47976878]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 94 reward=0 new_state=[0 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5165291  -0.4604552  -1.1324708  -0.03432946  0.12644698 -1.2658637\n",
            " -1.260392   -0.48446283  0.03022319 -1.6558331  -0.8864205  -0.74205047\n",
            " -0.57255137 -0.14034858 -2.0703526   3.0990057  -0.44451526  2.687496\n",
            " -0.3879771 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 95 reward=-1 new_state=[0 1 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.308427   -0.37220487 -0.7870783  -0.22013389  0.23500286 -1.106293\n",
            " -1.2150372  -0.41612408 -0.01416396 -1.3499249  -0.747349   -0.68205625\n",
            " -0.55795187 -0.48008746 -1.9732044   2.7373977  -0.27487892  1.8440125\n",
            " -0.3706676 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 96 reward=-1 new_state=[0 1 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.8193461  -0.6851139  -1.1258603  -0.18893614  0.04477712 -1.3044685\n",
            " -1.5215341  -0.56916535  0.27170914 -1.9297441  -0.9564626  -0.92455286\n",
            " -0.9250773  -0.68426865 -2.118857    3.3334098  -0.38516057  2.146153\n",
            " -0.5067594 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 97 reward=-2 new_state=[0 1 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6877891  -0.20830679 -0.9014274   0.03429515  0.3756602  -0.9385139\n",
            " -1.28828    -0.7126617  -0.0717589  -1.5737807  -0.63742334 -0.60987747\n",
            " -0.7458021  -0.64171314 -1.9592326   2.818839   -0.19623831  1.8873222\n",
            " -0.46638328]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 98 reward=-2 new_state=[0 1 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0410024  -0.14658004 -0.7422182   0.05359149  0.3594646  -0.5764638\n",
            " -1.0172238  -0.3624326  -0.06287996 -1.2426231  -0.61498004 -0.61966085\n",
            " -0.4788696  -0.29491603 -1.5500038   2.2622106  -0.16649519  1.6580391\n",
            " -0.2885183 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 99 reward=-2 new_state=[0 1 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7683315  -0.49983916 -1.1727211   0.07115438 -0.00394119 -0.98135513\n",
            " -1.5817881  -0.3776954  -0.01735982 -2.1431468  -0.9526384  -0.8948687\n",
            " -0.9712878  -0.72507375 -2.3038378   3.5405314  -0.20223281  2.2146497\n",
            " -0.54250574]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 100 reward=-2 new_state=[0 1 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.719883   -0.43134817 -1.1423255   0.09817032  0.08406005 -0.8597609\n",
            " -1.5636089  -0.38256773  0.06272698 -1.9176022  -0.8762655  -0.82575375\n",
            " -0.9095417  -0.7748392  -2.1292658   3.4295754  -0.1615192   2.0359402\n",
            " -0.5023475 ]\n",
            "Epsilon reduced to 0.16000000000000003\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.6346085  -0.5809264  -0.95638466  0.05334619  0.13057753 -0.7343834\n",
            " -1.3681082  -0.3450421   0.3096697  -1.6301126  -0.8412866  -0.76267785\n",
            " -0.748534   -0.26922435 -1.664298    3.6818388  -0.23363122  2.5738919\n",
            " -0.44215387]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.905078   -0.62531257 -1.074017    0.03780349  0.5719484  -0.85496056\n",
            " -1.6245731  -0.70366454  0.04677235 -2.0394788  -0.9345729  -0.83083105\n",
            " -0.82219726 -0.34814912 -2.3067813   4.0252      0.1359809   2.7216222\n",
            " -0.56312525]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7802217  -0.6653582  -1.0572673   0.04593561  0.33536977 -0.8213514\n",
            " -1.5437889  -0.47346857  0.10143264 -1.904843   -0.89598995 -0.94373214\n",
            " -0.78228056 -0.17832665 -2.2220209   4.0489817   0.00611406  2.7233942\n",
            " -0.46451285]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9737902  -0.66683716 -1.167706    0.14762586  0.47521937 -0.77986383\n",
            " -1.7183601  -0.5403256   0.04637101 -2.0767438  -0.9661597  -0.8366603\n",
            " -0.852893    0.10266078 -2.3039973   4.297175    0.16397585  2.7633765\n",
            " -0.521412  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8826128  -0.51774496 -1.0231156   0.17749414  0.19862051 -0.6905345\n",
            " -1.516938   -0.24361601  0.10878864 -1.9589984  -1.0259207  -0.88012296\n",
            " -0.7771124   0.35672954 -2.1547663   4.170967   -0.18108404  2.4586105\n",
            " -0.4875082 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.0695517  -0.5301826  -1.2486237   0.38254598  0.4176324  -0.62366855\n",
            " -1.681241   -0.5399407   0.16479565 -2.311694   -1.1043153  -0.9068663\n",
            " -0.9131715   1.0049853  -2.1852913   4.5325756  -0.24822216  2.8112767\n",
            " -0.59632385]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7202512  -0.62631494 -1.0315949   0.15681666  0.3588776  -0.6240046\n",
            " -1.4755375  -0.34120122  0.09251039 -1.9880828  -0.8686217  -0.9209334\n",
            " -0.7886362   1.0839936  -2.1584666   4.023872   -0.05030625  2.3040488\n",
            " -0.47335833]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8900372e+00 -6.7277908e-01 -1.1221489e+00  2.2394526e-01\n",
            "  5.4274029e-01 -5.9911126e-01 -1.6234303e+00 -4.7918558e-01\n",
            " -1.4801958e-03 -2.0057852e+00 -9.7448534e-01 -8.1649667e-01\n",
            " -8.5673434e-01  1.5290750e+00 -2.0475707e+00  4.3072767e+00\n",
            "  2.1046828e-01  2.4912803e+00 -5.4682451e-01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.6059039  -0.67572296 -0.9059669   0.1663751   0.35016647 -0.6171241\n",
            " -1.3971305  -0.2263873   0.16510117 -1.7447543  -0.80186963 -0.82321495\n",
            " -0.8747389   1.6701591  -1.8042935   3.8237088  -0.07610138  2.1377797\n",
            " -0.42192557]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 10 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.8272111  -0.57856405 -1.0642968   0.2935098   0.367892   -0.37059233\n",
            " -1.4626094  -0.11214017  0.13309503 -1.9607161  -1.0696921  -0.898077\n",
            " -0.77398485  2.2895076  -1.8664597   4.365006   -0.11457337  2.6850023\n",
            " -0.43773586]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5401446  -0.5000111  -0.8748258   0.29513013  0.1356175  -0.46450627\n",
            " -1.3247154   0.10983896  0.18071233 -1.5924646  -0.76666737 -0.7142866\n",
            " -0.6458813   1.9438455  -1.5051945   3.7232852  -0.08259604  2.0559556\n",
            " -0.38815668]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5434448  -0.55685604 -1.0231711   0.45011106  0.15342273 -0.29042408\n",
            " -1.176279    0.06284289  0.23776068 -1.7478219  -0.9713877  -0.66683626\n",
            " -0.75203055  2.6588418  -1.2415992   4.2280884  -0.29640174  2.6937842\n",
            " -0.42537665]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6657642  -0.549393   -1.044493    0.5705446   0.06352812 -0.2608029\n",
            " -1.1315402   0.1892298   0.2826827  -1.7918127  -1.018712   -0.74039155\n",
            " -0.71048135  2.7795506  -1.1405753   4.3147798  -0.18877944  2.6871715\n",
            " -0.51617974]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.4947352  -0.5111956  -0.95044774  0.4866547   0.10544682 -0.18087585\n",
            " -0.8693307   0.06419019  0.27906585 -1.6476289  -0.93613005 -0.6524319\n",
            " -0.7036054   3.0847342  -1.153237    4.14347    -0.20913902  2.6960583\n",
            " -0.38005802]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5625615  -0.5114734  -1.0265632   0.49291062  0.06783395 -0.2606915\n",
            " -0.745951    0.06639342  0.35315096 -1.6672782  -0.86875576 -0.7361839\n",
            " -0.7693929   3.2939565  -1.2598698   4.1643124  -0.33077854  2.5685384\n",
            " -0.42062825]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.3710622  -0.43987238 -0.9798941   0.37638238  0.23273383 -0.20701402\n",
            " -0.44991985  0.13130662  0.3183333  -1.4391416  -0.83956206 -0.7102625\n",
            " -0.62521017  3.3515959  -1.1016039   3.9489303  -0.0470926   2.6745007\n",
            " -0.42072153]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5269046  -0.4383903  -0.916936    0.30151308  0.25106835 -0.41153705\n",
            " -0.6459335   0.12146471  0.2236441  -1.6600595  -0.8010551  -0.7777316\n",
            " -0.6620998   3.584383   -1.4655471   3.995833    0.02644339  2.3214\n",
            " -0.47414568]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6244216  -0.57560384 -1.0424769   0.562179    0.23869407 -0.22924761\n",
            " -0.47927055  0.12576605  0.2720842  -1.7978512  -1.0486221  -0.70398563\n",
            " -0.8003707   4.300622   -1.142819    4.5755825  -0.22987658  3.0301323\n",
            " -0.45957595]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7287033  -0.6484804  -1.101236    0.6263712   0.09043963 -0.21670939\n",
            " -0.4073569   0.21940179  0.338304   -1.7861258  -0.9545439  -0.76581186\n",
            " -0.8362349   4.5256095  -1.2459917   4.469376   -0.36028427  2.812494\n",
            " -0.4163157 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 20 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.463051   -0.5392661  -0.9957532   0.46697316  0.19711472 -0.14633822\n",
            " -0.18825133  0.29218283  0.3425879  -1.5743674  -0.9020902  -0.7153786\n",
            " -0.70251775  4.372208   -1.0998363   4.3109393  -0.15270217  2.9380193\n",
            " -0.4893215 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 21 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2135754  -0.30965474 -0.81158674  0.67371875  0.18416998  0.07680855\n",
            " -0.18518028  0.26581585  0.2898824  -1.2201452  -0.78561217 -0.5940929\n",
            " -0.528566    3.8860765  -0.6177639   3.9475102  -0.16271245  3.2061527\n",
            " -0.36401752]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 22 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2770327  -0.39652312 -0.8317764   0.67142653  0.2523508   0.02048368\n",
            " -0.16469787  0.29136935  0.46430916 -1.2307606  -0.79573774 -0.61874\n",
            " -0.56930494  4.0494876  -0.6027247   4.0715475  -0.20869856  3.1605773\n",
            " -0.41287714]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 23 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2015318  -0.34638008 -0.78441334  0.51711416  0.3515177  -0.14115538\n",
            " -0.2218853   0.13839217  0.50811255 -1.2512171  -0.68277794 -0.67908543\n",
            " -0.47597238  4.138938   -0.8814366   3.9302123   0.09933609  3.1641905\n",
            " -0.41737208]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2259774  -0.30924955 -0.7219297   0.5739857   0.10397588 -0.04870093\n",
            " -0.14509891  0.316768    0.5955676  -1.1247414  -0.61497897 -0.5499324\n",
            " -0.5700057   3.7493613  -0.73164433  3.8320515  -0.16317618  2.5174854\n",
            " -0.24281316]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.254272   -0.33715284 -0.7762411   0.52003044  0.10434779 -0.08717584\n",
            " -0.13181068  0.55219877  0.6724622  -1.1444554  -0.71089643 -0.56441176\n",
            " -0.5314595   3.748854   -0.72889584  3.9101338  -0.30153412  2.494062\n",
            " -0.26624542]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.5178163  -0.36554322 -0.9263954   0.5065497   0.09824054 -0.06620876\n",
            " -0.07874933  0.7166815   0.7491888  -1.4403611  -0.8241153  -0.7536052\n",
            " -0.6066246   4.4955745  -1.2329886   4.5325155  -0.2673581   2.6117346\n",
            " -0.27565807]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.5879648  -0.31439117 -0.9649216   0.57788885  0.28970847 -0.13739385\n",
            " -0.25265566  0.59561586  0.74186516 -1.6844182  -0.9094437  -0.72472686\n",
            " -0.88292605  4.647655   -1.3553421   4.9510894  -0.07149807  2.3243532\n",
            " -0.41976297]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.7216662  -0.52459323 -1.1042753   0.8293579   0.19111687 -0.2092856\n",
            " -0.26518524  0.9721885   1.0869508  -1.7399704  -0.88433367 -0.7952917\n",
            " -0.85127026  4.6515675  -0.98289186  5.173483   -0.32391402  2.342558\n",
            " -0.46912467]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.5397353  -0.4801812  -1.1025846   0.63419455  0.2249768  -0.11306766\n",
            "  0.01898316  1.1323411   1.2516899  -1.5661625  -0.9354917  -0.737215\n",
            " -0.6367731   5.394454   -0.9936733   5.3961253  -0.3696361   3.3038046\n",
            " -0.46187553]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.5679997  -0.37802818 -0.9092404   0.6438674   0.11900097  0.03346827\n",
            " -0.03382378  1.1946621   1.0298058  -1.5162761  -0.9155411  -0.71506625\n",
            " -0.55497503  5.260636   -1.1988155   5.1620255  -0.2904002   2.7594664\n",
            " -0.39635623]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.6418966  -0.34482315 -0.82395756  0.6959487   0.31618676 -0.01740978\n",
            " -0.16581342  1.0090683   1.1188054  -1.7241076  -0.98357755 -0.7468733\n",
            " -0.83913547  5.6092954  -1.3661441   5.559814    0.02049938  2.5933883\n",
            " -0.41592735]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.7430934  -0.54489726 -0.7843042   0.74720126  0.1111896  -0.21758479\n",
            " -0.18688577  1.2953631   1.4449577  -1.710194   -0.93705356 -0.79336315\n",
            " -0.8517769   5.197397   -0.9528876   5.5567565  -0.32079437  2.463876\n",
            " -0.45189407]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.4858885  -0.3765064  -0.44964597  0.69441664  0.37860602 -0.19807823\n",
            " -0.06614531  1.4645101   1.4374701  -1.6359757  -0.9347576  -0.70328134\n",
            " -0.62038505  5.9888234  -0.96827114  5.6759334  -0.10442208  3.58285\n",
            " -0.5498854 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.4774972  -0.29919294 -0.28801844  0.6497007   0.19950852  0.06504092\n",
            " -0.03017609  1.2299919   1.1928526  -1.514347   -0.8172915  -0.6510667\n",
            " -0.6713904   5.4915967  -1.1883423   5.233662   -0.12923393  2.2439618\n",
            " -0.3526276 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.3296518  -0.88986325 -0.5631761   0.73272043  0.41628397 -0.5848037\n",
            " -0.23907329  1.5920655   2.044041   -2.7488198  -1.518018   -1.1224638\n",
            " -1.094655    7.6983013  -2.2631335   7.635553   -0.09495912  3.20895\n",
            " -0.7307581 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.9182776  -0.6802851  -0.34599826  1.0721861   0.45607674 -0.23781963\n",
            " -0.21269204  1.6355525   2.1731298  -2.0358703  -1.0613977  -0.9167407\n",
            " -0.58695     6.6144238  -1.3153181   6.56727    -0.13476343  3.191398\n",
            " -0.63333   ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.0243416  -0.7063765  -0.27451423  0.8316136   0.24108554 -0.40538502\n",
            " -0.2137317   1.7617471   1.8314998  -1.9930347  -1.0990807  -0.9784656\n",
            " -0.65737873  6.386187   -1.0997057   6.1189885  -0.5065989   2.7612944\n",
            " -0.6003095 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.5172322  -0.84411335 -0.1347835   0.7242539   0.6640628  -0.46933672\n",
            " -0.13512751  2.0480087   2.2035475  -2.9255216  -1.6333795  -1.3356448\n",
            " -0.37197778  7.996826   -1.9910065   7.4975333  -0.35560724  3.502684\n",
            " -0.7970625 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.9337995  -0.6953054   0.05369127  0.6934748   0.7233241  -0.20809926\n",
            " -0.0119892   1.705952    1.8620727  -2.229478   -1.2431556  -0.87213194\n",
            " -0.12468279  7.33481    -2.0659187   6.227381   -0.01706656  2.27448\n",
            " -0.54291254]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.0808506  -0.7037101  -0.05646835  0.9818704   0.7946579  -0.3027451\n",
            " -0.28782782  1.748332    2.0482082  -2.2892737  -1.343361   -0.9945225\n",
            " -0.12772533  6.9622483  -1.1841906   6.160482   -0.43853843  3.3233018\n",
            " -0.66063035]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.8813882  -0.6209307   0.15297069  0.9739297   1.1686946  -0.2724075\n",
            " -0.172343    1.9379313   2.2054474  -1.9682102  -1.0554783  -1.0038681\n",
            "  0.10995772  6.7043686  -1.2808855   5.573174   -0.08719401  3.1779501\n",
            " -0.44086024]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.9148256  -0.6765033   0.23197035  0.6359512   0.92215157 -0.26967847\n",
            "  0.03521131  2.1071067   1.8428863  -2.0314517  -1.0968655  -0.891333\n",
            "  0.1577312   6.594849   -1.4175091   5.3424525  -0.34965095  2.4717197\n",
            " -0.2596786 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 43 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.3985045  -0.8807399   0.39544204  0.7248264   1.3205363  -0.40551803\n",
            "  0.05494044  2.6280425   2.2907114  -2.85991    -1.6031836  -1.2167397\n",
            "  0.46228057  8.164053   -1.9732805   6.1193614  -0.1508375   3.1821754\n",
            " -0.09138707]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.512333   -0.49442935  0.3828946   0.7611152   0.99776256 -0.19820875\n",
            " -0.02890468  2.1442368   1.7315068  -1.53473    -0.8412892  -0.735937\n",
            "  0.31839254  5.6719685  -0.6274538   4.4562545  -0.31191564  2.4629266\n",
            "  0.13669683]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.015511   -0.6527921   0.4342504   0.90177965  1.4236108  -0.21028556\n",
            "  0.01958169  2.7381246   2.332965   -2.1577153  -1.1847159  -1.0203639\n",
            "  0.5115676   7.1522417  -0.6710082   5.3567467  -0.37156096  3.6098812\n",
            "  0.3784839 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.0015337  -0.6079535   0.39732105  0.731193    1.7017297  -0.29820693\n",
            " -0.0781322   2.6146      2.0517426  -2.4528937  -1.1991624  -0.9056961\n",
            "  0.5844753   7.8705473  -1.6625532   5.0973015   0.07519055  2.3791275\n",
            "  0.56076103]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.00964    -0.75070775  0.4505787   0.71551996  1.4914327  -0.30590212\n",
            "  0.10899998  2.7980769   2.140861   -2.1903062  -1.0747638  -0.9816829\n",
            "  0.63327354  7.3295403  -0.7797441   5.0702415  -0.24048087  2.7082422\n",
            "  0.64922667]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.7230583  -0.5328523   0.423153    0.8585921   1.7439446  -0.27641487\n",
            " -0.05304389  2.72292     2.155001   -1.9399313  -1.0889257  -0.98037577\n",
            "  0.52969635  6.854458   -0.36833233  4.743235   -0.0288404   3.2857468\n",
            "  0.8034402 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.5437396  -0.57328314  0.49905753  0.78002405  1.5075219  -0.16183345\n",
            " -0.15337674  2.4814675   1.8363187  -1.5509937  -0.84604216 -0.70781666\n",
            "  0.554975    5.7905087  -0.04909467  4.312928   -0.28446996  2.4554443\n",
            "  0.7253619 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.2958698  -0.8777471   0.62337774  0.7208545   2.0457785  -0.29728112\n",
            "  0.12144247  3.5730138   2.6282692  -2.8471358  -1.5228672  -1.1171254\n",
            "  1.0234363   8.628343   -0.8504556   5.6281247  -0.05494329  3.3571908\n",
            "  1.3942385 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 51 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.5902358  -0.4835306   0.46320152  0.723626    1.5436959  -0.13145289\n",
            " -0.04468657  2.6075048   1.5919367  -1.7737967  -0.89329755 -0.6146856\n",
            "  0.8262161   6.3526034  -0.42756814  4.255016    0.03342355  1.972849\n",
            "  0.88263434]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 52 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.7272207  -0.61957103  0.6135112   0.85667247  1.715021   -0.13931061\n",
            " -0.04679981  2.8324535   2.2287316  -1.8207934  -1.0089383  -0.81110084\n",
            "  0.71162564  6.6396194   0.28396696  5.157514   -0.38890016  3.0419822\n",
            "  1.2113997 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 53 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.7116266  -0.48088357  0.71009505  0.9352472   1.7244732  -0.17180564\n",
            "  0.12899294  2.978324    2.2483528  -1.77193    -1.0421343  -0.9110664\n",
            "  0.8690978   6.9086885   0.17141363  5.2011433  -0.11072774  3.444036\n",
            "  1.2344027 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 54 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.9157767  -0.7984851   0.78412014  0.54942316  1.9405277  -0.42061424\n",
            "  0.03540277  3.6209874   2.0802925  -2.2131395  -1.175134   -0.95653445\n",
            "  1.0525126   7.6714396  -0.46902466  5.284525   -0.00858207  2.4449174\n",
            "  1.3987617 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 55 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.0990987  -0.77283514  0.761716    0.8055857   2.1347482  -0.24908516\n",
            "  0.1537105   3.653228    2.5327973  -2.4460506  -1.3565441  -1.0397484\n",
            "  1.3340017   7.7474275   0.20618276  5.734437   -0.30487293  3.4172604\n",
            "  1.587115  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 56 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.4576741  -0.37970987  0.54288965  0.9490243   1.9976689  -0.17780024\n",
            " -0.01892136  2.80355     1.9193019  -1.5787299  -0.88018495 -0.73205847\n",
            "  0.89714235  6.2987933   0.2477799   5.117324    0.0072915   2.2928464\n",
            "  1.1807064 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 57 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.939818   -0.72316915  0.7262337   0.91926414  2.3475702  -0.24156882\n",
            "  0.0890214   3.431547    2.483802   -2.145436   -1.0614141  -0.93314433\n",
            "  0.9393476   7.502111    0.4284335   6.1264563  -0.2672753   3.161262\n",
            "  1.6697885 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 58 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.12148    -0.5791733   0.7991189   0.7995365   2.3370328  -0.35339072\n",
            "  0.05178834  3.6646137   2.3265123  -2.52214    -1.4051651  -1.0351901\n",
            "  1.2262154   8.475143   -0.19258755  5.868119    0.02058195  2.8928342\n",
            "  1.8101913 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 59 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.8948113  -0.73383594  0.76613647  0.7977188   2.240693   -0.28212097\n",
            "  0.06813138  3.2791822   2.3743625  -1.9875149  -1.048595   -0.9027243\n",
            "  1.0038278   7.3846245   0.8324394   6.232699   -0.26135844  3.035935\n",
            "  1.6330659 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 60 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.9004474  -0.5393629   0.7659744   1.0006912   2.2273073  -0.2521227\n",
            "  0.04253657  3.497915    2.322561   -2.1406345  -1.1811844  -0.95249456\n",
            "  0.97731256  7.7200356   0.9108021   6.197945   -0.18401359  3.518931\n",
            "  1.9328077 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 61 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.7831783  -0.6824178   0.81065696  0.79664826  2.051325   -0.07667912\n",
            " -0.01301994  3.4268818   1.9181284  -1.8408381  -0.9703397  -0.7902022\n",
            "  1.1849626   7.0315437   0.78644145  5.9814467  -0.20158021  2.5762196\n",
            "  1.6102481 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 62 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.325066   -0.92112875  1.1305313   0.72800624  2.8506968  -0.47901714\n",
            "  0.20289364  4.379992    2.742825   -2.877664   -1.5606588  -1.1527547\n",
            "  1.6163543   9.136988    0.67656755  6.643212    0.06817933  3.3553534\n",
            "  2.3500986 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 63 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.9743729  -0.61456376  0.69561994  0.8947033   2.3253157  -0.21564078\n",
            "  0.0258984   3.6235917   2.2772694  -1.9864976  -1.1896605  -0.9403081\n",
            "  1.3195591   7.827964    0.87783414  6.5946665  -0.15243542  3.0847774\n",
            "  1.7117858 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 64 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.0069053  -0.6671237   0.9072311   0.814841    2.3607314  -0.38330752\n",
            " -0.07243113  3.8279467   2.3367238  -1.9383997  -1.1184214  -0.9291426\n",
            "  0.99105084  7.6179295   1.6822209   6.444324   -0.50420064  2.871569\n",
            "  1.989994  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 65 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.1580148  -0.8264405   0.959642    0.9470444   2.864846   -0.2949003\n",
            "  0.11523563  4.3126807   2.707799   -2.4265516  -1.3428426  -1.0507731\n",
            "  1.3476832   8.541048    1.5545043   6.728523   -0.31696865  3.8360522\n",
            "  2.116262  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 66 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.1085248  -0.78092456  1.283263    0.655077    2.470258   -0.2957873\n",
            "  0.22297834  4.302124    2.5138245  -2.5386872  -1.2117683  -0.9363825\n",
            "  1.583486    8.834168    0.7785438   7.057115   -0.02725275  2.6064801\n",
            "  2.1340907 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 67 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.3427076  -0.7952933   1.318573    0.9878297   2.9524179  -0.26632297\n",
            "  0.12499113  4.2830634   2.5819035  -2.7530694  -1.3360425  -1.0464464\n",
            "  1.305547    8.982282    2.016857    7.121478   -0.30710876  3.336381\n",
            "  2.426613  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 68 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.695304   -0.45705444  1.3467135   1.0311806   2.4829118  -0.13975532\n",
            "  0.06350987  3.552134    2.3117144  -1.8097425  -1.1429967  -0.7435699\n",
            "  1.3572114   7.5515847   1.6232101   6.7215376  -0.16083041  3.168961\n",
            "  1.6694206 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 69 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.9363515  -0.67470807  1.6352453   0.88324565  2.223805   -0.20675017\n",
            " -0.05198234  3.5832658   2.13492    -1.9505854  -1.0575942  -0.873778\n",
            "  1.2898597   7.4895983   2.1867695   6.4440207  -0.45059812  2.8318014\n",
            "  2.1136355 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 70 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.2248101  -0.78051066  2.326547    0.77102077  2.9441583  -0.36510754\n",
            "  0.0119354   4.6011186   2.7402363  -2.7665546  -1.521093   -1.1491727\n",
            "  2.2705426   9.064382    1.633878    6.517462    0.02616782  3.7212896\n",
            "  2.5422716 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 71 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.8471123  -0.66272825  1.8086413   0.78988934  2.3772118  -0.13831273\n",
            "  0.12081584  3.8503697   2.2745674  -2.0319715  -0.9712744  -0.7158392\n",
            "  1.8686881   7.6840816   1.7509571   6.7956653  -0.22697502  2.7166498\n",
            "  1.891654  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 72 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.1408842  -0.7418527   2.2413816   1.0843934   2.6596346  -0.18783833\n",
            " -0.14689976  4.0422573   2.5290692  -2.3742964  -1.3049152  -0.9366891\n",
            "  2.1579764   8.513918    2.53963     7.082263   -0.3798515   3.6192842\n",
            "  2.6287239 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 73 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.0120425  -0.7026051   2.2836332   1.2071942   2.8208077  -0.18598315\n",
            " -0.1595545   4.248065    2.6142213  -2.133482   -1.0689793  -1.1034992\n",
            "  2.3611479   8.387048    2.1125262   6.897312   -0.19352758  3.8020005\n",
            "  2.2454438 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 74 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.2406032  -0.879765    2.7485151   0.59888756  2.5089998  -0.4226648\n",
            "  0.1870348   4.639027    2.5840597  -2.5056603  -1.342554   -1.0502932\n",
            "  2.671698    9.112919    1.9059081   7.2150383  -0.14594007  2.9590318\n",
            "  2.5326345 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 75 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.1074674  -0.63584274  2.621351    0.8706129   2.9482412  -0.32179862\n",
            "  0.04173727  4.369338    2.614947   -2.561844   -1.2806112  -0.963898\n",
            "  2.8112395   8.825985    2.3092046   6.734424    0.02763871  3.8942258\n",
            "  2.47702   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 76 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.6209669  -0.50414115  2.1894798   0.92383707  2.4802022  -0.16118343\n",
            " -0.16247949  3.7285867   2.0440857  -1.7843406  -0.8746388  -0.7131259\n",
            "  2.2281427   7.383424    1.9155566   6.293525   -0.067198    2.4633293\n",
            "  1.9359274 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 77 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.1833313  -0.73188764  2.8397994   1.1103871   2.856093   -0.18952815\n",
            " -0.05888192  4.2825465   2.6584852  -2.488926   -1.2669998  -0.92977095\n",
            "  3.0026407   8.663461    2.7905607   7.0059342  -0.45000377  3.6052885\n",
            "  2.6671188 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 78 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.2206695  -0.7132055   2.9033458   0.9250003   2.9236565  -0.3316586\n",
            "  0.03215903  4.645892    2.6220033  -2.5709298  -1.4893353  -1.0265578\n",
            "  3.367359    9.603914    1.6514465   6.7987533   0.03262042  3.0353456\n",
            "  2.5649567 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 79 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.9444764  -0.68819577  2.6306205   0.7286587   2.3333337  -0.26134512\n",
            "  0.17264941  4.060815    2.3846283  -2.0650437  -1.0501056  -0.8562174\n",
            "  2.8101156   7.8934197   2.4429197   6.7467527  -0.3535695   2.8588228\n",
            "  2.1528857 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 80 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.9401976  -0.67057306  3.04622     1.0947454   2.9586291  -0.20155123\n",
            " -0.0868652   4.290227    2.5965185  -2.315104   -1.2331766  -0.8739903\n",
            "  3.2452612   8.402578    2.7837093   6.4917274  -0.07805519  3.9425206\n",
            "  2.4620223 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 81 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.5719738  -0.509971    2.4268658   1.0341302   2.4563878  -0.0962288\n",
            " -0.15974024  3.6458452   1.9788934  -1.7042621  -0.9205259  -0.6101479\n",
            "  2.4950004   7.148687    2.0564425   6.2766104  -0.15459372  2.5793276\n",
            "  1.8620524 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 82 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.3305266  -0.8027514   3.5257287   0.91523206  3.2935574  -0.267333\n",
            "  0.03927208  4.707049    2.8838067  -2.9056184  -1.5871822  -1.0475856\n",
            "  4.072202    9.911534    2.2537549   6.8995314   0.2077929   3.6679823\n",
            "  2.83981   ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 83 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.2104282  -0.7949876   3.1080968   0.9325626   2.6949408  -0.3339292\n",
            " -0.12413152  4.45458     2.5366793  -2.3738573  -1.2470497  -1.134569\n",
            "  3.5686798   8.700068    2.1640382   6.7011886  -0.12743986  3.366135\n",
            "  2.4194999 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 84 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.0106409  -0.7580208   3.3023868   0.9737544   2.495253   -0.29784667\n",
            "  0.05350946  4.2697506   2.361263   -2.0576997  -1.1250612  -0.85291296\n",
            "  3.125735    8.36801     2.9511328   7.1819596  -0.47308704  3.2160823\n",
            "  2.44181   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 85 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.172476   -0.72129583  3.968565    1.1271945   3.2801871  -0.3395911\n",
            " -0.08404562  4.7046328   2.7676327  -2.4587164  -1.3118148  -1.0712144\n",
            "  3.8823333   8.867429    3.0226462   6.8341856  -0.22057685  4.3229175\n",
            "  2.6740139 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 86 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.1036055  -0.7463967   3.7661402   0.5980936   2.6223717  -0.40645963\n",
            "  0.0128714   4.6813455   2.4352944  -2.4786122  -1.3206149  -0.8246943\n",
            "  3.5892096   8.876424    1.6955911   6.8586187  -0.08664869  2.6363099\n",
            "  2.499728  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 87 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.1146266  -0.7203064   3.5835881   0.9439212   2.74638    -0.32465455\n",
            " -0.0415409   4.2984776   2.773495   -1.913738   -1.2298743  -0.8368765\n",
            "  3.433958    8.337502    2.8868954   7.1912084  -0.55173177  3.2273808\n",
            "  2.5241892 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 88 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.4232113  -0.30786902  2.9281313   0.8535242   2.1056101  -0.2094757\n",
            " -0.19512853  3.501707    1.7839229  -1.0271376  -0.8720582  -0.61723876\n",
            "  2.4965503   6.4336863   2.1700106   5.566666   -0.2523582   2.3020015\n",
            "  1.8457457 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 89 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.6957828e+00 -3.7817803e-01  3.7783532e+00  5.9485859e-01\n",
            "  2.4208376e+00 -2.8323421e-01 -9.3861848e-02  3.7924659e+00\n",
            "  1.9129403e+00 -9.8568130e-01 -9.8202920e-01 -5.2212757e-01\n",
            "  2.9478214e+00  7.5783916e+00  1.8329076e+00  5.6858139e+00\n",
            " -3.4316909e-03  2.3016827e+00  2.1551392e+00]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 90 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.2658284  -0.47025263  4.8779674   0.8548599   2.8279865  -0.44345367\n",
            " -0.08824269  3.888141    2.2699564  -1.2547829  -1.3446414  -0.86422503\n",
            "  3.7302613   9.301047    1.9583873   6.566764   -0.12164692  2.4380238\n",
            "  2.5802467 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 91 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.125401   -0.71940327  3.9610727   0.85550344  2.6848671  -0.340626\n",
            " -0.07016256  4.098279    2.5075445  -1.0376906  -1.2655346  -0.9007599\n",
            "  3.5969338   8.3670025   2.8847322   6.0945306  -0.54677874  3.061816\n",
            "  2.5723848 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 92 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.6505314  -0.43711722  3.4413552   0.87063724  2.2403955  -0.25806373\n",
            " -0.08463094  3.8241887   2.1137433  -0.4005918  -1.0030533  -0.74758816\n",
            "  2.8149085   7.091897    2.3025365   5.8947086  -0.31059518  2.7953742\n",
            "  1.9720781 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 93 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.482628   -0.5527357   5.1014795   0.7772566   2.8351016  -0.5457546\n",
            " -0.09002433  4.275308    2.3052208  -0.6562017  -1.3565705  -0.93401796\n",
            "  3.8560622   9.773085    2.0373306   6.4490976  -0.10896608  2.5978615\n",
            "  2.7417383 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 94 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.6445084  -0.8754011   5.6813273   0.7618009   3.065542   -0.60607046\n",
            "  0.28144634  4.713236    3.0197403  -0.7000395  -1.4928269  -1.2557178\n",
            "  4.2244577  10.444679    2.353949    6.726493   -0.37079555  3.5148704\n",
            "  3.39774   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 95 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.5818262  -0.45943817  3.6157744   0.7614877   2.211082   -0.24440388\n",
            "  0.0277832   3.7985425   1.9884382   0.10107606 -0.8825329  -0.69767183\n",
            "  2.7722917   6.970812    2.3758883   5.3746824  -0.27198568  2.6992252\n",
            "  2.3211167 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 96 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.8545837e+00 -4.7313395e-01  4.3491058e+00  6.8709868e-01\n",
            "  2.4614587e+00 -2.0376697e-01  7.5869530e-02  3.9100733e+00\n",
            "  2.3207514e+00 -5.7733748e-03 -1.0610129e+00 -6.9779658e-01\n",
            "  3.3675642e+00  8.1661158e+00  2.2887437e+00  5.5473156e+00\n",
            " -3.3622641e-02  2.6616745e+00  2.7813325e+00]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 97 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.8551471  -0.7763019   6.092794    0.93055004  3.139914   -0.53938675\n",
            " -0.20684783  4.6356144   2.773768   -0.29448947 -1.5808473  -1.0014769\n",
            "  4.612042   10.4229965   2.9419708   6.4086857  -0.3996144   3.0200632\n",
            "  4.3348346 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 98 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.6699452  -0.40079316  3.7574227   0.8244388   2.265996   -0.19994162\n",
            "  0.04942845  3.8539486   2.0286472   0.20415822 -0.9545131  -0.62882596\n",
            "  3.0017917   7.1882277   2.4579875   5.0556955  -0.4247382   2.5756574\n",
            "  2.848689  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 99 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.3808117  -0.3454456   3.3883586   0.7285652   2.1704247  -0.19631538\n",
            " -0.02155578  3.6215081   1.8917425   0.32811436 -0.9087718  -0.64580166\n",
            "  2.6895108   6.592672    2.0907824   4.52842    -0.15908395  2.4480486\n",
            "  2.7004771 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 100 reward=-1 new_state=[0 0 0 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-2.660073   -0.8849667   6.072547    0.82510996  3.173153   -0.3997553\n",
            "  0.20902075  4.168298    3.0882163   0.02335918 -1.4530662  -1.0175666\n",
            "  4.4760337  10.3768835   2.8724384   5.949306   -0.25012723  3.3212566\n",
            "  4.449494  ]\n",
            "Epsilon reduced to 0.12800000000000003\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1673346  -0.5615466   5.582163    0.94306856  3.021976   -0.08673652\n",
            "  0.50842875  3.4474313   2.8265553   0.03724346 -1.4259216  -0.8640793\n",
            "  4.2800436   9.662986    2.4116468   4.891944    0.06529982  3.891806\n",
            "  3.9738245 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.7348421  -0.5773839   4.9843364   0.8148138   2.7584376  -0.24607147\n",
            "  0.15815698  3.8420572   2.169148    0.44648936 -1.0514237  -0.8182227\n",
            "  3.5335476   8.093594    2.2582908   3.6047554   0.26142752  3.6356015\n",
            "  3.8721883 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.9875224  -0.4751417   5.1384964   1.0306646   2.974388    0.0156225\n",
            "  0.54075617  3.4135106   2.4649806   0.26446474 -1.2943108  -0.79722285\n",
            "  4.055187    9.159161    2.4538786   4.7424235  -0.05099839  3.833063\n",
            "  3.9827511 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.9556373  -0.71067023  5.0458627   0.92415667  2.9010541  -0.18654275\n",
            "  0.21937354  3.9871511   2.3111022   0.54994583 -1.1391658  -0.89267606\n",
            "  3.731529    8.681616    2.3050961   4.1658754   0.14972644  3.7689993\n",
            "  4.209951  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.0363710e+00 -5.7224226e-01  5.3544836e+00  1.0372723e+00\n",
            "  3.1130548e+00  8.7519269e-03  2.7176028e-01  3.7914200e+00\n",
            "  2.4846904e+00  5.6079578e-01 -1.2697235e+00 -7.7790755e-01\n",
            "  4.0745001e+00  9.1177034e+00  2.7858846e+00  5.6815715e+00\n",
            "  2.4035376e-02  3.9489865e+00  4.2024035e+00]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.6832993  -0.5542999   4.709111    0.7318204   2.488391   -0.12254386\n",
            "  0.19790938  3.5090542   2.009635    0.51715773 -1.0740212  -0.7340335\n",
            "  3.2656755   7.529875    2.1648538   3.9843588   0.06726909  3.1802382\n",
            "  3.8875911 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.0665     -0.5762412   5.666174    0.78556585  3.3554826  -0.24336013\n",
            "  0.1166473   3.8779013   2.3404872   0.79695904 -1.1290398  -0.88586533\n",
            "  4.244036    9.301131    2.6868892   5.759968    0.4134804   3.8433692\n",
            "  4.466135  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.87248    -0.76941967  5.4479785   0.89070255  3.0196187  -0.21758048\n",
            "  0.16598168  4.47467     2.55254     0.82753855 -1.10964    -0.8707815\n",
            "  4.1647663   8.502304    2.417185    5.3202224  -0.14224297  3.9781682\n",
            "  4.25465   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.0298324  -0.5481309   5.714481    0.8218297   3.333381   -0.24442057\n",
            "  0.30846584  3.3045459   2.342474    0.6728556  -1.1875633  -0.87507874\n",
            "  4.1559906   9.243393    2.6061537   6.127213    0.2506126   3.8843322\n",
            "  4.395598  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.9633753  -0.6991256   5.5654426   0.98375314  3.1470566  -0.20977834\n",
            "  0.27401122  3.9766958   2.4135628   0.7125974  -1.2625879  -0.8995598\n",
            "  3.9397213   8.9598055   2.5637484   6.0639396   0.11598652  4.0079308\n",
            "  4.584714  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1273284  -0.54381907  5.6738334   1.0943296   3.3632574  -0.04721067\n",
            "  0.33021647  4.195099    2.78192     0.7251522  -1.3232585  -0.85540193\n",
            "  4.3278375   9.840785    2.7729003   7.698963    0.12958564  4.1380796\n",
            "  4.747777  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.9801917  -0.67994785  5.4807463   0.9792027   2.8197343  -0.06468661\n",
            "  0.34684163  4.45784     2.6326668   0.8138453  -1.3049738  -0.8668703\n",
            "  3.9630089   9.042566    2.5867555   7.199207    0.01514213  4.0351086\n",
            "  4.772794  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1977372e+00 -5.7386965e-01  5.8171587e+00  1.1697264e+00\n",
            "  3.3904607e+00 -2.7421189e-02  5.2101481e-01  3.8099587e+00\n",
            "  2.8471022e+00  6.3774353e-01 -1.4378359e+00 -9.0305465e-01\n",
            "  4.3591852e+00  1.0067895e+01  2.6365809e+00  8.3780947e+00\n",
            "  8.5049402e-03  4.3223109e+00  4.8280234e+00]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.8530552  -0.561272    5.078159    0.8093977   2.7573829  -0.15278676\n",
            "  0.13870771  4.245225    2.3738995   0.8449733  -1.1366818  -0.8661261\n",
            "  3.7513986   8.403372    2.4649744   7.034907    0.04069211  3.3952224\n",
            "  4.5733614 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.27812    -0.60604066  5.9865603   1.2285664   3.514622   -0.03312543\n",
            "  0.48961213  3.791663    2.8799324   0.7893687  -1.4256438  -0.9131956\n",
            "  4.518446   10.036554    2.8051617   8.903377   -0.1663691   4.41068\n",
            "  4.9657617 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.0550048  -0.7756485   5.7614746   0.8874192   3.3009276  -0.24851185\n",
            "  0.03862314  4.556435    2.4906857   1.0779967  -0.91748583 -0.9907781\n",
            "  4.4579325   8.978703    2.3664818   7.4990516   0.08312456  3.876815\n",
            "  4.687715  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.168741   -0.5682675   5.842445    1.0816327   3.2320557  -0.05992297\n",
            "  0.63305587  4.1834273   2.8267689   0.891299   -1.297999   -1.0116925\n",
            "  4.357845    9.958786    3.1387174   9.00482    -0.09682982  4.1338224\n",
            "  4.8237004 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3871846  -0.6251208   6.3789253   1.2580178   3.502437   -0.04951582\n",
            "  0.5571981   4.3440757   2.9789438   0.89236546 -1.4505893  -1.0160736\n",
            "  4.610061   10.582401    3.2928863   9.825529    0.05147195  4.698919\n",
            "  5.3484917 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.0625572  -0.5089192   5.891421    1.0462788   3.277547   -0.05941824\n",
            "  0.66130066  3.9308705   2.759285    0.86936164 -1.3135383  -0.991265\n",
            "  4.360383    9.864835    3.57625     9.08133    -0.01807402  4.1952276\n",
            "  4.741272  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.11996961e+00 -6.82427227e-01  6.15338135e+00  1.15065753e+00\n",
            "  3.49872375e+00 -3.10365856e-03  1.08361974e-01  4.66748857e+00\n",
            "  2.86212015e+00  1.01736879e+00 -1.25349975e+00 -1.03852570e+00\n",
            "  4.67134428e+00  9.57877636e+00  3.38605714e+00  8.98782063e+00\n",
            " -1.60526671e-02  4.48580647e+00  4.96522903e+00]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1168017  -0.6939869   5.991692    1.0429577   3.3436353  -0.13733125\n",
            "  0.46004936  3.9966176   2.8238714   0.94959646 -1.2122443  -0.96123487\n",
            "  4.342774    9.81788     3.7271426   9.017613   -0.11183847  4.120341\n",
            "  4.7151036 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.8455297e+00 -5.5095547e-01  6.1466012e+00  1.2293355e+00\n",
            "  3.3928742e+00 -4.6911915e-03  5.3403211e-01  4.0832148e+00\n",
            "  2.9126329e+00  8.0738157e-01 -1.4092646e+00 -9.2758209e-01\n",
            "  4.4811554e+00  1.0152837e+01  3.8804977e+00  9.2953463e+00\n",
            "  3.8473003e-02  4.4671879e+00  5.1749148e+00]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.3738573  -0.7409288   6.103664    1.1505752   3.4217618  -0.09356387\n",
            "  0.23002003  4.248924    2.654416    1.0437082  -1.2952465  -0.8604912\n",
            "  4.454183   10.006316    4.149632    9.137498    0.20354721  4.194044\n",
            "  5.0117383 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.0958515  -0.603713    5.8963737   1.1640989   3.3525755  -0.01880724\n",
            "  0.26250756  4.6963797   2.8885732   0.9457105  -1.3467098  -0.95415187\n",
            "  4.426311   10.008877    3.9694777   9.39911     0.4550009   4.244072\n",
            "  5.389297  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.9434071  -0.6811142   5.9622865   1.0112594   3.41124    -0.19927624\n",
            "  0.29740584  3.666229    2.427425    0.9685916  -1.199198   -0.92595065\n",
            "  4.5586925   9.973703    4.069607    8.638045    0.91587734  3.9437394\n",
            "  4.9723444 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.27715784 -0.70912135  5.9652905   1.1524457   3.5726087   0.02130131\n",
            "  0.15413256  4.7282524   2.8559947   1.0468196  -1.2633846  -0.99162644\n",
            "  4.836737    9.597199    3.9214225   8.1905155   1.0461409   4.3137083\n",
            "  4.9817257 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.40819654 -0.624397    5.7181544   1.0526018   2.9765928  -0.0218143\n",
            "  0.4985542   3.768335    2.667537    0.9881044  -1.1592454  -0.96625775\n",
            "  4.189528    9.300492    4.243788    7.79368     1.2148694   3.8436065\n",
            "  4.6382003 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.01720269 -0.70677507  5.4899545   0.9722325   3.0850391  -0.12589207\n",
            "  0.24187945  4.1687474   2.722096    1.0414124  -1.0586009  -0.81583816\n",
            "  4.0170217   8.4487      3.7585309   6.9363427   1.2934095   3.9681666\n",
            "  4.4196067 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.02579947 -0.6263224   5.7515736   1.0561725   3.0997095  -0.01687614\n",
            "  0.5266353   3.7059996   2.5698125   0.8920406  -1.2438959  -0.8040999\n",
            "  4.238626    9.522508    4.4241443   7.7019796   1.7004663   4.0648746\n",
            "  4.660811  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.4579849  -0.6814153   5.196522    0.9233324   2.9270687  -0.03028456\n",
            "  0.08940633  4.2024784   2.5277963   1.0703448  -0.940545   -0.80867714\n",
            "  3.894032    7.91161     3.7397761   6.626144    1.5390131   3.5701792\n",
            "  4.162742  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.3249079  -0.56618094  5.546439    1.038384    2.898374    0.05223644\n",
            "  0.4781732   3.4021857   2.4534674   0.8527044  -1.1775563  -0.7818521\n",
            "  4.0689993   9.059131    4.348402    7.068351    2.0846047   3.898416\n",
            "  4.4622674 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.7150039  -0.56897414  5.4629836   1.0968691   3.0022144   0.03075964\n",
            "  0.26244554  4.1204395   2.5229425   0.86162984 -1.2498976  -0.83460754\n",
            "  3.9566488   9.036865    4.151325    7.443406    2.40228     4.1238227\n",
            "  4.8811827 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.51320857 -0.6639732   5.602059    1.0831416   3.0651183  -0.03472297\n",
            "  0.4692045   3.5986705   2.4854193   0.90236616 -1.2328911  -0.7862513\n",
            "  4.106078    9.3350525   4.5628552   7.069898    2.3347769   4.0339775\n",
            "  4.6361275 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.98575157 -0.4902368   5.4213567   1.0265777   3.0498974   0.02243725\n",
            "  0.33734658  4.233121    2.610237    0.99124706 -1.2720898  -0.90373737\n",
            "  4.2454734   9.134887    4.079424    6.9988365   2.7524555   4.036576\n",
            "  4.845886  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.0235928  -0.6297225   5.495181    1.0361416   3.1251783   0.03831127\n",
            "  0.20803468  4.044863    2.53302     0.9946439  -1.1023804  -0.78325456\n",
            "  4.2076902   9.134763    4.2300444   6.974576    2.5917222   3.8698\n",
            "  4.6071997 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.98153037 -0.68567514  5.442105    0.87770915  3.126058   -0.1942968\n",
            "  0.15557072  3.880643    2.3215258   0.99435794 -0.6089938  -0.86073464\n",
            "  4.136101    8.225229    3.7095656   6.1633983   2.4939036   3.6932242\n",
            "  4.329681  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.84516186 -0.52055144  5.326413    0.9997555   2.705929    0.0845456\n",
            "  0.528263    3.3922672   2.3458226   0.8922173  -0.43276274 -0.7661067\n",
            "  3.9558995   8.756536    4.317221    6.4061823   2.7398767   3.8953915\n",
            "  4.3617067 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.1745666  -0.56220776  5.2493777   0.9338293   2.729341    0.01024896\n",
            "  0.32629934  4.1528754   2.4801059   0.954677   -0.05654415 -0.78136885\n",
            "  3.6521454   8.552347    4.0032387   6.9856234   2.8985324   3.9376242\n",
            "  4.6320543 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.8257693  -0.56090415  5.3292503   0.82455087  2.8504653  -0.13400829\n",
            "  0.29397956  3.033409    2.0542982   0.8934933   0.0882923  -0.76563615\n",
            "  3.9086514   8.487674    4.140121    6.189988    2.9522333   3.5148258\n",
            "  4.2532253 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.4727198  -0.66380495  5.1761      0.8805287   2.8372335  -0.04853635\n",
            "  0.12153374  4.062108    2.4379578   1.0679445   0.4518098  -0.7906091\n",
            "  3.9482155   7.946741    3.8299818   6.0683994   2.5608268   3.759684\n",
            "  4.4357595 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.3292489e+00 -5.2972728e-01  5.4924817e+00  9.5936763e-01\n",
            "  3.1137927e+00  3.0221296e-03  3.0420145e-01  3.8648248e+00\n",
            "  2.5171347e+00  9.9332231e-01  6.0446543e-01 -7.1536398e-01\n",
            "  3.9763951e+00  8.8832932e+00  4.6213341e+00  6.7698188e+00\n",
            "  3.1026127e+00  3.8833058e+00  4.9774942e+00]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.1254952  -0.5893857   5.05968     0.86437964  2.7115211  -0.1824735\n",
            "  0.11441275  3.814488    2.1796484   1.0719843   0.71357644 -0.7807916\n",
            "  3.4430273   7.9566345   3.729559    6.56041     3.011837    3.5915108\n",
            "  4.938962  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.1803632  -0.49524078  5.379894    1.0533539   2.9920115   0.01931483\n",
            "  0.41720092  3.2970145   2.4176648   0.9115535   0.8212667  -0.8158007\n",
            "  3.8699675   8.904197    4.753439    6.6090107   3.2694516   3.8779166\n",
            "  5.0637894 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.4719585  -0.6845017   5.009686    0.94335073  2.9196064  -0.04625052\n",
            "  0.0603918   4.1024485   2.4568455   1.1222394   1.178741   -0.8066425\n",
            "  3.8022962   7.813216    3.934696    6.3980184   2.6592867   3.505544\n",
            "  4.791883  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.2535032  -0.5265801   5.3822722   1.002658    2.862       0.05148824\n",
            "  0.48302346  3.3528626   2.403686    0.90066606  1.1805913  -0.7790272\n",
            "  4.0092754   8.882628    4.5498247   6.4798555   3.2931275   3.772796\n",
            "  5.1118884 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.6954519e+00 -6.7139637e-01  5.3599701e+00  8.3414799e-01\n",
            "  2.9603457e+00 -1.7577296e-01 -4.2657116e-03  4.0960350e+00\n",
            "  2.2539763e+00  1.2413003e+00  1.5271567e+00 -8.5707688e-01\n",
            "  3.9828715e+00  7.9638872e+00  3.6819978e+00  6.5531282e+00\n",
            "  2.9320908e+00  3.6764283e+00  5.2250624e+00]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.7842404  -0.5432311   5.4915023   0.97720927  3.0053236   0.01536228\n",
            "  0.43765017  3.9459217   2.4179258   1.0431597   1.5780127  -0.7345254\n",
            "  4.1133447   9.082664    4.6176934   6.951154    3.4637673   3.8625073\n",
            "  5.6347027 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.6276062  -0.46172002  4.92106     0.9381384   2.7457745  -0.2751391\n",
            "  0.13968268  3.5013201   1.9468738   0.93127674  1.5277534  -0.69988227\n",
            "  3.2885115   7.6794834   3.7663622   6.2776585   3.2581508   2.8210049\n",
            "  5.380897  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.6399204  -0.42023674  4.6971884   0.7365143   2.5953894  -0.2451729\n",
            "  0.18502948  3.063687    1.7932457   0.91725737  1.4321623  -0.33810624\n",
            "  3.189915    7.078244    3.6551826   5.5014977   2.8380916   2.5776823\n",
            "  4.5798473 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 2.0577786  -0.43151778  4.863683    0.7890786   2.6347744  -0.17838076\n",
            "  0.02081438  3.821528    1.8922018   0.94970685  1.8343431   0.00792344\n",
            "  3.277309    7.433298    3.6577961   6.403489    3.2558339   2.8139966\n",
            "  5.6708746 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 51 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.7038276  -0.2833279   4.5709605   0.7547819   2.422111   -0.1451579\n",
            "  0.25757536  2.9199538   1.7707655   0.91046107  1.4923747   0.15413871\n",
            "  3.140643    6.911616    3.7119663   5.3061986   2.8474188   2.460359\n",
            "  4.6138124 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 52 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 2.2793446  -0.49898466  4.610806    0.7943308   2.6367104  -0.15118939\n",
            " -0.02987629  3.317839    1.8995647   0.93769044  1.6031272   0.38373756\n",
            "  3.3174582   6.611662    3.4674506   5.522817    2.7440205   2.5439923\n",
            "  4.6201696 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 53 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.9654344  -0.4617042   4.8232703   0.898965    2.6929846  -0.21524273\n",
            "  0.12883404  3.102144    1.8896955   0.9898569   1.740331    0.61190253\n",
            "  3.3308313   7.592236    4.0269866   6.124278    3.1699638   2.5536287\n",
            "  5.1927676 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 54 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 2.7964082  -0.5533428   4.838041    0.85779417  2.9437935  -0.1733901\n",
            " -0.17302677  3.9123068   2.0494452   1.1412163   2.0132802   0.7849362\n",
            "  3.585577    7.164244    3.5665913   6.4946284   2.850958    2.7148085\n",
            "  5.2822328 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 55 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 2.2003198  -0.49413538  4.895383    0.88012606  2.7323012  -0.18481007\n",
            "  0.11796285  3.3494382   1.943567    0.8877825   1.8823233   1.1524498\n",
            "  3.4754095   7.730167    4.136382    6.273478    3.249422    2.6046288\n",
            "  5.3861637 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 56 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 2.61032    -0.48983166  4.9476166   0.84534514  2.763671   -0.20914723\n",
            " -0.0177385   3.8519022   1.9836658   0.9653712   2.1742408   1.379309\n",
            "  3.258524    7.648598    3.8489048   7.1878295   3.3511422   2.933241\n",
            "  6.0469737 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 57 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 2.2854068  -0.41292295  5.0411987   0.83583915  2.7867625  -0.173289\n",
            "  0.15976341  3.1442158   1.9674729   1.0358292   1.9684932   1.2933276\n",
            "  3.5056825   7.622204    4.205911    6.640791    3.2849565   2.4928997\n",
            "  5.3091025 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 58 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 3.1621387  -0.53580606  5.1968265   0.8719957   2.9743946  -0.1578822\n",
            " -0.1300879   4.1061482   2.0466805   1.1588811   2.335165    1.5454323\n",
            "  3.8462777   7.444611    3.7420835   7.137052    3.1132872   2.8544383\n",
            "  5.743164  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 59 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 2.3365347  -0.3146629   5.0014687   0.821136    2.7429667  -0.16034569\n",
            "  0.2296316   3.0590925   1.9756572   0.9901809   1.9844948   1.592907\n",
            "  3.375882    7.5623446   4.266588    6.940118    3.3768232   2.6089568\n",
            "  5.3988147 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 60 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 3.1551785  -0.535837    4.876168    0.7980878   2.8164847  -0.12756225\n",
            " -0.1796842   3.9453068   2.1057627   1.1166108   2.1933966   1.5888824\n",
            "  3.6310117   7.0971193   3.7224236   7.1128626   2.8861864   2.7525551\n",
            "  5.2766128 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 61 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.3119912  -0.25722924  4.882118    0.83510244  2.63748    -0.13486144\n",
            "  0.2852951   3.2628503   1.9121202   0.97524905  1.8390574   1.6140423\n",
            "  3.3969402   7.4777837   3.9322116   6.5983305   3.0901911   2.6797578\n",
            "  5.1563177 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 62 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.3195088  -0.24557029  4.575624    0.75860965  2.523642   -0.02611223\n",
            "  0.3465411   3.2800527   1.8023288   0.8858178   1.7569873   1.6014459\n",
            "  3.236002    7.252022    3.5346751   6.585722    2.8673258   2.9915822\n",
            "  4.913278  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 63 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.3926632  -0.22523144  4.783576    0.77611536  2.6550198  -0.05526655\n",
            "  0.35467696  3.2552066   1.8622633   0.96611077  1.8554385   1.7469207\n",
            "  3.374016    7.498447    3.7530563   6.7583656   3.02718     3.1867208\n",
            "  5.055525  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 64 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.6996176e+00 -3.0464476e-01  4.6363549e+00  8.0518502e-01\n",
            "  2.6770349e+00 -7.6122857e-03  3.1220353e-01  3.5098128e+00\n",
            "  1.8969399e+00  9.7495264e-01  1.9141752e+00  1.7804275e+00\n",
            "  3.3678689e+00  7.6365762e+00  3.6308243e+00  7.0882154e+00\n",
            "  2.9293253e+00  3.4554670e+00  5.1653457e+00]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 65 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.9038064  -0.34482285  5.02037     0.90660334  2.845436   -0.04468756\n",
            "  0.35533008  3.6421618   1.9855791   1.0728642   2.01905     2.0729194\n",
            "  3.6576905   8.154862    4.041723    7.55213     3.1612496   3.76719\n",
            "  5.591737  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 66 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 3.3925676  -0.29836923  5.0367866   0.74107003  2.8804297  -0.03418856\n",
            "  0.23115145  4.389267    2.005781    1.2028946   2.4413369   2.3206196\n",
            "  3.853288    8.251679    3.7665067   7.807727    3.0946238   3.9968114\n",
            "  6.0648384 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 67 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 3.3133469  -0.2976372   5.221035    0.80049425  2.8714933  -0.10663449\n",
            "  0.39086133  3.7117705   2.014001    1.1544888   2.1187913   2.1771162\n",
            "  3.6558459   8.209126    4.0680537   7.880768    3.2885442   4.137631\n",
            "  5.7037325 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 68 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 3.640924   -0.41614822  5.7323213   0.7729663   2.9070158  -0.19291869\n",
            "  0.45561218  3.6190243   2.4178362   0.8038858   2.1956913   2.336777\n",
            "  3.931705    9.185538    3.970717    8.585356    3.4542117   4.005994\n",
            "  5.986068  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 69 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 3.8447344  -0.4276985   5.492681    0.711433    2.6414385  -0.10657284\n",
            "  0.26853225  4.21422     2.3107371   0.88808215  2.1289337   2.3095345\n",
            "  3.8059964   8.500249    3.8738558   8.003552    3.4647427   4.180859\n",
            "  5.7727985 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 70 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 3.9317474  -0.36107448  5.316342    0.8230004   3.0111387  -0.05567142\n",
            "  0.04942058  4.2253847   2.0909243   1.2484115   2.465339    2.6746545\n",
            "  3.8395221   8.350979    4.5227313   8.468467    3.2361403   4.654986\n",
            "  6.1285963 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 71 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 4.062362   -0.25192133  5.1560345   0.8296032   2.9617329  -0.01171679\n",
            "  0.36758086  3.9900634   2.0951545   1.1398581   2.2778058   2.3080719\n",
            "  3.7326734   8.288859    4.450252    8.466675    3.3002412   4.527126\n",
            "  5.759833  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 72 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 4.1869535  -0.32828197  5.5213876   0.8608369   3.007955   -0.02402397\n",
            "  0.34358716  4.035885    2.1890724   1.2098845   2.3293984   2.472113\n",
            "  4.041898    8.768363    5.1271896   9.098243    3.5579658   5.1153483\n",
            "  6.149863  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 73 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 4.2433934  -0.2814032   5.184838    0.7555504   2.894952    0.04077792\n",
            "  0.2466544   4.1596823   2.1175957   1.2264535   2.3679302   2.502566\n",
            "  3.7376626   8.347127    4.8057313   8.733687    3.2206397   5.278905\n",
            "  5.875152  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 74 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 4.691253   -0.3980898   5.7497096   0.9151695   3.3860154  -0.06710961\n",
            "  0.10385349  4.7615643   2.3719964   1.3779336   2.7987843   3.119847\n",
            "  4.347828    9.343908    5.5469027   9.757498    3.5479193   6.329537\n",
            "  6.7946777 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 75 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 4.6420164  -0.36218148  5.4922166   0.9216229   3.1209185  -0.02481271\n",
            "  0.2497719   4.2968035   2.624133    1.2514192   2.3739464   2.6367536\n",
            "  3.9016304   8.862827    5.4477463   9.484262    3.4265957   6.535008\n",
            "  6.199686  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 76 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 5.27909    -0.5763412   6.7873497   0.81233513  3.5555868  -0.33705652\n",
            "  0.425688    5.1450243   3.728429    1.2345599   3.1529722   3.6035213\n",
            "  4.9280057  11.15299     5.917727   10.873225    3.989523    7.4857364\n",
            "  7.8000255 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 77 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 5.5628786  -0.64320564  6.9385524   0.81672245  3.2402341  -0.23624162\n",
            "  0.39703438  5.129602    4.1351094   1.1065936   2.9114954   3.3629293\n",
            "  4.836047   11.135541    6.2315946  11.17141     4.0012436   8.053999\n",
            "  7.710796  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 78 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 5.212004   -0.3589859   6.2371845   0.8874279   3.461481   -0.04763409\n",
            "  0.11492207  4.9643197   3.8931692   1.4585117   2.9683762   3.363398\n",
            "  4.554141    9.691145    6.7174144  10.414389    3.8285086   8.6761055\n",
            "  7.1295547 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 79 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 5.137445   -0.34107116  5.746889    0.9309679   3.1203182   0.02213341\n",
            "  0.21636492  4.8322096   4.0074754   1.2926817   2.5506623   2.8443527\n",
            "  4.232493    9.305019    6.6770663  10.161855    3.4621372   8.584326\n",
            "  6.4118714 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 80 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 5.6127157  -0.31965145  6.537618    1.0168495   3.6229568  -0.08695582\n",
            "  0.40552127  4.5999866   4.5792127   1.4024029   2.775805    3.1821315\n",
            "  4.69877    10.218546    7.899294   11.080119    4.151203    9.778704\n",
            "  7.112763  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 81 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 5.685255   -0.43841392  6.3826933   0.95646816  3.54777    -0.05407944\n",
            "  0.35103962  5.21311     4.786715    1.3608278   2.781115    3.279282\n",
            "  4.6207504  10.253988    7.743845   11.245258    3.84468    10.285061\n",
            "  7.222771  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 82 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 5.924236   -0.33328402  6.7925615   1.0526032   3.7633197  -0.09229098\n",
            "  0.41245317  4.7950063   5.212858    1.4653378   2.8979614   3.3438342\n",
            "  4.892533   10.612548    8.641028   11.56261     4.310315   11.16056\n",
            "  7.4051166 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 83 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 5.5097003  -0.319996    5.819662    1.0219824   3.369951    0.02697594\n",
            "  0.18315442  5.2627316   5.2604136   1.5139223   2.8307216   3.0585644\n",
            "  4.5033283  10.131273    7.85803    10.996238    3.5979493  10.912345\n",
            "  6.649986  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 84 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 5.5731196  -0.41363645  5.8753448   1.1782337   3.5914514  -0.02058386\n",
            "  0.13180536  5.4502816   5.598129    1.6079006   2.812385    3.272192\n",
            "  4.579964   10.652937    8.329066   11.336103    3.5747051  11.593937\n",
            "  6.795372  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 85 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 5.689203   -0.48539573  6.1447444   1.2609646   3.6982214   0.01405428\n",
            "  0.17030399  5.797984    5.9680567   1.729864    2.9574914   3.4756343\n",
            "  4.8153133  11.484565    8.848217   11.870507    3.738737   12.395233\n",
            "  7.2183614 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 86 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 5.728807   -0.4901695   6.157296    1.2312098   3.6063206   0.02243951\n",
            "  0.12668674  5.712112    6.028749    1.6913599   2.8893983   3.4764264\n",
            "  4.7328963  11.816154    8.995441   11.804135    3.701269   12.747255\n",
            "  7.156013  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 87 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 5.8210998  -0.42906702  6.3295197   1.108169    3.6038334  -0.05322253\n",
            "  0.15998785  5.751882    6.2157454   1.7734994   2.9543903   3.44745\n",
            "  4.6865516  12.314756    9.152509   11.779453    3.8091197  13.141499\n",
            "  7.211909  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 88 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 5.8912988e+00 -3.7055543e-01  6.2780313e+00  1.1386415e+00\n",
            "  3.6090157e+00 -1.4148420e-03  1.3241863e-01  5.5930672e+00\n",
            "  6.3268652e+00  1.6610686e+00  2.9089794e+00  3.3982115e+00\n",
            "  4.7051387e+00  1.2781469e+01  9.1763716e+00  1.1637219e+01\n",
            "  3.8022094e+00  1.3259019e+01  7.0068688e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 89 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.107763   -0.35743317  6.40043     1.1595699   3.7612505  -0.02359997\n",
            "  0.15471454  5.7665577   6.6456313   1.7346467   3.0596702   3.476265\n",
            "  4.894829   13.806986    9.434279   11.995488    3.9341974  13.803281\n",
            "  7.2370353 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 90 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.119265   -0.42881975  6.5640507   1.192658    3.703576    0.02098578\n",
            "  0.14596437  5.9289207   6.788165    1.7871735   3.0746937   3.6068459\n",
            "  4.953691   14.543782    9.77463    12.248402    3.989835   14.329166\n",
            "  7.460345  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 91 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.2365427e+00 -3.9467359e-01  6.6121483e+00  1.1886542e+00\n",
            "  3.7977371e+00 -1.4698104e-02  1.2747133e-01  5.8744063e+00\n",
            "  6.9370852e+00  1.7380430e+00  3.0678065e+00  3.6001868e+00\n",
            "  4.9607520e+00  1.5123854e+01  9.9099321e+00  1.2357469e+01\n",
            "  4.0020542e+00  1.4564097e+01  7.3934960e+00]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 92 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.361484   -0.40412518  6.7394576   1.2086031   3.870966   -0.02079136\n",
            "  0.1234597   5.9829073   7.147301    1.7655609   3.1274157   3.6755054\n",
            "  5.0581007  15.891117   10.1645155  12.863181    4.0773234  15.0081835\n",
            "  7.53931   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 93 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.599076   -0.3927434   6.891397    1.2360471   4.043732   -0.04603034\n",
            "  0.14410524  6.184264    7.482866    1.8441685   3.2923295   3.7681143\n",
            "  5.2731957  16.937744   10.441032   13.711017    4.225889   15.575948\n",
            "  7.8029637 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 94 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.6384411e+00 -4.7084469e-01  7.1013689e+00  1.2777364e+00\n",
            "  4.0113721e+00 -5.2331425e-03  1.2818609e-01  6.3907084e+00\n",
            "  7.6554346e+00  1.9032834e+00  3.3264108e+00  3.9254754e+00\n",
            "  5.3654566e+00  1.7653444e+01  1.0830191e+01  1.4598616e+01\n",
            "  4.3079476e+00  1.6160007e+01  8.0782452e+00]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 95 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.9450874  -0.48260915  7.234165    1.3604497   4.3142853  -0.09375249\n",
            "  0.07005223  6.476041    7.9897723   1.9369563   3.4521232   3.9891803\n",
            "  5.526071   18.770622   11.202161   15.665563    4.3971124  16.780535\n",
            "  8.29734   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 96 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.1431313  -0.5752085   7.49565     1.4503442   4.461762   -0.05517774\n",
            "  0.11975629  6.9354725   8.37802     2.035147    3.6171303   4.2891335\n",
            "  5.867728   20.09661    11.731873   17.171682    4.550541   17.616806\n",
            "  8.803498  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 97 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.249151   -0.52208394  7.6397405   1.4063644   4.4761934  -0.03413262\n",
            "  0.17421761  7.065264    8.583158    2.0435052   3.6841984   4.3518515\n",
            "  5.990209   20.603262   11.838854   18.10473     4.665868   17.890463\n",
            "  8.865684  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 98 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.250226   -0.52395105  7.7748084   1.389938    4.4024973  -0.04415043\n",
            "  0.0950667   6.9738      8.61054     2.0403812   3.6362417   4.316748\n",
            "  5.8804083  20.726532   12.007936   18.848484    4.701936   18.149506\n",
            "  8.84728   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 99 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.4515815  -0.51377374  7.969526    1.35704     4.62241    -0.17745474\n",
            "  0.07936159  7.056168    8.839203    2.0670097   3.68692     4.368341\n",
            "  5.8977566  21.377304   12.31046    19.75933     4.7548027  18.579514\n",
            "  8.96944   ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 100 reward=2 new_state=[0 0 0 1 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.634557   -0.5592321   8.5918455   1.2797035   4.4626245  -0.25285035\n",
            "  0.03517753  7.4663897   9.133689    1.7432873   3.6675637   4.570434\n",
            "  6.503796   22.541574   12.162064   20.278788    5.011816   18.310242\n",
            "  9.379567  ]\n",
            "Epsilon reduced to 0.10240000000000003\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 9.832799   -0.8477142  11.244667    1.6766431   5.956018   -0.31189936\n",
            "  0.981768    7.3352623  11.894744    1.9937339   5.198164    6.1645126\n",
            "  8.48838    28.582033   15.813009   25.429926    7.295485   23.385052\n",
            " 12.959287  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [10.626376   -1.0840493  11.611107    2.0428596   6.9250426  -0.31246305\n",
            "  0.4095844   6.7153673  12.081996    2.2560902   5.217931    6.2288136\n",
            "  8.94785    29.258146   16.484444   27.246067    7.511917   24.814245\n",
            " 12.941534  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 9.555544   -0.99404764 11.125497    1.798042    5.86423    -0.24063666\n",
            "  0.47551948  6.0266123  11.458532    2.479865    4.9256997   5.8085713\n",
            "  8.380702   27.713182   16.377417   26.178797    6.6750784  23.398302\n",
            " 12.612963  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 9.8371592e+00 -9.8125780e-01  1.1412654e+01  1.8393688e+00\n",
            "  6.1692433e+00 -2.8762449e-02  3.2902941e-01  6.1315799e+00\n",
            "  1.2050961e+01  2.2031183e+00  5.6321588e+00  6.9588723e+00\n",
            "  8.5425282e+00  2.9400288e+01  1.6142359e+01  2.8113470e+01\n",
            "  7.6928701e+00  2.4302885e+01  1.3930551e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 9.165803   -0.92647016 10.9839945   1.5357957   5.8651376  -0.47942945\n",
            "  0.29397714  4.949303   11.122423    2.4193394   4.8190837   5.624302\n",
            "  7.9304814  27.473978   15.664658   26.999336    6.698948   21.810526\n",
            " 12.190349  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 9.93185    -0.92828995 11.632668    1.7715461   6.416877   -0.40946424\n",
            "  0.3950877   4.50706    11.987484    2.1914096   5.1110153   6.3691573\n",
            "  8.516498   30.113256   16.621174   29.464224    7.9520698  22.829643\n",
            " 13.507945  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 7 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.867016   -0.69634974  9.908564    1.6245064   5.41563     0.05708754\n",
            "  0.6681174   4.6461263  10.925439    2.0345452   4.5280604   5.4021974\n",
            "  7.595016   27.24903    14.84407    26.748743    6.113388   20.607925\n",
            " 11.399957  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 8 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.24464    -0.81581295 10.553163    1.6816975   5.9808736   0.06785095\n",
            "  0.45035836  4.4739437  11.21934     2.1384344   5.0678935   6.2917876\n",
            "  8.27992    27.723774   15.374151   28.243795    6.4570365  22.126534\n",
            " 12.3484335 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.690463   -0.65205365  9.926152    1.3480949   5.5266185  -0.22769691\n",
            "  0.5015288   3.762409   10.52822     2.198302    4.3175697   4.9942403\n",
            "  7.5295477  24.966654   14.206449   26.773354    6.1416144  20.343365\n",
            " 10.924399  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 10 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.983972   -0.76808506 10.406999    1.5208813   5.727324    0.03340879\n",
            "  0.50310814  3.8792822  10.987624    2.2342606   5.032377    6.1097865\n",
            "  7.996549   24.955225   14.995333   28.129393    6.409522   21.441174\n",
            " 12.15363   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.81501    -0.72613627  9.64004     1.6553327   5.45675     0.11199389\n",
            "  0.74079007  3.433225   10.677036    2.1102507   4.573637    5.2766523\n",
            "  7.49315    23.551311   14.449314   27.598722    6.14421    20.134573\n",
            " 11.320691  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.8903074e+00 -5.9806508e-01  9.8130226e+00  1.5925624e+00\n",
            "  5.5427246e+00 -1.8104611e-02  8.8007301e-01  2.8119450e+00\n",
            "  1.0615847e+01  1.9536377e+00  4.4405279e+00  5.1792798e+00\n",
            "  7.5761495e+00  2.2380442e+01  1.4699482e+01  2.7344194e+01\n",
            "  6.2687211e+00  1.9674107e+01  1.1039868e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.538678   -0.62600654  9.394663    1.5231645   5.2379932   0.13575086\n",
            "  0.7756128   2.9126837  10.299848    1.9302518   4.3773236   5.057404\n",
            "  7.2770014  20.827925   13.882695   26.294634    6.0103555  19.177786\n",
            " 10.683716  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.67029    -0.5844033   9.670057    1.6363457   5.3725224   0.08007611\n",
            "  0.83435476  2.3727384  10.345282    1.8874044   4.326227    5.1032853\n",
            "  7.3670063  20.522253   14.468012   26.927912    6.229935   19.339193\n",
            " 10.8308115 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.094403   -0.6410567   9.259741    1.2125053   5.197934   -0.27198997\n",
            "  0.5159199   2.5329175   9.932564    2.091825    4.1013055   4.681794\n",
            "  6.8657765  19.301884   13.390263   26.29356     5.6717567  18.76877\n",
            " 10.289763  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.4822226e+00 -7.3249453e-01  9.6438179e+00  1.5019993e+00\n",
            "  5.4600873e+00 -2.7035447e-02  4.7139427e-01  2.5728683e+00\n",
            "  1.0371995e+01  2.1123557e+00  4.7276516e+00  5.5796447e+00\n",
            "  7.3420873e+00  1.9824558e+01  1.4185556e+01  2.7355309e+01\n",
            "  5.9389949e+00  1.9724277e+01  1.1304315e+01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 17 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.2728953e+00 -4.8558661e-01  7.8512473e+00  1.2245519e+00\n",
            "  4.4172368e+00 -1.2504338e-02  2.3486251e-01  2.1890292e+00\n",
            "  8.8244658e+00  1.7701247e+00  3.7137117e+00  4.2953372e+00\n",
            "  5.9816442e+00  1.7332060e+01  1.1925435e+01  2.4053049e+01\n",
            "  4.7995019e+00  1.6191046e+01  8.9163218e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 18 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 6.7698207e+00  1.3245814e-01  7.3029413e+00  1.3072532e+00\n",
            "  4.1990814e+00 -9.0186102e-03  8.8744499e-02  1.9614931e+00\n",
            "  8.4018812e+00  1.8382657e+00  3.4757261e+00  4.0237370e+00\n",
            "  5.5304527e+00  1.6670902e+01  1.1452365e+01  2.3339378e+01\n",
            "  4.3847890e+00  1.5386767e+01  8.1150761e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 19 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.727045    0.7399296   8.73101     1.2363467   4.8384075  -0.03146951\n",
            "  0.20780256  2.1240022   9.456855    1.8924819   3.9419577   4.618119\n",
            "  6.8714814  18.752617   12.763609   25.828249    5.288302   17.61164\n",
            "  9.576295  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 20 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.04819     1.4353546   9.295197    1.4204379   5.1992106  -0.25148466\n",
            "  0.570981    1.7786686   9.85875     2.0371778   4.153213    4.8186855\n",
            "  6.7929845  19.696991   13.996479   27.247166    5.7214255  18.356688\n",
            " 10.472631  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 21 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.170628    2.0031064   8.7870035   1.5086038   4.9114504   0.1507707\n",
            "  0.65083486  2.3953228  10.057772    1.9007508   4.3794665   4.850644\n",
            "  7.178446   20.561108   13.26603    26.824743    5.516978   17.953306\n",
            " 10.124     ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 22 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.9886370e+00  2.7566655e+00  9.8989534e+00  1.7284027e+00\n",
            "  5.6269202e+00 -2.1656103e-02  8.2580304e-01  1.6571519e+00\n",
            "  1.0861529e+01  1.9776142e+00  4.5490775e+00  5.2289343e+00\n",
            "  7.6003718e+00  2.2548409e+01  1.5079259e+01  2.9174797e+01\n",
            "  6.4260697e+00  1.9592495e+01  1.1285362e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 23 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.222337    2.762391    9.286956    1.4316112   5.0929136  -0.29901227\n",
            "  0.28222904  1.723736   10.178962    2.0312755   4.347127    4.7748146\n",
            "  6.714456   21.042606   13.682835   27.4011      5.774103   17.585764\n",
            " 10.515613  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 24 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.5571785   3.4815948   9.591437    1.5455512   5.286564   -0.04196189\n",
            "  0.5058342   1.6387928  10.570246    1.9710009   4.362009    5.2455177\n",
            "  7.2771497  22.447643   14.448643   27.885292    6.0702305  18.757347\n",
            " 10.91311   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 25 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.742191    3.9402723  10.371129    1.3692842   5.3886967  -0.45195085\n",
            "  0.6172748   1.9032699  10.864957    1.6806631   4.349283    5.2068257\n",
            "  7.5217714  23.65811    14.431283   29.063156    6.3995695  18.707598\n",
            " 11.165889  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 26 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.088446    3.87882     9.345859    1.361039    4.8503566  -0.14632091\n",
            "  0.49471113  2.4698992  10.3468895   1.7563071   4.171333    5.0688534\n",
            "  6.9882565  22.886036   13.4120245  27.347948    5.5299397  17.520844\n",
            " 10.232071  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 27 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.035601    4.617116   10.589393    1.5541643   5.911019   -0.41244435\n",
            "  0.33005488  1.206871   11.059534    2.0794077   4.5466394   5.5699477\n",
            "  7.953088   24.522474   15.569962   30.256489    6.655355   19.743465\n",
            " 11.722571  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 28 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.33186     4.5823154   9.510165    1.4171098   5.0900536  -0.2945483\n",
            "  0.45385224  1.4596324  10.228475    1.9919195   4.3059115   4.9323053\n",
            "  6.9777126  22.824512   14.259514   28.107798    5.9090214  18.449516\n",
            " 10.78352   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 29 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.758811    4.986404   10.140316    1.1765387   5.2978015  -0.39586014\n",
            "  0.33183792  1.8680375  10.896016    1.7105067   4.223042    5.15739\n",
            "  7.8136806  24.695805   14.015924   28.916798    6.2920823  18.8154\n",
            " 10.823196  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 30 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.153525    5.590804   10.6912      1.5084593   5.5196357  -0.29077396\n",
            "  0.89437133  1.9586898  11.377303    1.9051064   4.6874623   5.641087\n",
            "  8.045441   25.577385   15.101055   30.610643    6.6034083  19.487436\n",
            " 11.770939  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 31 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.381452    5.2643013   9.508366    1.7627246   5.072962   -0.03586458\n",
            "  0.32678375  1.9406503  10.807769    2.1788192   4.6212797   5.272721\n",
            "  7.25319    24.136496   14.670882   29.039852    5.719945   18.388885\n",
            " 11.006407  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 32 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.21538     6.219181   10.45145     1.7409984   5.8096576  -0.15742075\n",
            "  0.68569905  0.96849126 11.249227    2.1199846   4.769635    5.6315618\n",
            "  7.883993   24.94971    15.669212   29.966831    6.796991   19.683586\n",
            " 11.887065  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 33 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.474844    5.495681    9.608845    1.1875005   4.9182024  -0.27127847\n",
            "  0.50767565  1.5589633  10.122763    1.4996846   4.1914177   4.9230294\n",
            "  7.0829563  22.912348   13.413586   27.688007    5.9959044  17.761679\n",
            " 10.443255  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 34 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.570014    5.822812    9.870006    1.2138236   5.139147   -0.3112878\n",
            "  0.37660065  1.8934815  10.742151    1.7598842   4.1725225   5.08724\n",
            "  7.668044   23.831377   13.845533   28.705175    6.0717955  18.530844\n",
            " 10.545447  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 35 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.647787    6.098997   10.328469    1.6150821   5.4170346  -0.30971172\n",
            "  0.57711667  1.3820405  10.942484    2.1219344   4.6437373   5.362564\n",
            "  7.459141   23.24226    15.368067   29.651226    6.286568   18.864363\n",
            " 11.526228  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 36 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.947504    5.7024794   8.999353    1.5692631   4.779546   -0.03340526\n",
            "  0.45267868  2.030255   10.032158    1.945648    4.259759    4.949479\n",
            "  6.7743607  21.932339   13.570411   27.502756    5.2546177  17.274231\n",
            " 10.283942  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 37 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.079667   6.740389  10.412482   1.519013   5.5262766 -0.1516707\n",
            "  0.7496584  1.1681814 10.983046   1.7138256  4.469291   5.562011\n",
            "  7.9035363 23.551891  14.8045845 29.038372   6.687986  19.117973\n",
            " 11.4650755]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 38 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.532971    6.361827    9.822492    1.2913601   5.1771755  -0.3401751\n",
            "  0.52539647  1.4926301  10.412159    1.6954635   4.323305    5.068535\n",
            "  7.2981253  22.498741   13.832116   28.519539    6.1812387  18.13834\n",
            " 10.654897  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 39 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.248215    6.4258204   9.578683    1.6725317   4.9931836   0.03573711\n",
            "  0.26615268  1.6836478  10.3488245   1.9675312   4.4434347   5.293044\n",
            "  7.4607716  22.284132   14.277028   28.206886    5.740753   17.478094\n",
            " 10.96396   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 40 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.253171    6.2856016   9.897025    1.451727    5.1799126  -0.2388109\n",
            "  0.18829381  2.562578   10.205593    2.0589173   4.368331    5.08567\n",
            "  7.1264853  21.375105   13.841677   27.710638    5.7080708  17.45625\n",
            " 11.327776  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 41 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.7494597  6.169502   9.265998   1.2334431  4.584179  -0.2127003\n",
            "  0.4459782  3.7512968 10.226714   1.8196144  4.0918727  4.83252\n",
            "  7.2183847 21.980978  13.146771  27.233345   5.3349705 16.94305\n",
            " 10.612808 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 42 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.628689    7.102769   10.653131    1.3032172   5.200268   -0.56433374\n",
            "  0.43440765  4.0618877  11.382198    2.071928    4.6594324   5.5354347\n",
            "  8.102543   23.351046   14.558498   28.946249    6.386126   18.676643\n",
            " 11.90037   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 43 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.1371269e+00  6.4272141e+00  9.3879433e+00  1.4534917e+00\n",
            "  5.2645950e+00 -3.3330247e-01  2.2219967e-02  4.4203391e+00\n",
            "  1.0352774e+01  1.7713976e+00  4.4267902e+00  5.0231113e+00\n",
            "  7.1504812e+00  2.1427153e+01  1.3713279e+01  2.8134733e+01\n",
            "  5.5124335e+00  1.6549072e+01  1.0809519e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 44 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.293749    6.756823   10.15207     1.3446851   5.1376834  -0.53394884\n",
            "  0.25065997  5.0731726  10.266436    1.7812071   4.405477    5.156796\n",
            "  7.419494   20.773697   13.6947975  27.688713    5.883865   17.402033\n",
            " 11.259717  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 45 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.422955    6.7527986  10.249049    1.1748086   5.2026043  -0.511707\n",
            "  0.38004288  5.8077993  10.57451     1.7983327   4.2057743   4.942805\n",
            "  7.715191   22.21432    13.219097   27.543816    6.0084624  18.383944\n",
            " 10.964652  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 46 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.060913    6.628247    9.809158    1.3677592   5.0278854  -0.38717416\n",
            " -0.03124788  5.506154   10.450141    1.7096183   4.2118335   5.0903\n",
            "  7.0627894  20.606709   13.943415   27.858229    5.73031    16.572275\n",
            " 10.829215  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 47 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.5817451e+00  6.2260876e+00  9.0920286e+00  1.4789439e+00\n",
            "  4.9051642e+00 -2.9466951e-01 -6.5256906e-04  5.7541924e+00\n",
            "  9.9887905e+00  1.9625723e+00  4.0550437e+00  4.7040091e+00\n",
            "  6.8315754e+00  2.0197905e+01  1.3778622e+01  2.6730072e+01\n",
            "  4.9751353e+00  1.6583946e+01  1.0394038e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 48 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.774398    7.28099    10.695996    1.3717828   5.404538   -0.45786873\n",
            "  0.341076    7.1104     10.822719    2.1075544   4.548449    5.307161\n",
            "  8.091751   21.328932   13.969842   28.284098    6.516312   19.066711\n",
            " 11.65685   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 49 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.612452    7.010594   10.564848    1.4180139   5.0200353  -0.3695208\n",
            "  0.36491117  6.8751144  10.771432    1.3058609   4.484596    5.1614213\n",
            "  7.4910297  21.08285    13.871301   29.142736    6.219052   17.413578\n",
            " 11.614474  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 50 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.7115693e+00  6.5164261e+00  9.1270590e+00  1.4630809e+00\n",
            "  5.2728777e+00 -4.9135792e-01 -1.2394814e-02  6.9657464e+00\n",
            "  1.0202645e+01  1.9618176e+00  4.2481189e+00  4.9191847e+00\n",
            "  7.1587043e+00  2.0491051e+01  1.3669739e+01  2.7305569e+01\n",
            "  5.1196041e+00  1.6636990e+01  1.0801139e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 51 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.722539    7.5036926  10.320394    1.4589183   5.634753   -0.43766588\n",
            "  0.26446146  7.8484144  10.785549    1.8944341   4.7444506   5.4823804\n",
            "  8.1517105  21.695055   14.364972   28.335377    6.116758   18.534733\n",
            " 11.909363  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 52 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.495539    7.069404    9.924986    1.2495395   5.0171204  -0.39020324\n",
            "  0.34914872  7.5685716  10.438514    1.4745069   4.4806676   5.158508\n",
            "  7.430438   21.07112    13.202714   28.914999    5.8947334  17.553349\n",
            " 11.323782  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 53 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.760928    6.4695106   9.590606    1.3688576   4.8536882  -0.5884246\n",
            " -0.03942591  7.627332   10.341941    1.8986824   4.0777755   4.8821898\n",
            "  6.8235726  20.35463    13.336397   27.408312    5.4539547  16.810837\n",
            " 10.567567  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 54 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.773315    7.4402246  10.67232     1.5362155   5.834278   -0.5058542\n",
            "  0.06729642  8.086682   10.759427    2.0789127   4.624376    5.4239664\n",
            "  7.9357963  21.657635   14.755375   28.40955     6.2133703  18.739418\n",
            " 11.840913  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 55 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.519781    7.399403   10.227805    1.1982554   5.3465986  -0.5488494\n",
            "  0.14839604  8.300684   10.699567    1.7828524   4.5429525   5.2069087\n",
            "  7.472027   21.346966   13.757527   28.635044    6.2421575  18.070274\n",
            " 11.76619   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 56 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.632128    6.4079666   9.407789    1.3387296   4.592796   -0.4287773\n",
            "  0.27292407  8.276906   10.326177    1.62779     3.8865535   4.677535\n",
            "  6.9248962  20.97121    12.776439   26.971933    5.2885447  16.426275\n",
            " 10.20418   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 57 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.000331    7.8106093  10.763124    1.461638    5.8752193  -0.47987407\n",
            "  0.16600622  9.017638   11.2286825   1.8660727   4.855389    5.5702424\n",
            "  8.362336   22.674915   14.6969595  28.47593     6.498639   18.889986\n",
            " 12.2721405 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 58 reward=-1 new_state=[1 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.328566    7.220542    9.920203    1.3930649   5.523381   -0.46453676\n",
            "  0.04347796  8.160409   10.53984     1.8171793   4.635665    5.182357\n",
            "  7.260055   21.118305   13.855743   28.775955    5.9938684  17.685017\n",
            " 11.540958  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 59 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.7557864   7.6454754   9.76095     1.2001663   4.9317484  -0.5951361\n",
            "  0.26991463  8.854044   10.692113    1.619929    4.0738163   4.866076\n",
            "  7.337867   22.542616   13.728021   27.737316    5.128947   17.026993\n",
            " 11.091015  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 60 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.139763    8.950016   11.194179    1.4290812   5.3824835  -0.69716513\n",
            "  0.22119877  9.976434   11.631607    2.7411172   4.831686    5.6664343\n",
            "  8.033515   23.132318   14.6152725  29.851799    6.4245415  18.442015\n",
            " 12.803625  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 61 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.300795    8.372582    9.508135    1.4412912   4.974975   -0.50145495\n",
            " -0.22806512  8.841859   10.337348    3.1409602   4.291795    5.280614\n",
            "  6.9857974  21.33087    13.149667   27.937963    6.06121    16.842817\n",
            " 11.3519535 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 62 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.9013343   8.647678    9.307204    1.524353    4.9873066  -0.5933078\n",
            " -0.13853781  8.3911495  10.426664    4.010501    4.0737095   5.0138674\n",
            "  6.959494   21.786648   13.657864   28.43647     5.265739   17.200962\n",
            " 10.763139  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 63 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.092376    9.061866    9.65156     1.4896293   4.9003596  -0.73087114\n",
            " -0.29575154  8.11432    10.419578    4.6555862   4.101548    4.715022\n",
            "  6.704177   21.063412   13.969958   27.676659    5.239936   16.823158\n",
            " 10.766534  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 64 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.656609   10.038848   10.019989    1.3593757   5.4289355  -0.68270165\n",
            "  0.04414289  9.884399   11.158616    5.1024547   4.505506    5.214288\n",
            "  7.524341   21.881163   14.034826   28.743923    5.839562   17.946318\n",
            " 11.724054  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 65 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.478874   10.289566   10.22452     1.2525318   5.183617   -0.48715279\n",
            "  0.36005977  9.380355   10.888279    5.4242473   4.2763443   5.231487\n",
            "  7.481728   21.571665   13.610568   28.526318    5.7924457  17.879568\n",
            " 11.714849  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 66 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.9983897   9.534366    9.571792    1.3783839   4.760864   -0.60657567\n",
            " -0.09978443  7.990668   10.426908    5.959715    4.0821047   4.7662187\n",
            "  6.6944914  19.63865    13.439774   27.270803    5.260292   16.728525\n",
            " 10.465424  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 67 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.8303566  10.217002    9.080876    1.573941    5.110133   -0.5409915\n",
            " -0.44839886  8.594375   10.062963    5.9573584   4.147262    5.0366197\n",
            "  6.6513844  20.04535    13.443867   27.524614    5.4876876  16.236391\n",
            " 10.93614   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 68 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.978645   10.256102    9.507718    1.38568     5.1055894  -0.58857423\n",
            "  0.05320349  8.978271   10.606458    6.684963    3.9916105   5.0056124\n",
            "  7.0279574  20.64193    13.827662   28.62867     5.2087617  17.32808\n",
            " 10.887867  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 69 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.187882  12.5224905 11.355835   1.5368221  5.2773795 -0.6667019\n",
            "  0.201114  10.167636  11.692993   7.5965915  4.887195   5.6999583\n",
            "  8.1029    21.734537  14.847457  30.440971   6.147891  19.008009\n",
            " 12.822823 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 70 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.81659    11.177525    9.644818    1.2571522   5.7278657  -0.6737089\n",
            " -0.22668788  9.371028   10.69845     7.380545    4.5781484   5.09885\n",
            "  7.1646233  19.82898    13.165635   28.374496    5.911719   17.657404\n",
            " 11.440874  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 71 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.008346   10.771506    9.42696     1.4015912   4.6740355  -0.46022224\n",
            " -0.07351044  9.085534   10.5105715   7.256516    4.11888     4.9547524\n",
            "  7.112168   19.691816   13.323363   28.075743    5.463558   16.693388\n",
            " 10.6820135 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 72 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.871671   12.50228    10.8456135   1.5488095   5.291437   -0.62755877\n",
            "  0.20503071 10.436353   11.823152    8.4985485   4.9433904   5.5108705\n",
            "  8.094327   21.536663   14.928517   30.3711      6.0900455  18.794996\n",
            " 12.747724  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 73 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.362773   13.439369   10.632957    1.4528402   6.1976166  -0.7868875\n",
            "  0.13293444 10.861758   11.544494    8.402476    5.1302104   5.9345217\n",
            "  7.9893837  22.298864   14.769925   30.761631    6.5862436  18.87442\n",
            " 13.312839  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 74 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.229786  11.681907   9.910445   1.340848   4.762808  -0.5896527\n",
            " -0.1768518  9.263515  10.312599   8.050222   4.320877   5.024107\n",
            "  7.1766896 20.66743   13.832486  28.53908    5.2553887 17.021114\n",
            " 11.240736 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 75 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.43327045e+00  1.16675415e+01  1.02740030e+01  1.57801592e+00\n",
            "  5.10211897e+00 -5.96427500e-01 -2.30634541e-04  9.28813362e+00\n",
            "  1.12765474e+01  8.73109341e+00  4.38986111e+00  5.25795889e+00\n",
            "  7.15600157e+00  2.11276035e+01  1.44086208e+01  2.97965145e+01\n",
            "  5.76009178e+00  1.81460400e+01  1.13657417e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 76 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.418078   13.408134   10.799948    1.4504341   5.9144764  -0.6276065\n",
            "  0.16457464 10.638734   11.747362    9.294712    4.9196067   5.6499767\n",
            "  8.054764   21.35364    14.722115   30.284887    6.1522803  19.366661\n",
            " 12.578326  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 77 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.875005   12.300731   10.348361    1.3996018   5.115929   -0.4264532\n",
            "  0.09026574  9.709408   10.766813    8.614707    4.385887    5.2811236\n",
            "  7.5555673  21.29667    13.673422   29.517006    5.7354555  18.005318\n",
            " 11.629843  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 78 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.276376   12.153025    9.773386    1.580716    5.042951   -0.64118135\n",
            " -0.44060865  9.1790695  10.499017    9.063126    4.2691197   5.181392\n",
            "  7.10374    20.957619   14.3512     28.770805    5.474646   16.767097\n",
            " 11.12882   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 79 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.816453   12.653829    9.915297    1.5728893   5.6655154  -0.5393846\n",
            " -0.08393019 10.302188   11.412423    9.349534    4.7291584   5.391726\n",
            "  7.6010113  22.066502   14.548671   30.811066    5.687932   18.834621\n",
            " 11.814491  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 80 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.093464   14.022803   11.401382    1.4181651   5.561876   -0.8681627\n",
            "  0.10146672 10.763893   11.570563   10.131211    4.9989653   5.8392262\n",
            "  8.078365   22.697824   15.132326   31.009144    6.149496   19.759195\n",
            " 13.372055  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 81 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.609936   12.575275   10.235675    1.5042795   4.9066625  -0.4108746\n",
            " -0.10313498  9.676121   11.062522    9.460326    4.5185294   5.267741\n",
            "  7.50729    22.167185   14.43282    30.138931    5.930183   17.674229\n",
            " 11.711345  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 82 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.3818445 12.053722   9.445043   1.4953074  5.5413175 -0.5762295\n",
            " -0.2642011  9.538265  10.800479   9.109127   4.4359083  5.2868147\n",
            "  7.0256705 21.903706  13.602892  28.96712    5.8938966 17.611124\n",
            " 11.362096 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 83 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.341759  14.456306  11.470145   1.4635152  5.5785527 -0.6995402\n",
            "  0.3133814 11.001736  12.06423   10.3391     4.9537377  6.0116534\n",
            "  8.579183  23.989897  15.545568  31.933834   6.2849545 19.838137\n",
            " 13.2323675]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 84 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.3167276e+00  1.3759606e+01  1.1034545e+01  1.5063068e+00\n",
            "  5.4545598e+00 -5.2910388e-01 -2.7775971e-02  1.0201073e+01\n",
            "  1.1757605e+01  1.0186833e+01  4.7900758e+00  5.8601933e+00\n",
            "  7.8083420e+00  2.3185493e+01  1.5262243e+01  3.1562002e+01\n",
            "  6.6058068e+00  1.9183275e+01  1.3052649e+01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 85 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.690862   12.159512    9.610478    1.4487576   5.5745983  -0.581229\n",
            " -0.26375687  9.588049   10.895651    9.255833    4.3358164   4.985165\n",
            "  7.2490773  21.681046   13.360068   28.263521    5.768026   16.830288\n",
            " 10.824604  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 86 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.201835   12.194369    9.818712    2.288365    5.159812   -0.5905949\n",
            " -0.08670621  8.997197   10.84862     9.730555    4.2697263   4.967974\n",
            "  7.0241833  22.062365   14.110651   28.944996    5.475863   17.412582\n",
            " 11.027008  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 87 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.661529  12.764312   9.804431   3.0595355  5.495336  -0.6272129\n",
            " -0.3639371 10.1411705 10.898412   9.329056   4.3202906  4.9958086\n",
            "  7.4246745 22.018452  13.695055  28.428165   5.699862  16.726234\n",
            " 10.8567295]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 88 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.78575    13.551728   10.55009     4.0666294   5.4516435  -0.60336655\n",
            "  0.19012998  9.813763   11.62002    10.118811    4.6818385   5.707902\n",
            "  7.609952   23.113976   15.146819   30.841518    5.9399796  18.790037\n",
            " 12.466534  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 89 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.147383   14.089947   11.063552    4.9065366   5.571707   -0.7191911\n",
            " -0.10500101 10.424715   11.5423765  10.805096    4.779689    5.601196\n",
            "  7.8736076  22.626318   14.852058   30.78747     5.913001   19.315533\n",
            " 12.985395  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 90 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.4658     13.088349    9.972687    5.361849    5.297152   -0.54859215\n",
            " -0.08874703  9.765525   11.128554   10.047855    4.6297636   5.595744\n",
            "  7.159092   21.78601    14.157468   30.248186    6.4101872  17.350056\n",
            " 11.933548  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 91 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.200937   12.657885    9.924092    5.602872    5.0878725  -0.67539734\n",
            " -0.05980467  9.486422   10.722157    9.797839    4.409688    5.0064836\n",
            "  7.2546015  21.622005   14.184636   28.646885    5.437804   17.176619\n",
            " 11.462467  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 92 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.557291   13.022712    9.857788    6.2715907   5.2079253  -0.4656998\n",
            " -0.12795977 10.005495   11.081325    9.663009    4.579812    5.448887\n",
            "  7.069377   20.932402   13.991825   29.329947    6.3309703  17.151333\n",
            " 11.703931  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 93 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.670398   13.206295    9.937555    7.4266396   5.6606746  -0.59536076\n",
            "  0.24922538  9.418706   11.622194   10.181499    4.5767207   5.4704328\n",
            "  7.410689   21.461819   14.645419   30.557732    5.8182836  18.08639\n",
            " 11.859791  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 94 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.340988  14.41074   11.519777   8.659566   5.5231276 -0.7548848\n",
            " -0.184482  10.644232  11.830518  11.051227   4.8508215  5.6389966\n",
            "  8.154306  20.69628   15.603174  31.737612   6.26183   19.305656\n",
            " 12.757337 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 95 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.648093   13.081897    9.775603    8.610004    5.0656548  -0.687469\n",
            " -0.08639132  9.805623   11.117549    9.722229    4.6840916   5.3518972\n",
            "  7.195019   20.171719   13.726682   29.38187     6.289235   17.069231\n",
            " 11.522504  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 96 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.363091   12.538154    9.964888    9.232891    4.8347964  -0.4280325\n",
            " -0.20431454  9.462407   10.760418    9.7982855   4.3805127   5.2265334\n",
            "  7.252501   20.007505   14.276697   29.780584    5.5860248  17.074905\n",
            " 10.962687  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 97 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.455924   13.84507    10.241874   10.369646    5.445201   -0.43410578\n",
            "  0.2045932  10.774389   11.56311    10.108442    4.5363054   5.7305593\n",
            "  7.6693215  20.5227     14.628232   29.691113    6.2590775  17.774881\n",
            " 12.433266  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 98 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 9.379344   14.558996   11.029989   11.582957    5.7453184  -0.7420359\n",
            "  0.23579483 10.826902   12.080326   10.613367    4.9668903   5.987407\n",
            "  7.969599   21.551962   14.805936   31.769438    6.60772    19.184078\n",
            " 13.278748  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 99 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.87127304e+00  1.36146173e+01  1.03749657e+01  1.16983919e+01\n",
            "  5.28770542e+00 -5.72009325e-01 -2.21356843e-02  9.89053249e+00\n",
            "  1.13023109e+01  1.04573460e+01  4.57742453e+00  5.28526115e+00\n",
            "  7.66509962e+00  2.08334026e+01  1.58399725e+01  3.07422276e+01\n",
            "  5.51806593e+00  1.81739082e+01  1.19251690e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 100 reward=-3 new_state=[1 1 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 8.856059   12.9004135   9.820981   11.554133    5.7740045  -0.44913673\n",
            " -0.2264209  10.37396    11.326295   10.109079    4.4624996   5.218475\n",
            "  7.662765   20.30224    15.207156   29.53617     5.885135   17.369503\n",
            " 11.365257  ]\n",
            "Epsilon reduced to 0.08192000000000003\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 9.323599   13.128698   11.036239   11.670945    5.7907095  -0.51783395\n",
            "  0.30166453  9.606792   11.715442   10.460227    5.210548    6.053481\n",
            "  8.004407   19.653423   16.840029   31.058409    6.721404   20.512953\n",
            " 12.321819  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 9.610632   12.913679   10.8581085  11.787031    5.660961   -0.23396179\n",
            "  0.70233005  9.952344   11.472884    9.918318    4.8823385   5.877296\n",
            "  8.095663   19.027359   17.21949    29.515131    6.769881   19.075302\n",
            " 11.834317  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 3 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [ 9.316748   13.102204   10.658151   12.022003    5.6853065  -0.10422322\n",
            "  0.7740462   9.753107   11.485147   10.252366    5.132548    6.0428615\n",
            "  8.101912   18.287077   17.63081    28.477978    6.6462517  20.17803\n",
            " 12.094615  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 4 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.3866625  11.010874    9.273981   10.670443    5.0075383   0.08217405\n",
            "  0.66209614  8.646157   10.12344     8.777677    4.162213    4.9410844\n",
            "  6.773695   16.7024     16.661095   26.156107    5.644104   18.526855\n",
            " 10.283844  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 5 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.788028   11.91228     9.974101   11.506779    5.525915   -0.14746216\n",
            "  0.50875366  8.521819   10.380123    9.380342    4.205301    5.1371436\n",
            "  7.530822   16.42169    17.822834   25.978233    6.278584   19.908747\n",
            " 10.586815  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 6 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.100954   10.645418    8.917018   10.680278    4.8169355   0.14651684\n",
            "  0.51176155  8.700835    9.80629     8.740446    4.24821     4.9831734\n",
            "  6.764854   15.448655   16.500908   23.723795    5.2047977  18.574253\n",
            " 10.17286   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 7 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.325628   11.542346    9.665458   11.488616    5.388438   -0.2053724\n",
            "  0.39609724  8.616528   10.171931    9.421125    4.376276    5.3170767\n",
            "  7.286256   15.381577   17.50528    23.099342    5.8561583  20.132244\n",
            " 10.631842  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 8 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.135328   10.718901    8.736248   11.245162    4.8015294   0.20475967\n",
            "  0.63831514  8.679282    9.698517    8.556727    4.084853    4.9282103\n",
            "  6.845527   14.959276   17.100378   21.61021     5.2714725  19.125622\n",
            " 10.059545  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.4537325  11.458755    9.469044   11.964363    5.159429    0.07630486\n",
            "  0.8398791   8.558032   10.097603    9.071726    4.3923206   5.1228642\n",
            "  7.168439   14.460286   18.23418    20.717497    5.9939923  20.384989\n",
            " 10.633451  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 10 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.3186126   9.628614    8.08557    10.442031    4.306883    0.15887702\n",
            "  0.49026415  7.9509106   9.022222    8.065218    3.8294916   4.440313\n",
            "  6.0573144  13.900538   16.09339    18.58053     4.760095   18.096935\n",
            "  9.007873  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 11 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.406012    9.961378    8.146672   10.660673    4.718044   -0.10612163\n",
            "  0.23976696  7.5715313   9.08695     8.272619    3.6685386   4.392903\n",
            "  6.299411   14.063961   16.224194   17.540632    4.9570413  18.570683\n",
            "  8.79694   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 12 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.9266386   9.151128    7.4943957  10.024481    4.3194733   0.04505134\n",
            "  0.3503273   7.4595976   8.651161    7.6297665   3.5914466   4.0653114\n",
            "  5.669365   13.463007   15.45493    16.417282    4.457963   17.39194\n",
            "  8.344417  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 13 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.222749    9.659004    7.89117    10.488039    4.538321   -0.02115659\n",
            "  0.23216742  7.354044    8.824506    7.946906    3.5579655   4.26439\n",
            "  6.179503   13.75415    15.978571   15.678167    4.840357   18.223738\n",
            "  8.480474  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 14 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.9048514   9.0237875   7.6709824  10.21102     4.1259933   0.1339852\n",
            "  0.45673198  7.4679465   8.580996    7.674983    3.5807302   4.2680755\n",
            "  5.6832013  13.268514   15.77849    14.934987    4.499251   17.755507\n",
            "  8.44813   ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 15 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.7959557   8.810542    7.235672    9.854983    4.144626    0.03330986\n",
            "  0.3066724   7.109442    8.359371    7.393721    3.3485878   3.9691627\n",
            "  5.3846292  13.249958   15.422714   14.551732    4.326933   17.375435\n",
            "  8.5994625 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 16 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [6.71895027e+00 8.58731174e+00 6.91709709e+00 9.78494453e+00\n",
            " 4.06506824e+00 1.12938723e-02 1.33781642e-01 6.79753256e+00\n",
            " 8.07869816e+00 7.25550175e+00 3.24634337e+00 4.51107407e+00\n",
            " 5.27007294e+00 1.29469194e+01 1.52859955e+01 1.40646257e+01\n",
            " 4.14803410e+00 1.70994606e+01 8.86010456e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 17 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.6760197   8.563552    6.94669     9.81106     3.969878    0.06605338\n",
            "  0.21082291  6.9121547   8.093213    7.296028    3.299648    5.068664\n",
            "  5.333299   12.960627   15.28653    13.700078    4.2093277  17.159569\n",
            "  9.353969  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 18 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.703678    8.584402    6.9747806   9.907484    3.9715068   0.02489223\n",
            "  0.1122626   6.7986937   8.062513    7.325523    3.2949622   5.481006\n",
            "  5.239525   13.180292   15.416868   13.4712515   4.2044253  17.358082\n",
            "  9.781069  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 19 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.6805286   8.453234    6.9191356   9.637493    3.943933    0.01845155\n",
            "  0.15399155  6.6316466   8.010427    7.165595    3.2441337   5.7666936\n",
            "  5.186546   13.354411   15.19737    12.79542     4.1905427  17.139103\n",
            "  9.85272   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 20 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.706391    8.487075    6.946389    9.708378    3.9588625   0.01791041\n",
            "  0.15385734  6.6559935   8.0405245   7.1950994   3.2571473   6.1634946\n",
            "  5.2072754  13.644404   15.309828   12.55169     4.206808   17.258993\n",
            " 10.207264  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 21 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.7758603   8.603675    7.0625825   9.891269    4.0522766  -0.06063831\n",
            "  0.16198383  6.7319555   8.13481     7.385586    3.3059459   6.6018205\n",
            "  5.216382   13.993775   15.584933   12.341934    4.240093   17.653278\n",
            " 10.691305  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 22 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.900404    8.780944    7.1027937  10.1158695   4.1507077  -0.07891229\n",
            "  0.17813821  6.867572    8.282044    7.5131583   3.4133434   6.986661\n",
            "  5.3342934  14.629729   15.746069   12.346624    4.319918   17.852734\n",
            " 11.150854  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 23 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.942792    8.839872    7.1491356  10.208618    4.1774735  -0.08022531\n",
            "  0.17768972  6.911055    8.335614    7.562361    3.4353652   7.317346\n",
            "  5.3697286  15.049033   15.888728   12.201508    4.3476214  18.006157\n",
            " 11.465605  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 24 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.90568924e+00  8.79066181e+00  7.13866997e+00  1.02338848e+01\n",
            "  4.12167501e+00 -1.00801121e-02  9.20569077e-02  6.84678459e+00\n",
            "  8.26229382e+00  7.44599533e+00  3.34036565e+00  7.57669020e+00\n",
            "  5.33837414e+00  1.52774553e+01  1.60504856e+01  1.20727625e+01\n",
            "  4.28898764e+00  1.80104942e+01  1.15915794e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 25 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.9332104   8.842708    7.1693726  10.283284    4.1523542   0.03066099\n",
            "  0.19186866  7.0157123   8.360098    7.4798336   3.373188    7.914365\n",
            "  5.4789386  15.663891   16.154837   11.836524    4.3301277  18.056797\n",
            " 11.861497  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 26 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 6.993308    8.97635     7.3575225  10.52129     4.163317   -0.03639824\n",
            "  0.17676891  7.111445    8.460097    7.7523365   3.4767964   8.2787485\n",
            "  5.466468   16.119137   16.431076   11.91858     4.4279766  18.586252\n",
            " 12.370712  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 27 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.0270367   9.043879    7.3284903  10.640633    4.1855826   0.05918729\n",
            "  0.20869233  7.270524    8.530536    7.7026715   3.480326    8.574726\n",
            "  5.6273313  16.559694   16.57847    11.880527    4.4377236  18.531487\n",
            " 12.585118  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 28 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.051491    9.036275    7.3806276  10.602577    4.149801    0.0398623\n",
            "  0.16704929  7.1698165   8.5291195   7.7070866   3.4939294   8.685604\n",
            "  5.5650744  16.982206   16.566095   11.8108      4.4791007  18.626297\n",
            " 12.701799  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 29 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.1405621e+00  9.1230345e+00  7.3985405e+00  1.0702782e+01\n",
            "  4.2735105e+00 -1.6404802e-02  8.7356508e-02  7.0944357e+00\n",
            "  8.5659428e+00  7.7176614e+00  3.4618144e+00  8.8360806e+00\n",
            "  5.5365858e+00  1.7495497e+01  1.6770325e+01  1.1718806e+01\n",
            "  4.4427652e+00  1.8780436e+01  1.2842077e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 30 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.1770725   9.238058    7.563007   10.885916    4.282343   -0.04190832\n",
            "  0.174042    7.30815     8.699947    7.9684653   3.5730448   9.197832\n",
            "  5.622719   17.869516   16.986351   11.700458    4.5505524  19.184725\n",
            " 13.292887  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 31 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [7.1822333e+00 9.1522064e+00 7.4699755e+00 1.0684292e+01 4.2612700e+00\n",
            " 5.6405533e-03 1.4679477e-01 7.1535640e+00 8.6503630e+00 7.7431970e+00\n",
            " 3.5029998e+00 9.1394339e+00 5.6071367e+00 1.8201588e+01 1.6819088e+01\n",
            " 1.1491805e+01 4.5197425e+00 1.8874926e+01 1.3105686e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 32 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.25514     9.27978     7.593966   10.864617    4.360145   -0.07516661\n",
            "  0.1545099   7.2346992   8.75211     7.9438257   3.554528    9.398669\n",
            "  5.619538   18.601288   17.08214    11.43402     4.5557556  19.265882\n",
            " 13.442061  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 33 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [7.2500639e+00 9.2482662e+00 7.5452247e+00 1.0815407e+01 4.3051181e+00\n",
            " 3.8132600e-03 1.4564084e-01 7.2258511e+00 8.7387705e+00 7.8215275e+00\n",
            " 3.5382137e+00 9.4556036e+00 5.6646466e+00 1.8852173e+01 1.7020388e+01\n",
            " 1.1426446e+01 4.5647011e+00 1.9090733e+01 1.3428327e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 34 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.3652329e+00  9.4140549e+00  7.5722322e+00  1.1013444e+01\n",
            "  4.3989387e+00 -1.5344015e-02  1.6378503e-01  7.3529396e+00\n",
            "  8.8757963e+00  7.9379520e+00  3.6427214e+00  9.6468287e+00\n",
            "  5.7765298e+00  1.9428591e+01  1.7128513e+01  1.1567093e+01\n",
            "  4.6389699e+00  1.9231407e+01  1.3732823e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 35 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.3206615   9.391059    7.598711   11.045062    4.4024243   0.02164234\n",
            "  0.18683065  7.430586    8.864948    7.9280396   3.5739958   9.838817\n",
            "  5.8099203  19.3566     17.327415   11.360457    4.5862145  19.307129\n",
            " 13.809708  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [7.3148141e+00 9.3396149e+00 7.6168818e+00 1.0944638e+01 4.3467755e+00\n",
            " 2.0722584e-03 1.4454517e-01 7.2945008e+00 8.8228207e+00 7.8962440e+00\n",
            " 3.5717912e+00 9.8074541e+00 5.7194257e+00 1.9517895e+01 1.7219204e+01\n",
            " 1.1325123e+01 4.6075325e+00 1.9249506e+01 1.3778333e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 37 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.3248897   9.423897    7.684993   11.143841    4.326062    0.03278715\n",
            "  0.16291861  7.4636393   8.886637    8.025491    3.6368554  10.061156\n",
            "  5.798257   19.686745   17.392279   11.48186     4.6617503  19.362017\n",
            " 14.080692  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 38 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.377239    9.599704    7.8404202  11.429485    4.4332614  -0.06228596\n",
            "  0.1484172   7.617163    9.047062    8.256805    3.7314055  10.360353\n",
            "  5.810786   19.976973   17.732006   11.555507    4.6181893  19.69338\n",
            " 14.497007  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 39 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [7.3707199e+00 9.3795109e+00 7.6966305e+00 1.1248529e+01 4.3737354e+00\n",
            " 5.6605823e-03 8.8613883e-02 7.4060779e+00 8.8671703e+00 8.0270729e+00\n",
            " 3.6221361e+00 1.0205457e+01 5.8009238e+00 1.9740116e+01 1.7579081e+01\n",
            " 1.1218417e+01 4.5954747e+00 1.9187784e+01 1.4194171e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 40 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.57093143e+00  9.61004066e+00  7.80650282e+00  1.12540455e+01\n",
            "  4.41823196e+00 -5.04731387e-03  1.64356440e-01  7.40908623e+00\n",
            "  8.98025227e+00  8.07331276e+00  3.70292187e+00  1.02010708e+01\n",
            "  6.58070850e+00  2.00022659e+01  1.78424759e+01  1.09669113e+01\n",
            "  4.75147676e+00  1.93739891e+01  1.44224110e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 41 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.169153   10.508311    8.712619   12.081574    4.913414   -0.09335776\n",
            "  0.46663788  7.7875514   9.425152    8.551954    3.9046454  10.97015\n",
            "  7.809468   20.159967   19.008425   10.710215    5.47356    20.343212\n",
            " 15.707308  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 42 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.102587   10.37662     8.544901   11.866083    4.812494   -0.09115423\n",
            "  0.41699523  7.9293427   9.352155    8.500735    3.9397247  10.947925\n",
            "  8.212311   20.117237   18.486965   10.60719     5.2655997  20.123474\n",
            " 15.650237  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 43 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.194888   10.52768     8.81002    12.0858      4.9303164  -0.19411029\n",
            "  0.4323036   7.6871223   9.430278    8.675467    3.9329371  11.037393\n",
            "  8.806069   20.385458   19.044453   10.627997    5.503816   20.021587\n",
            " 15.842695  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 44 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.4547415   9.525553    7.773609   10.99167     4.213241    0.0454166\n",
            "  0.17241949  7.3974347   8.636011    7.869637    3.651119   10.124822\n",
            "  8.530496   19.615002   17.319176   10.381356    4.594549   17.673758\n",
            " 14.320339  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 45 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.090502   10.705354    8.666182   12.143553    4.9630356  -0.07665918\n",
            "  0.19664577  8.057905    9.544441    8.928372    4.3152094  11.384312\n",
            "  9.948928   20.747698   18.707836   10.444265    5.3535485  19.00835\n",
            " 16.264868  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 46 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [7.7396326e+00 9.9019022e+00 8.0027523e+00 1.1543546e+01 4.4526019e+00\n",
            " 7.7336468e-03 2.8785741e-01 7.7696486e+00 9.1706648e+00 8.2416134e+00\n",
            " 3.8527982e+00 1.0676632e+01 9.6327333e+00 2.0643042e+01 1.7946167e+01\n",
            " 1.0929632e+01 4.9855709e+00 1.7689131e+01 1.5090368e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 47 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.1690645  10.566805    8.768849   12.294373    4.8741693  -0.10653497\n",
            "  0.37982175  7.8001714   9.412643    8.670301    3.9460554  11.284907\n",
            " 10.493487   20.566772   19.168892   10.851328    5.4943223  18.296291\n",
            " 16.071308  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 48 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.908399   10.085823    8.235866   11.525092    4.5882125  -0.05970401\n",
            "  0.17863885  7.6651487   9.183127    8.256243    3.8220541  10.762061\n",
            " 10.18896    20.370056   18.043941   10.527982    5.0565543  17.454227\n",
            " 15.245111  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 49 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.044655   10.6507845   8.75995    12.144697    4.9540386  -0.18167982\n",
            "  0.08110894  7.8629403   9.441096    9.008176    4.241227   11.458504\n",
            " 11.069091   20.706978   18.86026    10.290855    5.2864933  18.247585\n",
            " 16.314302  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 50 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.141745   10.446351    8.609975   11.966403    4.844576   -0.15598021\n",
            "  0.41602716  7.8782363   9.375528    8.569446    3.9316325  11.211948\n",
            " 11.186071   20.480991   18.640059   10.380651    5.320694   17.64994\n",
            " 15.946241  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 51 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.9970794  10.569352    8.663067   12.001019    4.882346   -0.05829772\n",
            "  0.1768282   7.9540324   9.436507    8.837575    4.2213507  11.448283\n",
            " 11.602057   20.656586   18.690075   10.1508465   5.292877   17.74331\n",
            " 16.193188  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 52 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.001356   10.204496    8.363474   11.75326     4.730353   -0.06350827\n",
            "  0.27771252  7.6477933   9.255601    8.35027     3.8303463  10.9309435\n",
            " 11.193102   20.70488    18.340118   10.608305    5.182331   17.405182\n",
            " 15.532724  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 53 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.9916487  10.519027    8.68232    11.896372    4.8270826  -0.07797237\n",
            "  0.13448621  7.818303    9.394749    8.80626     4.2201185  11.36717\n",
            " 11.871123   20.725548   18.574192   10.155345    5.314974   17.86859\n",
            " 16.118607  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 54 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [7.5858736e+00 9.7230291e+00 7.9043689e+00 1.1165791e+01 4.3135848e+00\n",
            " 7.5806351e-03 1.5909097e-01 7.4997373e+00 8.7829247e+00 8.0394316e+00\n",
            " 3.7029576e+00 1.0490160e+01 1.1090583e+01 2.0115273e+01 1.7453836e+01\n",
            " 1.0270428e+01 4.7986789e+00 1.6654871e+01 1.4703925e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 55 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.3529673   9.589117    7.5860143  11.09818     4.3941116  -0.02226966\n",
            "  0.10028136  7.2293997   8.862231    8.015617    3.6600482  10.422909\n",
            " 11.08396    20.622292   17.319828   10.624824    4.6544237  16.390491\n",
            " 14.336386  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 56 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.098571   10.8798065   8.547093   12.130292    4.764045   -0.07026173\n",
            "  0.4007644   7.7734933   9.356038    8.607657    3.9798212  11.261653\n",
            " 12.283004   20.89531    18.665894   10.795427    5.4680653  17.963667\n",
            " 16.048216  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 57 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.091644   11.139475    8.7509165  12.199946    4.8105035  -0.08542534\n",
            "  0.39320996  7.7161627   9.39201     8.721507    3.96751    11.387476\n",
            " 12.50251    20.70002    19.050995   10.523702    5.5672607  18.236368\n",
            " 16.097458  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 58 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.37467527e+00  1.01264048e+01  7.68803692e+00  1.12561464e+01\n",
            "  4.43094110e+00 -3.05122212e-02  1.72407739e-02  7.23522663e+00\n",
            "  8.87459087e+00  8.08285522e+00  3.60690141e+00  1.05857124e+01\n",
            "  1.14453897e+01  2.06234303e+01  1.76743507e+01  1.06723785e+01\n",
            "  4.61944866e+00  1.71419773e+01  1.44617405e+01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 59 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 8.163461   12.432478    9.253049   12.954362    4.7970443  -0.2717169\n",
            "  0.20191175  8.867969   10.153189    9.38479     4.355434   11.955862\n",
            " 13.25027    21.358057   19.911268   10.836953    5.357827   19.298428\n",
            " 17.15027   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 60 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.7394547  11.99442     8.837818   12.811208    4.7827     -0.42065218\n",
            "  0.13391718  8.591663    9.969943    9.159828    4.029621   11.823825\n",
            " 12.905726   21.557886   19.65474    10.810628    5.010377   18.796535\n",
            " 16.440878  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 61 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 8.291241   12.310163    9.033047   12.495212    4.6565065  -0.29072604\n",
            "  0.22765684  8.892396    9.69524     8.850792    4.0202084  11.81179\n",
            " 13.225976   20.82439    18.636871    9.956257    5.48412    18.733051\n",
            " 16.67051   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 62 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.9454284  11.768293    8.341723   11.974266    4.578603   -0.11289384\n",
            "  0.24540615  8.280185    9.3987665   8.486885    3.9070308  11.256771\n",
            " 12.646909   20.968422   18.498503   10.937635    5.031128   18.351658\n",
            " 15.811602  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 63 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.7211084  12.174763    8.717613   12.628766    4.674542   -0.2949374\n",
            "  0.16461599  8.424406    9.906367    8.964565    3.997801   11.642143\n",
            " 12.96064    21.36141    19.500946   11.04514     5.046899   18.551947\n",
            " 16.130962  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 64 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.6834507  12.307587    8.687274   12.714254    4.789111   -0.43054724\n",
            "  0.09269582  8.32202     9.834751    9.1858      3.987954   11.704416\n",
            " 13.061917   21.150806   19.46331    10.622605    4.9965906  18.757374\n",
            " 16.263504  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 65 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.719448   11.756413    8.440644   12.014836    4.4703116  -0.29160833\n",
            "  0.11535938  8.235412    9.458504    8.534976    3.7524412  11.352825\n",
            " 12.827951   20.96746    18.240465   10.287102    5.086129   18.117435\n",
            " 15.581934  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 66 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.680885   11.635939    8.011689   12.047881    4.6142454  -0.06201141\n",
            "  0.12850466  7.979564    9.404986    8.533004    3.8889832  11.301653\n",
            " 12.751718   21.66582    18.464422   11.721798    4.8816085  18.33591\n",
            " 15.526315  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 67 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.840169   12.637033    8.749965   12.833733    4.772067   -0.31558526\n",
            "  0.183992    8.5581665  10.050101    9.088543    4.1058536  11.773942\n",
            " 13.349242   21.733084   19.610933   11.244358    5.124609   19.042362\n",
            " 16.398434  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 68 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.7869244  12.7038355   8.825603   13.084336    4.772369   -0.30784023\n",
            "  0.10281862  8.695731   10.00427     9.19872     4.0609236  12.023816\n",
            " 13.514019   21.503984   19.826511   11.135258    5.0463305  19.515745\n",
            " 16.660324  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 69 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.9740314  12.385307    8.676114   12.230758    4.4687543  -0.2568259\n",
            "  0.16495192  8.535922    9.5102      8.678145    3.9104638  11.504271\n",
            " 13.298573   20.887363   18.472559   10.229863    5.2294483  19.124786\n",
            " 16.088974  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 70 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 8.267792   12.710233    8.718181   12.321333    4.842638   -0.21732979\n",
            "  0.21015586  8.530892    9.646432    8.876288    4.148491   11.675197\n",
            " 13.450675   21.451458   18.669611   10.860092    5.271967   19.902542\n",
            " 16.604454  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 71 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.8318286  12.742794    8.965505   12.870685    4.746263   -0.3625157\n",
            "  0.18560964  8.585438   10.082664    9.228084    4.1034403  12.02753\n",
            " 13.548261   21.478706   19.995317   10.921523    5.0841374  20.009668\n",
            " 16.606758  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 72 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.753376   12.610753    8.6400795  12.77068     4.759306   -0.25276232\n",
            "  0.181811    8.546397    9.871836    9.024412    3.9581072  11.791911\n",
            " 13.404674   20.981392   19.44544    10.781972    5.0076895  19.324219\n",
            " 16.242231  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 73 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.72275    12.127379    8.445225   12.023835    4.4728956  -0.29157647\n",
            "  0.11546898  8.239909    9.462546    8.539843    3.7542827  11.379863\n",
            " 13.102806   20.64968    18.252817   10.270992    5.0880404  18.831944\n",
            " 15.608986  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 74 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.814494   12.223576    8.164644   11.960815    4.5807433  -0.09566759\n",
            "  0.15243699  8.11309     9.305999    8.46198     3.8896735  11.251092\n",
            " 13.014564   20.817032   18.45591    11.030028    4.8721323  18.956161\n",
            " 15.679482  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 75 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 8.306818   13.705268    9.42126    13.093907    4.8297358  -0.3370255\n",
            "  0.19479516  9.084586   10.178365    9.29451     4.2882967  12.161907\n",
            " 14.021026   20.748087   19.96149    10.792391    5.488021   20.015575\n",
            " 17.30904   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 76 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.813817   12.982655    8.786391   12.837656    4.76515    -0.33472133\n",
            "  0.1455495   8.642577   10.063008    9.076186    4.1046667  11.834937\n",
            " 13.601877   21.192999   19.668865   11.257328    5.113753   19.041037\n",
            " 16.451555  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 77 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.715064   12.223723    8.503412   12.040431    4.427089   -0.3247399\n",
            "  0.15116039  8.250231    9.463816    8.494212    3.7754865  11.366167\n",
            " 13.200134   20.438938   18.16622    10.160186    5.10595    18.268158\n",
            " 15.656556  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 78 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.759996   12.331866    8.1536455  12.098729    4.629098   -0.18058436\n",
            "  0.16346921  8.161048    9.392682    8.784788    3.9799504  11.38416\n",
            " 13.279396   20.536922   18.441292   11.002975    4.9876933  18.400831\n",
            " 15.847204  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 79 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 8.306988   13.660114    9.416025   13.227649    4.917884   -0.40605494\n",
            "  0.12756012  8.969674   10.089606    9.441529    4.265976   12.19166\n",
            " 14.027635   20.70508    20.033125   10.749502    5.4045835  19.50964\n",
            " 17.417198  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 80 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.822982   13.058983    8.786758   13.006555    4.7551184  -0.29373586\n",
            "  0.16712727  8.604416    9.903071    9.37708     4.128206   11.936138\n",
            " 13.81434    20.27319    19.898727   10.946341    5.108436   18.642263\n",
            " 16.709211  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 81 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.685506   12.153026    8.361754   11.950756    4.4029207  -0.1496615\n",
            "  0.21289757  8.43717     9.455724    8.2699585   3.711973   11.359666\n",
            " 13.183469   20.401075   18.12332    10.274059    5.0200167  17.439787\n",
            " 15.504451  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 82 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.7960854  12.443792    8.1054325  12.025665    4.635418   -0.12265465\n",
            "  0.28189325  8.381444    9.504195    8.491669    3.933123   11.34932\n",
            " 13.292187   20.532808   18.461441   10.97529     4.9444404  17.52744\n",
            " 15.713821  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 83 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 8.224923   13.46266     9.271613   12.831849    4.707751   -0.26673633\n",
            "  0.26807016  8.817133    9.995253    9.142222    4.243489   12.009572\n",
            " 13.858165   20.315613   19.62868    10.369431    5.393227   18.444262\n",
            " 17.04226   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 84 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 7.5837107  12.828786    8.625091   12.712549    4.693038   -0.39270136\n",
            "  0.11285617  8.366239    9.744975    9.17695     3.9945347  11.7588005\n",
            " 13.524941   20.157234   19.336674   10.634643    4.9755015  17.752329\n",
            " 16.293604  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 85 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.734984   12.335102    8.39401    12.056534    4.352203   -0.19824982\n",
            "  0.22617637  8.505494    9.532703    8.346832    3.848403   11.411937\n",
            " 13.345866   20.202219   18.137924   10.340856    5.0942903  17.282295\n",
            " 15.643148  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 86 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.5034084  11.494003    7.7576165  11.316465    4.4643598  -0.02675127\n",
            "  0.13029668  7.472931    9.041954    8.092067    3.7133553  10.7126045\n",
            " 12.726065   20.33699    17.642113   10.786487    4.7056694  16.911652\n",
            " 14.721014  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 87 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.8428063  13.115042    8.791428   13.006154    4.7161655  -0.27414572\n",
            "  0.29939273  8.780206   10.12702     9.148672    4.1696763  12.073702\n",
            " 14.453442   20.341251   19.965122   10.877185    5.0926585  17.77185\n",
            " 16.649471  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 88 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.271197   14.136328    9.576696   13.582694    5.097131   -0.33799255\n",
            "  0.08937861  8.906183   10.212912    9.85402     4.5649652  12.627507\n",
            " 15.400091   20.184034   20.422697   10.1349325   5.6301465  19.08115\n",
            " 17.925556  ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 89 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 7.9536567  12.885281    8.75507    12.344261    4.410188   -0.24347652\n",
            "  0.26567414  8.762072    9.609745    8.508359    3.9060442  11.609501\n",
            " 14.492976   19.617449   18.596893    9.891645    5.2693043  17.728056\n",
            " 16.287472  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 90 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.4581995  11.623464    7.7704554  11.315443    4.474824   -0.08920837\n",
            "  0.06721121  7.393588    9.01676     8.179185    3.6603644  10.7399845\n",
            " 13.449247   19.56514    17.565968   10.716661    4.7098646  16.932259\n",
            " 14.6596575 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 91 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.470476   14.115859    9.740918   13.704538    5.2478423  -0.42776218\n",
            "  0.41867357  8.582745   10.335251    9.745157    4.302698   12.556108\n",
            " 16.066786   20.21034    20.407425   10.380951    5.9112706  18.42981\n",
            " 17.78946   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 92 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.478517   13.784034    9.359216   13.16923     4.998109   -0.3257091\n",
            "  0.47293046  9.057734   10.743885    9.05263     4.1550584  12.213817\n",
            " 15.81311    20.553936   18.801237   10.567246    5.841597   18.626532\n",
            " 17.30479   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 93 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.029558   12.887044    8.633445   12.1903925   4.4945626  -0.2298095\n",
            "  0.26361266  8.657262   11.011448    8.47153     3.9366987  11.521974\n",
            " 14.968702   20.41252    17.507187   11.273439    5.3154435  17.59191\n",
            " 16.04154   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 94 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.298307   12.972178    8.887492   12.527243    4.9454923  -0.10818239\n",
            "  0.43915594  8.039721   11.49349     8.846345    4.0977926  11.692892\n",
            " 15.360844   20.576483   17.845001   11.953693    5.6554747  17.827513\n",
            " 16.592125  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 95 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.9396567  13.228902    9.073532   12.843611    4.7230024  -0.34893686\n",
            "  0.14783223  8.405793   12.282804    9.32388     4.192642   11.987781\n",
            " 15.690819   20.044859   18.096127   11.393325    5.181154   17.36626\n",
            " 16.631485  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 96 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.3577795  14.1091385   9.420942   13.189105    5.0121856  -0.38589334\n",
            "  0.29290593  9.124967   13.026234    9.5822115   4.440572   12.3375225\n",
            " 16.52501    20.123926   17.649202   11.152764    5.671355   18.335773\n",
            " 17.678675  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 97 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.4236765  13.72298     9.557464   13.04881     4.842137   -0.37247667\n",
            "  0.39196816  8.943535   13.286492    9.056457    4.2121572  12.363435\n",
            " 16.389004   20.436037   17.659245   11.77086     5.891369   17.950329\n",
            " 17.361427  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 98 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 7.6639547  11.892165    8.053471   11.705114    4.5562077  -0.11028661\n",
            "  0.06394572  7.736326   12.571567    8.389398    3.798174   11.043729\n",
            " 14.524322   20.322916   16.306374   12.595951    4.7376757  16.516798\n",
            " 15.273259  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 99 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.472219   14.147061    9.783359   13.728747    5.265856   -0.49733007\n",
            "  0.3117254   8.494888   14.511614    9.788725    4.3185806  12.564094\n",
            " 16.94368    20.24466    18.650238   12.29914     5.866346   17.125965\n",
            " 17.77074   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 100 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 8.022046   13.110694    8.748938   12.524084    4.6106224  -0.25829932\n",
            "  0.23649     8.757664   13.929767    8.687723    3.933748   11.5890875\n",
            " 15.568428   19.805874   16.575697   12.573601    5.2267494  16.601213\n",
            " 16.271     ]\n",
            "Epsilon reduced to 0.06553600000000002\n",
            "Total reward: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZU8aGmBaWM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "496327c3-a561-4d17-c64e-f1f51f5ac750"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=100, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=5, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_21(env), epsilon_decay=0.9)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_14 (Reshape)         (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_14 (Conv1D)           (None, 187, 32)           128       \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 5984)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 19)                113715    \n",
            "=================================================================\n",
            "Total params: 113,843\n",
            "Trainable params: 113,843\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |████████████████████--------------------------------------------------------------------------------| 20.0% \rPERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 1 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.06353027  0.11294562  0.08529295 -0.0272862   0.01068775 -0.10373767\n",
            "  0.00607626  0.12164999  0.04722446 -0.02234321  0.03287448 -0.08721256\n",
            "  0.08931717 -0.00913929 -0.03238335 -0.06365645  0.08664208  0.0420693\n",
            "  0.11720253]\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 2 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.12811172  0.09511397  0.02537734  0.02988244 -0.0476623  -0.04661671\n",
            "  0.05215574  0.12634474  0.05179977 -0.01501575  0.02397656 -0.03898726\n",
            "  0.14170147 -0.01362125  0.01055681  0.00434801  0.06412188 -0.03806687\n",
            "  0.08984053]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 3 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.00958494  0.00667237  0.02529179 -0.02870087 -0.03089339  0.04624971\n",
            "  0.04097076 -0.02879587  0.04025622  0.00382982 -0.01199199  0.05128189\n",
            "  0.02148931 -0.07505313 -0.0247143   0.02676741  0.06157359  0.01225356\n",
            " -0.02555649]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 4 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.00823835  0.01361356  0.01490774 -0.04302068 -0.04333159 -0.01854158\n",
            "  0.02643367 -0.06560498  0.04561741 -0.05056728 -0.02757523 -0.04058171\n",
            " -0.00905082 -0.07355111  0.00835053 -0.07107501  0.10582218 -0.0370666\n",
            "  0.04200238]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 5 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.14242542  0.08729832  0.01511026  0.0237811  -0.08923198 -0.10666717\n",
            "  0.05142401 -0.02940205  0.01683231 -0.20536773 -0.00567713 -0.00323841\n",
            " -0.02493481 -0.0257068  -0.10391847 -0.05765543  0.11249764 -0.01264053\n",
            "  0.05757152]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 6 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.07355944  0.05712935  0.03128376  0.01820598 -0.07290737 -0.00736422\n",
            "  0.0412662  -0.08466025  0.01323178 -0.11723533 -0.0445395   0.05000477\n",
            " -0.0142961  -0.05410434 -0.16947214  0.0128868   0.04341163  0.01849989\n",
            " -0.01225606]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 7 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.00549721 -0.00476162  0.03536874 -0.0495853  -0.01263219  0.0579625\n",
            "  0.01048437 -0.10532352  0.01772894 -0.04401733  0.00079677 -0.01731081\n",
            " -0.02358308 -0.05648196 -0.05519874 -0.01655822  0.07673825 -0.01512586\n",
            " -0.02777211]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 8 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.08848535  0.0814256   0.10296432 -0.08854173 -0.0065133   0.01362355\n",
            "  0.04088577 -0.14364886 -0.03207638 -0.18855992  0.04644759 -0.02192354\n",
            " -0.03347418 -0.03693512 -0.01137576 -0.03567649  0.0850789  -0.08307244\n",
            " -0.04740361]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 9 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.03209686  0.10303705  0.07443541 -0.08411995 -0.05242415  0.01512587\n",
            "  0.02342745 -0.16868123  0.02371158 -0.2587476   0.01545384 -0.03126234\n",
            " -0.06277916 -0.01473112 -0.10073964 -0.05408323  0.1008184  -0.05971219\n",
            "  0.02169528]\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 10 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.01892654  0.02420868 -0.03593802 -0.01770057  0.00978946 -0.07557873\n",
            "  0.04784968 -0.18561217  0.02830775 -0.22934513 -0.01167405 -0.11013757\n",
            " -0.08610083 -0.0564912  -0.19933113 -0.11364186  0.14375985 -0.04476658\n",
            "  0.02089431]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 11 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.03373487  0.02097672 -0.02922418 -0.05999477 -0.04356107  0.04247604\n",
            " -0.00362261 -0.13494305 -0.02163681 -0.11743835 -0.00631971 -0.0360708\n",
            " -0.04038651 -0.07168663 -0.12298268 -0.04810299  0.06843503 -0.05964639\n",
            " -0.05331923]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 12 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.08361602 -0.09466092  0.01676458  0.0003664   0.0040019  -0.04638313\n",
            "  0.03970627 -0.16827917  0.01304856 -0.1658911  -0.01284623 -0.059436\n",
            " -0.06910569 -0.04348046 -0.24363972  0.00173785  0.05929063  0.0451194\n",
            "  0.00093862]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 13 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.04120507 -0.01125274 -0.06433087 -0.0688017  -0.00913506 -0.10873536\n",
            "  0.00355792 -0.25475296 -0.03592335 -0.4593595   0.06228685 -0.05447838\n",
            " -0.19755892 -0.01494175 -0.29032195 -0.06002641  0.10038589 -0.06404752\n",
            "  0.06491132]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 14 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05859269 -0.06191387 -0.07515057 -0.06409417 -0.07668275 -0.0502162\n",
            "  0.03070105 -0.21644646 -0.02006986 -0.32067722 -0.00433837 -0.12071396\n",
            " -0.15809442 -0.03284813 -0.31320056 -0.05874105  0.08233677 -0.06719808\n",
            " -0.00884956]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 15 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1418854  -0.08393873 -0.1261059   0.03223203 -0.00073029 -0.02084838\n",
            "  0.03665687 -0.16972475 -0.00906486 -0.15030216 -0.04569761 -0.0017192\n",
            " -0.06770647 -0.03894231 -0.32151356 -0.00890109  0.01004701  0.01172028\n",
            "  0.04375127]\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 16 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.20576528 -0.11042988 -0.14429256 -0.02723614 -0.05072044 -0.05236348\n",
            "  0.00464745 -0.35162395  0.03886059 -0.45659626 -0.11768863 -0.05360236\n",
            " -0.16906941 -0.05392133 -0.60913163 -0.06168234  0.05066684  0.03852065\n",
            "  0.03991209]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 17 reward=-4 new_state=[1 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10112815 -0.01011014 -0.19746366 -0.11964685 -0.03033223 -0.08218178\n",
            "  0.11865374 -0.40811488 -0.03668348 -0.5996452  -0.12172452 -0.03719436\n",
            " -0.25800198 -0.05723021 -0.6603173  -0.08196165  0.07911067 -0.09008068\n",
            " -0.01619233]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 18 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.12621884 -0.11712284 -0.1536984  -0.03832161  0.03278431 -0.00533989\n",
            "  0.02063048 -0.22793631  0.01863841 -0.21280278 -0.121727   -0.05908018\n",
            " -0.1406424  -0.03550564 -0.37093598 -0.03423969  0.09585464 -0.06563814\n",
            " -0.05492588]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 19 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.17986228 -0.22282808 -0.08093179  0.01470402  0.01874682 -0.11095385\n",
            "  0.06586868 -0.2419329   0.06073494 -0.2787507  -0.09543667 -0.07739591\n",
            " -0.16918294  0.02230006 -0.6084919   0.02789452  0.07159077  0.05604334\n",
            " -0.01345265]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 20 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.19399236 -0.21710986 -0.26697233 -0.00832924  0.02323883 -0.1698285\n",
            " -0.07902386 -0.41247758  0.01111908 -0.721311   -0.2227307  -0.09993391\n",
            " -0.44432563  0.01355495 -0.9127453  -0.10076374  0.07676496 -0.01939073\n",
            " -0.05249506]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 21 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.15608917 -0.15327594 -0.15645753 -0.07884402  0.03132647 -0.1275935\n",
            " -0.02716455 -0.34666428  0.01035232 -0.40288642 -0.24585702 -0.16208358\n",
            " -0.23598935  0.02135469 -0.7070867  -0.09334785  0.04743794 -0.07067142\n",
            " -0.06019899]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 22 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.22086662 -0.23860186 -0.14906621 -0.01038591  0.02775452 -0.11078576\n",
            " -0.1039037  -0.2848848   0.03844094 -0.38502502 -0.19952045 -0.03541346\n",
            " -0.1675676   0.05427619 -0.82444715  0.01002819  0.04163999  0.0467304\n",
            " -0.11491586]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 23 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.17118634 -0.23110712 -0.3093255  -0.02612555  0.00607428 -0.16048513\n",
            " -0.177393   -0.42258722  0.04796177 -0.77252364 -0.2944424  -0.05769363\n",
            " -0.39005902  0.05763537 -1.2144471  -0.06323747  0.12889704 -0.05390596\n",
            " -0.11650395]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 24 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.30334657 -0.3296603  -0.27455696  0.02728663 -0.00353617 -0.1232498\n",
            " -0.15126109 -0.40666595  0.06414764 -0.5390084  -0.34218168 -0.08614746\n",
            " -0.3494275  -0.06182706 -1.1428779  -0.01953932  0.02209799 -0.0081335\n",
            " -0.16694607]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.4540152e-01 -3.6108410e-01 -4.4640085e-01  2.4331172e-03\n",
            "  2.8916482e-02 -1.3080379e-01 -2.8822637e-01 -3.8637510e-01\n",
            "  1.2303495e-01 -9.3023849e-01 -3.6623296e-01 -3.1011453e-04\n",
            " -4.3862998e-01  1.0041775e-01 -1.6540468e+00 -6.0275316e-02\n",
            "  8.6967193e-02 -5.4705091e-02 -3.9964122e-01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.29520226 -0.32921597 -0.26966748  0.03798541  0.01723821 -0.0637655\n",
            " -0.18874533 -0.42054206  0.00950714 -0.611007   -0.273256   -0.04294711\n",
            " -0.4386174   0.03551979 -1.4085124  -0.00701671  0.0626404   0.083751\n",
            " -0.24667822]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.49702257 -0.30851433 -0.3865085  -0.05522905  0.1092658   0.04890088\n",
            " -0.3116234  -0.34247145 -0.06237289 -0.75440735 -0.32774693 -0.04563729\n",
            " -0.4410731   0.06006462 -1.712444   -0.02811899  0.02325505  0.10295634\n",
            " -0.2560025 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.45642462 -0.3187971  -0.37863812 -0.01065715  0.10696813  0.02521472\n",
            " -0.26689732 -0.38316572 -0.12551221 -0.66848075 -0.32192457 -0.11311579\n",
            " -0.41413537 -0.11291023 -1.6397967  -0.02146322  0.02047046  0.10860647\n",
            " -0.34180143]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.52674925 -0.2736316  -0.38468748 -0.06917864  0.2602684  -0.01104704\n",
            " -0.369205   -0.4382352  -0.2432343  -0.5985411  -0.37328696  0.00347784\n",
            " -0.2892588  -0.09867521 -1.7720991  -0.06141096  0.05364055  0.05480539\n",
            " -0.361139  ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5139907  -0.35088125 -0.37153003  0.04872624  0.31768933  0.01127932\n",
            " -0.28758943 -0.35963157 -0.14488395 -0.6346496  -0.32119638  0.01842145\n",
            " -0.44231018 -0.15750884 -1.7060068  -0.00856381  0.0491984   0.08688518\n",
            " -0.3723142 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.62194395 -0.36264676 -0.4311605  -0.01530796  0.39287785 -0.02631747\n",
            " -0.3248168  -0.3921078  -0.16932714 -0.7252117  -0.3418257   0.15382384\n",
            " -0.4422562  -0.1679906  -1.8419439  -0.01619457  0.11088899  0.1037002\n",
            " -0.39105943]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5394526  -0.42660803 -0.3076346  -0.01762351  0.41966304 -0.04405753\n",
            " -0.37512138 -0.39375597 -0.27107638 -0.5335147  -0.30603373  0.22437847\n",
            " -0.37481993 -0.2102888  -1.8050652  -0.03393641  0.02359641  0.09279639\n",
            " -0.3543901 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6324775  -0.30948034 -0.4133177  -0.08166381  0.75812066 -0.0052468\n",
            " -0.44641528 -0.4617059  -0.41519696 -0.62841797 -0.40657252  0.15897928\n",
            " -0.34013334 -0.24599576 -2.0113196  -0.05124891  0.04939431  0.07721498\n",
            " -0.3879073 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.64176905 -0.44986132 -0.44556987  0.08101574  0.7045147  -0.05621662\n",
            " -0.47985628 -0.45362106 -0.42315665 -0.64191914 -0.43482664  0.3098996\n",
            " -0.39785263 -0.30381408 -2.0705369  -0.06603471  0.13835105  0.03052385\n",
            " -0.37007973]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7028003  -0.3422236  -0.4614616   0.00455701  0.81168425 -0.04257625\n",
            " -0.4580935  -0.43748125 -0.39183715 -0.66958314 -0.41543356  0.28138447\n",
            " -0.41676408 -0.3147234  -2.0654414  -0.02659898  0.159289    0.01451753\n",
            " -0.410734  ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6751192  -0.41464534 -0.46675977  0.05977008  0.83454883 -0.05867638\n",
            " -0.39740732 -0.36224753 -0.33428434 -0.5439839  -0.4590468   0.39125264\n",
            " -0.4222617  -0.36733255 -1.9749916   0.01213934  0.10778608 -0.00692166\n",
            " -0.37398154]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.62689275 -0.39870924 -0.42481467 -0.04119201  1.1866865   0.02332414\n",
            " -0.38480252 -0.36687472 -0.4819908  -0.55943686 -0.4713282   0.3702713\n",
            " -0.5097479  -0.43111002 -2.1049194   0.01610322  0.06974377  0.12984627\n",
            " -0.41355297]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.820189   -0.38554582 -0.5009828   0.06206691  1.146078   -0.02802891\n",
            " -0.4221208  -0.38896796 -0.5394451  -0.5888694  -0.53913987  0.38347155\n",
            " -0.48962542 -0.5511546  -2.214699    0.00541151  0.05627719  0.00315874\n",
            " -0.42331678]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-7.71870255e-01 -5.10760903e-01 -5.50685823e-01  6.41529411e-02\n",
            "  1.22974408e+00 -2.38174945e-02 -5.28338790e-01 -3.17686886e-01\n",
            " -5.43185771e-01 -7.54048586e-01 -5.78316689e-01  4.42085624e-01\n",
            " -5.23048639e-01 -6.63847923e-01 -2.42349148e+00 -2.07018759e-03\n",
            "  1.05543435e-01 -2.76129018e-03 -4.42360163e-01]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7300702  -0.44315666 -0.3895027  -0.14317729  1.3206567  -0.1047179\n",
            " -0.6019141  -0.36185166 -0.5962641  -0.84425026 -0.6103142   0.45589018\n",
            " -0.55026084 -0.514954   -2.5031137  -0.11706746  0.13451177  0.04986528\n",
            " -0.42998624]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7967994  -0.45988056 -0.45830542 -0.02083996  1.2957568  -0.06514807\n",
            " -0.52157986 -0.24429402 -0.5421295  -0.7602982  -0.518451    0.5134956\n",
            " -0.44770017 -0.64561814 -2.3521836   0.01872049  0.08665986  0.04700006\n",
            " -0.43221655]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.79269856 -0.43879682 -0.5030734   0.05642906  1.46114     0.00774544\n",
            " -0.4928552  -0.14471665 -0.63558483 -0.6600269  -0.54485697  0.4865757\n",
            " -0.5581108  -0.56105655 -2.3306544   0.04550558  0.09799404  0.08012971\n",
            " -0.40170214]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7917391  -0.46878403 -0.5649352   0.05698337  1.3654507  -0.08103777\n",
            " -0.5236795  -0.07753394 -0.53109914 -0.7645296  -0.50323176  0.606927\n",
            " -0.5559528  -0.59584624 -2.4003346  -0.02770841  0.14816843 -0.06511903\n",
            " -0.44100422]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9554978  -0.5287827  -0.4578913  -0.08121178  1.4038572  -0.10276023\n",
            " -0.58700496 -0.20582059 -0.7380363  -0.9611465  -0.6992379   0.5606079\n",
            " -0.602046   -0.7786762  -2.9236367  -0.11168645  0.08031506  0.09256519\n",
            " -0.52714074]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8452997  -0.44345698 -0.5171999  -0.04474637  1.4810215  -0.03949726\n",
            " -0.61054033 -0.1114921  -0.61861426 -0.9335671  -0.54530466  0.67266726\n",
            " -0.5403639  -0.5954929  -2.5369494  -0.02627391  0.21143635 -0.01809715\n",
            " -0.5121884 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7869239  -0.36385345 -0.5013638  -0.04416927  1.1902195  -0.02369054\n",
            " -0.53598607 -0.03365802 -0.63811237 -0.58018446 -0.5642531   0.55079037\n",
            " -0.42688948 -0.7021932  -2.4428494   0.06031493  0.07009922  0.03378782\n",
            " -0.4772028 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8688872  -0.44108707 -0.50812435  0.00603745  1.2734222  -0.01475234\n",
            " -0.5761664  -0.21305057 -0.7253984  -0.673095   -0.61259556  0.61966646\n",
            " -0.61305857 -0.67791724 -2.6179492  -0.01438326  0.0580884   0.09436506\n",
            " -0.50014234]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.91236603 -0.5021762  -0.5317696  -0.14769378  1.459465   -0.08242267\n",
            " -0.6428801  -0.14513817 -0.6816646  -0.935742   -0.64867     0.65881824\n",
            " -0.6475348  -0.7256999  -2.9040077  -0.08056254  0.1548403  -0.03548398\n",
            " -0.4755584 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9567632  -0.497563   -0.5034778   0.00553397  1.1347462  -0.05652677\n",
            " -0.53753865 -0.03642984 -0.73465776 -0.87400454 -0.59302926  0.6395047\n",
            " -0.49635288 -0.7974022  -2.7101815  -0.05499209  0.04130256  0.04592892\n",
            " -0.51948595]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.79447913 -0.46162483 -0.50744814  0.01515415  1.2011795   0.01606531\n",
            " -0.5183552   0.04527282 -0.6154668  -0.7797521  -0.5454728   0.7136434\n",
            " -0.4839681  -0.6399209  -2.4264789  -0.00953049  0.19265813 -0.04620748\n",
            " -0.52516854]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 51 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-7.7782339e-01 -4.5438847e-01 -5.3079683e-01 -2.1207981e-02\n",
            "  8.7445891e-01 -5.0845896e-03 -5.3702432e-01 -6.7635909e-02\n",
            " -5.5937648e-01 -6.7157793e-01 -5.6934065e-01  6.8787390e-01\n",
            " -5.1467335e-01 -6.7529666e-01 -2.4621487e+00  3.6654598e-04\n",
            "  7.9560399e-02  5.9902009e-02 -4.4263285e-01]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 52 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.91024923 -0.45129395 -0.5266717  -0.05612779  0.99293923 -0.07029343\n",
            " -0.6974827  -0.00688805 -0.8667601  -0.9316214  -0.70906997  0.6477177\n",
            " -0.6695034  -0.72480243 -2.8575392  -0.06443597  0.10858735  0.07724562\n",
            " -0.4800726 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 53 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.77050203 -0.42282173 -0.55230105  0.08922943  0.72856486 -0.08341806\n",
            " -0.5919114  -0.00819725 -0.6665508  -0.7925114  -0.5583586   0.65796196\n",
            " -0.4781764  -0.6097963  -2.4492567   0.04161633  0.10720216  0.00845977\n",
            " -0.44965675]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 54 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8059116  -0.41573453 -0.46441442  0.04077361  0.7352737   0.01536272\n",
            " -0.40513477  0.00642008 -0.5664685  -0.52713615 -0.49648482  0.5167368\n",
            " -0.4161961  -0.64680225 -2.2636147  -0.03229464  0.05005365  0.15288445\n",
            " -0.49641815]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 55 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.689108   -0.39454436 -0.35551828 -0.02521189  0.6917704   0.04139636\n",
            " -0.4756011  -0.02816368 -0.48916078 -0.5472818  -0.48557368  0.5278968\n",
            " -0.44613516 -0.46317348 -2.0437071  -0.02835342  0.15227176  0.21895665\n",
            " -0.39353728]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 56 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.83976984 -0.4635308  -0.39103916 -0.06471393  0.4063239  -0.10175015\n",
            " -0.5785511  -0.0715674  -0.63426864 -0.7592547  -0.6220455   0.61009103\n",
            " -0.43318674 -0.69231355 -2.466111   -0.07311777  0.14350215  0.35213453\n",
            " -0.40887094]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 57 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.7305224  -0.40543267 -0.41895938  0.02645841  0.7868621  -0.03952137\n",
            " -0.5115968   0.09768217 -0.6760942  -0.68488985 -0.5010058   0.6511547\n",
            " -0.41687152 -0.6301793  -2.3321383   0.03379713  0.11225487  0.4403258\n",
            " -0.41851622]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 58 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.76465416 -0.39129215 -0.44062984  0.06457728  0.5904614   0.0185436\n",
            " -0.40528876  0.1404137  -0.52074707 -0.47242966 -0.42673212  0.5511011\n",
            " -0.3552127  -0.56299984 -2.008054   -0.00752112  0.09494559  0.35957283\n",
            " -0.34376812]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 59 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.87396574 -0.49935275 -0.41837013  0.06511953  0.8113933  -0.02114118\n",
            " -0.53530264  0.21107824 -0.6681269  -0.6884548  -0.59325373  0.7923571\n",
            " -0.44659942 -0.6695856  -2.3899703  -0.04318839  0.10876392  0.56723005\n",
            " -0.43009418]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 60 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.82595795 -0.42800096 -0.37852886 -0.04077277  0.5860454  -0.11837275\n",
            " -0.599287    0.07591249 -0.6549796  -0.73919487 -0.5866547   0.83940923\n",
            " -0.49458957 -0.6172002  -2.5760608  -0.12521939  0.21482916  0.6047831\n",
            " -0.4604985 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 61 reward=0 new_state=[0 0 1 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8482871  -0.50139475 -0.5241902   0.04052504  1.1264788  -0.00325187\n",
            " -0.5582437   0.388059   -0.69192564 -0.7987789  -0.58896905  0.8781738\n",
            " -0.42645594 -0.7441565  -2.4939926  -0.03369239  0.08737426  0.62818694\n",
            " -0.45628324]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 62 reward=0 new_state=[0 0 1 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.75686496 -0.3643697  -0.45722607  0.03343343  1.1319488   0.17282729\n",
            " -0.43856713  0.29527083 -0.5505484  -0.6023213  -0.54063565  0.80301625\n",
            " -0.40830287 -0.6173484  -2.1988907   0.00574188  0.1286835   0.6382045\n",
            " -0.32766938]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 63 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.9492828  -0.48259646 -0.4373389   0.02302835  0.9549393   0.14687812\n",
            " -0.58977085  0.22530057 -0.6385957  -0.66400546 -0.58661175  1.1117374\n",
            " -0.47435746 -0.661818   -2.5238516  -0.07559918  0.10720589  0.77454084\n",
            " -0.48482084]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 64 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8578026  -0.50043297 -0.5016698   0.05446466  1.1517832   0.22422652\n",
            " -0.58254766  0.38073596 -0.78282887 -0.75854397 -0.59973234  1.1476607\n",
            " -0.51277006 -0.51461273 -2.6406198  -0.06044032  0.12570347  0.7698997\n",
            " -0.4998003 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 65 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.90290165 -0.4223765  -0.5753835   0.02379871  1.1478206   0.2962965\n",
            " -0.63701916  0.34966284 -0.7327658  -0.80415636 -0.5845194   1.1328443\n",
            " -0.5035151  -0.41357213 -2.5980175  -0.03229471  0.13437466  0.6835747\n",
            " -0.47673726]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 66 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.90151036 -0.43082905 -0.48448902  0.0475591   1.3055617   0.39910015\n",
            " -0.5004197   0.5650849  -0.64178246 -0.4882134  -0.44526154  1.1754708\n",
            " -0.43628743 -0.34496558 -2.3969412  -0.00755154  0.01488474  0.80448526\n",
            " -0.5075081 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 67 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-7.3204190e-01 -3.8918668e-01 -3.8819981e-01  4.0934447e-02\n",
            "  1.3107920e+00  4.4885957e-01 -4.1239488e-01  5.2047515e-01\n",
            " -5.6687433e-01 -5.0533277e-01 -4.5721236e-01  1.2301301e+00\n",
            " -4.0384832e-01 -2.2761995e-01 -2.0225132e+00 -4.9065350e-04\n",
            "  1.3969338e-01  6.5140396e-01 -4.1836953e-01]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 68 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.9781793  -0.50824875 -0.51864296 -0.03968741  1.3924687   0.5068578\n",
            " -0.72677505  0.37399125 -0.79801047 -0.9442649  -0.7511702   1.7957287\n",
            " -0.52261627 -0.2137037  -2.8577843  -0.08380079  0.15215115  0.94580835\n",
            " -0.40203074]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 69 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.9740523  -0.5203269  -0.59767836  0.06575533  1.6327009   0.68685293\n",
            " -0.6569183   0.61119044 -0.9276939  -0.92984754 -0.7282368   1.9755827\n",
            " -0.63721    -0.1801952  -2.9513175  -0.03918329  0.10313305  1.1194643\n",
            " -0.5491929 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 70 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.92994744 -0.39980707 -0.521529    0.00522994  1.5930786   0.6310562\n",
            " -0.53445     0.6374282  -0.606771   -0.6023389  -0.52567893  1.7625345\n",
            " -0.37560117 -0.12237454 -2.383237   -0.04740832  0.10914257  0.7025945\n",
            " -0.48638964]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 71 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.9595332  -0.52384156 -0.5209752   0.00854911  1.9012302   0.64847237\n",
            " -0.5431442   0.78301036 -0.7135625  -0.6269304  -0.58029264  1.9707432\n",
            " -0.46568733 -0.0749295  -2.6632767   0.00341916  0.06107049  0.9792501\n",
            " -0.5294005 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 72 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.9728137  -0.51340544 -0.54822034 -0.10740408  1.9660046   0.92232466\n",
            " -0.88283753  0.5928346  -0.9440908  -0.96425015 -0.81862444  2.7220645\n",
            " -0.6495548   0.07354017 -3.17916    -0.12275839  0.20569569  1.279506\n",
            " -0.5043364 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 73 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0120963  -0.5245551  -0.64072984  0.0038027   1.9255701   0.8246751\n",
            " -0.71426094  0.6059608  -0.9243989  -0.9536085  -0.7979185   2.7333646\n",
            " -0.64737123  0.05794453 -3.0101008  -0.04825476  0.09891319  1.1077243\n",
            " -0.54086643]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 74 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.9884365  -0.5184951  -0.52575904  0.04419852  2.241598    0.8341645\n",
            " -0.5435149   0.92363095 -0.7840201  -0.60247964 -0.559488    2.7803535\n",
            " -0.4848607   0.07127685 -2.7200186  -0.03460206  0.09654526  1.061982\n",
            " -0.56138945]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 75 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8902614  -0.362367   -0.535377   -0.0502553   2.004048    0.8704689\n",
            " -0.5123083   0.67785174 -0.64932525 -0.55329424 -0.5102209   2.5463607\n",
            " -0.44585767  0.10198417 -2.423337   -0.02947977  0.10375696  0.8192323\n",
            " -0.5033626 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 76 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.1382203  -0.6196062  -0.6348708  -0.08876781  2.3991811   0.993091\n",
            " -0.86054116  0.68101263 -0.8179389  -1.0963069  -0.86668044  3.7214348\n",
            " -0.6295772   0.18993104 -3.4217308  -0.11096255  0.13383876  1.3478935\n",
            " -0.513296  ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 77 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0560578  -0.59990567 -0.68837065  0.04718084  2.5043437   1.1957495\n",
            " -0.8488525   0.86786187 -0.7270158  -1.055668   -0.7995125   4.0650315\n",
            " -0.6604073   0.25220394 -3.3489194  -0.0653291   0.20846412  1.341686\n",
            " -0.6038116 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 78 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0465357  -0.46927515 -0.5708521   0.01469321  2.4477699   1.0166405\n",
            " -0.6242018   0.8958826  -0.42887396 -0.55164415 -0.5178324   3.5108676\n",
            " -0.49180052  0.22097155 -2.7675345   0.03511867  0.06028462  1.0745076\n",
            " -0.5869611 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 79 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0696021  -0.50718623 -0.5776582   0.06846621  2.6389604   1.13652\n",
            " -0.63768905  1.1915734  -0.38301566 -0.72749496 -0.5852236   4.0234947\n",
            " -0.51656574  0.29213658 -2.9805357  -0.05626815  0.10915776  1.2682668\n",
            " -0.6677966 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 80 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.231012   -0.5275863  -0.6602228  -0.08750138  2.5507498   1.2390704\n",
            " -0.8487881   0.6475263  -0.34447023 -1.0838829  -0.90161014  4.652574\n",
            " -0.66149426  0.27704424 -3.43783    -0.18725325  0.20510796  1.3177809\n",
            " -0.630691  ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 81 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2787932  -0.6289788  -0.71192837  0.015848    2.5821605   1.270747\n",
            " -0.92495704  0.81074834 -0.3347003  -1.1513836  -0.9229614   5.04004\n",
            " -0.68813735  0.28836215 -3.6347725  -0.08163011  0.14665894  1.5094373\n",
            " -0.65461487]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 82 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0646472  -0.54666007 -0.58998877 -0.05233965  2.8983612   1.182871\n",
            " -0.65082145  0.95807475 -0.11521354 -0.73276126 -0.7016688   4.381929\n",
            " -0.6033672   0.323712   -3.0204391   0.01491645  0.1343387   1.2797085\n",
            " -0.5770276 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 83 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.1398937e+00 -5.1997626e-01 -5.6626451e-01  2.9107388e-02\n",
            "  2.7157981e+00  1.0822530e+00 -6.5080732e-01  9.9125606e-01\n",
            "  3.5312311e-03 -6.8968493e-01 -6.2954742e-01  4.4674854e+00\n",
            " -4.9100792e-01  3.0931097e-01 -2.9322245e+00  7.6092975e-03\n",
            "  7.6615602e-02  1.1545783e+00 -5.9615433e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 84 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2811279  -0.676334   -0.6154615  -0.08217069  3.0696204   1.2891344\n",
            " -0.8384426   0.92473793 -0.01427212 -1.0501631  -0.87461346  5.2936463\n",
            " -0.6971784   0.36739635 -3.5991786  -0.0937437   0.09295732  1.5990793\n",
            " -0.61375   ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 85 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2588104  -0.550368   -0.71589136 -0.08523856  2.835536    1.4616393\n",
            " -0.9685534   0.78792614 -0.03516996 -1.2089951  -0.876591    5.5846295\n",
            " -0.6904452   0.46159768 -3.698658   -0.15930149  0.14873895  1.5066348\n",
            " -0.6000896 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 86 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.1728277  -0.52928984 -0.6098007   0.01697835  2.9565094   1.1863263\n",
            " -0.6339247   1.0126143   0.22122824 -0.65135765 -0.6467522   5.0105395\n",
            " -0.55532193  0.33663985 -3.0433233   0.00766708  0.10170139  1.254326\n",
            " -0.60279256]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 87 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0564725e+00 -5.8893734e-01 -5.9377021e-01  1.2759592e-03\n",
            "  3.0560334e+00  1.3241407e+00 -6.7225266e-01  1.1751834e+00\n",
            "  2.5245854e-01 -7.2361988e-01 -6.4212024e-01  5.1779494e+00\n",
            " -5.3566575e-01  4.1844192e-01 -3.0902812e+00 -1.8103883e-02\n",
            "  1.8205394e-01  1.3146058e+00 -6.4760613e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 88 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.214324   -0.5346855  -0.66717064 -0.08185961  2.8439429   1.414953\n",
            " -0.9193577   0.92672545  0.22586028 -1.0471686  -0.81792015  5.6415343\n",
            " -0.63506126  0.47874928 -3.487291   -0.13152954  0.14852771  1.5081965\n",
            " -0.6202075 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 89 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.3647423  -0.69015974 -0.7410462   0.05741392  3.3513443   1.438128\n",
            " -0.925717    1.086341    0.17559978 -1.2125078  -0.92834723  6.0077915\n",
            " -0.7623716   0.57217336 -3.7923114  -0.10625475  0.12512513  1.6582942\n",
            " -0.68493676]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 90 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.1961055  -0.45560408 -0.6341121  -0.03994856  3.0003693   1.3333397\n",
            " -0.7245833   0.97189146  0.33168992 -0.7645116  -0.68104357  5.183146\n",
            " -0.5484037   0.452066   -3.1392858  -0.07078897  0.14243235  1.1871269\n",
            " -0.59299856]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 91 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2441694e+00 -5.7720095e-01 -6.4351755e-01  4.5215909e-04\n",
            "  2.9645162e+00  1.3533171e+00 -7.2253585e-01  1.1331450e+00\n",
            "  4.8294389e-01 -7.6920289e-01 -7.0310026e-01  5.5677137e+00\n",
            " -5.0078523e-01  3.8576517e-01 -3.2727578e+00  9.1980929e-03\n",
            "  9.2756003e-02  1.4035755e+00 -6.8823290e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 92 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2564961  -0.69447845 -0.6083547  -0.1333047   3.1410222   1.5848742\n",
            " -0.9214782   1.0059786   0.71387607 -1.0590442  -0.86017746  6.1219926\n",
            " -0.7186651   0.5417372  -3.7496145  -0.15797189  0.16703588  1.6943427\n",
            " -0.71431464]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 93 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2906711  -0.5799604  -0.76875365  0.04580291  2.9817083   1.5556127\n",
            " -0.95551413  0.9223357   0.7508085  -1.1067779  -0.8012727   6.0723796\n",
            " -0.71766317  0.5661795  -3.6975787  -0.12376137  0.12621838  1.5858825\n",
            " -0.6556197 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 94 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2527344  -0.60825473 -0.62968427  0.04071779  3.410778    1.4060049\n",
            " -0.7001003   1.243381    1.0251061  -0.72293454 -0.6881467   6.0639486\n",
            " -0.63415766  0.52061504 -3.3777635  -0.06149025  0.10968542  1.6516521\n",
            " -0.70096254]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 95 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.1640921  -0.48550612 -0.55476445 -0.0495351   3.2184649   1.3840201\n",
            " -0.6968691   1.1183352   1.0450436  -0.7171727  -0.6698189   5.7177863\n",
            " -0.5568238   0.52970344 -3.1653845  -0.03927347  0.12986611  1.5513604\n",
            " -0.70071846]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 96 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.3591791  -0.62569886 -0.6807385  -0.13373828  3.4114246   1.4994352\n",
            " -0.9363284   0.9792434   1.2927462  -1.0813899  -0.9491265   6.422403\n",
            " -0.76635027  0.60235524 -3.835913   -0.14421323  0.10315236  2.254712\n",
            " -0.65632004]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 97 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2786844  -0.6501045  -0.7171885   0.0405964   3.5288594   1.6867735\n",
            " -0.9944617   1.055632    1.383241   -1.2213343  -0.9017405   6.517861\n",
            " -0.7648838   0.7136597  -3.9738955  -0.07073253  0.18475021  2.5543306\n",
            " -0.6550228 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 98 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2056322  -0.5385209  -0.68870157  0.02887701  3.25756     1.3663888\n",
            " -0.75912344  1.1402714   1.2935816  -0.72705346 -0.6314054   5.754932\n",
            " -0.5730706   0.54653096 -3.2385285  -0.02722614  0.13122135  1.9963162\n",
            " -0.58709604]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 99 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.3790201e+00 -5.6424260e-01 -6.7830759e-01  4.1684724e-02\n",
            "  3.8124177e+00  1.5267397e+00 -7.6716912e-01  1.4544362e+00\n",
            "  1.5009117e+00 -8.2235658e-01 -7.4547541e-01  6.5283027e+00\n",
            " -5.7956910e-01  5.6821209e-01 -3.5133519e+00 -4.9838312e-03\n",
            "  1.2654077e-01  2.4055490e+00 -7.0048648e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 100 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.3188068  -0.61488855 -0.64320225 -0.1389141   3.2940774   1.6093552\n",
            " -0.9438756   0.95974904  1.5990745  -1.0879648  -0.93750185  6.212012\n",
            " -0.7531798   0.67136216 -3.8164277  -0.19307089  0.18562672  2.679641\n",
            " -0.66245264]\n",
            "Epsilon reduced to 0.09000000000000001\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.4022809  -0.90424746 -0.7449607   0.04705154  4.0017653   1.7466478\n",
            " -1.1095031   1.4349507   1.946072   -1.2670869  -0.87367797  6.8614492\n",
            " -0.8129      0.7769677  -4.2509933  -0.18410087  0.06646704  3.065757\n",
            " -0.7828177 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.3130904  -0.61186683 -0.7183481  -0.02525132  3.6768012   1.4806894\n",
            " -0.97298145  1.1341783   1.7026628  -1.0459976  -0.7288977   5.8640647\n",
            " -0.69331616  0.7302422  -3.8298693  -0.12985893  0.0171887   2.7912414\n",
            " -0.7399525 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.2827047  -0.7935504  -0.72003114  0.09415923  3.4752758   1.5289705\n",
            " -0.8909451   1.0973248   1.8882033  -1.0574228  -0.72698176  5.2102313\n",
            " -0.69100666  0.4869786  -3.6906333  -0.12815543  0.13612086  2.761448\n",
            " -0.74223953]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.28815    -0.5271447  -0.63418627 -0.07073576  3.4351509   1.403032\n",
            " -0.93273884  1.0497501   1.745857   -0.9573146  -0.7445407   5.146137\n",
            " -0.66371953  0.73560977 -3.6322236  -0.1366052   0.04763591  2.765561\n",
            " -0.629801  ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.2294668  -0.59253466 -0.6450761   0.06737854  3.6185064   1.429201\n",
            " -0.7847429   1.0961494   1.9281697  -1.1204773  -0.6952764   4.6210713\n",
            " -0.72435594  0.6628408  -3.5755062  -0.08702798 -0.01885387  2.8727036\n",
            " -0.6975735 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.2413398  -0.6074816  -0.62196714  0.01445031  3.4009323   1.3341221\n",
            " -0.8530626   1.0043093   1.892365   -1.0727173  -0.64512014  4.2920346\n",
            " -0.67866594  0.6669295  -3.4415488  -0.10959775  0.13999815  2.7816672\n",
            " -0.5536305 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1317937  -0.7081344  -0.6509612   0.03155479  3.3089283   1.4116322\n",
            " -0.82304984  1.0361235   1.8824384  -1.0593926  -0.6190398   3.996715\n",
            " -0.5680965   0.70494485 -3.5030463  -0.16953509  0.07313953  2.9040601\n",
            " -0.65684783]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.191878   -0.6145765  -0.62352216  0.05843931  3.1096246   1.2951264\n",
            " -0.76708657  1.006085    2.0154293  -0.9539486  -0.57693416  3.7113645\n",
            " -0.6510777   0.5520655  -3.2435853  -0.12114388  0.11793438  2.7775972\n",
            " -0.6227764 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1902765  -0.7220258  -0.7119275   0.09916355  3.1075518   1.43897\n",
            " -0.7986102   0.97858196  1.9436837  -1.0921713  -0.6599387   3.3503954\n",
            " -0.569738    0.46114567 -3.3992662  -0.14084275  0.14075123  2.8839836\n",
            " -0.7092973 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1391858  -0.48786506 -0.56016105 -0.08339107  2.953993    1.233866\n",
            " -0.8450157   0.9025943   1.6890332  -0.91240996 -0.7267468   3.042305\n",
            " -0.5671692   0.6274712  -3.2316911  -0.13596246  0.04504374  2.677143\n",
            " -0.558419  ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0554149  -0.6209754  -0.54216176  0.09103446  2.853318    1.2499744\n",
            " -0.7630371   0.9842922   1.8057748  -0.9657276  -0.5856318   2.6637743\n",
            " -0.5711206   0.56323504 -3.1380386  -0.14709355  0.04938869  2.7005804\n",
            " -0.56120837]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.92556936 -0.4650629  -0.5120071  -0.14415431  2.4201581   1.1210167\n",
            " -0.7917941   0.8006668   1.5575345  -0.78635347 -0.60732925  2.2515934\n",
            " -0.51680535  0.5467898  -2.8319662  -0.14572072  0.04185154  2.3375547\n",
            " -0.5067949 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0765351  -0.4707924  -0.5427186   0.01500535  2.7787418   1.1967192\n",
            " -0.68238664  0.96156996  1.7117108  -0.84879524 -0.6282507   2.228162\n",
            " -0.5414846   0.46209502 -2.9622169  -0.0785059   0.03764476  2.5309105\n",
            " -0.572297  ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.85181403 -0.33780977 -0.5185576  -0.09191335  2.0066602   1.0360578\n",
            " -0.70961815  0.6582355   1.4610677  -0.66696435 -0.4804731   1.7182862\n",
            " -0.4713      0.47588238 -2.5648894  -0.13244653  0.05039498  2.1936982\n",
            " -0.5214439 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.77831435 -0.38252977 -0.44190347 -0.08533607  1.9305878   0.937472\n",
            " -0.65964115  0.63933843  1.4367992  -0.60602677 -0.46756497  1.5540472\n",
            " -0.4747849   0.36945552 -2.4485428  -0.13974242 -0.0325375   2.184118\n",
            " -0.4588718 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8952235  -0.40597004 -0.5280301  -0.01310336  1.8628173   1.0264292\n",
            " -0.65242773  0.7059888   1.5884142  -0.78781015 -0.3747429   1.4349494\n",
            " -0.4476329   0.39042556 -2.4301126  -0.1241629   0.15004833  2.2576091\n",
            " -0.49861097]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8753935  -0.38911155 -0.48555428 -0.05300596  1.8852773   1.0337963\n",
            " -0.64233404  0.7509958   1.4622476  -0.62406933 -0.5221459   1.394375\n",
            " -0.44688782  0.32617262 -2.5108216  -0.07923362  0.02132621  2.308287\n",
            " -0.47438994]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8179688  -0.32706964 -0.48895806 -0.07805237  1.6927431   1.0197327\n",
            " -0.70629317  0.64906573  1.4359328  -0.6454907  -0.45154697  1.2653935\n",
            " -0.41312924  0.4851625  -2.4814687  -0.10202754  0.06177116  2.5041738\n",
            " -0.46169534]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-7.5033444e-01 -4.1065678e-01 -4.3118677e-01  1.5026859e-03\n",
            "  1.5604908e+00  8.9789313e-01 -5.8046651e-01  6.7199498e-01\n",
            "  1.4657385e+00 -7.1065706e-01 -4.7682831e-01  1.0977157e+00\n",
            " -4.6254903e-01  4.4938999e-01 -2.3450813e+00 -9.8227628e-02\n",
            "  3.2805979e-02  2.4215121e+00 -4.1036570e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.85289866 -0.4288635  -0.50191957 -0.02554105  1.4880626   0.9294312\n",
            " -0.62592816  0.7350984   1.6429895  -0.76584107 -0.4245514   1.1372124\n",
            " -0.45257184  0.39802453 -2.350797   -0.15048298  0.12553792  2.5534306\n",
            " -0.5072945 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.78617465 -0.39578447 -0.4820526  -0.00475809  1.4554379   0.9665697\n",
            " -0.60735     0.72161865  1.5562862  -0.73784065 -0.5197701   0.9949299\n",
            " -0.5136931   0.43342224 -2.462808   -0.08221207  0.00348907  2.7777984\n",
            " -0.49142852]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.79681534 -0.36777627 -0.45330015 -0.11907087  1.3861892   0.9400031\n",
            " -0.69783694  0.68707824  1.4453648  -0.6491528  -0.49524784  0.9782401\n",
            " -0.4132173   0.48153114 -2.4362316  -0.16466658  0.06477816  2.9035726\n",
            " -0.42946035]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.78241277 -0.4100526  -0.44997293  0.01484353  1.3517545   0.9258307\n",
            " -0.6051209   0.72237045  1.5380695  -0.7236528  -0.48817685  0.87906647\n",
            " -0.46775118  0.4339006  -2.3934355  -0.09656354  0.03427437  2.9186676\n",
            " -0.46385655]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.81216484 -0.4205776  -0.52325064 -0.0675261   1.4075565   0.9638595\n",
            " -0.6387854   0.68986934  1.6258333  -0.7792599  -0.45826077  0.8180208\n",
            " -0.5284103   0.43165362 -2.4581344  -0.13320793  0.08881299  3.2614386\n",
            " -0.46475822]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8609262  -0.3995977  -0.5490618  -0.05190806  1.5720794   1.0593592\n",
            " -0.662237    0.85466045  1.5831761  -0.63811713 -0.45499766  1.0056145\n",
            " -0.4912541   0.4259299  -2.539338   -0.08746003  0.01578676  3.3895564\n",
            " -0.5336782 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8160149  -0.3732466  -0.4995868  -0.0120467   1.3423482   0.93755376\n",
            " -0.5931402   0.62494195  1.6317694  -0.7048983  -0.36746448  0.7537168\n",
            " -0.5130639   0.3870811  -2.2828267  -0.11817734  0.1270541   3.3101969\n",
            " -0.5088271 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8128881  -0.43042916 -0.4563428   0.01656264  1.4342142   0.97242373\n",
            " -0.6339154   0.6759554   1.5209775  -0.6568329  -0.4799619   0.70726764\n",
            " -0.47420132  0.36489624 -2.3757439  -0.06395631  0.1172947   3.3194933\n",
            " -0.4542554 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8098211  -0.37509903 -0.45281538 -0.11129444  1.2944047   0.97322685\n",
            " -0.71431637  0.71704     1.5220225  -0.6552128  -0.49765238  0.74424297\n",
            " -0.41161904  0.5048494  -2.5070505  -0.15369849  0.07903378  3.8065195\n",
            " -0.42538923]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7788751  -0.3782169  -0.478984    0.01110135  1.2794251   0.9874197\n",
            " -0.61373705  0.72418094  1.5991672  -0.6929385  -0.45525187  0.6238596\n",
            " -0.49809507  0.47255465 -2.4239354  -0.05981587 -0.00389582  3.8731904\n",
            " -0.45339707]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.88348997 -0.43895027 -0.5517344  -0.05015966  1.2497594   1.0238556\n",
            " -0.680475    0.75896955  1.721349   -0.82991046 -0.49589548  0.7650578\n",
            " -0.5018747   0.4469564  -2.5911968  -0.12938082  0.09828597  4.2112756\n",
            " -0.5028195 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.78809196 -0.4080288  -0.4950311  -0.0440772   1.196273    0.99230844\n",
            " -0.62809134  0.75271547  1.6255771  -0.7455507  -0.49358892  0.70702183\n",
            " -0.44377843  0.47315124 -2.51185    -0.11980883  0.02761055  4.174294\n",
            " -0.49530944]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8541497  -0.45559615 -0.51572794 -0.06953635  1.2394195   0.9786433\n",
            " -0.6644161   0.7488927   1.7038153  -0.8237694  -0.51010436  0.63444114\n",
            " -0.5394986   0.45013085 -2.57322    -0.1372738   0.10535727  4.4793806\n",
            " -0.4554879 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.83823663 -0.4194342  -0.52594423 -0.0523798   1.3478713   1.0409297\n",
            " -0.6533708   0.6905759   1.6091976  -0.69339985 -0.46733063  0.6451827\n",
            " -0.47657162  0.4297454  -2.5127692  -0.10781749  0.10021412  4.3187337\n",
            " -0.53321546]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.91838026 -0.4039742  -0.52496934 -0.04881648  1.2461884   0.95268106\n",
            " -0.6635737   0.8503222   1.791379   -0.76252896 -0.4715952   0.67589283\n",
            " -0.5146537   0.40981653 -2.5453558  -0.1501498   0.1563799   4.676393\n",
            " -0.4363129 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.83084774 -0.41535088 -0.532117    0.01090258  1.2035867   0.9874327\n",
            " -0.6406452   0.7191893   1.6954188  -0.81671107 -0.43387657  0.60218173\n",
            " -0.44573075  0.45148587 -2.4857461  -0.17483495  0.07459832  4.6904964\n",
            " -0.530528  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8835236  -0.4256888  -0.5163584  -0.0450043   1.2640358   0.993367\n",
            " -0.682839    0.83833915  1.8519396  -0.7777078  -0.470792    0.6182213\n",
            " -0.50054383  0.43774712 -2.5513475  -0.1454267   0.06034946  5.088266\n",
            " -0.4929065 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-8.2865530e-01 -4.1746724e-01 -5.2598780e-01 -2.7185248e-03\n",
            "  1.2306212e+00  9.7920716e-01 -6.4968759e-01  7.3993200e-01\n",
            "  1.7119215e+00 -8.0472076e-01 -4.5199791e-01  5.2853805e-01\n",
            " -4.7843328e-01  4.7909030e-01 -2.5184140e+00 -1.5801241e-01\n",
            "  5.9837226e-02  4.9979787e+00 -4.8758119e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-7.9998946e-01 -3.8373560e-01 -4.7240379e-01 -1.1644257e-01\n",
            "  1.1904541e+00  1.0001363e+00 -7.7740008e-01  7.8179723e-01\n",
            "  1.6140587e+00 -7.1481079e-01 -5.5012995e-01  6.3284987e-01\n",
            " -4.2313656e-01  5.0177163e-01 -2.6131988e+00 -1.5516296e-01\n",
            " -4.4569238e-03  5.0319972e+00 -4.4277096e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8914221  -0.41744453 -0.58993936  0.00723332  1.2762835   1.0871602\n",
            " -0.6424779   0.7029017   1.6359361  -0.8116177  -0.48309115  0.63831407\n",
            " -0.45916674  0.36421    -2.4798923  -0.06231033  0.07103623  4.680117\n",
            " -0.49745584]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.87476575 -0.4206541  -0.52241844 -0.02882754  1.3255571   1.043558\n",
            " -0.7047577   0.82971174  1.7778174  -0.7986054  -0.48962677  0.5880452\n",
            " -0.47285014  0.47806373 -2.6038628  -0.09769148  0.03791056  5.327414\n",
            " -0.44870657]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-8.7737775e-01 -4.3521395e-01 -5.3969014e-01  1.0809242e-03\n",
            "  1.1395597e+00  9.8231930e-01 -6.6066945e-01  7.4571562e-01\n",
            "  1.6881278e+00 -8.1859046e-01 -4.3880141e-01  5.6465197e-01\n",
            " -4.6290502e-01  4.5990497e-01 -2.4977326e+00 -1.6855946e-01\n",
            "  1.0856470e-01  5.0173926e+00 -5.1681644e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8595903  -0.43403813 -0.5094752  -0.04492662  1.2226318   0.98347324\n",
            " -0.68541765  0.7989892   1.740728   -0.8063639  -0.50150645  0.55093074\n",
            " -0.48988768  0.4426255  -2.539668   -0.1298522   0.04021684  5.2585583\n",
            " -0.45874465]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.85581446 -0.44592866 -0.5274146   0.00874787  1.1407138   0.989493\n",
            " -0.6630333   0.7515185   1.7221739  -0.8497742  -0.45024508  0.55274874\n",
            " -0.4400504   0.4595809  -2.5360417  -0.18121603  0.092339    5.205843\n",
            " -0.5227947 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9383139  -0.37356678 -0.54713947 -0.10741317  1.305169    1.0707661\n",
            " -0.73590446  0.8499419   1.589096   -0.6719851  -0.60574853  0.7865407\n",
            " -0.4179797   0.42409387 -2.6622     -0.10635052  0.02966158  5.076704\n",
            " -0.51455057]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 45 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.86734724 -0.444757   -0.4537995  -0.04117627  1.1891873   1.0213828\n",
            " -0.70766354  0.78005594  1.7819945  -0.8123683  -0.46047783  0.47354412\n",
            " -0.5111794   0.428351   -2.529974   -0.19403799  0.07477055  5.1785197\n",
            " -0.48938656]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.88263637 -0.3936277  -0.53571165 -0.02341413  1.1873084   1.0746703\n",
            " -0.7569904   0.78889894  1.7216886  -0.6983826  -0.6438788   0.61599886\n",
            " -0.5764832   0.42454404 -2.7275927  -0.09930322  0.03095759  5.4294357\n",
            " -0.4566725 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.85222    -0.44677454 -0.5403528  -0.10067089  1.0992516   1.165536\n",
            " -0.787973    0.76661235  1.6730396  -0.9439083  -0.7033523   0.7518558\n",
            " -0.50268507  0.44698742 -2.778393   -0.13337043  0.15239066  4.7525377\n",
            " -0.4456475 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8747775  -0.40029678 -0.47024557 -0.14024425  1.169336    1.1018308\n",
            " -0.6564143   0.9024098   1.6410443  -0.71279967 -0.6515947   0.87292415\n",
            " -0.53177625  0.3946594  -2.644861   -0.12382877  0.1244313   4.7891135\n",
            " -0.50688004]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0077553e+00 -4.2234254e-01 -5.5095834e-01  1.3795738e-02\n",
            "  1.3746548e+00  1.0602304e+00 -7.2296536e-01  1.0627158e+00\n",
            "  1.6068143e+00 -9.2874444e-01 -6.1845773e-01  1.0143549e+00\n",
            " -4.4962066e-01  4.5206371e-01 -2.7505007e+00  1.5972324e-02\n",
            " -1.7805201e-03  4.5442095e+00 -5.2637130e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.96804607 -0.4470173  -0.5776524  -0.07059741  1.2160019   1.1394826\n",
            " -0.740188    1.1594208   1.6364042  -0.92493    -0.45278546  0.77496475\n",
            " -0.48180133  0.49968505 -2.815748   -0.08019187  0.0665233   4.694121\n",
            " -0.57849526]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 51 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9298976  -0.3759586  -0.45790598 -0.0179179   1.0505692   1.132416\n",
            " -0.6975497   1.0112147   1.5613565  -0.66986907 -0.35870695  1.0041642\n",
            " -0.45924836  0.33333027 -2.4528224  -0.11948154  0.1292215   4.309912\n",
            " -0.48199633]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 52 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.81903964 -0.3578965  -0.48932958 -0.01600909  0.948302    1.1161317\n",
            " -0.65113693  1.1883992   1.5072367  -0.6890493  -0.16907796  0.85852414\n",
            " -0.36462688  0.31475723 -2.4347258  -0.07722232  0.0689419   4.046395\n",
            " -0.5282046 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 53 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.99399894 -0.4051998  -0.62031466  0.0692735   1.291913    1.082231\n",
            " -0.7948426   1.3646467   1.6013128  -1.0113099  -0.24438763  1.041959\n",
            " -0.5054838   0.4702283  -2.77641     0.01384502  0.04340348  4.0290194\n",
            " -0.5488481 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 54 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7870738  -0.37025258 -0.44852564 -0.10284676  0.9008074   1.0268226\n",
            " -0.6574988   1.2286769   1.4128991  -0.7747988  -0.12068223  0.61096126\n",
            " -0.43152314  0.38825685 -2.4300942  -0.08395136  0.07311503  3.312324\n",
            " -0.47797891]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 55 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-8.06858838e-01 -3.90142828e-01 -3.70713681e-01  2.47588195e-03\n",
            "  1.04368865e+00  1.03036571e+00 -5.30678689e-01  1.26882184e+00\n",
            "  1.45454490e+00 -5.57221115e-01  9.12279636e-02  8.30715537e-01\n",
            " -4.92600292e-01  2.54701108e-01 -2.34830689e+00 -7.85414800e-02\n",
            "  1.14095435e-01  3.80833387e+00 -3.04125696e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 56 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.774849   -0.3018901  -0.46618536 -0.04620352  0.94084513  1.0125635\n",
            " -0.62622213  1.2717654   1.3103286  -0.7106585   0.12544975  0.79566044\n",
            " -0.32645163  0.26815525 -2.1908245  -0.05355272  0.05279936  2.9864388\n",
            " -0.31057504]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 57 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-8.3440888e-01 -4.1694355e-01 -4.3885028e-01  2.4619183e-02\n",
            "  1.1281976e+00  9.5544469e-01 -6.9851792e-01  1.2089189e+00\n",
            "  1.5142404e+00 -9.0936905e-01  6.0581408e-02  7.5884664e-01\n",
            " -5.3588748e-01  4.4435996e-01 -2.4821346e+00 -6.4944342e-02\n",
            "  1.0197896e-01  2.9807227e+00  6.5474957e-04]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 58 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.65650624 -0.38197562 -0.42009535 -0.02639188  1.038656    0.9390435\n",
            " -0.62622064  1.3413314   1.4663224  -0.66184056  0.208514    0.64413816\n",
            " -0.5033585   0.3322581  -2.2975922  -0.08836509  0.09609499  3.157598\n",
            "  0.01088212]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 59 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6683204  -0.30251765 -0.3462135  -0.13051155  0.88941264  0.77146703\n",
            " -0.47377452  1.0519811   1.0635515  -0.623049    0.13168116  0.64632976\n",
            " -0.27850148  0.25576124 -1.9024777  -0.07586643  0.0358661   1.8733325\n",
            "  0.10265769]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 60 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.70259327 -0.35472053 -0.37898928 -0.02614669  0.9294223   0.88348454\n",
            " -0.62299424  1.4630069   1.3623415  -0.7191934   0.3612809   0.5102764\n",
            " -0.5379494   0.44574243 -2.2762985  -0.10110897  0.08970313  2.6926053\n",
            "  0.2948817 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 61 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7371244  -0.33397147 -0.39479572  0.02163201  1.1591979   0.9252512\n",
            " -0.5239437   1.216362    1.2307215  -0.64643127  0.22919221  0.849875\n",
            " -0.25599235  0.29269361 -2.042433   -0.02674873 -0.03131682  1.9202003\n",
            "  0.14480045]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 62 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.72182244 -0.38419655 -0.35318276  0.00640449  0.9310583   0.78594476\n",
            " -0.5518277   1.6665971   1.5742393  -0.804011    0.3922566   0.5460977\n",
            " -0.44606045  0.49079335 -2.0950158  -0.10014043  0.07351578  2.4901195\n",
            "  0.29328075]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 63 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.52693844 -0.21069784 -0.2865538   0.05205475  0.7568721   0.65894693\n",
            " -0.33232322  1.0007871   1.0724524  -0.5182273   0.3589501   0.20376474\n",
            " -0.37210447  0.26810732 -1.5306679  -0.06332923  0.0668919   1.613715\n",
            "  0.28790855]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 64 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5949505  -0.26501355 -0.3437872   0.04841637  0.74240094  0.6773107\n",
            " -0.30204332  1.420503    1.3655882  -0.7299526   0.4442861   0.39707157\n",
            " -0.4183437   0.40867248 -1.8382357  -0.06757926  0.09802809  1.8945987\n",
            "  0.37631568]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 65 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5481004  -0.29876694 -0.27918342 -0.08218177  0.6292583   0.6734811\n",
            " -0.26633623  1.2709042   1.0496547  -0.6390567   0.33967796  0.17158426\n",
            " -0.3678245   0.362791   -1.6076483  -0.13954227  0.04770122  1.6238819\n",
            "  0.43024564]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 66 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5952223  -0.26695395 -0.35204473  0.04266572  0.7702681   0.67672193\n",
            " -0.15485874  1.2308772   1.2583387  -0.6804201   0.4342979   0.42885873\n",
            " -0.41380194  0.3024123  -1.7344056  -0.04748918  0.15487473  1.4709177\n",
            "  0.40905413]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 67 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.43118995 -0.22157082 -0.2533164  -0.0663069   0.4953879   0.53561956\n",
            " -0.14090924  0.9820471   0.8583124  -0.49949372  0.23581886 -0.00521418\n",
            " -0.31878778  0.2718305  -1.3545073  -0.09859478  0.04094946  1.0982684\n",
            "  0.359186  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 68 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.474161   -0.17673317 -0.31439552 -0.00997205  0.6356992   0.58882904\n",
            " -0.18864675  1.1070839   1.0424213  -0.59343594  0.374348    0.3336003\n",
            " -0.3120558   0.4100615  -1.6750219  -0.11297616  0.0266567   1.0803599\n",
            "  0.49249315]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 69 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.45067036 -0.24143757 -0.27255514 -0.02178045  0.48900256  0.5438969\n",
            " -0.05546528  1.011658    0.91256666 -0.579114    0.30391338  0.07092337\n",
            " -0.23666047  0.29824805 -1.359529   -0.14209701  0.09551237  1.0010802\n",
            "  0.38224146]\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 70 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4636264  -0.2062396  -0.28399208  0.07362583  0.65193886  0.54530436\n",
            " -0.03494691  0.83822095  0.92881984 -0.5335482   0.18690053  0.49057865\n",
            " -0.30002728  0.2754081  -1.5301695  -0.03671358  0.11060026  0.37258753\n",
            "  0.47475058]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 71 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.462753   -0.24909864 -0.25505438  0.03218451  0.4360218   0.58167034\n",
            " -0.02703689  0.8145094   0.80541146 -0.47690764  0.12469965  0.37638763\n",
            " -0.14652646  0.1856755  -1.4248179  -0.0764953   0.0878949   0.40571994\n",
            "  0.38942665]\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 72 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.41450608 -0.14071842 -0.2719175  -0.02389523  0.57944983  0.5667733\n",
            "  0.03606065  0.779375    0.73903835 -0.44415718  0.32029766  0.3279796\n",
            " -0.04043166  0.21375291 -1.24735    -0.00501172  0.06922599  0.36667797\n",
            "  0.3315316 ]\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 73 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.35961628 -0.1212645  -0.16636391 -0.09607723  0.40312204  0.48584402\n",
            "  0.08251214  0.5262195   0.596858   -0.34803683  0.25910336  0.1769804\n",
            " -0.04623933  0.12068486 -1.0479283  -0.03093154  0.01415596  0.38671428\n",
            "  0.28537005]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 74 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4231338  -0.2096161  -0.23714483  0.05573257  0.49779898  0.5779531\n",
            "  0.02775258  0.61216813  0.7754071  -0.38733903  0.2597444   0.25579265\n",
            " -0.11954834  0.1382875  -1.291812   -0.09717236  0.07694156  0.390437\n",
            "  0.43159428]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 75 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40011585 -0.2519617  -0.30959862  0.09407008  0.75723356  0.553057\n",
            "  0.14393042  0.5628301   0.7773371  -0.44869265  0.26201677  0.43679997\n",
            " -0.12971075  0.24634199 -1.3760729  -0.03920908  0.04716057  0.19455826\n",
            "  0.5384096 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 76 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3534009  -0.16945487 -0.24123508 -0.06758793  0.51361334  0.47564653\n",
            "  0.14435504  0.6157101   0.5823152  -0.40184876  0.30106103  0.29005334\n",
            " -0.03794913  0.17358352 -1.1327063   0.02572002  0.07412732  0.13113038\n",
            "  0.42625806]\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 77 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2877998  -0.2015695  -0.16205883 -0.02687649  0.2704274   0.41905248\n",
            "  0.01213975  0.3877214   0.42976293 -0.31911227  0.09928475  0.09066119\n",
            " -0.02278767  0.09249937 -0.978206   -0.0784306   0.06562892  0.05647934\n",
            "  0.361267  ]\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 78 reward=-2 new_state=[0 0 1 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31616375 -0.17857133 -0.24441202 -0.02963635  0.377734    0.5673414\n",
            "  0.06176041  0.45464647  0.7112077  -0.51011884  0.30559787  0.2427742\n",
            "  0.01395169  0.23238258 -1.2822016  -0.0343826   0.07854033  0.20110483\n",
            "  0.5589618 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 79 reward=-2 new_state=[0 0 1 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.35404    -0.14548217 -0.23898132  0.04067374  0.46857902  0.39902508\n",
            "  0.18535662  0.42254466  0.48808143 -0.3888506   0.23746477  0.364593\n",
            "  0.03471624  0.18672718 -1.0770637   0.0210454   0.03741175  0.07805061\n",
            "  0.42047465]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 80 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.25561282 -0.1106123  -0.1958236  -0.0428851   0.19153805  0.38909042\n",
            " -0.01528265  0.2252055   0.37827557 -0.45400196  0.23068918  0.05219463\n",
            "  0.10756874  0.17568856 -1.0640303  -0.05416663  0.09469452 -0.00227566\n",
            "  0.45929784]\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 81 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31062353 -0.20078394 -0.1423654  -0.04538872  0.30712372  0.3355187\n",
            "  0.13996942  0.27420312  0.48093376 -0.42322615  0.21998641  0.41663682\n",
            " -0.00708425  0.15247299 -1.1523097  -0.04307055  0.18117052  0.10540778\n",
            "  0.4418246 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 82 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.41315556 -0.18748727 -0.2526089   0.03751693  0.37549105  0.34043947\n",
            "  0.2751869   0.23708484  0.54584134 -0.39062932  0.318268    0.33312124\n",
            "  0.13610505  0.14232218 -1.2774427  -0.00959344  0.04222352  0.24161713\n",
            "  0.42951655]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 83 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3738854  -0.1892412  -0.21622795 -0.02897023  0.25022247  0.3741069\n",
            "  0.12024228  0.2223553   0.42042822 -0.48888537  0.2932097   0.31453156\n",
            " -0.00576592  0.15531158 -1.2441767  -0.03102527  0.11175229  0.10316744\n",
            "  0.39807817]\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 84 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.25076836 -0.22834845 -0.11228612 -0.02959757  0.04855281  0.31227446\n",
            "  0.0633911   0.46086273  0.28758365 -0.3897531   0.2415332   0.08499085\n",
            "  0.17298403  0.21036682 -1.1749395  -0.11773209  0.05621846  0.20788388\n",
            "  0.41067722]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 85 reward=0 new_state=[0 0 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.29677895 -0.11952759 -0.15440865 -0.10499217  0.20161104  0.28128764\n",
            "  0.11330654  0.05665787  0.22048691 -0.35137105  0.22535795  0.35011342\n",
            "  0.08613946  0.10829991 -0.91461307 -0.05831884  0.07431377 -0.05378878\n",
            "  0.24710366]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 86 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.32008764 -0.21352996 -0.12906651  0.01962453  0.38518816  0.24618217\n",
            "  0.25671571  0.20295613  0.3537943  -0.3087103   0.218811    0.5088876\n",
            " -0.00669674  0.16717774 -1.0945507   0.01302404  0.09891286  0.19459753\n",
            "  0.41301855]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 87 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18609908 -0.10220566 -0.10586703 -0.06518614  0.23554513  0.1156598\n",
            "  0.13576402  0.0992055   0.11561747 -0.16367514  0.14342204  0.30515784\n",
            "  0.07785118  0.04722198 -0.5993812   0.00258316  0.04219442  0.11977724\n",
            "  0.13665022]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 88 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1775447  -0.1562763  -0.0726595  -0.07087187  0.18272403  0.123704\n",
            "  0.12048249  0.25192654  0.2238726  -0.24120724  0.14931093  0.28808632\n",
            "  0.13970378  0.12263274 -0.7531718  -0.03449027  0.06121144  0.30239114\n",
            "  0.1908876 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 89 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.13208872 -0.06672458 -0.11469764 -0.06507525  0.06338912  0.12339601\n",
            "  0.05555859  0.21306053  0.15470693 -0.16709992  0.20431694  0.2987964\n",
            "  0.11750201  0.09203421 -0.69200873 -0.07579426  0.10207649  0.23893599\n",
            "  0.14944947]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 90 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.276106   -0.1947476  -0.09169219  0.01361926  0.31302935  0.09412989\n",
            "  0.21027489  0.10463046  0.20256434 -0.18755694  0.13754293  0.45708868\n",
            "  0.01327443  0.0874162  -0.7678169  -0.01858075  0.06067324  0.07369543\n",
            "  0.15263581]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 91 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.21175525 -0.13085201 -0.09113953 -0.05397511  0.1440993   0.20179228\n",
            "  0.13916065  0.12266069  0.13925205 -0.17755738  0.22276662  0.48708725\n",
            "  0.12742181  0.10741142 -0.80032593 -0.07680853  0.15480895  0.20943181\n",
            " -0.0172775 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 92 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.23318003 -0.07518271 -0.15145469 -0.04419043  0.12340302  0.20229791\n",
            "  0.10792     0.13973406  0.09360317 -0.183452    0.22039932  0.47182205\n",
            "  0.1641202   0.11911663 -0.6640449  -0.03128504  0.11194954  0.07075235\n",
            "  0.05006628]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 93 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.20325696 -0.12289428 -0.10427764 -0.02720486  0.22980726  0.09112985\n",
            "  0.13981777  0.12768252  0.12692381 -0.18605451  0.18717283  0.41390863\n",
            "  0.07908259  0.11388435 -0.74398553 -0.08369163  0.15373363  0.20293637\n",
            "  0.05433925]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 94 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.22984858 -0.14834744 -0.15767492  0.03804913  0.33530605  0.15845443\n",
            "  0.25670514  0.2106831   0.19596438 -0.24853262  0.18488751  0.52407366\n",
            "  0.07556235  0.1385275  -0.8515825  -0.00276451  0.06023319  0.2597244\n",
            "  0.19504681]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 95 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.12608506 -0.13108225 -0.08981159 -0.08214121  0.25213927  0.03662974\n",
            "  0.15365753  0.10871293  0.10849097 -0.15717064  0.09498567  0.46065852\n",
            "  0.0727274   0.0745263  -0.6213088  -0.01186532  0.03480715  0.0416897\n",
            "  0.01708584]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 96 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.06456795 -0.17831753 -0.13498117 -0.03377911  0.01648096  0.05507375\n",
            "  0.09723295  0.20969017  0.06491059 -0.23372667  0.1755193   0.39047086\n",
            "  0.15815537  0.23418438 -0.8990187  -0.05566072  0.10124727  0.15848722\n",
            "  0.08222119]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 97 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10823864 -0.08542676 -0.17104737 -0.05144127  0.10176077  0.06949753\n",
            "  0.11667728  0.06470272  0.02175673 -0.22712614  0.16299324  0.3572807\n",
            "  0.08590188  0.20805688 -0.7855578  -0.00328399  0.05433132  0.03184739\n",
            "  0.04457455]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 98 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08101477 -0.18697484 -0.11264384 -0.02571532  0.1617629   0.15021999\n",
            "  0.18731841  0.01489314  0.04065922 -0.37847033  0.20449066  0.719081\n",
            "  0.0538376   0.22100034 -1.0697753  -0.03678427  0.12979382  0.13663816\n",
            "  0.11499908]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 99 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02565296 -0.13286394 -0.12790631 -0.05438988  0.21054342  0.06565738\n",
            "  0.17504084  0.21639529  0.11981525 -0.2207054   0.14534177  0.60295457\n",
            "  0.09717187  0.12524064 -0.75386465 -0.00699662  0.07294687  0.2166925\n",
            "  0.11253732]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 100 reward=1 new_state=[0 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.05009559 -0.12199829 -0.13965082 -0.10443939  0.11584135 -0.04280203\n",
            "  0.13372602  0.11285561  0.02128782 -0.20510726  0.1280806   0.4561658\n",
            "  0.07681188  0.18060465 -0.7581031  -0.03635321  0.07112131  0.02061852\n",
            "  0.08228467]\n",
            "Epsilon reduced to 0.08100000000000002\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1329922  -0.11624348 -0.13172953 -0.0432645   0.25868428  0.13249525\n",
            "  0.19284226  0.26619348  0.17271419 -0.2874891   0.28902212  0.39574972\n",
            "  0.23652811  0.26703948 -1.0188426  -0.01604393 -0.05477941  0.4732761\n",
            "  0.04549495]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 2 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.12645319 -0.09695012 -0.14794013 -0.03820196  0.39588052  0.14120807\n",
            "  0.206591    0.19615665  0.14162938 -0.33127367  0.25703233  0.37952328\n",
            "  0.21929756  0.3263079  -1.0013628  -0.00293973 -0.0013275   0.33626842\n",
            "  0.09988295]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 3 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.15364078 -0.17423813 -0.19865623 -0.01925985  0.31229082  0.08653449\n",
            "  0.17122464  0.52392215  0.25102958 -0.37897334  0.3399461   0.35729015\n",
            "  0.21971251  0.36844847 -1.0411551  -0.04755283  0.0143108   0.56405497\n",
            "  0.11914124]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 4 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 3.8106080e-02 -1.1167857e-01 -1.4529462e-01 -9.1500692e-02\n",
            "  1.1478267e-01  4.8180930e-03  4.7809631e-04  2.1210515e-01\n",
            "  8.9692548e-03 -2.3058486e-01  1.9392654e-01  2.4810386e-01\n",
            "  2.6917076e-01  3.9766768e-01 -7.6020902e-01 -5.2166265e-02\n",
            "  6.8774661e-03  2.7724329e-01  5.1246233e-02]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 5 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.09164124 -0.1423407  -0.2098371  -0.07605698  0.17262061  0.027624\n",
            "  0.07666026  0.34309286  0.16665046 -0.31753987  0.32477933  0.22671396\n",
            "  0.1375676   0.44911388 -0.83405334 -0.07126515  0.05028555  0.48620206\n",
            "  0.0482991 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 6 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.2235246e-02 -9.2565253e-02 -1.5097798e-01 -1.0757730e-01\n",
            "  2.2666562e-01 -8.6997403e-05  2.9526211e-02  1.6331831e-01\n",
            "  5.3668402e-02 -1.4261998e-01  1.3824405e-01  2.7014044e-01\n",
            "  1.4830808e-01  2.5790966e-01 -6.0492492e-01  5.8870301e-02\n",
            " -2.5643436e-03  1.5029059e-01  5.2766845e-02]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 7 reward=1 new_state=[0 0 0 0 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.12517828 -0.14893492 -0.17630745 -0.04718462  0.24117    -0.03263501\n",
            "  0.10040795  0.23850064  0.13157678 -0.26209626  0.21351604  0.23470798\n",
            "  0.03242361  0.30104655 -0.661048    0.06498408  0.03686068  0.31175894\n",
            "  0.02596675]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 8 reward=1 new_state=[0 0 0 0 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.06910988 -0.09518091 -0.10788801 -0.02277613  0.14713943 -0.10590556\n",
            "  0.02134761  0.08748808  0.05008471 -0.13688867  0.04467686  0.20808038\n",
            " -0.00105583  0.22552624 -0.52830094  0.12149905  0.04180394  0.08312199\n",
            "  0.06485685]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 0 0 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.09975985 -0.08130177 -0.13474007 -0.00478996  0.05708684 -0.07727991\n",
            "  0.07787129  0.03392692  0.01829627 -0.18818593  0.06530076  0.36226442\n",
            "  0.0481033   0.29263398 -0.6532174   0.16960162  0.04300011  0.08579072\n",
            "  0.06961849]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 10 reward=1 new_state=[0 0 0 0 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.18112552 -0.07796471 -0.1676596   0.00736524  0.1254089   0.05642598\n",
            "  0.14360483  0.07355664 -0.02359907 -0.24949126  0.15427391  0.6927436\n",
            "  0.11551803  0.3324544  -0.8413209   0.1711711   0.03699067  0.10023917\n",
            "  0.14133519]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.15503098 -0.09035976 -0.19172986 -0.00463702  0.20932373  0.04259325\n",
            "  0.10632229  0.14567877  0.00894526 -0.21384726  0.22220534  0.38228422\n",
            "  0.08044643  0.5420787  -0.8560701   0.31537232  0.04912592  0.226391\n",
            "  0.03631944]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.10355701 -0.1038759  -0.11245936 -0.04182789  0.23585887 -0.0478206\n",
            "  0.08003743  0.14328292  0.09046295 -0.04871188  0.08745337  0.35998487\n",
            "  0.04196066  0.29691502 -0.5643208   0.28630733  0.03975212  0.08857816\n",
            "  0.04323769]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.11671129 -0.10988136 -0.1266643  -0.0578426   0.22478478 -0.04231074\n",
            "  0.07897149  0.1298896   0.05739911 -0.10868493  0.13114281  0.43359524\n",
            "  0.06750948  0.45919612 -0.72434974  0.41609117  0.01048926  0.08587787\n",
            "  0.0974556 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.18545733 -0.08736128 -0.19275899 -0.01602596  0.09955445 -0.03297042\n",
            "  0.10793329  0.18952215 -0.06754127 -0.1623485   0.21379882  0.5649141\n",
            "  0.15010104  0.6317258  -0.9210787   0.5119105   0.00380589  0.05084326\n",
            "  0.14930688]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.14190039 -0.07800438 -0.06844887 -0.05157575  0.2985738   0.07172114\n",
            "  0.15160048  0.14104393  0.07526676 -0.08160384  0.14052984  0.6129027\n",
            "  0.08378864  0.2604077  -0.63101244  0.38160408  0.03290464  0.12983082\n",
            "  0.0729786 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.1254342  -0.11581656 -0.14623944 -0.03899199  0.29700908  0.03825776\n",
            "  0.05750361  0.2607036   0.09435735 -0.12323528  0.21967314  0.41607726\n",
            "  0.06018432  0.6533285  -0.8744535   0.67380595  0.01783007  0.26697648\n",
            "  0.07531457]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 17 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.05949049 -0.0761609  -0.13276908 -0.05323405  0.18166922  0.01379629\n",
            "  0.08597495  0.09527773  0.01735705 -0.09195593  0.1642291   0.703799\n",
            "  0.13520628  0.5188677  -0.6858833   0.6082074  -0.02506739  0.15050608\n",
            "  0.08243235]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 18 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10246513 -0.11076831 -0.08378977 -0.03431563  0.12190313 -0.03650041\n",
            "  0.13907973  0.10588753 -0.09580481 -0.13008538  0.14511332  0.82917994\n",
            "  0.17434762  0.39349154 -0.7028727   0.61845356  0.01502023  0.01879646\n",
            "  0.0825022 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 19 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.11190103 -0.1140584   0.0254452  -0.03225213  0.33628276  0.05343374\n",
            "  0.18557918  0.15719345  0.07584051 -0.06081345  0.15227234  0.6617706\n",
            "  0.09817272  0.32022887 -0.63043106  0.5996899   0.04269968  0.14920218\n",
            "  0.04279222]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 20 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.08837906 -0.10596639  0.01751626  0.0070191   0.39574823  0.13403729\n",
            "  0.17641726  0.1973887   0.16509381 -0.04414877  0.21423914  0.7237061\n",
            "  0.09297842  0.5028123  -0.69539243  0.6492743   0.02664781  0.32927257\n",
            " -0.02391781]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 21 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.04654957 -0.07757898 -0.06694912 -0.03213203  0.21303254  0.0453895\n",
            "  0.15324007  0.05728994  0.0129265  -0.05408015  0.14112684  0.8399114\n",
            "  0.13047726  0.5087254  -0.611549    0.70635897  0.01541325  0.16664894\n",
            " -0.04412485]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 22 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.09724094 -0.11971113 -0.05562923 -0.04233937  0.10070219 -0.01832111\n",
            "  0.15203886  0.11354692 -0.12383664 -0.12721376  0.16778809  1.0712652\n",
            "  0.16548023  0.55012715 -0.7833595   0.901115    0.03905826  0.08060499\n",
            "  0.11559505]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 23 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.1410048  -0.12081412  0.14875892 -0.02748768  0.38937926  0.07347815\n",
            "  0.14908063  0.18358256  0.08046662 -0.06798321  0.21196713  0.9076544\n",
            "  0.11308759  0.46556064 -0.7421932   0.81221896  0.03253259  0.25835848\n",
            "  0.08488899]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02792311 -0.16297747  0.05342142 -0.04678207  0.25300282  0.03232425\n",
            "  0.11578634  0.14636125  0.05866668 -0.16160508  0.1655164   1.0109593\n",
            "  0.12044791  0.6209932  -0.81843346  0.78352743  0.1084123   0.29260036\n",
            " -0.08475634]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.06474742 -0.0911537   0.07848709 -0.00991861  0.11925978 -0.02775821\n",
            "  0.10044888  0.08918441 -0.02744469 -0.06222956  0.14196208  0.6552938\n",
            "  0.09335186  0.3190454  -0.48822704  0.5010176   0.0423321   0.15542851\n",
            " -0.02519084]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.09162716 -0.10519248  0.09135506 -0.00422076  0.3138304  -0.02536756\n",
            "  0.13846098  0.1339966  -0.03787845 -0.01644482  0.17678308  1.0888953\n",
            "  0.17614771  0.42784342 -0.6102544   0.87525266  0.03615731  0.06169839\n",
            "  0.02528145]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.04933766 -0.11856704  0.09869154 -0.0240094   0.36116722  0.08311533\n",
            "  0.14464988  0.1915703   0.0646826  -0.08066143  0.21319574  1.1152279\n",
            "  0.04477895  0.54838794 -0.71023446  0.91821975  0.05441454  0.29603\n",
            "  0.00269958]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 5.82560860e-02 -1.83224663e-01  1.11150779e-01 -4.86355238e-02\n",
            "  2.94362545e-01  4.65358868e-02  9.79537964e-02  1.72859773e-01\n",
            " -1.01129338e-03 -1.49666101e-01  2.16134772e-01  1.16295588e+00\n",
            "  1.13742985e-01  6.83919966e-01 -7.60660827e-01  9.04434264e-01\n",
            "  1.04894109e-01  4.24182773e-01 -8.02190229e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.01211642 -0.13256969  0.13349241 -0.00860323  0.2690987  -0.02052297\n",
            "  0.12928095  0.13722537  0.02685175 -0.05019825  0.15850583  1.0884795\n",
            "  0.13666442  0.5427869  -0.6404249   0.91718894  0.05927806  0.20603591\n",
            " -0.04628849]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.01682456 -0.12614705  0.07392231 -0.02935952  0.18603061 -0.04848623\n",
            "  0.13715222  0.16701543 -0.12236068 -0.11680231  0.21230887  1.5018805\n",
            "  0.14458252  0.66207874 -0.7630703   1.0224425   0.07244442  0.16713366\n",
            " -0.07736459]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02110575 -0.14265962  0.11006494 -0.06999823  0.23112983  0.08544197\n",
            "  0.20535211  0.18366304 -0.09265852 -0.13793403  0.38807628  1.4534197\n",
            "  0.10331006  0.92846    -0.991336    1.2591081   0.06344545  0.5635772\n",
            " -0.10969747]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11980516 -0.18971479  0.16712855 -0.04544936  0.32348406  0.03854302\n",
            "  0.18676922  0.2765498   0.1072707  -0.06390435  0.26628545  1.2487004\n",
            "  0.10621706  0.64364105 -0.7236925   0.99280536  0.04346311  0.44476736\n",
            " -0.01342382]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.06753538 -0.16331851  0.21248728 -0.06136619  0.23097254 -0.02501868\n",
            "  0.09691088  0.22947864 -0.00286294 -0.08748661  0.21740837  1.341114\n",
            "  0.1318016   0.67097276 -0.74282795  0.9845352   0.0857425   0.4365605\n",
            " -0.06937168]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-6.1591834e-02 -1.4405209e-01  3.2756016e-01 -8.3584664e-03\n",
            "  2.3217005e-01 -6.8206944e-02  1.1987005e-01  1.4858313e-01\n",
            "  7.6426193e-05 -8.7847404e-02  1.7686386e-01  1.6114545e+00\n",
            "  1.7652032e-01  6.3695693e-01 -6.8299693e-01  1.1535079e+00\n",
            "  7.8888722e-02  9.1706403e-02 -7.4402504e-03]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02603907 -0.14877953  0.4375686  -0.07105168  0.22977822  0.06487485\n",
            "  0.24075842  0.19854899 -0.05161204 -0.13316058  0.3464146   1.6676916\n",
            "  0.1477873   0.94053006 -0.84350324  1.3822582   0.07304931  0.47349253\n",
            " -0.08975237]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.01713648 -0.17708988  0.34134826 -0.09116612  0.27138016  0.05758209\n",
            "  0.14098476  0.19276486 -0.03556985 -0.14814062  0.33165222  1.7630811\n",
            "  0.09944297  0.8999003  -0.89468586  1.2314521   0.07233182  0.5661226\n",
            " -0.10787093]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02871401 -0.13698736  0.24520624 -0.01589881  0.33180976 -0.01389966\n",
            "  0.135793    0.23838419  0.08008368  0.01697077  0.15566018  1.3470069\n",
            "  0.12811957  0.5165764  -0.5413263   0.90274256  0.03858382  0.26727882\n",
            " -0.03351928]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02336788 -0.17986922  0.4453006  -0.03208411  0.1410756  -0.04374835\n",
            "  0.16846207  0.137873   -0.12289685 -0.15316425  0.1751755   2.0550416\n",
            "  0.22461204  0.7723097  -0.7762784   1.2486874   0.13048536  0.13291898\n",
            " -0.06889861]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.04226298 -0.22621174  0.4636105  -0.06741103  0.24295071  0.04228232\n",
            "  0.28396007  0.13749099 -0.03298255 -0.15512627  0.3087879   1.9033031\n",
            "  0.20834254  0.961618   -0.8957233   1.4006045   0.07630651  0.47896263\n",
            " -0.11642014]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.03912445 -0.20454162  0.5053287  -0.05516558  0.30978918  0.05650289\n",
            "  0.20053178  0.21097709  0.06544654 -0.09915822  0.32128292  1.7663172\n",
            "  0.13553657  0.8733681  -0.7802147   1.2678382   0.06711626  0.63803166\n",
            " -0.11065852]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.05435277 -0.1454557   0.46903962 -0.05278441  0.22568652 -0.02771892\n",
            "  0.13629335  0.20863695  0.04152953 -0.09337023  0.20938721  1.8625212\n",
            "  0.13814703  0.7296626  -0.79851097  1.1206754   0.09703479  0.49690247\n",
            " -0.10504413]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2507507e-01 -1.5086746e-01  4.9406609e-01 -4.4989954e-03\n",
            "  3.2509720e-01 -3.8037188e-02  1.6560560e-01  1.8081482e-01\n",
            " -3.0336671e-02 -1.3842501e-03  1.8198098e-01  1.8093013e+00\n",
            "  1.9236106e-01  4.8570102e-01 -5.3148711e-01  1.1904033e+00\n",
            "  9.0668485e-02  1.2818843e-01  6.2258095e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.00787649 -0.18353382  0.65684086 -0.05596815  0.18886416  0.01528653\n",
            "  0.24248771  0.13309093 -0.10464104 -0.17838453  0.3209846   2.2652311\n",
            "  0.19358078  0.9511819  -0.83767885  1.4485059   0.10362449  0.5569455\n",
            " -0.10783687]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 44 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02024462 -0.22146255  0.4684491  -0.06338829  0.32953018  0.041856\n",
            "  0.15477131  0.21486117  0.14281155 -0.05147387  0.28740254  1.6567396\n",
            "  0.1559636   0.86813235 -0.671537    1.1998501   0.07386881  0.51271415\n",
            " -0.03070522]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 45 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.15733872 -0.14872499  0.3833565  -0.08652487  0.21935615 -0.01269592\n",
            "  0.07529532  0.28859752  0.06348762 -0.06314822  0.24211723  1.6728894\n",
            "  0.31152394  1.0729511  -0.73127866  1.3290538   0.01947386  0.50376356\n",
            "  0.05920882]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 46 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.11813977 -0.1523052   0.4029454  -0.10313367  0.22788104 -0.02328312\n",
            "  0.07501733  0.43363884  0.13618758 -0.06568474  0.2331152   1.6812496\n",
            "  0.3384242   1.1444639  -0.8055465   1.3079522   0.00444623  0.54038006\n",
            "  0.12859423]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 47 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.10789239 -0.14007922  0.4006128  -0.0929947   0.21364534 -0.01821156\n",
            "  0.06713257  0.26408967  0.08852021 -0.04270864  0.24283151  1.7669914\n",
            "  0.274445    1.0415342  -0.7218538   1.3512728   0.0361946   0.49716103\n",
            "  0.04586937]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 48 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.06464764 -0.15428105  0.37951568 -0.07763477  0.35718036  0.12462797\n",
            "  0.26599932  0.2893466   0.13171189 -0.03482315  0.25608146  2.3059294\n",
            "  0.20406634  0.8633902  -0.89842737  1.2323297   0.04142088  0.49308774\n",
            "  0.05322699]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 49 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.09057558 -0.09437324  0.3969746  -0.01324418  0.23365703 -0.00610855\n",
            "  0.1321647   0.1578525   0.12825643 -0.00243899  0.20163034  1.525725\n",
            "  0.11943378  0.5331036  -0.47258383  0.9009617   0.00974189  0.37716174\n",
            "  0.04186645]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 50 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.00432932e-01 -1.10218614e-01  4.52306420e-01 -7.36825988e-02\n",
            "  2.36177847e-01  8.34220555e-04  1.39003143e-01  3.51650894e-01\n",
            "  1.61299929e-01 -1.04370732e-02  2.67187536e-01  1.98379421e+00\n",
            "  2.69173622e-01  1.09185600e+00 -7.93723881e-01  1.31791639e+00\n",
            "  8.92623365e-02  5.38191020e-01  8.32164660e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 51 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.14178064 -0.15275837  0.534192   -0.06348854  0.28892866  0.01875642\n",
            "  0.09914377  0.32139328  0.18957776 -0.03629275  0.29036093  2.0610769\n",
            "  0.3389889   1.1647847  -0.7399007   1.4662269   0.0457243   0.6010378\n",
            "  0.01195632]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 52 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1253068  -0.12954432  0.48866543 -0.06755966  0.21512176 -0.00556525\n",
            "  0.09476935  0.46863416  0.2366297  -0.06061924  0.22684123  1.9806783\n",
            "  0.33339503  1.1771286  -0.77041805  1.2922258   0.02266518  0.57141834\n",
            "  0.11711983]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 53 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.15723608 -0.16773877  0.49788553 -0.0902817   0.24871293 -0.01214374\n",
            "  0.09074003  0.33194944  0.20999825 -0.06310019  0.27382183  2.4094927\n",
            "  0.3544515   1.1919527  -0.8087967   1.512744    0.02394353  0.59090805\n",
            "  0.06721021]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 54 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.11479148 -0.1705644   0.532267   -0.09026612  0.27808964 -0.00313716\n",
            "  0.09622201  0.47958416  0.28581685 -0.05621677  0.2620832   2.405846\n",
            "  0.40144157  1.2825475  -0.84891176  1.4436897   0.02065899  0.58483607\n",
            "  0.11546624]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 55 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.09006668 -0.16453795  0.55303633 -0.04367629  0.3347207   0.03609862\n",
            "  0.18201432  0.2306047   0.24254853 -0.03383449  0.28363854  2.6273816\n",
            "  0.1682509   0.81254137 -0.7507141   1.3475835   0.07550835  0.5545191\n",
            " -0.00684324]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 56 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.11022718 -0.18029965  0.56024224 -0.09128026  0.29560244  0.00298437\n",
            "  0.10594334  0.51048994  0.32286465 -0.060415    0.27879348  2.7251008\n",
            "  0.42100328  1.3333136  -0.8925171   1.5083846   0.02241147  0.6152321\n",
            "  0.12320133]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 57 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.9904668e-02 -1.9927189e-01  5.6535810e-01 -6.7651160e-02\n",
            "  3.3597252e-01 -2.7625007e-03  1.7928715e-01  2.7412310e-01\n",
            "  2.9257202e-01 -6.4526819e-02  2.7760458e-01  2.9709306e+00\n",
            "  1.8472876e-01  7.8590757e-01 -8.4087604e-01  1.4001197e+00\n",
            "  7.8981765e-02  5.7000679e-01  5.7529349e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 58 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.12036905 -0.18113963  0.5718624  -0.08890951  0.33430743  0.04281718\n",
            "  0.13195379  0.5385143   0.33544225 -0.06531249  0.318299    3.239172\n",
            "  0.4632221   1.472944   -0.9785041   1.6457685   0.00851021  0.6783565\n",
            "  0.12129409]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 59 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 3.3216666e-02 -2.0892477e-01  5.3875595e-01 -4.9865600e-02\n",
            "  3.5962984e-01  4.6881273e-02  2.1445142e-01  2.6824796e-01\n",
            "  3.5045922e-01 -5.5363994e-02  2.5125068e-01  3.2408526e+00\n",
            "  2.7348700e-01  9.6250397e-01 -8.7470931e-01  1.5090213e+00\n",
            "  5.7496533e-02  4.6273282e-01  2.7522212e-03]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 60 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.04371316 -0.19740744  0.5622534  -0.12712237  0.45589763  0.11801623\n",
            "  0.3017881   0.48479706  0.33162597 -0.01590867  0.36855766  3.9285104\n",
            "  0.2046436   1.0329643  -1.1304038   1.6275326   0.02537814  0.75734425\n",
            "  0.13829379]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 61 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.04822738 -0.19341254  0.62392545 -0.10591243  0.29720265  0.00908051\n",
            "  0.15543936  0.42863026  0.45234874 -0.07973005  0.2821503   3.7768524\n",
            "  0.27831867  1.2752575  -1.028379    1.7346876   0.07897605  0.6250346\n",
            "  0.00686585]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 62 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.08240313 -0.226104    0.69103277 -0.0894416   0.42923257  0.06066523\n",
            "  0.16825406  0.66452456  0.44488844 -0.08176803  0.3794556   4.013586\n",
            "  0.51331025  1.5558556  -1.1177835   1.8140074   0.04245686  0.7975352\n",
            "  0.15263337]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 63 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.0554508  -0.21342935  0.6738136  -0.04843413  0.43957686  0.08289023\n",
            "  0.25040817  0.38708416  0.4026702  -0.05427954  0.37516734  4.090837\n",
            "  0.23191312  1.0279515  -0.997936    1.6601436   0.08686022  0.7142663\n",
            "  0.0305675 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 64 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.07207096 -0.23729174  0.6745267  -0.09519765  0.4025694   0.05877436\n",
            "  0.17733106  0.7177559   0.48902974 -0.0938311   0.38778296  4.4234185\n",
            "  0.51588273  1.6130395  -1.1698711   1.8437046   0.03537058  0.78848356\n",
            "  0.18207459]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 65 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.00881246 -0.24721132  0.71004766 -0.05104347  0.46115485  0.06411022\n",
            "  0.25442192  0.42842135  0.48578516 -0.06742921  0.35654265  4.3397403\n",
            "  0.2511006   1.0180085  -1.0339046   1.6625619   0.07883427  0.63568217\n",
            "  0.05293251]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 66 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.05814937 -0.25451726  0.70219314 -0.09562305  0.43247864  0.07837551\n",
            "  0.20210783  0.7856264   0.53130573 -0.10611895  0.42191875  4.948481\n",
            "  0.54173976  1.6962739  -1.2554829   1.9406178   0.0398954   0.83938724\n",
            "  0.20096022]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 67 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02763705 -0.23484558  0.7511353  -0.10288098  0.42759442  0.06237472\n",
            "  0.19977307  0.6096339   0.54875946 -0.07747499  0.36726302  4.8797765\n",
            "  0.33493215  1.4318191  -1.1452211   1.9128485   0.09387317  0.7044834\n",
            "  0.0472791 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 68 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.04032161 -0.2591724   0.6883235  -0.07278609  0.4507356   0.16340221\n",
            "  0.2807467   0.8216651   0.57633984 -0.13798285  0.47440165  5.8792477\n",
            "  0.618511    1.944418   -1.4559455   2.1667345   0.03685085  0.9269843\n",
            "  0.16718367]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 69 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 2.70288088e-03 -2.64143586e-01  7.31566548e-01 -8.32889900e-02\n",
            "  4.49621826e-01  1.22547187e-01  2.67098039e-01  6.42365396e-01\n",
            "  6.14661217e-01 -1.27719030e-01  3.87076378e-01  5.66455507e+00\n",
            "  4.29883868e-01  1.59820962e+00 -1.33815539e+00  2.05446649e+00\n",
            "  1.07962504e-01  7.09444880e-01  4.36367132e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 70 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 2.8401049e-02 -2.5867513e-01  7.0243317e-01 -8.6019337e-02\n",
            "  4.5165783e-01  1.8534237e-01  3.1561252e-01  9.1198605e-01\n",
            "  5.9754336e-01 -1.4743187e-01  5.5663931e-01  6.8790708e+00\n",
            "  6.0605758e-01  2.1191642e+00 -1.6277568e+00  2.4379017e+00\n",
            "  5.2448334e-03  1.1251194e+00  1.8753642e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 71 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 4.1767210e-03 -2.9373115e-01  7.8351068e-01 -1.0677552e-01\n",
            "  4.9164221e-01  7.9126395e-02  2.3155266e-01  7.1312886e-01\n",
            "  6.1089426e-01 -8.6324319e-02  4.7011241e-01  6.1216612e+00\n",
            "  3.2141292e-01  1.5176753e+00 -1.3650619e+00  2.1505265e+00\n",
            "  8.6594239e-02  8.2095808e-01  8.5459277e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 72 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.01295195 -0.2894998   0.8496557  -0.09044959  0.584043    0.19390084\n",
            "  0.3157137   1.0224274   0.63270795 -0.1256083   0.5949777   7.033293\n",
            "  0.62835455  2.0716128  -1.584295    2.4235566   0.04773526  1.151175\n",
            "  0.21495329]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 73 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.04226098 -0.3676085   0.8705165  -0.07416406  0.60587955  0.12342511\n",
            "  0.35486975  0.6884259   0.66559494 -0.11070259  0.48047346  6.539061\n",
            "  0.41370612  1.4249094  -1.517569    2.1505353   0.12839463  0.77926016\n",
            "  0.07277354]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 74 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.06138667 -0.32921904  0.8203157  -0.12803298  0.77401054  0.2857437\n",
            "  0.5105752   0.9348088   0.63097537 -0.07182977  0.59995997  7.5617747\n",
            "  0.41233405  1.4874257  -1.7360586   2.2256114   0.1013601   1.0486684\n",
            "  0.25453293]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 75 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0850037  -0.32397583  0.8046541  -0.07380971  0.6005694   0.16391352\n",
            "  0.391427    0.7769236   0.6460897  -0.09005303  0.519085    7.0671387\n",
            "  0.3673683   1.4442693  -1.4500972   2.228589    0.07353206  0.86239976\n",
            "  0.15022008]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 76 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02742099 -0.344693    0.837034   -0.09740996  0.57648367  0.18856908\n",
            "  0.343111    1.1568913   0.7420655  -0.17323007  0.61443484  8.001427\n",
            "  0.6745494   2.125743   -1.7227973   2.4451156   0.06091142  1.0927138\n",
            "  0.30830538]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 77 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.0194622  -0.35817498  0.88660645 -0.08978039  0.6296382   0.15956602\n",
            "  0.38934195  0.8215155   0.6674161  -0.13931715  0.62693757  8.325824\n",
            "  0.3897348   1.5762877  -1.7702836   2.3647072   0.11428392  1.0553437\n",
            "  0.16065884]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 78 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0444916  -0.36075324  0.86103094 -0.09777544  0.60044986  0.21075101\n",
            "  0.37005818  1.2281954   0.78148293 -0.18527141  0.6529929   8.624738\n",
            "  0.69931245  2.2039487  -1.8100808   2.5368512   0.06463941  1.1389004\n",
            "  0.33018866]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 79 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.04593097 -0.34919372  0.8961432  -0.0606232   0.5772539   0.24998\n",
            "  0.45218465  0.81204414  0.6951469  -0.13675189  0.67252654  8.397131\n",
            "  0.5596256   1.8376107  -1.7491891   2.4872057   0.13716093  0.95830333\n",
            "  0.05775896]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 80 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2023717  -0.35755137  0.8539197  -0.09395818  0.6338501   0.27198568\n",
            "  0.41064313  1.2685156   0.77149457 -0.19271727  0.7084288   9.445589\n",
            "  0.74730575  2.3712153  -1.9194498   2.698765    0.04961047  1.2153357\n",
            "  0.32822675]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 81 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.35132486 -0.3295704   0.928707   -0.05611688  0.5818577   0.29126635\n",
            "  0.48741826  0.8571611   0.69418246 -0.12048049  0.73201275  9.14673\n",
            "  0.5548928   1.9784145  -1.8196762   2.6547413   0.11689331  1.0799129\n",
            "  0.02788467]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 82 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.48282433 -0.34888574  0.8368264  -0.148244    0.76230484  0.32642734\n",
            "  0.6020617   1.171454    0.741519   -0.09518373  0.74796844 10.232664\n",
            "  0.41209     1.7877173  -2.0945034   2.6289005   0.06277785  1.3018296\n",
            "  0.34295687]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 83 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.6998137  -0.3378525   1.0667254  -0.10589866  0.6514307   0.2523192\n",
            "  0.4748705   1.0078758   0.73609364 -0.14080445  0.74581313  9.492733\n",
            "  0.58182627  1.9691572  -1.8197157   2.712504    0.14691886  1.2023306\n",
            "  0.11708815]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 84 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.9481937  -0.4068766   1.0095642  -0.09098003  0.71869105  0.28748974\n",
            "  0.43694454  1.447662    0.8807903  -0.20756298  0.7787562  10.449974\n",
            "  0.77743876  2.454627   -2.049164    2.8088186   0.06763036  1.3198205\n",
            "  0.37378645]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 85 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.0724806  -0.37088743  1.0192422  -0.05518521  0.6711491   0.3146604\n",
            "  0.5255857   0.93320835  0.730049   -0.18050641  0.8600409  10.644797\n",
            "  0.5897743   2.0148027  -2.100424    2.8437488   0.18692644  1.3459806\n",
            "  0.10307497]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 86 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.045198   -0.38740417  0.8803663  -0.14051226  0.8253865   0.3628289\n",
            "  0.856829    1.2433561   0.8169147  -0.11222282  0.7471003  10.573348\n",
            "  0.48231098  1.778819   -2.1174693   2.5560205   0.11016562  1.2000644\n",
            "  0.36922202]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 87 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.2310455  -0.34312865  1.0348146  -0.10681553  0.6056566   0.25209668\n",
            "  0.9199544   1.073781    0.78853214 -0.15452713  0.75984204 10.150855\n",
            "  0.5811523   2.0177703  -1.8784783   2.7205162   0.13304424  1.1747878\n",
            "  0.15141667]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 88 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.4320188  -0.37394828  0.9045564  -0.06211279  0.6893624   0.3429796\n",
            "  1.213279    1.506843    0.9153315  -0.21130309  0.736966   10.601788\n",
            "  0.71650815  2.4273214  -2.1280177   2.7533495   0.0418815   1.275398\n",
            "  0.45099476]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 89 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.97643876 -0.43171027  0.8006439   0.3151697   0.969705    0.36380246\n",
            "  1.4357814   1.2387775   0.9866095  -0.10891625  0.62745357  8.963637\n",
            "  0.3414131   1.6275611  -1.9305885   2.3728375   0.06006922  0.9155547\n",
            "  0.4359835 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 90 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.4644717  -0.32567114  1.0362651   0.38813305  0.7022374   0.3155685\n",
            "  1.3620203   1.0646179   0.7790449  -0.11067025  0.75302047  9.995955\n",
            "  0.47912806  1.791367   -1.8615525   2.4210916   0.14600113  1.0804604\n",
            "  0.11356791]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 91 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.6346989  -0.35723644  0.9506667   0.70784986  0.6032432   0.40317407\n",
            "  1.8016225   1.2526188   0.7725615  -0.25067246  0.892873   11.254353\n",
            "  0.54875785  2.216885   -2.2492414   2.8243732   0.04970368  1.4477553\n",
            "  0.37233612]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 92 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.989671   -0.4199481   0.8239343   1.0319661   0.55989957  0.17911825\n",
            "  2.0256248   1.3302927   0.8671714  -0.50018185  0.7287872  11.146918\n",
            "  0.6817911   3.0831413  -2.778188    3.6832807   0.0154005   1.156832\n",
            "  0.59731066]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 93 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.5190427  -0.43595746  0.8677273   1.3374131   0.84528273  0.28209737\n",
            "  2.116299    1.2384905   0.9698436  -0.15153989  0.7411291  10.39699\n",
            "  0.4129602   2.3698032  -2.4580724   3.2304003  -0.02787855  1.1320323\n",
            "  0.4656704 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 94 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.6181264  -0.36087683  0.901599    1.1343102   0.9144661   0.45155057\n",
            "  2.0930128   1.3001199   0.8229667  -0.0973832   0.73549694  9.99917\n",
            "  0.44705552  1.7733427  -2.1872907   2.558639    0.09107801  1.2623869\n",
            "  0.40634483]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 95 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.5565535  -0.39142865  0.94131315  1.1211444   0.7040405   0.21335387\n",
            "  1.8016285   0.97979337  0.78145194 -0.08097628  0.6992568   8.81352\n",
            "  0.54042464  1.6588746  -1.767888    2.4579823   0.1323263   0.9325333\n",
            "  0.19420709]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 96 reward=0 new_state=[0 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 2.4044628  -0.47580537  0.9159832   2.19998     0.34391242  0.3443875\n",
            "  2.974789    1.5862262   0.79670715 -0.65022504  0.80974704 10.656867\n",
            "  0.8034916   3.3052223  -2.9317896   3.8138714   0.16446035  0.9661468\n",
            "  0.48653647]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 97 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.6631433  -0.47572663  1.0521611   1.7557633   0.685561    0.3417444\n",
            "  2.2967465   0.8064835   0.81068516 -0.44591412  0.91684073  9.000609\n",
            "  0.54381305  1.8053594  -2.1831272   2.724455    0.10996766  0.76757836\n",
            "  0.34799713]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 98 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.6453736  -0.27564418  0.81116176  1.4771457   0.5508488   0.251586\n",
            "  2.0091944   1.03861     0.8407799  -0.12719923  1.0405667   8.27809\n",
            "  0.60405624  1.9286889  -1.738408    2.4809434   0.11952084  0.9638992\n",
            "  0.25255603]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 99 reward=1 new_state=[0 1 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.9255627  -0.30043235  0.73015684  1.996333    0.470583    0.22222187\n",
            "  2.3978071   0.9771298   0.8459689  -0.3555907   1.31486     8.672478\n",
            "  0.6819339   2.4385173  -2.361548    2.7982967   0.17137116  0.8624934\n",
            "  0.54713756]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 100 reward=0 new_state=[0 1 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.6083752  -0.47367188  0.5536251   2.0515916   0.727315    0.21126235\n",
            "  2.3560638   1.1019576   0.9792184  -0.21309394  1.2944587   7.640556\n",
            "  0.34835452  2.3891194  -2.3431246   3.12152     0.09793183  1.0091637\n",
            "  0.4688381 ]\n",
            "Epsilon reduced to 0.07290000000000002\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 1.8918165  -0.49296075  0.67173743  2.3257551   0.57424307  0.22980857\n",
            "  2.6061287   1.246858    1.0256367  -0.46267277  2.000329    7.120598\n",
            "  0.42862463  2.730221   -2.3054283   3.4035335   0.09371079  1.7074558\n",
            "  0.26637357]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.0243628  -0.5588832   0.7279018   2.3049464   0.64462775  0.18662736\n",
            "  2.6021073   1.2933209   0.9534144  -0.18856126  2.0177064   7.285119\n",
            "  0.5499985   2.277514   -2.4421222   2.7787666   0.07883517  1.6842806\n",
            "  0.52800614]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.0061955e+00 -3.9842093e-01  6.5232629e-01  2.3219345e+00\n",
            "  4.7374991e-01  3.8126432e-03  2.6305237e+00  1.2918555e+00\n",
            "  8.4066135e-01 -2.4436948e-01  2.1418281e+00  7.0402794e+00\n",
            "  8.6184174e-01  2.8896391e+00 -2.3936243e+00  3.1530144e+00\n",
            "  2.0572705e-02  1.5466436e+00  3.6149725e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.102182   -0.50017035  0.60207766  2.4160135   0.80654055 -0.17148681\n",
            "  2.6904306   1.2990386   1.0590694  -0.5647454   2.0097864   6.379559\n",
            "  0.48803374  2.7649944  -2.2962203   3.1248543   0.05721258  1.5068998\n",
            "  0.4927227 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 1.9746174  -0.36922905  0.6525136   2.29698     0.45989355 -0.20562138\n",
            "  2.5515032   1.1087201   0.82894945 -0.21776286  2.297198    6.3307424\n",
            "  0.77787626  2.828751   -2.34453     3.1492257   0.03586703  1.6407864\n",
            "  0.35007188]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.1849797e+00 -5.3217560e-01  6.6267890e-01  2.4754462e+00\n",
            "  3.4169284e-01 -2.1749136e-01  2.7035284e+00  1.4618444e+00\n",
            "  8.7277919e-01 -3.0102599e-01  2.4148250e+00  5.8493981e+00\n",
            "  7.1278572e-01  2.7377193e+00 -2.3156536e+00  3.0381634e+00\n",
            "  2.5432305e-03  1.7167723e+00  5.6437618e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 1.9263932  -0.4182099   0.57411903  2.368691    0.63750875 -0.38345078\n",
            "  2.633606    1.266339    1.0234525  -0.49405482  2.3616433   5.0730433\n",
            "  0.48329857  2.8592021  -2.2349513   3.0071895   0.1091776   1.6364882\n",
            "  0.3974637 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 1.6766074  -0.4447947   0.60804516  2.2453437   0.84466594 -0.3410373\n",
            "  2.4032543   0.9642707   0.9212064  -0.46695834  1.9743174   4.9932146\n",
            "  0.22672506  2.1093233  -2.0737424   2.5680323   0.1191995   1.2763162\n",
            "  0.39030716]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 1.9744606  -0.40068516  0.5302286   2.4043396   0.62481385 -0.46432638\n",
            "  2.6652799   1.31955     1.0731426  -0.52621704  2.4371064   4.721113\n",
            "  0.5796441   2.9377573  -2.2321014   2.9761996   0.04627262  1.6305732\n",
            "  0.36572665]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.129989   -0.5151848   0.57931733  2.5679176   0.3223756  -0.41632912\n",
            "  2.6712713   1.3908398   0.95040256 -0.48589993  2.5489006   4.5287194\n",
            "  0.5428763   2.7944982  -2.1344702   3.040354    0.06887044  1.7483587\n",
            "  0.5063192 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.6805776  -0.34972265  0.6050841   2.1091902   0.48531112 -0.4420506\n",
            "  2.178944    0.9588943   0.8514473  -0.3363034   2.2082927   3.9129562\n",
            "  0.34382346  2.3611698  -1.7733687   2.9211397   0.07803208  1.386222\n",
            "  0.10221687]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.7854916  -0.31736898  0.650646    1.7153206   0.24704754 -0.48693419\n",
            "  1.922153    0.8601534   0.5466591  -0.06725565  1.758098    4.215661\n",
            "  0.6811907   2.23248    -1.6357199   2.7316747   0.04411516  1.0450386\n",
            "  0.27115813]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.685411   -0.3709346   0.61413056  1.8457447   0.3772813  -0.45969725\n",
            "  1.9402148   0.88963556  0.7076237  -0.2618281   2.1498542   3.4119651\n",
            "  0.40831625  2.20054    -1.6114837   3.0134385   0.14340843  1.2889429\n",
            "  0.08906243]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.5014799  -0.30469894  0.5969813   1.5430179   0.34261352 -0.49215466\n",
            "  1.6527624   0.641682    0.5115362   0.01489102  1.637861    3.9804673\n",
            "  0.37056324  1.6370642  -1.5728894   2.4921503   0.07787754  0.88941455\n",
            "  0.2521278 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 1.6318785  -0.36264658  0.52771306  1.800555    0.2817804  -0.5968372\n",
            "  1.9129713   0.94633824  0.7113108  -0.26123148  2.083801    3.0878596\n",
            "  0.42324346  2.2571151  -1.599283    3.0924706   0.1318035   1.2170897\n",
            "  0.20001884]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 16 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.3669982  -0.25988558  0.60094243  1.3897084   0.5422149  -0.3291412\n",
            "  1.6708686   0.70870596  0.5128568  -0.01663207  1.72238     4.7908998\n",
            "  0.28671727  1.3141341  -1.4559587   2.433422    0.02347946  0.9352627\n",
            "  0.2951767 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 17 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.276202   -0.22857055  0.75248146  1.2063007   0.37151167 -0.35780653\n",
            "  1.4310222   0.46352857  0.5251216  -0.02537985  1.4962398   4.0533366\n",
            "  0.30989876  1.4569358  -1.1060183   2.3421695   0.09037955  0.7582209\n",
            "  0.1017336 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 18 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.2908669  -0.24445647  0.65930986  1.3721526   0.49067834 -0.3303286\n",
            "  1.7035449   0.63490695  0.53769445 -0.01546493  1.7078265   4.5517087\n",
            "  0.2797289   1.5746399  -1.4030969   2.4810107   0.04202017  0.93715036\n",
            "  0.18696484]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 19 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.2347466  -0.2582774   0.7761994   1.1516588   0.3710275  -0.34821466\n",
            "  1.3976481   0.4609027   0.49968433 -0.02918046  1.4901574   3.937907\n",
            "  0.33724084  1.545284   -1.07257     2.3263814   0.09879775  0.71605265\n",
            "  0.08055173]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 20 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.543203   -0.2582849   0.7391451   1.4347643   0.32798925 -0.43942556\n",
            "  1.7353882   0.8059893   0.5530367  -0.08441518  1.7433232   4.2242184\n",
            "  0.49048257  2.2644126  -1.2683967   2.796013    0.02801547  1.029465\n",
            "  0.21718793]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 21 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.2659347  -0.22577477  0.7516124   1.2051201   0.3698637  -0.3811707\n",
            "  1.420314    0.45057482  0.5202429  -0.02370876  1.5157379   3.979406\n",
            "  0.30752602  1.817353   -1.0922666   2.470376    0.08923525  0.75139743\n",
            "  0.09786965]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 22 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.3343548  -0.24942583  0.7307849   1.3688599   0.5798433  -0.34990317\n",
            "  1.7066753   0.65954906  0.49863443  0.00650602  1.7692692   4.408526\n",
            "  0.28181276  1.8586565  -1.3582916   2.646968    0.04279219  0.99713755\n",
            "  0.20945577]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 23 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.2718441  -0.22703564  0.7568105   1.2162821   0.37593588 -0.3919612\n",
            "  1.430552    0.4512849   0.52258825 -0.02373589  1.5362992   4.0564017\n",
            "  0.30994073  1.96218    -1.0983142   2.5372357   0.0891863   0.75575125\n",
            "  0.09751831]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 24 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.2527024  -0.26983088  0.6817272   1.2788498   0.5337485  -0.3877679\n",
            "  1.621754    0.653664    0.52531856 -0.01430088  1.6643428   4.3503532\n",
            "  0.28114873  1.8816738  -1.3339343   2.5292442   0.04793605  0.8631838\n",
            "  0.2549268 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 25 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.3561758  -0.22171901  0.7246066   1.3092152   0.378025   -0.3911304\n",
            "  1.5016916   0.4538739   0.49406913 -0.03506773  1.6455171   4.5031676\n",
            "  0.3176387   2.2115598  -1.2043009   2.7938108   0.05477705  0.8526634\n",
            "  0.11669116]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 26 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.6298729  -0.25106668  0.7373424   1.5360923   0.36602008 -0.44586545\n",
            "  1.8233258   0.80457795  0.5302394  -0.08478221  1.8842351   4.6925673\n",
            "  0.5272189   2.8377085  -1.3344511   3.1276283   0.00950826  1.0846394\n",
            "  0.20402744]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 27 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.4106475  -0.27934307  0.7138476   1.4316986   0.3216615  -0.5256925\n",
            "  1.6730632   0.58603096  0.5650765  -0.07779419  1.7432367   4.654839\n",
            "  0.41120097  2.6218922  -1.3500694   2.9778252   0.09078572  0.935785\n",
            "  0.20673466]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 28 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.5354967  -0.28063187  0.75353426  1.4100065   0.39670172 -0.5263148\n",
            "  1.7133896   0.7640106   0.5838005  -0.08268037  1.7361498   4.589924\n",
            "  0.53269535  2.7029533  -1.2709968   2.945282    0.04499279  0.9224928\n",
            "  0.24144445]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 29 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.4115928  -0.27746865  0.7658393   1.3672948   0.38744456 -0.523546\n",
            "  1.6231642   0.6058202   0.5393945  -0.04609855  1.7149212   4.548668\n",
            "  0.41547564  2.6467745  -1.2415582   2.8919592   0.09735388  0.88961196\n",
            "  0.21068351]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 30 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.6274914  -0.2742407   0.78235024  1.5489111   0.37586197 -0.5131406\n",
            "  1.8586379   0.8479753   0.5791753  -0.09172445  1.9112905   4.941309\n",
            "  0.5200843   2.9942398  -1.3495286   3.2248974   0.02815994  1.0897697\n",
            "  0.22711194]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 31 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.5978432  -0.22977848  0.78258026  1.5909805   0.4368382  -0.62853867\n",
            "  1.8811969   0.74069965  0.53418064 -0.04463976  1.975369    5.220703\n",
            "  0.42457244  2.8800056  -1.4472635   3.292387    0.09885155  1.1128817\n",
            "  0.2653605 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 32 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.1402417  -0.3027361   0.702802    1.2214928   0.6803963  -0.35261822\n",
            "  1.5073794   0.65891904  0.6007435   0.09422333  1.6032192   4.5102806\n",
            "  0.2955177   1.9590002  -1.0620455   2.671936    0.07412649  0.7184462\n",
            "  0.17273453]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 33 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.9726447  -0.26628396  0.58569     1.1469193   0.673743   -0.3242594\n",
            "  1.4560125   0.6279717   0.63774925  0.12872313  1.4885104   4.2860923\n",
            "  0.32910767  1.844919   -0.9988931   2.4781775   0.07541541  0.65327525\n",
            "  0.1226503 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 34 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.0112067  -0.27555114  0.5937888   1.1306479   0.68983775 -0.37254915\n",
            "  1.434558    0.6778122   0.61062926  0.13696718  1.5086489   4.358194\n",
            "  0.2870074   1.8386554  -1.0040605   2.5887983   0.04791913  0.79994106\n",
            "  0.1995971 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 35 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.0845369  -0.2649779   0.5785817   1.2090546   0.718405   -0.33995867\n",
            "  1.5022026   0.67698854  0.5860489   0.14035518  1.6015391   4.7070456\n",
            "  0.31796497  1.9982471  -1.0619439   2.7255664   0.02960405  0.91513026\n",
            "  0.1868079 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 36 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.2379044  -0.26212257  0.5726781   1.4662246   0.8055836  -0.31477365\n",
            "  1.7326514   0.762474    0.68161     0.1581207   1.8687484   5.261236\n",
            "  0.38945168  2.255591   -1.2589098   2.9951952   0.03220713  1.1011504\n",
            "  0.19267824]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 37 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.1062266  -0.27061036  0.59317523  1.311571    0.73965687 -0.3148297\n",
            "  1.6233803   0.6781954   0.64964265  0.13240033  1.6893525   5.1149426\n",
            "  0.37925032  2.164386   -1.1273094   2.7757385   0.05878116  0.9938908\n",
            "  0.12405477]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 38 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.1289696  -0.26583216  0.63458425  1.360192    0.73122466 -0.36788237\n",
            "  1.6675817   0.7162142   0.6795109   0.14901951  1.7500507   5.2595935\n",
            "  0.33823746  2.2085218  -1.127595    2.8855858   0.05880126  1.1435666\n",
            "  0.11087739]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 39 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.2082168  -0.27555323  0.72376686  1.3783326   0.84591246 -0.36320785\n",
            "  1.7132652   0.772753    0.65089154  0.17411453  1.825325    5.3124623\n",
            "  0.3452681   2.217871   -1.1127007   2.9849255   0.06369043  1.314206\n",
            "  0.14059977]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 40 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.2383066  -0.28226995  0.7350659   1.4190294   0.8631607  -0.37449288\n",
            "  1.7602794   0.7969367   0.6664473   0.17399544  1.8755991   5.5442896\n",
            "  0.3531135   2.282242   -1.1452523   3.057241    0.06466276  1.3915476\n",
            "  0.14681984]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 41 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.2154689  -0.31752038  0.7369585   1.3927146   0.88804215 -0.3852023\n",
            "  1.7670956   0.8224642   0.7050107   0.15313937  1.8492366   5.6223836\n",
            "  0.3838696   2.2500176  -1.1702914   2.9939394   0.08638895  1.3426511\n",
            "  0.19248313]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 42 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.21868    -0.32689708  0.66723007  1.4184698   0.8123006  -0.4601863\n",
            "  1.7635292   0.85244215  0.72800076  0.13523985  1.8671802   6.017107\n",
            "  0.3424401   2.3179557  -1.2388319   3.1119184   0.05422828  1.4023473\n",
            "  0.25348312]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 43 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.2539458  -0.33514324  0.67828697  1.4673727   0.83141893 -0.47407132\n",
            "  1.8192694   0.88242877  0.7476696   0.13416731  1.9270192   6.297499\n",
            "  0.35225356  2.390115   -1.2776122   3.1933787   0.05538171  1.4745101\n",
            "  0.26313573]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 44 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4026874  -0.33312744  0.7230214   1.6293367   0.9376013  -0.41007143\n",
            "  1.9952797   0.9059852   0.72188467  0.13970944  2.129935    6.939746\n",
            "  0.41741532  2.6206486  -1.3941222   3.4678917   0.05386278  1.7060665\n",
            "  0.23803583]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 45 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4224933  -0.33335748  0.75912195  1.6522202   0.98623174 -0.38956523\n",
            "  2.0425565   0.9294659   0.7464301   0.1532639   2.1640575   7.0463996\n",
            "  0.45492887  2.6645284  -1.3692675   3.4194152   0.07432065  1.6736587\n",
            "  0.2088652 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 46 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4939607  -0.3365378   0.6053466   1.9315889   0.8489033  -0.6091376\n",
            "  2.1079314   0.98487663  0.7693023   0.1471998   2.228625    6.70661\n",
            "  0.41289553  3.270744   -1.5969347   3.9839041   0.09548968  1.586078\n",
            "  0.32302976]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 47 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.7406037  -0.4423583   0.5573284   2.4182346   0.77297074 -0.7286185\n",
            "  2.4211044   1.1452451   0.97794795  0.08350051  2.4864247   6.625126\n",
            "  0.46230432  3.768453   -1.931854    4.461692    0.1298162   1.7456441\n",
            "  0.3705735 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 48 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8649944  -0.33697325  0.888088    2.4302714   0.7507593  -0.6529989\n",
            "  2.4690921   1.1190428   0.9526596  -0.07320711  2.6544285   8.472445\n",
            "  0.5729827   3.4583988  -1.7678231   3.7580416   0.07873025  2.0697434\n",
            "  0.22207838]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 49 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9461195  -0.36399034  0.8534901   3.2374597   0.8649131  -0.4426687\n",
            "  3.0072594   1.1952623   1.0472707  -0.16545184  3.0328107   8.671554\n",
            "  0.48776498  4.1918497  -2.3245173   4.6443133   0.0203194   2.466666\n",
            "  0.37782434]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 50 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0219178  -0.4639282   0.441467    3.8436277   0.93726575 -0.40611142\n",
            "  2.9882553   1.1406407   0.88228416 -0.39409328  3.0147214   5.9619417\n",
            "  0.48456547  4.4062057  -2.5257955   5.469181    0.07182351  1.839423\n",
            "  0.57028055]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 51 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8159889  -0.37324256  0.7202189   2.9620714   0.69307333 -0.14560789\n",
            "  2.541236    0.92238975  0.96296227 -0.2031944   2.563841    7.660142\n",
            "  0.41923398  3.779696   -2.0144598   4.4952955   0.1399021   1.7135855\n",
            "  0.4805028 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 52 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.1911423  -0.40767574  1.0818518   2.9492016   0.868235    0.29408872\n",
            "  2.678004    1.2319562   0.957208   -0.18965179  2.8867064   9.594708\n",
            "  0.5500754   3.9378867  -2.0397432   4.280594    0.12378961  2.5211704\n",
            "  0.20121208]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 53 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.861786   -0.49505174  0.7407179   3.754022    0.8998063   0.31681287\n",
            "  3.0993986   1.1208464   1.1419272  -0.258394    2.9058034   7.3029184\n",
            "  0.38622534  4.29754    -2.5263286   5.159994    0.05523275  2.0895293\n",
            "  0.6620496 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 54 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.2192945  -0.48137027  0.5877138   4.140504    1.0375437   0.5390837\n",
            "  3.0208116   1.1497749   0.91893196 -0.3569071   3.1904569   7.2784476\n",
            "  0.5693174   4.6152806  -2.5694425   5.719945    0.1716175   2.0768406\n",
            "  0.4585686 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 55 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.1210985  -0.4188228   1.0987498   3.2158103   0.87604624  0.67091\n",
            "  2.7896245   1.255923    0.98561144 -0.20927815  2.9632506   9.935516\n",
            "  0.5400129   4.161358   -2.218589    4.6159253   0.17063825  2.394682\n",
            "  0.24516518]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 56 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0699558e+00 -4.1027883e-01  9.6273959e-01  4.0212951e+00\n",
            "  1.0421360e+00  9.2896593e-01  3.2307472e+00  1.3659521e+00\n",
            "  1.1838672e+00 -2.7844402e-01  3.2776656e+00  9.8254013e+00\n",
            "  5.1492339e-01  4.2291479e+00 -2.4777155e+00  4.7598543e+00\n",
            "  6.1785174e-04  2.7943530e+00  3.5485882e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 57 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9045473  -0.5032404   0.5155441   4.599821    1.0645677   0.96955436\n",
            "  3.2789536   1.2215788   1.108345   -0.38808712  3.208821    6.940336\n",
            "  0.4865229   4.5247173  -2.7085347   5.5720534   0.0174295   1.9618946\n",
            "  0.6773785 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 58 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0263042  -0.48582876  0.7065333   3.8616216   0.72247195  0.9698512\n",
            "  2.7929883   0.9671966   0.99299306 -0.30657938  2.8830876   8.187776\n",
            "  0.5174414   4.374881   -2.4001203   5.172344    0.17829625  1.8826033\n",
            "  0.50956994]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 59 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.3634465  -0.45013496  1.1782166   3.6250432   0.96184605  1.2831352\n",
            "  3.0216599   1.3878258   1.0370204  -0.19749081  3.2487159  10.463818\n",
            "  0.5032313   4.3328247  -2.244972    4.7471967   0.10690258  2.9492712\n",
            "  0.1221841 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 60 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0148575e+00 -4.8661730e-01  7.2727954e-01  4.5257425e+00\n",
            "  8.9518726e-01  1.1879258e+00  3.3811793e+00  1.2390062e+00\n",
            "  1.3109159e+00 -3.2238200e-01  3.2114902e+00  7.4072495e+00\n",
            "  3.4423736e-01  4.5979395e+00 -2.7525079e+00  5.3458796e+00\n",
            " -3.6118017e-03  2.4505079e+00  6.5550286e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 61 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9793482  -0.36179715  0.39603668  4.536611    0.8728263   1.3418697\n",
            "  2.9666915   1.1625787   0.9028858  -0.39258698  2.9792426   6.454004\n",
            "  0.4461585   4.4772367  -2.5245013   5.497618    0.05305146  2.0373926\n",
            "  0.58580303]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 62 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8471364  -0.34669307  0.89697593  3.5036023   0.76992023  1.1299906\n",
            "  2.658096    1.1251777   1.0411822  -0.1888217   2.8427856   8.462314\n",
            "  0.5029778   3.8215916  -2.1509888   4.3799944   0.08262929  2.302464\n",
            "  0.34499097]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 63 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.2092495  -0.43205836  1.0929861   3.9065418   0.93724597  1.5910407\n",
            "  3.001694    1.403645    1.0135361  -0.274282    3.121562    9.192249\n",
            "  0.45669088  4.241101   -2.3087924   4.812477    0.05475986  2.9868805\n",
            "  0.25708845]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 64 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.986055   -0.55805534  0.62636423  4.685244    1.0152376   1.7276216\n",
            "  3.176246    1.168727    1.0167252  -0.37639606  3.1558251   6.974333\n",
            "  0.4381656   4.4866934  -2.7421362   5.750508    0.11101022  2.217541\n",
            "  0.72285455]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 65 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.923138   -0.35018662  0.6282044   4.0258484   0.81608856  1.4121414\n",
            "  2.7932982   1.0050129   0.96270645 -0.20661302  2.8523026   7.3257813\n",
            "  0.51395607  4.0052304  -2.2450724   4.874667    0.03455244  2.1081948\n",
            "  0.5890456 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 66 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.1668646  -0.43548432  1.0461924   3.5949595   0.9526944   1.5417634\n",
            "  2.747088    1.3265522   1.0407729  -0.22339317  2.989344    8.791764\n",
            "  0.5432802   4.0277753  -2.1349204   4.4535675   0.08592008  2.786497\n",
            "  0.2001667 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 67 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.760373   -0.42111447  0.70272374  4.2764997   0.9737836   1.5668433\n",
            "  3.0680661   1.2592069   1.2069486  -0.14578736  2.9984777   7.12082\n",
            "  0.46681708  4.2225227  -2.4470437   4.8853607   0.02049293  2.4112866\n",
            "  0.5557242 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 68 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0544071  -0.39368954  0.48686385  4.507771    0.79913163  1.8726215\n",
            "  2.9024053   1.0762045   0.8145445  -0.36662626  3.0353577   7.1635876\n",
            "  0.48097798  4.5519285  -2.5350327   5.5103126   0.11787301  1.942315\n",
            "  0.4265456 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 69 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8824793  -0.34458867  0.9803294   3.4798405   0.7222386   1.4613235\n",
            "  2.6312058   1.0592424   0.99002445 -0.21317278  2.793085    8.106654\n",
            "  0.5243165   3.8969183  -2.1206112   4.2662845   0.14808282  2.2419012\n",
            "  0.23297529]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 70 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9772017  -0.44540358  1.0244684   3.9992144   1.0797546   1.8767724\n",
            "  3.1192045   1.4237765   1.2036693  -0.24519774  3.1367202   7.4635644\n",
            "  0.47369483  3.98074    -2.3681176   4.4408274   0.04008876  2.7353246\n",
            "  0.3625987 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 71 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.724264   -0.43617305  0.48944682  4.4509063   0.92778915  1.8224828\n",
            "  2.9053674   1.0649451   0.96908385 -0.36128515  2.8357458   5.852962\n",
            "  0.4197317   4.1177225  -2.4455178   5.0877466   0.03391378  1.7896664\n",
            "  0.6248326 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 72 reward=1 new_state=[0 1 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8839148  -0.42236352  0.68262804  3.737793    0.77773124  1.6205175\n",
            "  2.6169035   0.9389332   0.8965537  -0.24299921  2.6674213   7.0639915\n",
            "  0.48649928  3.8887033  -2.143787    4.691034    0.12356989  1.8544137\n",
            "  0.4571597 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 73 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0680263  -0.37310484  1.0244255   3.2620037   0.7741996   1.553097\n",
            "  2.4770436   1.2170221   0.9186665  -0.14120297  2.756901    8.045183\n",
            "  0.49317434  3.707719   -1.8868679   4.0260735   0.07456581  2.5845544\n",
            "  0.2374321 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 74 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4730718  -0.28935453  0.7601844   2.6804519   0.77246165  1.2737757\n",
            "  2.0473917   1.1892345   0.8271319   0.07445507  2.2670183   6.099174\n",
            "  0.38634577  2.6902795  -1.3596823   3.1858728   0.02540359  1.9215455\n",
            "  0.4004013 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 75 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4883225  -0.37167567  1.1764818   3.034579    1.1082723   1.5394568\n",
            "  2.3600986   1.2032121   0.9364218   0.04920659  2.3530734   6.915585\n",
            "  0.44336408  2.9031258  -1.6342618   3.7423522   0.05869721  2.0830715\n",
            "  0.29025897]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 76 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6382217  -0.45208836  1.3698798   4.193583    0.83959216  1.9072596\n",
            "  2.8026288   0.9610681   1.004404   -0.3107896   2.65396     6.403214\n",
            "  0.4245734   4.030102   -2.3829315   4.9246187   0.04404106  1.8583885\n",
            "  0.50558954]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 77 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5604722  -0.27183035  1.4261363   3.3729296   0.7344643   1.5052614\n",
            "  2.3277707   0.8798161   0.76869166 -0.06004608  2.4117236   6.5501685\n",
            "  0.39029196  3.3646362  -1.7246583   4.202796    0.05152063  1.7733228\n",
            "  0.3706771 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 78 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.013474   -0.29658815  1.9018548   3.3482265   0.70713824  1.6520337\n",
            "  2.4647927   1.0594027   0.8631638  -0.15027787  2.7268562   8.133616\n",
            "  0.45847598  3.7251077  -1.8958648   4.1045523   0.07140877  2.5680287\n",
            "  0.09420042]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 79 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.7070111  -0.32939965  1.8844196   2.9707053   0.88794833  1.4601293\n",
            "  2.3087356   1.2505587   0.9736494  -0.02735894  2.4451847   7.0253305\n",
            "  0.44993827  3.2038174  -1.6299584   3.6115506   0.0388906   2.110545\n",
            "  0.30309454]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 80 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6324103  -0.3567627   2.034001    3.531834    0.8200188   1.6507066\n",
            "  2.4616716   0.95678264  0.8846312  -0.07093118  2.4096203   6.4991355\n",
            "  0.35817838  3.6317914  -1.9862727   4.5676227   0.01957762  1.9394165\n",
            "  0.35556364]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 81 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5582350e+00 -4.4248953e-01  2.2303870e+00  4.0098686e+00\n",
            "  8.5124433e-01  1.7155989e+00  2.7197816e+00  9.3330884e-01\n",
            "  9.6888751e-01 -2.7372888e-01  2.5528071e+00  5.6233788e+00\n",
            "  3.6206675e-01  3.6505499e+00 -2.2628691e+00  4.7380152e+00\n",
            " -1.4646642e-03  1.8677796e+00  6.7616731e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 82 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9276371  -0.35937014  2.330674    3.206236    0.8026486   1.565084\n",
            "  2.3774753   1.1380585   0.8935791  -0.1648199   2.545126    7.488403\n",
            "  0.48209876  3.5697625  -1.8461169   3.9979537   0.05547974  2.3485408\n",
            "  0.20050982]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 83 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.796782   -0.4394256   2.3407278   3.0851407   0.7200769   1.5003923\n",
            "  2.3652616   1.0747491   0.95163035 -0.10487687  2.5510166   8.120473\n",
            "  0.48626027  3.3569744  -1.8515676   3.920531    0.1026326   2.2053413\n",
            "  0.21537532]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 84 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.7756994e+00 -4.3274155e-01  2.4758804e+00  3.8923120e+00\n",
            "  7.7363253e-01  1.7641411e+00  2.7524199e+00  1.0234258e+00\n",
            "  1.0439982e+00 -3.3107836e-02  2.7447584e+00  7.2318411e+00\n",
            "  3.8461193e-01  3.9082026e+00 -2.2387562e+00  4.9017515e+00\n",
            "  7.1103070e-03  2.1947296e+00  3.6439332e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 85 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.77485    -0.42022815  2.8298936   4.46476     0.89003474  2.072711\n",
            "  2.9197352   1.0265368   1.0130756  -0.32183272  2.8149173   6.710704\n",
            "  0.39836863  4.1014085  -2.46374     5.199692   -0.01348225  2.2107081\n",
            "  0.6183603 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 86 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8998386  -0.39395148  2.6768606   3.3491673   0.93332195  1.636592\n",
            "  2.459594    1.1526586   0.9193939  -0.12664588  2.620605    7.720136\n",
            "  0.50046074  3.4649363  -1.8696221   4.062634    0.03720784  2.4175346\n",
            "  0.22021815]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 87 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.733342   -0.40806246  2.5470726   2.9799237   0.8383266   1.451528\n",
            "  2.276094    1.133396    0.9666328  -0.0187772   2.4419186   7.763526\n",
            "  0.46287158  3.2446995  -1.6565417   3.756965    0.11613474  2.0585802\n",
            "  0.26393765]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 88 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5483735  -0.32809782  2.590275    3.3046002   0.8970641   1.6276281\n",
            "  2.3332002   1.037122    0.85618794  0.02879965  2.4389558   7.6447883\n",
            "  0.48821098  3.3879733  -1.8166214   4.124325    0.10705229  1.8891076\n",
            "  0.3624155 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 89 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6814349  -0.5001995   3.0660644   4.163298    0.86766434  1.8938676\n",
            "  2.7422369   0.9801848   0.99556744 -0.3189567   2.6133614   6.903121\n",
            "  0.39618912  3.9008894  -2.340799    4.9325676   0.06695562  1.789614\n",
            "  0.6309599 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 90 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9564607  -0.3582924   3.009681    3.2741637   0.89450645  1.7172456\n",
            "  2.5509346   1.3133234   1.0067302  -0.15171076  2.6558263   7.6948943\n",
            "  0.50204074  3.611162   -1.901348    3.9535809   0.05185102  2.3813846\n",
            "  0.23209293]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 91 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8233504  -0.39826053  2.8313813   3.11952     0.8198652   1.4984727\n",
            "  2.3529644   1.1979814   0.940924   -0.10782093  2.566461    7.842822\n",
            "  0.47277308  3.3343146  -1.7665383   3.8623338   0.05927154  2.2780848\n",
            "  0.28491393]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 92 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5536152  -0.34961292  2.7985654   3.4313304   0.8753668   1.6116233\n",
            "  2.381887    1.1240201   0.9593356  -0.03790665  2.4108224   7.203991\n",
            "  0.38187325  3.5145142  -1.893597    4.3823185   0.05982777  2.013149\n",
            "  0.4412918 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 93 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.7553924  -0.5259718   3.4277468   4.3777757   0.9313158   2.0135589\n",
            "  2.9509184   1.0965725   1.0480579  -0.33562955  2.7982202   7.4653563\n",
            "  0.37400654  4.054531   -2.4301877   5.0516634   0.02962579  1.9383501\n",
            "  0.6380341 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 94 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0284846e+00 -3.8657507e-01  3.1174297e+00  3.5482929e+00\n",
            "  8.7192857e-01  1.7936122e+00  2.7103720e+00  1.2675772e+00\n",
            "  1.0887849e+00 -9.2713475e-02  2.8989079e+00  8.5577211e+00\n",
            "  4.9357319e-01  3.7357392e+00 -2.0678797e+00  4.2991638e+00\n",
            "  3.1068933e-03  2.5736673e+00  2.4830867e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 95 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0162814  -0.4124911   3.2884567   3.5792778   0.9318418   1.9196886\n",
            "  2.7716687   1.2628963   1.0326964  -0.07475898  2.9603858   9.300056\n",
            "  0.5683519   3.7478592  -2.0379498   4.2850084   0.0722693   2.6196854\n",
            "  0.18042919]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 96 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.7123636  -0.36474064  3.2109349   3.794396    0.90540123  1.8883824\n",
            "  2.6815324   1.0585767   0.9545896  -0.06338751  2.72263     8.877079\n",
            "  0.46289653  3.8092475  -2.0458472   4.6935673   0.11202179  2.2106225\n",
            "  0.330006  ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 97 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8632535  -0.5121459   3.684951    4.599833    1.0457009   2.17039\n",
            "  3.0418909   1.0896679   0.9895956  -0.4043832   2.8604753   8.423014\n",
            "  0.42676753  4.2003226  -2.513609    5.2726617   0.0776182   2.0028927\n",
            "  0.6622411 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 98 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8831712  -0.3852112   3.2859411   3.6390908   0.9448516   1.7659059\n",
            "  2.6368933   1.2280875   1.0371308  -0.12887003  2.692612    8.579016\n",
            "  0.42860845  3.677436   -2.0237243   4.340844    0.02509599  2.349144\n",
            "  0.32618883]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 99 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.126593   -0.4009437   3.4291568   3.5555816   0.9023818   1.9179387\n",
            "  2.7255695   1.3254088   0.99147636 -0.11538623  2.944019    9.539107\n",
            "  0.5981266   3.9179158  -2.0454493   4.2494426   0.08612359  2.519223\n",
            "  0.23627293]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 100 reward=2 new_state=[0 0 1 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6171681  -0.37572196  2.980853    3.3297863   0.9666376   1.651101\n",
            "  2.4275784   1.2089028   0.9508809   0.09761299  2.5032704   8.472151\n",
            "  0.49669698  3.4372077  -1.8304313   4.2364745   0.1370093   1.9927984\n",
            "  0.4153372 ]\n",
            "Epsilon reduced to 0.06561000000000002\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.4342632  -0.6338288   4.2800493   5.385012    1.1389719   2.6964414\n",
            "  4.0151277   1.4564255   0.84740496 -0.5980818   4.0105133   7.527941\n",
            "  0.7648357   5.0990686  -3.1306295   5.784407   -0.04421843  2.5506065\n",
            "  0.6229232 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 1.9169031  -0.5587267   3.6596189   4.77536     0.6556877   2.308341\n",
            "  3.378551    1.2200637   1.0913681  -0.6877132   3.3144      7.189424\n",
            "  0.46522227  4.994778   -2.82238     5.3899302  -0.01232039  2.6503205\n",
            "  0.47204366]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.8013723  -0.6745894   4.1358333   5.315945    0.70473886  2.6222248\n",
            "  3.9635324   1.9935565   1.4492662  -0.62134117  4.30304     7.7971554\n",
            "  0.78618264  6.3597665  -2.9880102   6.547411    0.0724124   4.115266\n",
            "  0.5557156 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.3081045  -0.51849955  3.6754699   4.7437735   0.8082758   2.138819\n",
            "  3.476598    1.5833565   1.2182491  -0.5946938   3.6238134   7.2909594\n",
            "  0.62356466  5.9323397  -2.720696    5.7682385   0.11621398  3.4835396\n",
            "  0.39033884]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.2562828  -0.6543798   3.814189    4.72478     0.81359017  2.3957734\n",
            "  3.4110813   1.5853653   1.3183596  -0.557975    3.7382452   6.552612\n",
            "  0.4486492   5.0194793  -2.6414707   5.504076    0.16204494  3.3470948\n",
            "  0.53044397]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.106046   -0.50035924  3.431784    4.386449    0.7942709   2.0814748\n",
            "  2.910269    1.2091837   0.9720053  -0.4161535   3.237221    6.5142703\n",
            "  0.43175176  5.1591096  -2.4809756   5.3023887   0.03063131  2.8749368\n",
            "  0.29311863]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.6889417  -0.5903435   3.9190128   5.00597     1.0240461   2.4365785\n",
            "  3.7974231   1.8507993   1.3118066  -0.7677805   3.942905    6.063545\n",
            "  0.8230159   6.112484   -2.8552482   5.8018627   0.03200708  3.6351728\n",
            "  0.5523026 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 8 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 1.887166   -0.5309906   3.3394408   3.601752    0.5126486   1.6611025\n",
            "  2.6297748   0.9961666   0.9433516  -0.22718427  2.7316456   6.571241\n",
            "  0.5177627   3.8864198  -2.0991914   4.5071316   0.03810464  2.16612\n",
            "  0.22033274]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 1.8808354  -0.6198597   3.290857    4.207844    0.9534808   2.1668656\n",
            "  3.1222742   1.7516025   1.3758783  -0.27893397  3.4446554   4.857935\n",
            "  0.4944132   4.3672686  -2.1716685   4.9730506   0.1391099   2.753758\n",
            "  0.49534175]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 10 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5274328  -0.3869585   2.9553022   3.5289373   1.2589141   1.8246064\n",
            "  2.5483143   1.1523011   1.0353879  -0.10534261  2.5409863   5.908117\n",
            "  0.43650404  3.4128664  -1.8826464   4.0287824   0.03928924  2.2816887\n",
            "  0.43940324]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 11 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.51763    -0.4318219   2.9328704   3.4611926   1.3449963   1.8589146\n",
            "  2.646368    1.2081615   1.0836269  -0.15995045  2.6556804   5.745231\n",
            "  0.45960677  3.054846   -1.7981267   3.8614259  -0.02365874  2.5042074\n",
            "  0.3705163 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 12 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3407265  -0.38091794  2.7886493   3.1807985   1.2318347   1.7254846\n",
            "  2.4494846   1.0846614   1.0561473  -0.12423716  2.4183311   5.835365\n",
            "  0.4023725   2.9954429  -1.7212625   3.8834488   0.04891574  2.4005752\n",
            "  0.32935235]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 13 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5415015  -0.55842125  2.980179    3.5959706   0.9287073   1.9419504\n",
            "  2.7770824   1.2432591   1.1724477  -0.05362475  2.8880262   5.6651516\n",
            "  0.53881925  3.1421223  -1.790861    4.6247044   0.01949915  2.8265712\n",
            "  0.33455178]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 14 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3592231  -0.37779385  2.7775433   3.232282    1.1126767   1.7478502\n",
            "  2.4619005   1.0786195   0.96265274 -0.06670601  2.41801     5.807557\n",
            "  0.3341149   3.021322   -1.779956    4.26618     0.01843726  2.5598814\n",
            "  0.31337407]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 15 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4402238e+00 -4.4418740e-01  2.8868349e+00  3.2901464e+00\n",
            "  1.3120867e+00  1.7477673e+00  2.5678849e+00  1.2049612e+00\n",
            "  1.1087391e+00 -1.6012171e-01  2.5540400e+00  5.2685814e+00\n",
            "  4.2588377e-01  2.8942952e+00 -1.7317762e+00  4.2407961e+00\n",
            " -4.7110487e-03  2.7238591e+00  3.8252550e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 16 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3639399  -0.4253505   2.7722318   3.1491134   1.1874834   1.6548895\n",
            "  2.4349012   1.1531949   0.99606514 -0.08101563  2.3777218   5.3348527\n",
            "  0.35327488  2.8135839  -1.725897    4.295197    0.0125066   2.5370305\n",
            "  0.3996996 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 17 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5637699  -0.44233453  3.01609     3.5753305   1.3655442   1.901184\n",
            "  2.694995    1.2076895   1.0628067  -0.1696132   2.7182453   5.889403\n",
            "  0.45481485  3.0776339  -1.8694952   4.7610908  -0.02512424  3.1343777\n",
            "  0.38667583]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 18 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5137678e+00 -3.9150590e-01  2.9800797e+00  3.4717207e+00\n",
            "  1.3858521e+00  1.8576894e+00  2.6247416e+00  1.1957994e+00\n",
            "  1.0454487e+00 -1.1540547e-01  2.6394496e+00  5.8962722e+00\n",
            "  4.4587126e-01  3.0652227e+00 -1.7672757e+00  4.6892123e+00\n",
            " -2.0731110e-03  2.9656386e+00  3.4959975e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 19 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5500494e+00 -4.3680683e-01  3.0171838e+00  3.5439956e+00\n",
            "  1.3990768e+00  1.9079005e+00  2.6911998e+00  1.2018127e+00\n",
            "  1.0704529e+00 -1.5701768e-01  2.6987338e+00  5.7508478e+00\n",
            "  4.8160875e-01  3.0560820e+00 -1.8138031e+00  4.8032198e+00\n",
            " -5.6921672e-03  3.1251230e+00  3.4977442e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 20 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3441316  -0.38013652  2.7988296   3.0878744   1.2675633   1.6499119\n",
            "  2.3756583   1.1096982   1.0177703  -0.09971475  2.3756306   5.6211762\n",
            "  0.37659627  2.902909   -1.6410979   4.56188     0.04005298  2.7883027\n",
            "  0.36370352]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 21 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4570339e+00 -4.4937205e-01  2.9320986e+00  3.3314040e+00\n",
            "  1.3264154e+00  1.7706912e+00  2.5978544e+00  1.2217493e+00\n",
            "  1.1201999e+00 -1.6329516e-01  2.5853515e+00  5.5357423e+00\n",
            "  4.3097189e-01  2.9279065e+00 -1.7536896e+00  4.7627487e+00\n",
            " -3.7050536e-03  3.0277560e+00  3.8763064e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 22 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4355055  -0.43975917  2.9148579   3.3228457   1.2717698   1.7545518\n",
            "  2.5508955   1.1846485   1.006265   -0.08301913  2.490522    5.7987676\n",
            "  0.3867938   2.8948202  -1.7917128   4.889414    0.03107521  2.926872\n",
            "  0.3916989 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 23 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4655077  -0.4549954   2.9704607   3.4566817   1.3012614   1.8644867\n",
            "  2.693668    1.2091738   1.1677252  -0.1905893   2.6516929   5.9596086\n",
            "  0.45997828  3.044765   -1.8510805   4.982366    0.00639229  3.1273108\n",
            "  0.35667232]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 24 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4314876  -0.4760505   2.8675392   3.1269698   0.9102479   1.6832372\n",
            "  2.375195    1.1530532   1.0000805   0.02554918  2.4313338   5.898569\n",
            "  0.39515877  2.99414    -1.667522    5.2122846   0.02487538  2.8026154\n",
            "  0.2746302 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 25 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.539389   -0.42801392  3.0227053   3.4700854   1.3439124   1.8622447\n",
            "  2.6936476   1.2500165   1.1174922  -0.14732538  2.7197568   6.117237\n",
            "  0.41549537  3.0877683  -1.8025125   5.1923447  -0.02393619  3.3465688\n",
            "  0.35705274]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 26 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5476668  -0.5643083   3.0654593   3.4419162   1.0306963   1.8307636\n",
            "  2.6245039   1.2374413   1.0352045  -0.04138994  2.648451    6.176133\n",
            "  0.46152428  3.0243566  -1.8653754   5.6731696   0.04486645  3.1077452\n",
            "  0.39155763]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 27 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5273293  -0.47049424  3.0496666   3.4992213   1.3383029   1.8386381\n",
            "  2.7034688   1.2828985   1.1493564  -0.18565324  2.709577    6.4826107\n",
            "  0.42096138  3.064706   -1.8838009   5.3257093  -0.02155094  3.3931694\n",
            "  0.44283947]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 28 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.416838   -0.41757432  2.959153    3.4070947   1.2237549   1.8104173\n",
            "  2.4985442   1.1257614   0.9249256  -0.02932266  2.473556    6.8091836\n",
            "  0.4375807   3.1201673  -1.7989607   5.3411903   0.03231047  3.0566647\n",
            "  0.34641576]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 29 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5846424  -0.45064154  3.0971222   3.6260214   1.4004612   1.9508563\n",
            "  2.7659295   1.2741361   1.1276251  -0.1718515   2.780408    6.7557397\n",
            "  0.48035088  3.1900487  -1.8861827   5.422075   -0.02046415  3.4255023\n",
            "  0.39038652]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 30 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4104121e+00 -4.2318496e-01  2.9307375e+00  3.3920434e+00\n",
            "  1.1452758e+00  1.7728038e+00  2.4781644e+00  1.1517446e+00\n",
            "  9.4048727e-01 -4.7579914e-02  2.4764879e+00  7.1578498e+00\n",
            "  3.9142239e-01  3.1680336e+00 -1.8590504e+00  5.5035758e+00\n",
            " -4.5156428e-03  3.1090984e+00  4.0871170e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 31 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5515749e+00 -4.7462061e-01  3.1200314e+00  3.5565009e+00\n",
            "  1.4021989e+00  1.8955585e+00  2.7687113e+00  1.3150039e+00\n",
            "  1.1816376e+00 -1.7913359e-01  2.7587719e+00  6.8191457e+00\n",
            "  4.5998654e-01  3.1192098e+00 -1.8756365e+00  5.4438987e+00\n",
            " -9.4292499e-04  3.4319234e+00  4.1781813e-01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3989069e+00 -4.0993246e-01  2.8035479e+00  3.1146629e+00\n",
            "  9.7926611e-01  1.6074455e+00  2.3287339e+00  1.2295566e+00\n",
            "  9.1554999e-01  3.0562673e-02  2.2695808e+00  6.9376788e+00\n",
            "  4.4357350e-01  2.9091303e+00 -1.6711766e+00  5.2893982e+00\n",
            "  6.6093388e-03  2.7358100e+00  2.9429454e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3739578e+00 -4.5081761e-01  3.2465653e+00  3.7877722e+00\n",
            "  8.7552035e-01  1.9538952e+00  2.7183354e+00  9.2634934e-01\n",
            "  1.0149100e+00 -2.8208771e-01  2.5535812e+00  7.0816717e+00\n",
            "  4.7102249e-01  3.0771396e+00 -2.1149197e+00  5.9662924e+00\n",
            " -2.8238859e-04  2.6368673e+00  5.2561474e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.635524   -0.46643022  3.4840546   4.0364127   0.90514255  2.3610654\n",
            "  2.8775718   1.1483531   1.2243694  -0.21013929  2.8972921   6.7709723\n",
            "  0.59488934  3.5966926  -2.0350711   6.675449    0.0907964   3.1970792\n",
            "  0.5688855 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.2727075  -0.4105746   3.823039    4.329969    0.85003114  2.6239028\n",
            "  2.9435966   1.2338952   1.0541711  -0.34374768  3.0944045   6.771313\n",
            "  0.47583476  4.8643093  -2.4511135   7.087169    0.03461604  3.9992127\n",
            "  0.39489087]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5324229  -0.44993407  3.0936599   3.4778252   0.95501703  2.2507837\n",
            "  2.5835404   1.2861419   1.0429109  -0.13319814  2.5074782   6.786213\n",
            "  0.40782318  3.2246976  -1.9064096   5.6603     -0.02100783  2.98729\n",
            "  0.45134786]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5085593  -0.4317804   3.4732368   4.133374    1.0599738   2.6958969\n",
            "  2.8894522   1.0882965   1.0355345  -0.36162227  2.768912    8.079081\n",
            "  0.4171876   3.3615494  -2.2327566   6.4176006  -0.01405076  3.1188087\n",
            "  0.511147  ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8065966  -0.46141064  3.803385    4.5095153   1.3964146   3.2903068\n",
            "  3.2298725   1.298665    1.2603917  -0.35339552  3.2715364   8.566661\n",
            "  0.53891456  3.7691061  -2.3181207   6.6911964   0.02233952  4.063383\n",
            "  0.58783734]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.2573347  -0.46751577  4.0235004   4.590793    0.88635164  3.1121593\n",
            "  3.0131378   1.3219715   1.0691863  -0.42026395  3.1322734   7.5261707\n",
            "  0.50078225  4.848482   -2.5679655   7.0274744   0.04751726  4.0727067\n",
            "  0.36792007]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8784935  -0.51340705  3.653169    4.1833134   1.0649086   3.049344\n",
            "  3.0117877   1.338286    1.1424816  -0.19612245  2.9885063   7.6242814\n",
            "  0.46561095  3.99954    -2.3794463   6.239882    0.04893898  3.6409686\n",
            "  0.43319425]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9250455e+00 -4.9651849e-01  3.7911425e+00  4.5660186e+00\n",
            "  1.2905248e+00  3.4857388e+00  3.2751551e+00  1.2533977e+00\n",
            "  1.1123396e+00 -3.9614078e-01  3.2015164e+00  8.8445406e+00\n",
            "  4.3149295e-01  3.8105919e+00 -2.4049811e+00  6.6865501e+00\n",
            " -4.1512214e-03  3.6026926e+00  5.2511525e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6526216  -0.5347107   3.7432885   4.344682    1.130951    3.4107566\n",
            "  3.2446449   1.2997653   1.2983671  -0.3122762   3.17169     7.5930324\n",
            "  0.61573833  3.7010746  -2.2250886   6.483322    0.0518322   3.6348243\n",
            "  0.50294507]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.1660953  -0.5017045   3.7746344   4.3873777   0.62125075  3.1681054\n",
            "  2.890785    1.3514448   1.0024726  -0.360477    2.9055715   7.387653\n",
            "  0.5100289   4.7844214  -2.5578897   6.5916343   0.0317963   3.6451082\n",
            "  0.34582934]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0635464e+00 -5.1475137e-01  3.7481947e+00  4.3295150e+00\n",
            "  1.0422286e+00  3.5250585e+00  3.2047710e+00  1.7063652e+00\n",
            "  1.3601459e+00 -1.4980766e-01  3.4415658e+00  6.9567556e+00\n",
            "  5.0028867e-01  4.0491462e+00 -2.2675428e+00  6.7134085e+00\n",
            " -5.1945895e-03  4.0719872e+00  6.4237863e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 45 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5801185  -0.36393484  3.4627178   4.0719223   1.0544488   3.3321013\n",
            "  2.9450274   1.1564921   1.1970936  -0.2793441   2.9212062   7.9870963\n",
            "  0.4924732   3.4378846  -2.216238    5.7448144  -0.06013411  3.3247986\n",
            "  0.46604526]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5901983  -0.5138602   3.5356143   4.033287    1.117841    3.2839968\n",
            "  2.9671524   1.3657392   1.1295756  -0.21532217  2.8924415   6.7172894\n",
            "  0.4682312   3.2977374  -2.1514993   5.915328   -0.01817476  3.3238797\n",
            "  0.56711364]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.3090248  -0.5942977   4.0763297   4.53033     1.0783021   3.789562\n",
            "  3.1796863   1.4187926   1.1700637  -0.46114823  3.2419987   6.9956465\n",
            "  0.5677248   4.8291063  -2.5422518   6.4162374   0.11699242  4.19082\n",
            "  0.44730213]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.7411921  -0.4492435   3.4997077   4.102021    0.95925474  3.3078034\n",
            "  2.836891    1.3010553   1.0828614  -0.24012698  2.812584    7.4162393\n",
            "  0.47907534  3.932981   -2.3074584   5.6556463  -0.01525747  3.5334673\n",
            "  0.39573318]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8325728  -0.4794208   3.783946    4.5395136   1.0313119   3.8247695\n",
            "  3.213552    1.3112113   1.2141942  -0.31442523  3.3485568   7.4218707\n",
            "  0.5954414   3.6604822  -2.279053    6.5089946   0.02040657  3.7755835\n",
            "  0.5260544 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5095109  -0.4088541   3.5875604   3.8729475   1.2120081   3.4487329\n",
            "  2.844619    1.0410898   1.1311145  -0.28403544  2.7727015   7.421925\n",
            "  0.5156287   3.3358648  -2.0914006   5.3015885   0.05847926  3.2670398\n",
            "  0.41961733]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 51 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.2635403  -0.6422064   4.02979     4.503406    0.7768084   3.666847\n",
            "  3.1365023   1.5143689   1.1328571  -0.35591692  3.211263    6.194349\n",
            "  0.8590424   4.7349086  -2.6127934   6.6163654   0.09910334  3.8372042\n",
            "  0.49958655]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 52 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.1250644  -0.54055446  3.7823107   4.3697786   1.0229795   3.8248816\n",
            "  3.1052718   1.61192     1.3335325  -0.2576058   3.2644463   6.7108507\n",
            "  1.047122    4.1055846  -2.239798    6.0751624   0.02146095  4.0972815\n",
            "  0.56410176]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 53 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5325608  -0.41852525  3.2662916   3.7330985   0.81317484  3.1972845\n",
            "  2.6966019   1.0442327   1.0177126  -0.17446055  2.5906146   7.7404\n",
            "  1.1597942   3.2572064  -2.0903068   5.3766274  -0.01558398  2.9946535\n",
            "  0.2921334 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 54 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5791379  -0.3862125   3.5001132   3.967549    1.4358307   3.742973\n",
            "  2.9098506   1.3598484   1.2182338  -0.3447085   2.9066205   6.6825085\n",
            "  1.4115      3.295684   -2.0967848   5.008366   -0.02883205  3.6048818\n",
            "  0.5576805 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 55 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.1443994  -0.4915497   3.8153272   4.218098    0.75689393  3.5300713\n",
            "  2.8157384   1.323691    1.1340187  -0.3667462   2.9934788   6.2415957\n",
            "  1.7038463   4.682263   -2.413468    5.8636413   0.06192622  3.7837417\n",
            "  0.42569968]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 56 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6679448  -0.52976567  3.3446453   3.7234643   0.9651556   3.2995613\n",
            "  2.7555022   1.3363755   1.1295786  -0.13290267  2.6781714   6.1876745\n",
            "  1.6020281   3.557063   -2.050605    5.268415    0.08318956  3.2066913\n",
            "  0.4395749 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 57 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6151383  -0.46195197  3.520882    3.9756196   1.3347572   3.8632529\n",
            "  2.9529197   1.1665301   1.2202643  -0.3927135   2.8743672   6.9073725\n",
            "  1.9020175   3.2045603  -2.1173434   4.8381605   0.03272263  3.414648\n",
            "  0.4754604 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 58 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4918725  -0.42821497  3.3580842   3.8597333   0.92762476  3.3309178\n",
            "  2.7119784   1.1014048   0.97436965 -0.20962518  2.57733     6.7497406\n",
            "  1.9761566   3.363397   -2.1113672   5.1938806  -0.02013724  2.8787587\n",
            "  0.37485814]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 59 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.296713   -0.49798867  4.0255094   4.671474    1.054736    4.2783704\n",
            "  3.2863321   1.5179404   1.1652389  -0.4761671   3.2408564   6.1333323\n",
            "  2.3664663   4.7981725  -2.7097273   5.4802637   0.01872798  4.1627555\n",
            "  0.4546296 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 60 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9513655  -0.4085794   3.5873435   4.034498    1.1658248   3.6722841\n",
            "  2.8824315   1.3431515   1.1339314  -0.23132922  2.942225    6.202407\n",
            "  2.3352795   3.9878109  -2.1249783   4.989946   -0.0148384   3.9174244\n",
            "  0.3756753 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 61 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.376445   -0.44635743  3.2534215   3.5272522   0.9905541   3.186751\n",
            "  2.5800889   1.14015     0.9868769  -0.17135717  2.452954    6.2355843\n",
            "  2.169426    3.0220592  -1.9031632   4.7754474   0.0069952   2.797489\n",
            "  0.3610511 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 62 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.7372911  -0.4472419   3.6481035   4.167306    1.3900139   4.0521374\n",
            "  3.0376568   1.1110142   1.1345145  -0.41387787  2.9974654   6.886566\n",
            "  2.5245833   3.2957156  -2.166323    4.9071693   0.01968761  3.5861003\n",
            "  0.45709053]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 63 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0956879  -0.41955218  3.559393    3.92238     0.59320337  3.2928154\n",
            "  2.618376    1.3102574   0.9372091  -0.23496975  2.695766    5.852199\n",
            "  2.463986    4.4310927  -2.278031    5.34013    -0.01573706  3.546769\n",
            "  0.3388637 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 64 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9896747  -0.52959985  3.6873925   4.049581    1.1998459   3.8608263\n",
            "  2.9672716   1.3441429   1.1957481  -0.37638295  2.9917443   6.4263625\n",
            "  2.466622    3.9489877  -2.3524182   4.8137975   0.06848929  3.876324\n",
            "  0.46120694]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 65 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5403708  -0.42353365  3.4498527   3.852055    1.3532324   3.8040519\n",
            "  2.8909545   1.1249381   1.1197888  -0.38419947  2.770706    6.165355\n",
            "  2.5875015   3.1927078  -2.032884    4.559893    0.03341904  3.347831\n",
            "  0.43873772]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 66 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3644387  -0.40768668  3.2659833   3.5581427   0.8875334   3.1415741\n",
            "  2.5085158   1.0856459   0.97838634 -0.24662533  2.3859577   5.7118692\n",
            "  2.623726    2.9315164  -1.8860624   4.654839   -0.01511188  2.7345293\n",
            "  0.43765187]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 67 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.108489   -0.56186295  4.1736836   4.288694    1.1180215   3.9906487\n",
            "  3.0256162   1.3941417   1.2091492  -0.45141652  3.0797122   5.7025943\n",
            "  2.6866162   4.3708997  -2.4277923   5.0996866   0.03611923  3.9284065\n",
            "  0.4461484 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 68 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6478862  -0.4041177   3.496199    3.5426984   0.7068626   2.9970646\n",
            "  2.428855    1.2115139   1.0068645  -0.1487202   2.370732    5.863446\n",
            "  2.5275812   3.6733294  -2.0220585   4.675392    0.02553592  3.0023785\n",
            "  0.35176662]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 69 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6186795  -0.4509908   3.9794397   4.105862    1.3803316   4.064394\n",
            "  3.0690367   1.261833    1.1072901  -0.27526534  2.9482899   6.341897\n",
            "  2.880856    3.3406012  -2.2536132   4.601648    0.02884857  3.5734477\n",
            "  0.38596582]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 70 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6531191  -0.45639414  4.174945    4.1853123   1.3601316   4.037978\n",
            "  3.0713253   1.0980605   1.143579   -0.43319473  2.9358065   6.3820667\n",
            "  3.0576406   3.342765   -2.1930223   4.7692394   0.05116375  3.424718\n",
            "  0.45363095]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 71 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9210554  -0.5114836   4.1164484   3.9596393   0.69636714  3.2328942\n",
            "  2.6172915   1.2991593   0.9635145  -0.26766846  2.6306434   5.3125596\n",
            "  2.851379    4.1134834  -2.2615697   5.1812763   0.02096607  3.330857\n",
            "  0.39131773]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 72 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9781781  -0.50266916  4.580782    4.059337    1.1350265   3.8711143\n",
            "  2.9515505   1.2986326   1.1152233  -0.30019024  3.020807    5.8391438\n",
            "  2.940307    4.031812   -2.2258694   4.9533134   0.06672397  3.868505\n",
            "  0.38729915]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 73 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3462415  -0.40456676  3.804242    3.5723808   0.88312185  3.290279\n",
            "  2.5867393   1.0977777   1.0399387  -0.13023324  2.5960298   5.8517056\n",
            "  2.801741    3.0212512  -1.8732078   4.7778215   0.01392424  2.9544256\n",
            "  0.44856316]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 74 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.617898   -0.47933975  4.416501    4.0967426   1.0358222   3.7115433\n",
            "  2.9348855   1.2796237   1.0969366  -0.31156722  2.8950083   5.1294794\n",
            "  3.4050946   3.186276   -2.1957362   5.20343    -0.05656918  3.127207\n",
            "  0.6653578 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 75 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0192304  -0.5470842   4.981416    4.2772      1.1363887   3.9248977\n",
            "  2.9854763   1.3943586   1.1771022  -0.3700673   3.0039792   5.2501397\n",
            "  2.9955962   4.3824244  -2.3082192   5.2772894   0.05979501  3.8477154\n",
            "  0.49658754]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 76 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9387996  -0.5019625   4.721541    3.910135    0.72692555  3.4979398\n",
            "  2.799717    1.3783582   1.0548946  -0.24124005  2.7701037   5.690884\n",
            "  2.9022803   4.1620297  -2.25724     5.076158    0.05469901  3.382174\n",
            "  0.36116922]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 77 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5863088  -0.5214888   4.43794     3.8329766   1.0236528   3.619007\n",
            "  2.8761      1.4160631   1.2147249  -0.16578123  3.0223606   5.216316\n",
            "  3.3031974   3.212899   -1.990249    5.3901896   0.0666274   3.3130984\n",
            "  0.53200024]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 78 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4610404  -0.4031632   3.9537191   3.327324    0.8256164   3.0040417\n",
            "  2.3566582   0.9493696   0.92540324 -0.21045588  2.3193412   5.9611487\n",
            "  2.7844346   2.8609502  -1.8563794   4.6107855   0.04722415  2.409746\n",
            "  0.3775454 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 79 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.1055427  -0.5872989   5.5198255   4.424456    1.0301179   4.097448\n",
            "  3.1416426   1.4083462   1.193313   -0.44743955  3.0937808   5.3910584\n",
            "  3.1089582   4.4245152  -2.6032155   5.3522086   0.06021269  3.8239808\n",
            "  0.45226717]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 80 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9889574  -0.4689      5.473545    4.1375365   1.1726522   4.0213137\n",
            "  2.989227    1.4543501   1.2590212  -0.36354828  3.0507782   5.3711596\n",
            "  3.1438146   3.9483902  -2.3177369   4.947022   -0.01321289  3.9438026\n",
            "  0.5257656 ]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 81 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4494084  -0.42977124  4.2554903   3.453073    0.92409045  3.1574008\n",
            "  2.531462    1.195623    0.9837033  -0.03740126  2.4802775   5.960949\n",
            "  2.8305018   3.0060835  -1.9202046   4.948276    0.01514466  2.9832625\n",
            "  0.32215062]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 82 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.7605633  -0.47036973  4.936383    4.1891303   0.96299845  3.850539\n",
            "  2.9921854   1.3270886   1.2002735  -0.3003773   3.1564646   5.0887213\n",
            "  3.7835104   3.330118   -2.119972    6.1361394  -0.0248342   3.4210892\n",
            "  0.66974604]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 83 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.8464907  -0.43180332  4.9586463   4.008627    0.8065161   3.4391832\n",
            "  2.6431632   1.3236396   1.1061202  -0.27515745  2.7327945   5.00409\n",
            "  3.0027242   4.24123    -2.1946082   5.580172   -0.01408887  3.5103717\n",
            "  0.47071683]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 84 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.1896737  -0.5324618   5.8459063   4.344448    1.0626422   4.1971064\n",
            "  3.1922102   1.4685285   1.1632057  -0.3830704   3.1314445   5.338\n",
            "  3.1575499   4.388069   -2.5739274   5.5193305   0.02614352  3.9570503\n",
            "  0.48039117]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 85 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5090923e+00 -4.0928927e-01  5.0601187e+00  3.8071799e+00\n",
            "  1.3168119e+00  3.7842648e+00  2.8202100e+00  1.3068151e+00\n",
            "  1.1253780e+00 -2.6293868e-01  2.8094153e+00  4.9456534e+00\n",
            "  3.1908600e+00  3.1730995e+00 -1.9658785e+00  5.0117950e+00\n",
            " -2.6042219e-03  3.5370860e+00  5.0119710e-01]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 86 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.5755267  -0.42085874  4.590594    3.7968905   0.9781576   3.4169157\n",
            "  2.6868854   1.0447226   0.90645176 -0.22521314  2.5834587   5.97503\n",
            "  3.1169531   3.1166167  -2.0073519   5.0806656  -0.01347405  2.8313725\n",
            "  0.35145256]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 87 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.183144   -0.4873093   5.615391    4.426802    0.9906751   3.954934\n",
            "  3.0187578   1.5563502   1.2914517  -0.30447838  3.1489635   4.6684723\n",
            "  3.3733962   4.549614   -2.3464615   5.658798   -0.04994676  3.999641\n",
            "  0.5080689 ]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 88 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.0809186  -0.4773717   5.3777385   4.0825334   0.80513036  3.7733018\n",
            "  2.8935056   1.3062091   1.1581811  -0.26602355  3.0115812   5.483098\n",
            "  3.263406    4.4403944  -2.3826914   5.596893    0.03689362  3.6356487\n",
            "  0.38866496]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 89 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4997203  -0.50929326  4.767219    3.759648    1.0749917   3.5434132\n",
            "  2.8501067   1.3567759   1.102755   -0.13599595  2.8087556   4.8142858\n",
            "  3.1947472   3.1346958  -2.0372598   5.235294    0.0281974   3.1136417\n",
            "  0.47984442]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 90 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6643606  -0.5814536   5.00624     4.0885806   0.8745643   3.7794142\n",
            "  2.988337    1.1341044   1.2126654  -0.35070723  3.0051835   5.2099614\n",
            "  3.6612408   3.2113924  -2.110452    5.576756    0.09116586  3.1207266\n",
            "  0.5141552 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 91 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.7516195  -0.43436703  4.730739    3.8427992   0.8206432   3.3218656\n",
            "  2.6402504   1.350032    1.0126213  -0.11339519  2.5421464   5.058124\n",
            "  2.997842    3.784368   -2.1128397   5.1709757  -0.04223787  3.1905262\n",
            "  0.49049678]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 92 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.165267   -0.44664901  5.8668246   4.326152    1.1947621   4.180555\n",
            "  3.1378827   1.4839604   1.1874722  -0.4006008   3.2521555   5.494147\n",
            "  3.2344532   4.4230666  -2.3896255   5.327267    0.07645141  4.1906614\n",
            "  0.39253786]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 93 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4144968  -0.35210654  4.5714874   3.4109266   1.0515826   3.2997174\n",
            "  2.5305738   1.2718081   1.1196879  -0.07830311  2.6202393   4.94064\n",
            "  3.0122602   3.0757618  -1.8015705   4.5773087  -0.01666556  3.1969988\n",
            "  0.40254852]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 94 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4982706 -0.4961893  4.7468514  3.8009915  0.9596186  3.4783907\n",
            "  2.7591555  1.0756732  1.0352706 -0.2966816  2.6418483  4.957981\n",
            "  3.2027297  3.030617  -2.0410852  4.9262004  0.0487048  2.7649019\n",
            "  0.7353875]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 95 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9462298  -0.5102762   5.48395     4.2071133   1.1857837   4.043088\n",
            "  2.978045    1.4656628   1.2506083  -0.36569852  2.9582388   4.893539\n",
            "  2.9801645   4.1666903  -2.281349    5.3468885   0.04094218  3.7664905\n",
            "  1.128678  ]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 96 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.9834294  -0.45364988  4.8653293   3.76261     0.6526427   3.365521\n",
            "  2.6689124   1.2941729   0.9740822  -0.19542545  2.6458578   5.3819847\n",
            "  2.9737046   4.1457667  -2.2507434   5.1808586   0.01360829  3.2671494\n",
            "  1.1073375 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 97 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6762317  -0.5370639   4.7353015   3.8793948   1.0545161   3.763512\n",
            "  2.9427824   1.416843    1.227186   -0.13425995  3.015225    5.059952\n",
            "  3.3986216   3.1574423  -1.9376897   5.1635723   0.03041904  3.4149811\n",
            "  1.3151152 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 98 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3731687  -0.44457585  4.5706573   3.975801    0.8095552   3.5285244\n",
            "  2.7901087   1.0617403   0.80452573 -0.29469004  2.6706145   5.3395452\n",
            "  3.202656    3.15921    -2.0545793   4.8670573   0.05014958  2.4348724\n",
            "  1.5168633 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 99 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 2.153514   -0.56040525  6.041663    4.6583037   0.94045794  4.276289\n",
            "  3.3052452   1.3795716   1.2152731  -0.5085161   3.3195357   5.323442\n",
            "  3.4054554   4.503786   -2.5780368   5.733988    0.06853497  3.7300336\n",
            "  2.0998755 ]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 100 reward=1 new_state=[0 0 1 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 1.7836109  -0.43754315  5.435126    3.9092822   1.1502122   3.8078277\n",
            "  2.868126    1.320067    1.215748   -0.3013812   3.0143204   5.670733\n",
            "  3.0370443   3.8710349  -2.233147    5.002757    0.06572403  3.712345\n",
            "  1.963053  ]\n",
            "Epsilon reduced to 0.05904900000000002\n",
            "Total reward: 99\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gzR_sAk4Qkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_conv_22(env):\n",
        "    input_shape = env.observation_shape()\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape + (1, ), input_shape=input_shape))\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(env.action_space.n))\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9NPUfkYJQaP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64f51273-da0b-48ec-df00-11c4538b7682"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=100, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=4, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_22(env), epsilon_decay=0.9)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_2 (Reshape)          (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 187, 64)           256       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 11968)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 19)                227411    \n",
            "=================================================================\n",
            "Total params: 227,667\n",
            "Trainable params: 227,667\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |█████████████████████████---------------------------------------------------------------------------| 25.0% \r\n",
            "Taking action 15\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.02137801  0.02758127  0.01183015 -0.18208072 -0.00535161 -0.08244883\n",
            " -0.07539311 -0.03217891 -0.26704416 -0.04881733  0.03941587  0.12783237\n",
            "  0.16516472 -0.0140975  -0.0493666  -0.10247707  0.00145816  0.11309139\n",
            "  0.04486777]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.10189294  0.11875295  0.1105452  -0.12095204 -0.06489269 -0.02679845\n",
            "  0.00669724  0.00263422 -0.18942113 -0.09260189  0.0960787   0.02838783\n",
            "  0.18992384  0.01613689  0.09485781 -0.12207305  0.04297644  0.0408645\n",
            " -0.03469747]\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 3 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.02370226  0.054863    0.0818892  -0.09598646  0.04664199 -0.01283799\n",
            "  0.0154898  -0.08683307 -0.24257591 -0.07504214  0.0139834   0.07613802\n",
            " -0.03720796 -0.06092687  0.09210228 -0.16281526 -0.02528499  0.09748486\n",
            "  0.08828158]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 4 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.09451835  0.0149406   0.04269731 -0.03148867  0.02763854  0.0230613\n",
            "  0.00125195 -0.06960057 -0.1094422  -0.04370287 -0.00555704  0.03159226\n",
            " -0.00882831 -0.09159213 -0.00708667 -0.1117241   0.01286956 -0.00231227\n",
            "  0.0378124 ]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.54120028e-01 -2.27070615e-01  3.69283371e-02 -1.35522917e-01\n",
            " -3.56230361e-04  3.90988514e-02  1.57220036e-01 -7.16167092e-02\n",
            " -1.66324481e-01 -1.06072128e-01  5.19825146e-02  1.24664344e-01\n",
            " -1.47680134e-01 -4.79930826e-02  9.21077728e-02 -4.35977101e-01\n",
            "  1.02806203e-01  7.31480941e-02 -1.53137103e-03]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.06978077 -0.18560788  0.06357281 -0.19619896 -0.01120592 -0.05892197\n",
            "  0.11127987 -0.05546319 -0.16697825 -0.13228658 -0.01036762  0.14468335\n",
            " -0.34021896 -0.03998105  0.05680497 -0.5675096   0.07927442  0.11248299\n",
            "  0.0963399 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 7 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.11003181 -0.35064128  0.05552066 -0.18589582  0.19153112 -0.06472552\n",
            "  0.19715548 -0.1396066  -0.24155144 -0.06603707  0.01765336  0.09139895\n",
            " -0.28391182 -0.05607793  0.078041   -0.65630007  0.08613101  0.14445743\n",
            "  0.05805787]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 8 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.1759519  -0.23900118  0.03542029 -0.150117    0.15489146  0.00810529\n",
            "  0.10236151 -0.07297606 -0.12624636 -0.10862742 -0.03788068  0.21433347\n",
            " -0.29365256 -0.02744068  0.06914224 -0.63204086  0.04760474  0.12964562\n",
            "  0.09621252]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.15868153 -0.4632247   0.04786584 -0.20473017  0.35814157 -0.04501088\n",
            "  0.20911272 -0.11185084 -0.20720528 -0.06231388  0.00111574  0.3318291\n",
            " -0.36907554 -0.06126678  0.06688729 -0.7312235   0.10302397  0.15617037\n",
            "  0.00828714]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 10 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 2.36423448e-01 -3.59281182e-01  4.03190404e-02 -2.05458194e-01\n",
            "  3.82031560e-01 -9.37056728e-03  1.68819457e-01 -7.06354529e-02\n",
            " -1.84574693e-01 -7.45687783e-02  4.69996128e-04  4.09686983e-01\n",
            " -4.36754525e-01 -7.56116882e-02  7.10524693e-02 -7.31487513e-01\n",
            "  1.13839686e-01  1.21789083e-01  6.45555630e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.27958226 -0.51450425  0.0227374  -0.22143446  0.5656419  -0.09089885\n",
            "  0.08012062 -0.12995508 -0.1720149  -0.1169524  -0.02461047  0.5641556\n",
            " -0.5093051  -0.08940718  0.00114011 -0.88020843  0.10146688  0.15944703\n",
            "  0.03869044]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.28199998 -0.43368697  0.05311662 -0.13596557  0.6102505   0.01448381\n",
            "  0.12190265 -0.07256402 -0.15830235 -0.08471934 -0.00719834  0.6370788\n",
            " -0.41470703 -0.03801355  0.08199108 -0.82064617  0.03549933  0.08989301\n",
            "  0.08350237]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.274665   -0.638708    0.06826632 -0.20775811  0.9359745  -0.04476688\n",
            "  0.19998273 -0.14309049 -0.0786142  -0.09830087  0.0030161   0.91723734\n",
            " -0.48367694 -0.05147364  0.07809368 -0.9656445   0.10686615  0.16854247\n",
            " -0.03671521]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.29698494 -0.5093005   0.05197333 -0.14594431  0.8650187   0.03626738\n",
            "  0.14814945 -0.10525392 -0.0102233  -0.07072075  0.02522081  0.8680257\n",
            " -0.45726815 -0.0560158   0.11437632 -0.91722953  0.03481096  0.11922892\n",
            "  0.04643113]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 3.47860605e-01 -7.35800683e-01  8.76957029e-02 -2.08555907e-01\n",
            "  1.22082138e+00 -7.33175278e-02  2.12899551e-01 -1.36351749e-01\n",
            "  1.14098862e-01 -8.06559771e-02 -1.20236911e-03  1.32130361e+00\n",
            " -5.40862143e-01 -5.33732399e-02  8.55365023e-02 -1.11017787e+00\n",
            "  1.07379586e-01  1.83367118e-01 -1.44785605e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 3.91303152e-01 -5.82373500e-01  4.30940948e-02 -2.10884228e-01\n",
            "  1.08751905e+00 -1.13452785e-04  1.77946612e-01 -8.98325220e-02\n",
            "  2.11232856e-01 -1.02086417e-01 -1.56847164e-02  1.43852746e+00\n",
            " -5.59531569e-01 -5.13182282e-02  8.88842568e-02 -1.06429577e+00\n",
            "  1.22004926e-01  1.26666248e-01  4.23601642e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 4.8240387e-01 -8.7265855e-01  6.4651407e-02 -9.5359638e-02\n",
            "  1.0569776e+00 -6.7900851e-02 -4.0070262e-02 -8.5653134e-02\n",
            "  3.5113880e-01 -6.0939342e-02 -6.3331285e-04  1.5005440e+00\n",
            " -5.9458029e-01 -4.1052110e-02 -1.5123610e-02 -1.0805663e+00\n",
            " -1.9572519e-02  7.7212431e-02 -4.2143404e-03]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.45135623 -0.6534311   0.03485129 -0.21022546  1.2758355  -0.02797891\n",
            "  0.14689296 -0.11588417  0.36335728 -0.12113583 -0.04817857  1.9409248\n",
            " -0.609155   -0.03057423  0.08280235 -1.1685145   0.10450393  0.11422421\n",
            "  0.06254666]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.41516623 -0.92954516  0.05595426 -0.18911998  1.6281445  -0.03085062\n",
            "  0.22899096 -0.13094826  0.4280342  -0.07789551  0.01720632  2.2254686\n",
            " -0.5859838  -0.07741701  0.08592604 -1.2568026   0.09732614  0.12285621\n",
            " -0.01938392]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.4592254  -0.75285804  0.06279363 -0.1910521   1.4015485   0.02568677\n",
            "  0.1445737  -0.08759672  0.43658978 -0.09202982  0.02423924  2.2903428\n",
            " -0.6615462  -0.0535841   0.11655849 -1.2134267   0.03283164  0.151769\n",
            "  0.06870797]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.5323019  -1.0055302   0.09298386 -0.1407013   1.5195969  -0.06171133\n",
            " -0.0054865  -0.06202323  0.6125819  -0.07532966  0.01972535  2.5212524\n",
            " -0.7520208  -0.07621031 -0.02360989 -1.2968727  -0.00919202  0.19099434\n",
            "  0.04887231]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.5768052  -0.898476    0.10070838 -0.133705    1.5569117  -0.04142557\n",
            "  0.03401134  0.08964366  0.5672988  -0.1476312   0.01520698  2.6788523\n",
            " -0.8169144   0.03660744  0.00706756 -1.3718431  -0.00676171  0.16716364\n",
            "  0.0864162 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 6.4075619e-01 -7.7859306e-01  4.2571038e-02 -1.8843023e-01\n",
            "  1.5233841e+00 -2.6649768e-02  4.5850359e-02  1.5290602e-01\n",
            "  5.8201683e-01 -6.5859921e-02  2.7072197e-04  2.6901445e+00\n",
            " -7.0103252e-01 -6.4770445e-02  1.0157775e-03 -1.1685523e+00\n",
            "  6.8605267e-02  9.5978774e-02  7.4110083e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.54018617 -0.7685747   0.12418582 -0.13638271  1.4975759  -0.0748503\n",
            "  0.05808789  0.22164686  0.5512242  -0.04479615  0.01471238  2.4421299\n",
            " -0.57764786 -0.11314455  0.09496688 -1.1206093   0.02052466  0.11117172\n",
            "  0.05985922]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.56780237 -0.96645653  0.13883941 -0.11837375  1.7999516  -0.02591581\n",
            " -0.00493708  0.47820732  0.80738676 -0.09303805  0.1167033   3.0332952\n",
            " -0.8652801  -0.13975975  0.13709855 -1.5349957  -0.01061086  0.14217305\n",
            "  0.10224833]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 6.4013141e-01 -9.8197949e-01  1.0870164e-01 -4.5805253e-02\n",
            "  1.8158998e+00 -9.6448194e-03 -2.1286165e-02  6.2616968e-01\n",
            "  7.5557977e-01 -9.7863980e-02  5.7228405e-02  2.9554632e+00\n",
            " -1.0291713e+00  2.2941097e-03  3.1657070e-02 -1.5777290e+00\n",
            " -1.3680905e-02  1.9938822e-01  1.3698135e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 7.0225132e-01 -1.0100992e+00  1.0026240e-01 -1.3747531e-01\n",
            "  1.7682649e+00  2.0253488e-03  3.5300657e-02  6.0234052e-01\n",
            "  7.7815896e-01 -7.3324159e-02 -1.0775984e-02  3.0186236e+00\n",
            " -7.6629251e-01 -8.6133191e-03 -4.4877441e-03 -1.2979991e+00\n",
            "  9.2564207e-03  1.1041867e-01  1.1908488e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 6.0420364e-01 -8.2481766e-01  9.7524337e-02 -1.0231145e-01\n",
            "  1.4815741e+00 -7.3196873e-02  3.3815831e-02  5.0506991e-01\n",
            "  7.2201306e-01 -8.3517596e-02  5.5109695e-02  2.4013677e+00\n",
            " -5.7909346e-01  6.8125979e-04  5.0871313e-02 -1.1877522e+00\n",
            " -9.6669989e-03  7.7421360e-02  1.0451208e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 6.3745195e-01 -1.0905102e+00  5.3903539e-02 -1.2927991e-01\n",
            "  2.1318147e+00 -4.4166580e-02  8.3154336e-02  7.9523194e-01\n",
            "  9.2050743e-01 -1.9698775e-01 -2.5687046e-02  3.3473830e+00\n",
            " -8.8172972e-01 -4.1046236e-02  5.0615031e-02 -1.6245862e+00\n",
            " -1.3033403e-03  1.6411671e-01  5.8767717e-02]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.71050316 -1.1056389   0.01942078 -0.1549893   2.2479334   0.05216246\n",
            "  0.09434649  1.0075346   1.0269886  -0.12257537  0.06400984  2.9006758\n",
            " -1.1478302  -0.1266871   0.06269709 -1.7633289   0.1203484   0.24637894\n",
            "  0.11961865]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 6.1167336e-01 -1.0237899e+00  7.3764302e-02 -1.6825061e-01\n",
            "  1.9261159e+00 -6.4399436e-02  8.9162134e-02  8.4475839e-01\n",
            "  7.7249622e-01 -6.6112801e-02  1.3986819e-03  2.5629389e+00\n",
            " -8.7550223e-01 -5.7873245e-02  8.8013791e-02 -1.4483255e+00\n",
            "  6.3507464e-03  1.0085435e-01  9.7260565e-02]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.5151806  -0.91451085  0.06104305 -0.10613112  1.6147357   0.00702505\n",
            "  0.16871777  0.7352994   0.68672085 -0.05672763  0.01438298  2.1746607\n",
            " -0.6638721   0.01587565  0.00364229 -1.1378399   0.08408602  0.12975979\n",
            " -0.04384955]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.5733811  -0.8130667   0.14801715 -0.04316759  1.5701661   0.03397173\n",
            " -0.09083138  0.7867674   0.8113685  -0.14591609  0.11557831  1.7365081\n",
            " -0.7600762  -0.08386975  0.10425038 -1.164155   -0.02270969  0.08909059\n",
            "  0.14663823]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.65740454 -1.3157293   0.02884026 -0.18838736  2.373927   -0.08874676\n",
            "  0.06679437  1.2944643   1.1073006  -0.15719539  0.05892098  2.4217\n",
            " -1.2075728   0.04118922  0.04251198 -1.4427903  -0.00549324  0.1903346\n",
            "  0.00538221]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 6.19825959e-01 -8.24148953e-01  9.76562873e-02 -1.45718247e-01\n",
            "  1.63420868e+00 -6.30452763e-04  1.07229196e-01  8.33884835e-01\n",
            "  7.12885797e-01 -5.75439632e-02  3.13991420e-02  1.77609634e+00\n",
            " -6.42249167e-01 -1.03355698e-01  6.30677193e-02 -8.40727448e-01\n",
            "  1.19014390e-01  1.53363228e-01  1.04977824e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.49574485 -0.83628064  0.0759214  -0.17930456  1.4651842  -0.0631132\n",
            "  0.10917717  0.8849936   0.70685583 -0.08039452  0.03105552  1.6597763\n",
            " -0.69723386 -0.00654195  0.09484793 -0.74131644  0.04587326  0.0581519\n",
            "  0.01556212]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.6113967  -1.0843419   0.07289682 -0.04186067  1.6916988   0.0223369\n",
            "  0.00473687  1.1609278   0.8846352  -0.11279602  0.05450194  1.965838\n",
            " -0.91470903 -0.01140607  0.06780998 -0.8242947  -0.06990007  0.12873797\n",
            "  0.08824741]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.61484635 -0.8987928   0.11419795 -0.09564415  1.3386446  -0.07188015\n",
            " -0.03667631  1.0019157   0.9234139  -0.16863118  0.11028489  1.0916812\n",
            " -0.9430913  -0.02013169  0.06410477 -0.7031403   0.03977901  0.16606003\n",
            "  0.09052525]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.5545665  -0.975575    0.00787581 -0.13428196  1.3468759  -0.09417504\n",
            "  0.10052016  0.97586215  0.7785869  -0.07335906 -0.01195991  1.7880116\n",
            " -0.659193    0.0542655   0.03095479 -0.38881972  0.01808664  0.10184309\n",
            " -0.02808181]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.54693455 -0.76857924  0.05449817 -0.1618435   1.2453558   0.01486448\n",
            "  0.11870516  0.78421617  0.73122346 -0.05948918  0.01291932  1.1210581\n",
            " -0.6942364  -0.11565732  0.06210626 -0.2587543   0.09945458  0.09489807\n",
            "  0.03554773]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.51164305 -0.8850694   0.08614406 -0.08046875  1.2448848   0.0279366\n",
            "  0.02815495  0.9098048   0.6637583  -0.11452245  0.06722646  1.1419481\n",
            " -0.867013   -0.0619241   0.10650275 -0.27199835  0.01781517  0.10342078\n",
            "  0.10490758]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.64877856 -1.1720742   0.04972233 -0.11168726  1.3728738  -0.0347736\n",
            "  0.09697375  1.2146956   0.9451862  -0.06555133  0.02611612  1.0800004\n",
            " -1.170845    0.0091962   0.06925821 -0.22312891  0.01872203  0.24429688\n",
            "  0.02720143]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.52196074 -0.6572957   0.13320751 -0.12454329  0.93813646 -0.07847793\n",
            "  0.00883463  0.68230027  0.7031819  -0.13249603  0.03982907  0.93382925\n",
            " -0.5833469  -0.04176335  0.06930084 -0.04037292  0.05807152  0.03120081\n",
            "  0.05299596]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.5000783  -0.85206527  0.03875608 -0.18738319  0.8822549  -0.07470428\n",
            "  0.11492246  0.8548278   0.70030016 -0.10478531  0.01999084  1.1208216\n",
            " -0.696307   -0.00349253  0.05408772  0.11666034  0.02531495  0.06572003\n",
            " -0.00899963]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.5528493  -0.8856619   0.09260911 -0.11356094  1.0486814   0.08061612\n",
            "  0.10287244  0.8888716   0.78890425 -0.11589243  0.01161448  0.99888235\n",
            " -0.8169499  -0.1060346   0.09988912 -0.04748122  0.12720165  0.22650981\n",
            "  0.1122247 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 4.8277146e-01 -8.8838768e-01  1.1302860e-01 -9.9958293e-02\n",
            "  7.1120352e-01 -5.5505823e-02  4.6460155e-02  9.7360969e-01\n",
            "  7.3509747e-01 -8.3072014e-02  4.0619619e-02  5.8486313e-01\n",
            " -9.2548990e-01  2.4339119e-02  9.6300401e-02  5.0953262e-02\n",
            " -7.5529894e-04  1.7791699e-01  9.4415314e-02]\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.52864146 -0.7784534   0.00551887 -0.09409157  0.5458774   0.02651455\n",
            "  0.13454546  0.75922537  0.7733993  -0.10796826 -0.01738331  0.8444531\n",
            " -0.63528293 -0.04247869  0.04182731  0.3548897  -0.00435602  0.13273035\n",
            " -0.04555416]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 4.56583619e-01 -5.89408338e-01  1.21824704e-01 -5.16210273e-02\n",
            "  5.61044216e-01 -5.65972291e-02 -1.26535277e-04  4.66607481e-01\n",
            "  5.92310071e-01 -9.58197936e-02  6.69030622e-02  5.62584102e-01\n",
            " -4.52754438e-01 -3.99559923e-02  8.23284537e-02  1.87578693e-01\n",
            "  1.12162428e-02  6.12359717e-02  8.83979499e-02]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.51702285 -0.82449144  0.02595802 -0.09966061  0.6174782  -0.03671013\n",
            "  0.09806478  0.7491754   0.80912286 -0.17689657  0.00335684  0.9322233\n",
            " -0.7476373  -0.01805531  0.081287    0.43915075 -0.00544957  0.1258915\n",
            "  0.01943283]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.54926664 -0.93204904  0.10698122 -0.07926796  0.60314727 -0.0125031\n",
            "  0.08854768  0.8002872   0.7809812  -0.05559669  0.01267013  0.62535274\n",
            " -0.90169597  0.01963694  0.04472958  0.06540477  0.03805759  0.21629694\n",
            "  0.16067311]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 51 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.43649316 -0.7150967   0.09626093 -0.156268    0.530507   -0.07686219\n",
            "  0.07773561  0.58840746  0.64135265 -0.09062932  0.04885771  0.650679\n",
            " -0.62251645 -0.00860032  0.065243    0.32190654  0.03508474  0.09138111\n",
            "  0.09938181]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 52 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.51579475 -0.7267298  -0.02653356 -0.12087259  0.243399    0.00458699\n",
            "  0.12835652  0.6160281   0.84031796 -0.08489595  0.02955006  0.7228064\n",
            " -0.68958056  0.00971709  0.06411969  0.5408079  -0.00524691  0.07064875\n",
            " -0.05024851]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 53 reward=-1 new_state=[0 0 0 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.4559586  -0.7367036   0.03183008 -0.15325335  0.16350499  0.05082386\n",
            "  0.00664366  0.46555775  0.672962   -0.10149848  0.11768017  0.16962112\n",
            " -0.7343474   0.00971878  0.12022989  0.34154314  0.01847893  0.06205149\n",
            "  0.03999432]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 54 reward=-1 new_state=[0 0 0 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.47402328 -1.0035963  -0.0111637  -0.20837307  0.17239039 -0.05636682\n",
            "  0.12082386  0.6023145   0.8370118  -0.20984793  0.06186476  0.3602438\n",
            " -1.0171293   0.05214394 -0.10289251  0.61176735  0.02622031  0.13757165\n",
            " -0.06321257]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 55 reward=-1 new_state=[0 0 0 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.45245978 -0.710454    0.07340644 -0.09360839  0.28301257 -0.05737117\n",
            "  0.0699285   0.47156593  0.4981256  -0.01917908 -0.02306443  0.29323837\n",
            " -0.67138964 -0.07544849 -0.0700709   0.27834937  0.12157163  0.09628185\n",
            "  0.0946833 ]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 56 reward=-1 new_state=[0 0 0 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.4807237  -0.795836    0.08537327 -0.10956948  0.18816322 -0.08154286\n",
            "  0.05874963  0.6697889   0.51346415 -0.02634781 -0.04438153  0.46075606\n",
            " -0.75741005 -0.0020817  -0.18884763  0.5661616   0.05288402  0.15534025\n",
            "  0.11835516]\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 57 reward=-1 new_state=[0 0 0 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.364578   -0.9124609   0.02501545 -0.07428809  0.02415305  0.03932235\n",
            "  0.05258198  0.37787834  0.24936451 -0.08265989 -0.05568076  0.27331865\n",
            " -0.76795083  0.00923348 -0.36352566  0.50959975 -0.00485787  0.09585653\n",
            "  0.06452538]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 58 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.27295223 -0.6846039   0.08253828 -0.05659373  0.06509972  0.04276023\n",
            " -0.01890606  0.26766947  0.21112278 -0.12676537  0.04351708 -0.12710755\n",
            " -0.695512   -0.02476311 -0.3110946   0.37399188  0.02976101  0.16578114\n",
            "  0.12236393]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 59 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.202127   -0.57872266 -0.0076366  -0.08503775  0.00959232 -0.00457575\n",
            "  0.05352267  0.10574698  0.11702672 -0.0577892   0.06788438  0.2588807\n",
            " -0.4610293  -0.00230521 -0.37091142  0.43974194  0.01229503  0.0951506\n",
            "  0.0458855 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 60 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.04566501 -0.48951015  0.08417216 -0.05382257  0.17967588  0.01927995\n",
            "  0.03361898  0.08527202 -0.02568768 -0.09719819  0.03409856  0.07387796\n",
            " -0.45583585 -0.10259613 -0.26951146  0.25988567  0.05648469  0.2978265\n",
            "  0.11794817]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 61 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 0.07431817 -0.77669835  0.11490053 -0.03467932  0.10812327 -0.01735814\n",
            "  0.00890384  0.04388335 -0.10127278 -0.12195756  0.06482822  0.24082796\n",
            " -0.86918855 -0.02489189 -0.60411626  0.33295503 -0.02853229  0.5364372\n",
            "  0.10749694]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 62 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 0.15056328 -0.46449462  0.14691854 -0.08559184  0.14823368 -0.00856852\n",
            " -0.0685675   0.03833059 -0.10405482 -0.13885327  0.05789083 -0.05765241\n",
            " -0.49712992 -0.04460872 -0.37749007  0.23166229  0.03615317  0.3941725\n",
            "  0.10860606]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 63 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 0.02176981 -0.56368667  0.09010322 -0.1052996   0.14922917 -0.05273392\n",
            "  0.06565909  0.14110127  0.00110747 -0.08637037  0.02753002  0.3778203\n",
            " -0.52989924 -0.03771244 -0.53152514  0.5685726   0.03589542  0.5774632\n",
            "  0.02812747]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 64 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 0.01952034 -0.4832902   0.11196207 -0.03308481  0.01595773 -0.02745299\n",
            "  0.0268472  -0.02082722 -0.08464704 -0.0366806  -0.02001456  0.22620082\n",
            " -0.4107559  -0.05162026 -0.5447642   0.51663107  0.04093789  0.60113424\n",
            "  0.00196904]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 65 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [ 0.03675435 -0.5527385   0.15333095 -0.06534667 -0.0747195  -0.01796893\n",
            "  0.00581635 -0.08224174 -0.36174002 -0.13367602  0.09180481 -0.17977206\n",
            " -0.57506984 -0.00833614 -0.55515486  0.61972964 -0.05414812  0.7383784\n",
            "  0.05731459]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 66 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.04257764 -0.54784536  0.12063815 -0.08781222  0.19909947 -0.02084591\n",
            "  0.04665888 -0.06157322 -0.250763   -0.1793198   0.0407111   0.32502165\n",
            " -0.4993163  -0.06043848 -0.67986465  0.7379779  -0.01253279  0.91590804\n",
            "  0.08238123]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 67 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.08677959 -0.44650248  0.09105946 -0.11761576  0.18001452  0.03480813\n",
            "  0.07074939  0.1657094  -0.04406175 -0.06773394  0.008123    0.16705993\n",
            " -0.37141865 -0.07519811 -0.56005055  0.7020767   0.05432776  0.7041509\n",
            "  0.11114007]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 68 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.17979783 -0.47799808  0.1090401  -0.07591401  0.1345946  -0.04853223\n",
            "  0.06692959 -0.02943668 -0.21403608 -0.06030259  0.02045462  0.16962703\n",
            " -0.36167666 -0.03279484 -0.5665682   0.82358176 -0.00323691  0.7838453\n",
            "  0.04401849]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 69 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.08431815 -0.5737338   0.10387954 -0.07448542 -0.0596863  -0.00542601\n",
            "  0.04507552 -0.14452927 -0.35674247 -0.1255056  -0.01658418 -0.09452508\n",
            " -0.60225534 -0.02896857 -0.84565216  1.1794803   0.01416915  1.245009\n",
            "  0.0152072 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 70 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.07469249 -0.49231195  0.10152812 -0.0141344   0.11971636  0.03252547\n",
            " -0.00327044 -0.12421081 -0.30905667 -0.1027236   0.06431864 -0.09452353\n",
            " -0.43572298 -0.03323876 -0.5844336   0.99649316 -0.02444731  1.1323771\n",
            "  0.10120288]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 71 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.2681692  -0.61295605  0.01202691 -0.16673075  0.00489808 -0.0553579\n",
            "  0.13558418 -0.06779475 -0.38232997 -0.14216572  0.06424804  0.2891151\n",
            " -0.68555164 -0.03892522 -1.1454697   1.5842059   0.06273086  1.5578753\n",
            " -0.01974461]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 72 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-3.5045391e-01 -5.3524917e-01  7.6074183e-02 -9.8352648e-02\n",
            "  6.7732356e-02 -8.9446496e-04  5.5199321e-02  3.3633173e-03\n",
            " -3.1737149e-01 -8.1209213e-02 -2.3646632e-02  9.8142333e-02\n",
            " -5.2004284e-01 -8.9269176e-02 -7.6262498e-01  1.3542815e+00\n",
            "  3.7732761e-02  1.4679941e+00  1.0882785e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 73 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.45676637 -0.6983943   0.07200872 -0.09569753 -0.20087011 -0.03584979\n",
            "  0.05666303 -0.5078027  -0.89904016 -0.14232415  0.04237955 -0.30702198\n",
            " -0.77071595 -0.03462274 -0.9680942   2.1101556   0.03723314  2.200889\n",
            "  0.02057675]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 74 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.26935196 -0.5941981   0.06192271 -0.05479889  0.11880129 -0.00621134\n",
            "  0.05446288 -0.22491795 -0.56021065 -0.14135186  0.04791605  0.17285077\n",
            " -0.5606089  -0.040066   -0.98329145  1.8278205   0.00510304  2.0114822\n",
            "  0.01013822]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 75 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.43906593 -0.6606703   0.08527033 -0.10413875 -0.03397053 -0.04400981\n",
            "  0.08400003 -0.22087944 -0.5027086  -0.07839433  0.05733339 -0.13488297\n",
            " -0.63798565 -0.0294227  -1.0551925   1.9195305   0.03636306  2.024\n",
            "  0.06761597]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 76 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.39644912 -0.63292146  0.13133185 -0.06724266 -0.11817462 -0.02890213\n",
            "  0.09165071 -0.28598008 -0.56316996 -0.06887785  0.04396699  0.04335947\n",
            " -0.56323045 -0.04364192 -0.98846686  2.0689476  -0.03052297  2.1743858\n",
            "  0.00516021]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 77 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.411928   -0.5894152   0.1885567  -0.05511495 -0.10328416 -0.04650672\n",
            " -0.02739224 -0.43858153 -0.7038674  -0.12971713  0.07000361 -0.42455527\n",
            " -0.6321783  -0.06532742 -0.705798    2.3514974   0.00593201  2.1046154\n",
            "  0.13610116]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 78 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-6.0249716e-01 -6.5309298e-01  1.1610765e-01 -7.3836029e-02\n",
            " -2.1266480e-01 -2.4297531e-03  2.5634658e-02 -6.6966593e-01\n",
            " -9.7462088e-01 -1.0657714e-01  4.9978845e-02 -3.1603050e-01\n",
            " -6.6513890e-01 -1.0512650e-01 -9.5344275e-01  2.6807697e+00\n",
            "  4.1354790e-02  2.7427430e+00  4.7457177e-02]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 79 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4278858  -0.6763513   0.06757546 -0.10636394 -0.1244395  -0.03360408\n",
            "  0.12004064 -0.28666353 -0.531447   -0.065022    0.01579073  0.11920677\n",
            " -0.64230406 -0.02187197 -1.2870734   2.684697    0.0820815   2.8809307\n",
            " -0.00409621]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 80 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-5.0649267e-01 -5.7438755e-01  9.3202703e-02 -5.2601915e-02\n",
            "  1.0859085e-01 -5.6119338e-02  5.5919185e-02 -2.2729462e-01\n",
            " -3.7414545e-01 -5.5725567e-02  3.6252040e-02 -1.2375906e-01\n",
            " -4.5783830e-01 -3.4961455e-02 -8.3973157e-01  2.4239769e+00\n",
            " -6.9023611e-04  2.3931775e+00  2.7486384e-02]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 81 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.43929744 -0.72810745  0.1328926  -0.08919667  0.0178744  -0.08075331\n",
            "  0.06871963 -0.49182147 -0.7497934  -0.17966135  0.02132848  0.00913103\n",
            " -0.7753874  -0.04614933 -1.135324    3.558102   -0.0217246   3.419195\n",
            "  0.06356449]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 82 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-6.1898255e-01 -7.3183781e-01  1.4734302e-01 -9.7479790e-02\n",
            " -1.5496732e-01  2.5277095e-02  2.9538361e-02 -4.1360566e-01\n",
            " -9.6072727e-01 -1.2579635e-01 -2.5019038e-03 -2.1502423e-01\n",
            " -8.2200676e-01 -9.1424242e-02 -1.0310771e+00  3.6249225e+00\n",
            "  6.1990015e-02  3.5919495e+00  1.5692657e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 83 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6297543  -0.72911185  0.10159979 -0.11481393 -0.28335938 -0.01886508\n",
            "  0.15370777 -0.40477154 -0.8061403  -0.06041148  0.09190731 -0.3720516\n",
            " -0.6277815  -0.03395125 -1.258388    3.3484175   0.04986615  3.5956395\n",
            " -0.01550787]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 84 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-4.7697061e-01 -7.1730787e-01  1.1023529e-01 -8.9579567e-02\n",
            " -1.6770346e-01 -5.4402199e-02  3.0657761e-02 -2.9205307e-01\n",
            " -6.4338607e-01 -1.2314146e-01  2.9718809e-02  8.4016256e-02\n",
            " -6.2316149e-01 -5.3049676e-02 -1.2156266e+00  3.5585124e+00\n",
            " -5.7776892e-03  3.6303642e+00  8.2540268e-05]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 85 reward=1 new_state=[0 0 0 1 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4503786  -0.63365     0.15600169 -0.00563953 -0.20238091  0.01657841\n",
            " -0.01209768 -0.5536386  -0.93011206 -0.13619909  0.10227419 -0.4363179\n",
            " -0.6576058  -0.05084299 -0.96817493  3.7908719  -0.04205402  3.8131888\n",
            "  0.08172836]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 86 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.81053615 -0.8868431   0.05868261 -0.05997837 -0.34216708 -0.01111056\n",
            " -0.02244118 -1.0157821  -1.292639   -0.13351516  0.06554228 -0.39906603\n",
            " -0.8825505  -0.07269213 -1.3432713   4.517245   -0.00760077  4.856361\n",
            "  0.04862919]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 87 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.71209043 -0.87382615  0.13883385 -0.18043514  0.07185677 -0.05726398\n",
            "  0.0294462  -0.46706784 -0.95294183 -0.15597565  0.21189766 -0.4496521\n",
            " -0.95349544 -0.03719704 -1.2607077   4.1514816   0.01932699  4.472172\n",
            "  0.16319469]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 88 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5296947  -0.72869664  0.14058755 -0.14236815  0.31391916 -0.09488127\n",
            "  0.02286082 -0.57564926 -0.87891215 -0.14527947  0.3296667  -0.020001\n",
            " -0.7318837  -0.0408113  -1.0655345   4.046955   -0.03610423  4.473484\n",
            "  0.10448425]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 89 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5848232  -0.7687839   0.06840794 -0.10626785  0.5276968   0.02942757\n",
            "  0.07304261 -0.34040967 -0.5521329  -0.04964596  0.48358867  0.07364634\n",
            " -0.6612652  -0.04038084 -1.0624739   3.8452873   0.05354082  4.196895\n",
            "  0.14962468]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 90 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.8941128  -0.88495106  0.03774236 -0.06797423  0.34617564  0.0337521\n",
            "  0.04492235 -0.65336406 -1.0904337  -0.09447399  0.7022488  -0.3172912\n",
            " -0.833455   -0.07593083 -1.1765012   4.4038324  -0.00966781  5.0648313\n",
            "  0.15630202]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 91 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-6.31456733e-01 -1.04644656e+00  6.55249804e-02 -1.04032204e-01\n",
            "  5.58717966e-01 -1.04860805e-01  1.51145887e-02 -6.00411952e-01\n",
            " -1.04407573e+00 -1.30850837e-01  8.37423623e-01 -1.74542427e-01\n",
            " -1.03382587e+00  3.07674538e-02 -1.50119734e+00  4.48927164e+00\n",
            "  4.15576436e-03  5.52354956e+00  6.67924136e-02]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 92 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.45852947 -0.67313087  0.11837867 -0.18732595  0.5239579  -0.00998745\n",
            " -0.00477543 -0.34288794 -0.77710587 -0.13065828  0.7618133  -0.6284651\n",
            " -0.6119789  -0.0779707  -1.0725721   3.779823    0.08953243  4.3437896\n",
            "  0.08210815]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 93 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-7.0328218e-01 -8.5396951e-01  6.2192556e-02 -1.5176263e-01\n",
            "  5.9988362e-01 -5.6819175e-05  6.4542986e-02 -6.2997901e-01\n",
            " -9.0978813e-01 -5.6845311e-02  9.6720266e-01 -2.1174845e-01\n",
            " -5.2642035e-01 -6.4164199e-02 -1.1762938e+00  4.1205988e+00\n",
            "  5.9171807e-02  4.9438953e+00  1.3651605e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 94 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.90157056 -0.94786906  0.09710103 -0.03894468  0.61121655 -0.03969579\n",
            " -0.01085859 -0.89659655 -1.4483228  -0.12417833  1.1427543  -0.3115364\n",
            " -0.40697324 -0.02858319 -1.4371943   4.655531    0.03544598  6.0062156\n",
            "  0.10822903]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 95 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-6.16854012e-01 -8.06567252e-01  1.36060610e-01 -1.23896085e-01\n",
            "  7.03344047e-01 -7.24908337e-03  5.38737476e-02 -5.21490932e-01\n",
            " -1.08243418e+00 -1.84379146e-01  1.16789258e+00 -2.90973604e-01\n",
            " -1.63728803e-01 -3.58585455e-03 -1.30638003e+00  3.76380110e+00\n",
            "  1.75623242e-02  5.18303013e+00  9.89373699e-02]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 96 reward=0 new_state=[0 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-4.4509217e-01 -7.0950103e-01  6.1927296e-02 -1.5883683e-01\n",
            "  9.1680384e-01  4.2610094e-03  9.6584715e-02 -4.4049940e-01\n",
            " -6.8429279e-01 -1.1572291e-01  1.0387565e+00 -5.6962483e-02\n",
            " -2.2094033e-03 -4.6022087e-02 -1.2470146e+00  3.8638098e+00\n",
            "  2.7805056e-02  4.7411299e+00  5.7931107e-02]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 97 reward=-1 new_state=[1 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.64923877 -0.579557    0.06470922 -0.06256829  0.64263606  0.04329651\n",
            " -0.0353329  -0.5612913  -0.8050631  -0.05612635  0.97643346 -0.3215149\n",
            "  0.09120431 -0.06688174 -0.7985491   3.3011677   0.06202383  4.0014067\n",
            "  0.20418118]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 98 reward=-1 new_state=[1 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-5.74947417e-01 -4.15849179e-01  5.35584539e-02  6.63020357e-04\n",
            "  4.62607414e-01  2.51234211e-02  4.23244499e-02 -5.15750945e-01\n",
            " -8.15065861e-01 -4.30472717e-02  8.79664898e-01 -3.62231553e-01\n",
            "  2.65937716e-01 -1.01313375e-01 -8.39303792e-01  2.60203981e+00\n",
            "  4.90691653e-03  3.59395075e+00  1.72209799e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 99 reward=-1 new_state=[1 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6455647  -0.51518434  0.1418384  -0.07555012  0.69249034 -0.03246005\n",
            "  0.06251758 -0.6726364  -1.1502783  -0.17913677  1.2973334  -0.32980064\n",
            "  0.29946667 -0.05025675 -1.1543549   3.2798269  -0.0184101   5.004745\n",
            "  0.10885908]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 100 reward=-1 new_state=[1 0 0 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-7.1955401e-01 -3.9892444e-01  7.7297233e-02 -8.8028252e-02\n",
            "  6.5771329e-01 -4.9831238e-02 -3.9835488e-03 -8.2563794e-01\n",
            " -1.2776645e+00 -1.6476469e-01  1.4885533e+00 -6.1305445e-01\n",
            "  3.1069109e-01 -1.0094562e-01 -1.2371805e+00  4.0860348e+00\n",
            "  1.4863516e-02  5.4764619e+00  5.7176460e-02]\n",
            "Epsilon reduced to 0.09000000000000001\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.81075406 -0.7040472   0.00533372 -0.06740842  0.9870391   0.14692047\n",
            "  0.04389717 -0.19075088 -0.99714196 -0.07763347  1.3445239  -0.01870789\n",
            "  0.25332034 -0.06971329 -1.1043544   2.7283938   0.11440682  4.4650397\n",
            " -0.02560573]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6898749  -0.48856214 -0.03885338 -0.12028884  0.8322826   0.16736591\n",
            "  0.04153863 -0.04837201 -0.9427436  -0.07110909  1.2473246  -0.2772777\n",
            "  0.32510984 -0.03160494 -1.0310318   2.4233608   0.12421771  4.224064\n",
            "  0.10057258]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.78329444 -0.56292844 -0.01105555 -0.16873576  0.94485664  0.10726947\n",
            "  0.03925107 -0.11496276 -0.8592142  -0.07749546  1.237872   -0.1335437\n",
            "  0.39303672 -0.0334101  -0.99029154  2.2603345   0.08338044  3.9066563\n",
            "  0.0686855 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7268672  -0.28391066 -0.08278456 -0.04039447  0.61246526  0.15745476\n",
            " -0.02928371 -0.13100286 -0.5264235  -0.08401524  1.0993024  -0.37261856\n",
            "  0.43614182 -0.07386248 -1.1080415   2.0951622   0.0431418   3.298634\n",
            "  0.07057861]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7491793  -0.4299783   0.00665994 -0.15097423  0.9068638   0.10932598\n",
            "  0.02476899 -0.13434559 -0.842454   -0.0976604   1.2146866  -0.12252513\n",
            "  0.5027612  -0.02680709 -0.9292003   1.9315677   0.07515872  3.0115838\n",
            "  0.04332543]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66263455 -0.27187848 -0.00477527 -0.05884504  0.8838397   0.13430905\n",
            "  0.07091681 -0.09643339 -0.87718636 -0.07250624  1.1518863   0.02568683\n",
            "  0.45967    -0.08090717 -0.80453545  1.8193219   0.09249064  2.2943392\n",
            " -0.02985684]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.753053   -0.31157482  0.0096845  -0.0587559   0.8620531   0.08785219\n",
            "  0.04736846 -0.27906263 -0.9765125  -0.072785    1.1604329   0.08067514\n",
            "  0.49119267 -0.09517558 -0.868411    1.595998    0.10411558  2.1635163\n",
            " -0.06925389]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.63971305 -0.249495   -0.02324982 -0.07719677  0.8303694   0.1380865\n",
            "  0.04232387 -0.01199587 -0.77887124 -0.0873438   1.1247207  -0.18394469\n",
            "  0.53592026 -0.01855991 -0.88782877  1.5942917   0.07691719  1.6978972\n",
            "  0.07120072]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.4360222  -0.11532994  0.02036094 -0.1558144   0.89134437  0.04238493\n",
            "  0.02862337  0.10240968 -0.46485558 -0.15569098  1.0066601  -0.03887102\n",
            "  0.48339903 -0.01040136 -0.8366831   1.4633203   0.05604725  1.4576102\n",
            "  0.0900984 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 10 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.40938264 -0.1545269   0.04572709 -0.05538999  0.9207238   0.07574975\n",
            "  0.05534595  0.05045114 -0.5030321  -0.09977572  0.91928387  0.14635561\n",
            "  0.49686465  0.11047537 -0.79856586  1.2456111   0.02987952  1.269894\n",
            "  0.00497819]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.3692096   0.00203202  0.03318727 -0.16822343  0.9490438   0.05928772\n",
            "  0.00892245  0.11339331 -0.4821283  -0.1295137   1.0663722  -0.01563775\n",
            "  0.49189726  0.34605303 -0.8643805   1.4345523   0.08756804  1.2033101\n",
            "  0.08501595]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.41220173 -0.07679966  0.04205393 -0.09546065  0.93905973  0.0791439\n",
            "  0.03696067  0.06992306 -0.5257455  -0.08064558  1.0715467   0.14936468\n",
            "  0.51508665  0.53393865 -0.8880783   1.3574728   0.06929095  0.92514724\n",
            "  0.01580377]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.41741222 -0.15783763  0.05236506 -0.0558673   0.94574505  0.08437211\n",
            "  0.03076905  0.07169927 -0.50759065 -0.09924639  1.030694    0.15719987\n",
            "  0.4842946   0.79868674 -0.8155636   1.2603345   0.09061805  0.7139985\n",
            " -0.00849548]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.40691966 -0.08158757  0.03754059 -0.09022366  0.8655451   0.07535821\n",
            "  0.0519545   0.07979543 -0.4646888  -0.07822279  0.9281761   0.16198038\n",
            "  0.5417476   1.0759897  -0.7830522   1.1079496   0.04351206  0.48344865\n",
            "  0.00517377]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 15 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.30887574  0.05903142  0.02820541 -0.12905122  0.8562201   0.01674654\n",
            "  0.01609933  0.12676784 -0.309694   -0.08308284  0.90120673  0.03836201\n",
            "  0.5328475   1.3226236  -0.6675376   1.026829    0.04084801  0.5101637\n",
            "  0.11271618]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 16 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.19029962  0.25603697  0.0435609  -0.04546162  0.681108    0.0332689\n",
            " -0.0080874   0.03170305 -0.20483156 -0.0474679   0.8115906  -0.07426277\n",
            "  0.54663926  1.0607232  -0.55390704  1.2799858   0.0018803   0.68399185\n",
            "  0.08917162]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 17 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.14875312  0.2087857   0.04622661 -0.0956597   0.68130726  0.03984896\n",
            " -0.05666111  0.05245242 -0.19846217 -0.09699789  0.88438356 -0.16632712\n",
            "  0.51127     1.4407927  -0.55710036  1.2699407   0.0226533   0.5522408\n",
            "  0.16684842]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 18 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.1459226   0.37155473  0.05872807 -0.0351741   0.7266412   0.05879625\n",
            " -0.05765932 -0.01428943 -0.21126531 -0.11296896  0.861934   -0.03936662\n",
            "  0.47191435  1.3120368  -0.579002    1.4591697   0.05723361  0.8488317\n",
            "  0.01640119]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 19 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.12778744  0.26616302  0.02926103 -0.10660864  0.69748354  0.05399016\n",
            " -0.06471415  0.11226527 -0.11482112 -0.13293691  0.8245657  -0.15740103\n",
            "  0.51403314  1.7686579  -0.55663145  1.5434909   0.01722644  0.3795559\n",
            "  0.13297194]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 20 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.12727171  0.28308755  0.04900613 -0.03818931  0.6422037   0.07085552\n",
            " -0.05640866 -0.03691381 -0.24081978 -0.09316092  0.78651994 -0.04810175\n",
            "  0.5201564   1.4376817  -0.4944496   1.3980653   0.04009715  0.81901276\n",
            "  0.04291891]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 21 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.13272485  0.2602111   0.02634209 -0.10026197  0.69081914  0.06106813\n",
            " -0.0470156   0.11077311 -0.10834372 -0.11229082  0.81993484 -0.1597521\n",
            "  0.5524538   2.075684   -0.53043133  1.7365822   0.00628385  0.2098499\n",
            "  0.13075042]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 22 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.1546572   0.39407793  0.05651964 -0.02994638  0.73584735  0.06566638\n",
            " -0.04043005 -0.0202245  -0.21189241 -0.09323333  0.87889886 -0.04888597\n",
            "  0.5298701   1.9645165  -0.5639051   1.7569673   0.04831557  0.6420002\n",
            "  0.01280526]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 23 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.13434659  0.3246891   0.03332853 -0.10568117  0.7786762   0.06367487\n",
            " -0.06347635  0.10108312 -0.16604859 -0.11601118  0.9927615  -0.18278421\n",
            "  0.5658929   2.640106   -0.63587487  2.2972538   0.03240356  0.21635738\n",
            "  0.14260855]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 24 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.15786679  0.35635376  0.06292586 -0.02597758  0.6943023   0.06835312\n",
            " -0.03860348 -0.03193862 -0.22439623 -0.09713864  0.85449976 -0.08946699\n",
            "  0.54056984  2.2122922  -0.531679    1.9843814   0.03642226  0.6034326\n",
            "  0.02845608]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 25 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.12884367  0.30731308  0.03435097 -0.02346485  0.6452961   0.04787906\n",
            "  0.02448172  0.11659943 -0.07511975 -0.07494684  0.8128517  -0.06759411\n",
            "  0.57409513  2.3138697  -0.5555258   2.1894634   0.0337515   0.10224567\n",
            "  0.03358481]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 26 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.14734416  0.35475126  0.0527969  -0.04285379  0.70027775  0.07486398\n",
            " -0.05971063 -0.04994862 -0.26842397 -0.09722703  0.8776831  -0.08419106\n",
            "  0.5979926   2.3322034  -0.5453762   2.2613146   0.04179727  0.73910415\n",
            "  0.04499333]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 27 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.1925377   0.33011162  0.0421584  -0.10384277  0.75915134  0.04346787\n",
            " -0.04521134  0.06656463 -0.16945034 -0.10350182  0.9419845  -0.1928363\n",
            "  0.6594152   3.1109977  -0.59535235  3.0491805   0.00710189  0.07776575\n",
            "  0.15144904]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 28 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.18972431  0.42060307  0.07112884 -0.04971071  0.75390315  0.04571857\n",
            " -0.07812975 -0.10192744 -0.33548006 -0.10739837  0.9678203  -0.09271742\n",
            "  0.6344758   2.7719383  -0.6123482   2.749714    0.05769207  0.88068205\n",
            "  0.06371715]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 29 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.1675359   0.38073346  0.03544709 -0.12029337  0.80835646  0.06369875\n",
            " -0.07183309  0.11626636 -0.14457633 -0.14705166  1.0020297  -0.23829311\n",
            "  0.6513649   3.533789   -0.66143996  3.7543392   0.01678738  0.07176711\n",
            "  0.14355779]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 30 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.18437673  0.43891844  0.06572807 -0.03350549  0.78071654  0.07207195\n",
            " -0.04271298 -0.0457156  -0.25960562 -0.10567803  0.9828813  -0.13835806\n",
            "  0.63860154  3.2014418  -0.6074558   3.28944     0.04005072  0.5611692\n",
            "  0.03133807]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 31 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.16649505  0.36818817  0.06074896 -0.10493441  0.7968421   0.08612106\n",
            " -0.08189437  0.07989103 -0.21772233 -0.15001911  1.1042186  -0.2911335\n",
            "  0.71667737  4.043438   -0.67507124  4.2021694   0.01183598  0.00797608\n",
            "  0.15126549]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 32 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.15649094  0.50547737  0.07953014 -0.05930443  0.8640614   0.07503488\n",
            " -0.07852249 -0.1143069  -0.37668395 -0.09440279  1.1734769  -0.0993657\n",
            "  0.64881486  3.6469102  -0.72465676  3.6630106   0.07292226  0.94430673\n",
            "  0.06152023]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 33 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.22431137  0.5281619   0.05816502 -0.04942628  1.0047266   0.079045\n",
            " -0.04839694  0.09311922 -0.24024515 -0.13569047  1.2445508  -0.08968803\n",
            "  0.75955117  4.7909923  -0.82164526  4.807434    0.04975387  0.11403897\n",
            "  0.03930229]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 34 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.17318627  0.47360176  0.07207348 -0.05970272  0.8237505   0.07513248\n",
            " -0.06095817 -0.10264017 -0.32945764 -0.09143208  1.0656948  -0.0997343\n",
            "  0.7057304   3.830656   -0.66234386  3.8143165   0.04379028  0.8941852\n",
            "  0.05696118]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 35 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-2.0469069e-01  4.1217640e-01  3.6593072e-02 -1.3131112e-01\n",
            "  9.1439730e-01  8.1452675e-02 -5.4745525e-02  1.4666115e-01\n",
            " -1.5157738e-01 -1.3134319e-01  1.1540600e+00 -2.8333884e-01\n",
            "  7.9642349e-01  5.2322106e+00 -7.4115533e-01  5.2739816e+00\n",
            "  2.5722079e-03 -1.4073870e-01  3.2807270e-01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 36 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.2180233   0.52065766  0.07080634 -0.04882987  0.9173073   0.08020881\n",
            " -0.04866009 -0.03205102 -0.29315856 -0.11418086  1.1791962  -0.18642026\n",
            "  0.7706138   4.8951917  -0.73512256  4.774242    0.04240477  0.55725753\n",
            "  0.37212768]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 37 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.24665299  0.50147647  0.06600419 -0.05229019  1.0395156   0.08829392\n",
            " -0.04923481  0.11355451 -0.26104605 -0.14320207  1.3390509  -0.16351283\n",
            "  0.84933424  5.9741244  -0.8695021   5.891957    0.03728784  0.07433797\n",
            "  0.70254105]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 38 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.2430498   0.5325419   0.09258696 -0.0653936   0.935982    0.05406713\n",
            " -0.05809653 -0.13284664 -0.4055138  -0.07619883  1.2473922  -0.12062045\n",
            "  0.8357513   5.090657   -0.7715658   4.874683    0.04462665  0.995984\n",
            "  0.68265396]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 39 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.236857    0.4919228   0.05023125 -0.15872878  1.0565552   0.07974478\n",
            " -0.0816471   0.19734056 -0.17240097 -0.16698112  1.3662673  -0.35406128\n",
            "  0.88401353  6.8038974  -0.90922993  6.7573533   0.01756188 -0.07651936\n",
            "  1.0314844 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 40 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.23575258  0.5108872   0.07166617 -0.07346487  1.082855    0.11019382\n",
            " -0.05286752  0.06806454 -0.30293104 -0.11576667  1.360571   -0.25876746\n",
            "  0.92912835  6.3464003  -0.87793046  6.3521934   0.01396342  0.4892862\n",
            "  1.10019   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 41 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-2.45962337e-01  4.80618238e-01  7.28250593e-02 -1.67719617e-01\n",
            "  1.12629783e+00  8.87377560e-02 -1.02474056e-01  2.03456372e-01\n",
            " -2.40053490e-01 -1.80334240e-01  1.58072507e+00 -4.35451508e-01\n",
            "  9.04494226e-01  7.79547930e+00 -1.03144181e+00  7.77883387e+00\n",
            "  3.76185514e-02 -7.24438578e-05  1.41526580e+00]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 42 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.27065152  0.6308543   0.02580541 -0.07599806  1.3209611   0.09211662\n",
            " -0.09867307  0.16600856 -0.36936852 -0.16990024  1.8167048  -0.4010648\n",
            "  1.0012017   7.84505    -1.1386008   7.87584     0.01170984  0.6104281\n",
            "  1.6374407 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 43 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.26720062  0.55215317  0.05203814 -0.1671426   1.3289745   0.07717296\n",
            " -0.06416187  0.3469952  -0.10752114 -0.1257258   1.6608397  -0.3654892\n",
            "  1.0166528   8.451706   -1.1471082   8.670671    0.03122533 -0.23050343\n",
            "  1.6998254 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 44 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.27158338  0.66065603  0.0810063  -0.09537401  1.2184265   0.09949736\n",
            " -0.0839579   0.07612377 -0.35970178 -0.12176991  1.6150069  -0.21325329\n",
            "  1.0359466   7.4085283  -1.073162    7.5957522   0.05983809  0.8176857\n",
            "  1.594995  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 45 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.29698133  0.5242339   0.06658538 -0.1884344   1.246342    0.08373165\n",
            " -0.08319796  0.3053272  -0.17416446 -0.18022412  1.6405743  -0.44020033\n",
            "  1.0484749   8.967853   -1.1290976   9.102805    0.01556305 -0.10342457\n",
            "  1.9424783 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 46 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.30123743  0.6440663   0.10462426 -0.09869625  1.2496552   0.08294348\n",
            " -0.07624158  0.10776681 -0.3367336  -0.15788037  1.6748484  -0.3246801\n",
            "  1.0181755   8.47921    -1.1250752   8.534832    0.0594446   0.73201925\n",
            "  1.929045  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 47 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.3182618   0.63598174  0.06777103 -0.20018461  1.3936776   0.08256952\n",
            " -0.08813612  0.35672772 -0.17554846 -0.1865004   1.8098565  -0.42885134\n",
            "  1.1079273   9.818521   -1.2707292  10.071181    0.02795235 -0.12319265\n",
            "  2.257374  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 48 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.30544057  0.5986677   0.12838139 -0.08035635  1.2405648   0.11290657\n",
            " -0.08665721  0.08631734 -0.42643654 -0.16159147  1.8150216  -0.38193095\n",
            "  1.1031821   9.179602   -1.1624696   9.116016    0.05045691  0.711109\n",
            "  2.11123   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 49 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.38081747  0.58498436  0.14508073 -0.07521907  1.3866334   0.0888629\n",
            " -0.08587729  0.19150712 -0.44192386 -0.17373325  2.0024197  -0.3501709\n",
            "  1.2127423  10.687441   -1.3104557  10.446772    0.04507915  0.21068305\n",
            "  2.79532   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 50 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.32618272  0.55203843  0.10817806 -0.10980058  1.3713013   0.11374187\n",
            " -0.06162053  0.1695718  -0.3868515  -0.12509733  1.9113067  -0.40336108\n",
            "  1.1109045  10.211627   -1.2605196  10.262459    0.03928351  0.5611852\n",
            "  2.49413   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 51 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.36605167  0.5352773   0.08593335 -0.08578843  1.4258376   0.10870608\n",
            " -0.03272675  0.33676046 -0.27632624 -0.13448308  1.8895382  -0.2987976\n",
            "  1.2340611  10.558031   -1.2890276  10.8650465   0.0192607  -0.09236725\n",
            "  2.8353486 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 52 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.3460445   0.5671063   0.09158531 -0.12538144  1.4132788   0.10109022\n",
            " -0.0537011   0.23027323 -0.31498173 -0.11927103  1.8587316  -0.37717637\n",
            "  1.1334289  10.186089   -1.2592485  10.46582     0.04963334  0.68442285\n",
            "  2.4901204 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 53 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-4.2694509e-01  4.8371091e-01  6.9932580e-02 -2.1856324e-01\n",
            "  1.4772694e+00  5.3030636e-02 -3.5022859e-02  3.8549620e-01\n",
            " -1.9708905e-01 -1.1257450e-01  2.3210802e+00 -4.8608899e-01\n",
            "  1.2175093e+00  1.1252283e+01 -1.3329282e+00  1.1677406e+01\n",
            "  9.0423627e-03 -1.7079428e-01  2.8023257e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 54 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.36232883  0.7092607   0.11802592 -0.11500642  1.5207664   0.1038433\n",
            " -0.07998144  0.19914359 -0.4163181  -0.14588189  2.8939009  -0.3976498\n",
            "  1.1805595  11.367757   -1.4289668  11.43442     0.07845055  0.7269037\n",
            "  2.8085444 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 55 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-3.8847175e-01  6.1606026e-01  6.2943943e-02 -2.1742210e-01\n",
            "  1.5693969e+00  9.4088592e-02 -6.4310484e-02  4.7646320e-01\n",
            " -1.6763486e-01 -1.6335568e-01  3.1429958e+00 -4.8576042e-01\n",
            "  1.2997987e+00  1.1696575e+01 -1.4475664e+00  1.2313913e+01\n",
            "  1.1510579e-02 -2.6590469e-01  2.9491844e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 56 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.43219367  0.6967424   0.1362986  -0.13166545  1.5162071   0.05745891\n",
            " -0.06622525  0.17576286 -0.4184438  -0.14143954  3.4855528  -0.40205562\n",
            "  1.2503504  11.868635   -1.3961986  11.448353    0.06215177  0.8654456\n",
            "  2.9572773 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 57 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.44951665  0.5746713   0.09846552 -0.23373513  1.5632565   0.05694812\n",
            " -0.07366484  0.4143627  -0.24062194 -0.1700168   3.8741539  -0.5532711\n",
            "  1.326262   12.359927   -1.4615587  12.60397     0.01628671 -0.03500615\n",
            "  3.1759453 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 58 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.40941802  0.6720282   0.16406924 -0.13785005  1.4775165   0.09364165\n",
            " -0.1358035   0.11309513 -0.56434375 -0.16810535  4.1923037  -0.44761938\n",
            "  1.3208907  12.072142   -1.4479709  11.589379    0.08040428  1.2364658\n",
            "  3.0362647 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 59 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.37511483  0.59413844  0.11116065 -0.09608651  1.5234766   0.1304198\n",
            " -0.05511542  0.3928849  -0.38474312 -0.17932208  4.7200594  -0.48353508\n",
            "  1.3655943  12.774568   -1.5322658  12.847615    0.02367328  0.08197829\n",
            "  3.4935594 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 60 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.39005554  0.64329773  0.13442825 -0.11358345  1.4921001   0.1212726\n",
            " -0.08862458  0.21401207 -0.45534182 -0.16572355  4.6116104  -0.4810675\n",
            "  1.3141663  12.078926   -1.4360129  12.291744    0.05500188  0.7656934\n",
            "  3.0427856 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 61 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-4.7424304e-01  5.3365523e-01  8.4468223e-02 -2.2535264e-01\n",
            "  1.5815232e+00  6.7080081e-02 -4.8875373e-02  4.4032249e-01\n",
            " -2.3079447e-01 -1.3765110e-01  4.8420525e+00 -5.4838330e-01\n",
            "  1.3968303e+00  1.1871728e+01 -1.4603158e+00  1.3289766e+01\n",
            " -2.1313489e-03 -1.7118318e-01  3.2457194e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 62 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.39934403  0.6502292   0.12602928 -0.15478323  1.6228825   0.11132898\n",
            " -0.09023288  0.27640858 -0.42112815 -0.15839821  5.314255   -0.52007717\n",
            "  1.2670103  12.677092   -1.5512785  13.390189    0.06315912  0.73508066\n",
            "  3.442998  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 63 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.4391585   0.50577986  0.05206627 -0.24680443  1.633052    0.08638746\n",
            " -0.04544599  0.5375521  -0.13961346 -0.13690263  5.2764344  -0.57450265\n",
            "  1.3288084  11.910871   -1.5172194  13.971005    0.01542504 -0.24888168\n",
            "  3.3262234 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 64 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.4130822   0.6072396   0.08692313 -0.16494137  1.5817454   0.10079869\n",
            " -0.07026079  0.29070076 -0.36257872 -0.09922389  5.28365    -0.39788625\n",
            "  1.3029033  11.745088   -1.4722055  12.815901    0.07166523  0.9343318\n",
            "  3.120057  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 65 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.41907     0.7296319   0.07911579 -0.11851561  1.7297387   0.10598669\n",
            " -0.02508937  0.5081001  -0.28992233 -0.15052441  6.0893474  -0.40577766\n",
            "  1.4215946  13.016402   -1.6673583  14.346947    0.02991459 -0.03210715\n",
            "  3.8197815 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 66 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.43427977  0.6609956   0.10511162 -0.13943216  1.6110677   0.10027637\n",
            " -0.05039375  0.3143688  -0.35851696 -0.13200468  5.690114   -0.44436505\n",
            "  1.3636203  12.668757   -1.4956344  13.476554    0.04895958  0.6813636\n",
            "  3.2539034 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 67 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.5021364   0.5994167   0.10211677 -0.25531492  1.7011572   0.05954308\n",
            " -0.07371431  0.48816192 -0.24721676 -0.1737399   6.206603   -0.61329496\n",
            "  1.4423964  13.255696   -1.6125267  14.81621     0.01840677 -0.02340323\n",
            "  3.6422625 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 68 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.44061744  0.71933043  0.12124414 -0.16090715  1.6758586   0.09119266\n",
            " -0.07525002  0.3311011  -0.37483066 -0.16679402  6.189818   -0.48510078\n",
            "  1.3622642  13.623237   -1.5889461  14.117216    0.06956519  0.8341013\n",
            "  3.4755898 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 69 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-5.0300866e-01  5.4145765e-01  1.1843981e-01 -2.3028657e-01\n",
            "  1.6666650e+00  9.3785957e-02 -8.7265745e-02  4.5190606e-01\n",
            " -3.4305701e-01 -1.7593703e-01  6.5935016e+00 -6.6504318e-01\n",
            "  1.5164511e+00  1.3946111e+01 -1.6250937e+00  1.5192635e+01\n",
            "  6.0526086e-03 -5.4851558e-02  3.6509440e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 70 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.40323886  0.64364326  0.14280808 -0.15053897  1.5773908   0.12235342\n",
            " -0.11148752  0.20550212 -0.5327644  -0.1460905   6.26378    -0.44365197\n",
            "  1.4030038  13.25728    -1.5683205  13.506505    0.0619329   1.2761661\n",
            "  3.2532566 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 71 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.43070614  0.6165206   0.11381543 -0.11563382  1.6857488   0.1342741\n",
            " -0.05648032  0.4843319  -0.4017202  -0.18436329  7.1932473  -0.5465051\n",
            "  1.5007638  14.7473545  -1.7162211  15.351009    0.02538443  0.10274979\n",
            "  4.0339365 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 72 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.47256643  0.76590836  0.09254231 -0.16598059  1.8360708   0.08651745\n",
            " -0.04418223  0.39810807 -0.3379514  -0.11305619  6.8379097  -0.4273767\n",
            "  1.3802829  14.664025   -1.6997912  15.146745    0.07883145  0.68857193\n",
            "  3.6386945 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 73 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-4.6334925e-01  6.0118884e-01  7.1373112e-02 -1.4791124e-01\n",
            "  1.7863495e+00  9.7578831e-02 -4.9057221e-03  5.7292700e-01\n",
            " -2.6347256e-01 -1.2485288e-01  7.3681798e+00 -4.8217231e-01\n",
            "  1.4362372e+00  1.4881849e+01 -1.7352816e+00  1.5700610e+01\n",
            "  3.2912679e-02 -7.7276118e-04  4.0982623e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 74 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.45867798  0.8454811   0.12125072 -0.167333    1.8218402   0.08946262\n",
            " -0.08177079  0.37671706 -0.3831097  -0.17164719  6.99994    -0.4605819\n",
            "  1.4122483  14.68263    -1.7282081  15.201522    0.08429754  0.8319097\n",
            "  3.7273705 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 75 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.4760195   0.65950674  0.06690796 -0.25095493  1.8175318   0.0998761\n",
            " -0.06595463  0.61406904 -0.17575687 -0.16866729  7.006497   -0.58200496\n",
            "  1.5017426  13.729959   -1.7203685  15.923901    0.01634996 -0.25923556\n",
            "  3.74669   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 76 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.46465188  0.76329535  0.09694476 -0.16262704  1.8455676   0.11209548\n",
            " -0.04737768  0.4224832  -0.35111642 -0.12045535  7.122431   -0.43566707\n",
            "  1.4742551  14.571724   -1.7037723  15.448631    0.0373754   0.5014216\n",
            "  4.348467  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 77 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-5.1858115e-01  6.4136767e-01  1.0642107e-01 -1.3378234e-01\n",
            "  1.7819508e+00  7.9323038e-02 -8.3005130e-03  4.9688447e-01\n",
            " -3.6437115e-01 -1.2619032e-01  7.7101846e+00 -4.7440669e-01\n",
            "  1.5479572e+00  1.4601154e+01 -1.7296573e+00  1.5815350e+01\n",
            "  1.4452401e-02  7.5816281e-02  5.2807455e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 78 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.5275746   0.74652237  0.14603734 -0.17220268  1.7771324   0.06049683\n",
            " -0.06557805  0.30589184 -0.45150068 -0.14361215  7.352335   -0.49804068\n",
            "  1.4629737  14.765801   -1.6804692  15.348357    0.06734656  0.9328802\n",
            "  5.3574924 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 79 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-4.82265979e-01  5.77899814e-01  1.04356006e-01 -2.75626689e-01\n",
            "  1.80151510e+00  1.04338206e-01 -1.14209637e-01  5.73525608e-01\n",
            " -2.66786814e-01 -2.10144505e-01  7.72959375e+00 -7.41512477e-01\n",
            "  1.45885706e+00  1.45188808e+01 -1.80460823e+00  1.67895889e+01\n",
            "  4.57465015e-02  2.86800507e-03  6.10875607e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 80 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.48230097  0.6821102   0.11015326 -0.1605645   1.751335    0.10252012\n",
            " -0.05177504  0.38670424 -0.37532702 -0.1323751   7.2600856  -0.48800763\n",
            "  1.4760313  14.480969   -1.6544777  15.362925    0.05219529  0.702376\n",
            "  5.95054   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 81 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-4.8296350e-01  5.4605049e-01  1.0243918e-01 -2.4661365e-01\n",
            "  1.7466961e+00  1.2875803e-01 -1.0255197e-01  5.6818944e-01\n",
            " -2.8755602e-01 -2.0098916e-01  7.6231861e+00 -7.1649599e-01\n",
            "  1.5641345e+00  1.4125386e+01 -1.7441643e+00  1.6535582e+01\n",
            "  1.0788724e-02 -1.4981724e-01  6.6486907e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 82 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.48403165  0.74246913  0.12713851 -0.18075472  1.8016169   0.09302046\n",
            " -0.07753664  0.39306334 -0.39110655 -0.16750237  7.578652   -0.5251946\n",
            "  1.4625325  14.998162   -1.7340344  15.809516    0.07328213  0.8517595\n",
            "  6.899616  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 83 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-5.3298563e-01  5.5787498e-01  7.3282748e-02 -2.9509282e-01\n",
            "  1.8982155e+00  7.7724747e-02 -5.0061777e-02  6.6882962e-01\n",
            " -1.4736481e-01 -1.2619697e-01  7.8921728e+00 -6.2944937e-01\n",
            "  1.5031451e+00  1.4655608e+01 -1.8108678e+00  1.7235031e+01\n",
            "  1.5755732e-02 -1.2686153e-01  7.5241871e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 84 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.44442073  0.8358096   0.12442027 -0.19478415  1.713601    0.05804935\n",
            " -0.0648623   0.31317908 -0.4386552   0.5645532   7.314944   -0.50233257\n",
            "  1.2864163  13.64541    -1.7622334  14.76163     0.03975453  1.4583789\n",
            "  6.982241  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 85 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-4.71753925e-01  7.94826090e-01  8.80613402e-02 -2.08659559e-01\n",
            "  1.81556869e+00  1.28128275e-01 -1.04673885e-01  5.13878882e-01\n",
            " -2.66821116e-01  1.05403411e+00  7.61062193e+00 -5.99394977e-01\n",
            "  1.57584488e+00  1.45899286e+01 -1.78819358e+00  1.61270733e+01\n",
            "  1.37869045e-02  7.52786875e-01  7.70612574e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 86 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-7.09979832e-01  9.14883018e-01  1.52009010e-01 -2.57774889e-01\n",
            "  2.04324651e+00  4.18149214e-03 -9.98760015e-02  3.30525488e-01\n",
            " -3.88118863e-01  1.66873932e+00  7.80590010e+00 -3.92225206e-01\n",
            "  1.48989570e+00  1.34608555e+01 -1.89772856e+00  1.47807941e+01\n",
            "  3.76379453e-02  2.54603219e+00  7.89756680e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 87 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-8.5380119e-01  6.3422513e-01  1.1551379e-01 -3.3117968e-01\n",
            "  2.1008623e+00 -1.4052395e-02 -9.5546804e-02  4.0455773e-01\n",
            " -4.0515941e-01  2.2188687e+00  8.2668428e+00 -2.8989998e-01\n",
            "  1.6255119e+00  1.3920086e+01 -1.8843429e+00  1.5481738e+01\n",
            "  5.1317357e-02  2.1970217e+00  8.4215670e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 88 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.9400239   0.50808305  0.01776059 -0.2888659   1.9579351   0.10604081\n",
            " -0.09158164  0.4195527  -0.5834841   3.0700493   9.16567    -0.5125706\n",
            "  1.7893089  15.518973   -1.9902042  17.252686    0.03709478 -0.13696156\n",
            "  9.723981  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 89 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-5.0870353e-01  6.8738174e-01  1.3468155e-01 -2.1082860e-01\n",
            "  1.6767151e+00  5.2444432e-02 -4.2986937e-02  2.1373290e-01\n",
            " -6.2238586e-01  2.5723798e+00  7.0672479e+00 -4.7791591e-01\n",
            "  1.3226705e+00  1.1982476e+01 -1.6308017e+00  1.3488284e+01\n",
            "  6.2864018e-04  1.8569171e+00  7.4699759e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 90 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.6959379   0.816848    0.14316203 -0.27904946  2.278227    0.08268902\n",
            " -0.13249092  0.54277104 -0.24369927  3.3321614   8.348371   -0.4143035\n",
            "  1.6077801  14.47662    -1.9727955  16.57083     0.05252502  2.1268957\n",
            "  9.274382  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 91 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-9.3991917e-01  6.6553301e-01  1.3707714e-01 -3.3804148e-01\n",
            "  2.2392697e+00  2.9209689e-03 -1.5594193e-01  3.5626578e-01\n",
            " -5.0436592e-01  3.8839393e+00  8.7559233e+00 -2.7607337e-01\n",
            "  1.6220539e+00  1.3941384e+01 -1.9933863e+00  1.5815360e+01\n",
            "  7.6422729e-02  2.4198182e+00  9.6104126e+00]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 92 reward=0 new_state=[0 0 0 1 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-8.5906988e-01  7.3633575e-01  7.9610851e-03 -3.0244741e-01\n",
            "  1.7604640e+00  1.4022316e-01 -5.1846620e-02  1.4803091e-01\n",
            " -8.9560819e-01  4.3493299e+00  9.0494003e+00 -9.1249079e-01\n",
            "  1.8729213e+00  1.3443604e+01 -2.0723834e+00  1.6384768e+01\n",
            "  1.0429610e-01  1.3180020e+00  9.8258896e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 93 reward=0 new_state=[0 0 0 1 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.86940855  0.8710874   0.11101738 -0.29893842  2.2757213   0.05102275\n",
            " -0.03191112  1.1770307  -0.1662282   4.167205    8.039299   -0.48436832\n",
            "  1.8292634  13.146256   -2.049297   15.931824    0.11136706  2.4964128\n",
            "  9.250183  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 94 reward=0 new_state=[0 0 0 1 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2046893   0.6494669   0.17475013 -0.37182134  2.3323689  -0.14788063\n",
            " -0.03937751  1.5573224  -0.54131323  4.851001    8.732113   -0.2745065\n",
            "  1.4803706  11.653141   -2.005327   14.084395    0.02043682  3.5738783\n",
            "  9.713361  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 95 reward=0 new_state=[0 0 0 1 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.10832930e+00  4.81578052e-01  8.48099589e-02 -2.45252892e-01\n",
            "  1.86436176e+00 -2.66956165e-03 -5.04203700e-02  2.01394463e+00\n",
            " -6.71429515e-01  5.64060402e+00  9.22660542e+00 -8.19998860e-01\n",
            "  1.65635347e+00  1.18104877e+01 -2.32544017e+00  1.50944052e+01\n",
            "  1.14175916e-01  1.32759959e-01  1.05812140e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 96 reward=0 new_state=[0 0 0 1 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.84282786  0.86189     0.12206586 -0.1527496   1.901823    0.05886837\n",
            " -0.10086852  2.2899776  -0.36844867  4.635143    7.981301   -0.7121919\n",
            "  1.9242045  11.891682   -1.8315715  15.457107    0.10732865  1.9760478\n",
            "  9.282778  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 97 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-9.4890976e-01  7.0818222e-01  1.9948453e-01 -2.9583004e-01\n",
            "  2.2390833e+00 -8.0484711e-04 -1.4154054e-01  3.1506276e+00\n",
            " -4.4016966e-01  5.4717145e+00  8.9786015e+00 -1.9532567e-01\n",
            "  1.6117920e+00  1.2915611e+01 -2.0185392e+00  1.6023191e+01\n",
            "  5.2059725e-02  3.1897805e+00  1.0363181e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 98 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-4.9845368e-01  8.7669003e-01  1.9460291e-01 -1.6586053e-01\n",
            "  1.4207593e+00  6.8853539e-03  5.1628160e-01  2.6216533e+00\n",
            " -8.8631445e-01  4.5625820e+00  7.2406487e+00 -8.2652956e-01\n",
            "  1.2705038e+00  1.0007297e+01 -1.8055165e+00  1.2949333e+01\n",
            " -2.0238543e-02  2.5978289e+00  8.3671513e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 99 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.5119974   0.6227421   0.12077974 -0.21107392  1.8464776   0.04249926\n",
            "  1.0699862   3.4163678  -0.21572493  4.642381    7.5345926  -0.3498925\n",
            "  1.6237309  11.120679   -1.662407   14.698664    0.06889272  1.6698948\n",
            "  8.712628  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 100 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-7.9185295e-01  6.4333123e-01  2.1769200e-04 -3.0425102e-01\n",
            "  1.6687222e+00  1.5732628e-01  1.9293468e+00  4.6206255e+00\n",
            " -8.7502760e-01  6.1353025e+00  9.2858515e+00 -7.0957470e-01\n",
            "  1.6506361e+00  1.2383266e+01 -2.1412830e+00  1.6851915e+01\n",
            "  1.3070834e-01  7.2899264e-01  1.0851490e+01]\n",
            "Epsilon reduced to 0.08100000000000002\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0157696   0.30754295  0.10739408 -0.15147306  1.4875616   0.10209796\n",
            "  2.8254046   5.320944   -0.38240966  6.625073    9.599698   -1.0082318\n",
            "  1.7545468  12.080571   -2.4520442  16.920452    0.09844123  0.28722057\n",
            " 11.218009  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0681067  -0.05092002  0.09955015 -0.09475192  2.111545    0.10076782\n",
            "  3.520906    5.7510695  -0.86417294  6.885567   10.028894   -0.4590209\n",
            "  1.7564036  12.52276    -2.1332583  17.193033    0.09303191 -0.70631826\n",
            " 11.584211  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.87216     0.2532956   0.13057737 -0.13585459  1.7640582   0.1451333\n",
            "  3.6798918   5.490579   -0.9066497   6.368231    9.115757   -0.710227\n",
            "  1.6640989  11.720138   -1.9679439  16.06003     0.17310329  1.5350899\n",
            " 10.610901  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0249023  -0.02880764  0.08931218 -0.25006768  2.113682    0.09405888\n",
            "  4.485566    6.459327   -0.74257     6.996763    9.597837   -0.63619375\n",
            "  1.7117592  10.7383     -2.1691039  17.51862     0.07787804  1.505144\n",
            " 11.346615  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.053384    0.41485235  0.16161355 -0.0944916   1.4124788   0.10934579\n",
            "  4.405387    5.8619337  -0.5616881   6.7088127   9.331416   -0.9193021\n",
            "  1.6961936  10.185277   -2.2133574  16.007227    0.12587236  4.554274\n",
            " 10.929155  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.167498    0.12890676  0.07142024 -0.28621417  1.4667586   0.04201099\n",
            "  5.2303      6.7127233  -0.25929108  7.3952756   9.749191   -1.052119\n",
            "  1.8370452   9.170787   -2.3502896  17.47147     0.04553929  5.293886\n",
            " 11.655948  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1346469   0.38354418  0.08120575 -0.17193933  1.5787398   0.20085233\n",
            "  5.219979    6.248317   -0.8245976   7.239504    9.676107   -0.8280719\n",
            "  1.6971565  10.34639    -2.3209918  16.476452    0.05502689  7.2354374\n",
            " 11.518328  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 8 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2099092e+00 -1.4164668e-02  1.2174565e-01 -9.3784302e-02\n",
            "  2.1618345e+00  1.1613983e-01  6.0859227e+00  7.1453214e+00\n",
            " -1.1227021e+00  7.8199177e+00  1.0618535e+01 -4.7250977e-01\n",
            "  1.9163688e+00  1.0859799e+01 -2.2245753e+00  1.7731670e+01\n",
            "  1.0156826e-01  8.3214598e+00  1.2279682e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.070345    0.30402717  0.09626558 -0.18893497  1.3954209   0.11633166\n",
            "  5.2563496   6.127254   -0.48623276  6.5306425   8.654098   -0.13410184\n",
            "  1.5912063   8.552142   -2.3247383  15.439456    0.02157622  9.176835\n",
            " 10.243878  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0857751e+00  8.2322724e-02  4.8623081e-02 -3.3255243e-01\n",
            "  1.8713316e+00  5.0157826e-02  5.8050318e+00  6.7957740e+00\n",
            " -4.9771324e-01  6.8969970e+00  8.7555599e+00  6.7982978e-01\n",
            "  1.3349471e+00  7.6276894e+00 -2.0052307e+00  1.5641968e+01\n",
            " -8.4414082e-03  9.7659674e+00  1.0375178e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2616109  -0.04375841  0.09599316 -0.24859506  2.4343133   0.03320891\n",
            "  6.8926544   8.025471   -0.76629466  8.1063     10.51023     1.9437791\n",
            "  1.5549921   9.775691   -2.4232929  17.585821    0.94467884 12.428924\n",
            " 12.304287  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.897361    0.32520223  0.10558735 -0.224494    1.7419113   0.10639527\n",
            "  5.8209414   6.4628334  -0.76892984  6.800886    8.848438    1.8744547\n",
            "  1.5275837   8.9731865  -2.011824   15.566147    1.460818   12.265997\n",
            " 10.371846  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1004992   0.28148615  0.03684347 -0.39497328  1.5625646   0.05157582\n",
            "  6.938465    7.8773046  -0.15072499  7.7899537   9.803907    2.5040824\n",
            "  1.6567185   7.528271   -2.628047   17.872541    2.4333687  14.367601\n",
            " 11.706541  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0513103   0.37437367  0.07352488 -0.2691846   1.9190351   0.08466417\n",
            "  6.7469544   7.2657323  -0.6870352   7.6105275   9.692989    3.0886335\n",
            "  1.5421177   8.965846   -2.3623774  16.874893    2.7926972  14.901508\n",
            " 11.515337  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 15 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0698388  -0.17690033  0.10287023 -0.3046711   2.1550379   0.0446352\n",
            "  7.5380173   8.259885   -0.783248    8.05732     9.959144    3.730387\n",
            "  1.6450322   7.840848   -2.2570693  17.25568     3.5750685  15.886292\n",
            " 11.731994  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 16 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.85480845  1.124018    0.10228524 -0.17894176  1.6245589   0.08486487\n",
            "  6.4442754   6.905028   -0.6902794   7.1258283   8.940855    3.4523041\n",
            "  1.627789    7.81049    -1.9856478  15.623942    3.5880916  16.296257\n",
            " 10.570933  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 17 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.77979356  2.1393304   0.07530263 -0.15179342  1.7616462   0.14068852\n",
            "  6.7108173   7.2542815  -0.58194774  7.3205934   9.297098    3.8655474\n",
            "  1.7012627   7.3416085  -2.0312035  16.517641    4.0732017  17.031055\n",
            " 11.05653   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 18 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8940787   3.0492258   0.11870093 -0.24981251  2.713686    0.09772228\n",
            "  7.388289    7.7561393  -0.74558866  7.960653   10.019888    4.4516973\n",
            "  1.9887197   7.88912    -2.3098102  17.431585    4.7643685  19.651686\n",
            " 11.874638  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 19 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.58148205  2.8316007   0.14597696 -0.19017972  2.7625291   0.07942103\n",
            "  6.125457    6.4535437  -0.7146      6.513565    8.247851    3.6684978\n",
            "  1.3273205   6.686633   -1.8742018  13.964872    4.284712   17.318035\n",
            "  9.554579  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 20 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6375116   3.5484183   0.07258254 -0.2953597   3.7627583   0.10615073\n",
            "  7.100635    7.8378024  -0.28153977  7.316577    8.982899    4.4273863\n",
            "  1.5890181   7.4268513  -1.9881588  17.307863    5.24842    17.93378\n",
            " 10.809236  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 21 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5659547   3.8363032   0.1168956  -0.23275395  4.005394    0.05399273\n",
            "  6.3285055   6.6374135  -0.6333046   6.6430235   8.280531    3.9391737\n",
            "  1.4003078   8.002732   -1.9814936  14.212816    4.7959914  18.207382\n",
            "  9.699377  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 22 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8667483   5.572253    0.07338642 -0.23411468  5.965862    0.1539772\n",
            "  8.243631    9.242679   -0.68277264  8.453191   10.63892     5.84425\n",
            "  2.133272   12.169301   -2.566642   19.375122    6.580056   23.510786\n",
            " 12.817831  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 23 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.1584582   6.768068    0.08727502 -0.26213986  7.3079324   0.05949138\n",
            "  9.362682   10.750354   -0.7306561   9.530716   11.769292    6.851038\n",
            "  2.330547   13.681204   -2.822592   20.295689    7.6795216  27.608212\n",
            " 14.141858  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 24 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0672666   6.7503247   0.15807459 -0.3258133   7.266849    0.09254111\n",
            "  8.840181   10.602139   -0.9095699   9.17762    11.247553    6.3966255\n",
            "  2.1264317  14.918922   -2.6983643  19.573618    7.517698   26.13762\n",
            " 13.439679  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 25 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-9.44036901e-01  6.15770483e+00  8.58807415e-02 -2.83600897e-01\n",
            "  6.99809265e+00 -7.16249365e-03  8.25249958e+00  1.00131035e+01\n",
            " -8.43874693e-01  8.50791359e+00  1.02225599e+01  6.09793663e+00\n",
            "  1.56619787e+00  1.51726437e+01 -2.56369686e+00  1.64993916e+01\n",
            "  6.94821405e+00  2.30999432e+01  1.21642647e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 26 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.3241483e+00  8.1573439e+00  2.1448568e-02 -3.7728718e-01\n",
            "  9.1305351e+00  6.0404222e-02  1.0340848e+01  1.2683093e+01\n",
            " -1.1330014e+00  1.0308231e+01  1.2655318e+01  7.9454103e+00\n",
            "  2.4845140e+00  2.0184446e+01 -3.4429395e+00  2.0369593e+01\n",
            "  9.0440102e+00  2.9572395e+01  1.5064712e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 27 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.4907237   9.311498    0.03302744 -0.3130461  10.563404    0.06183618\n",
            " 11.382768   14.982039   -0.914248   11.2265625  13.839646    8.896211\n",
            "  2.747158   22.67584    -3.3877282  23.308147   10.059482   32.12127\n",
            " 16.637691  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 28 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0282671   7.8888197   0.13172069 -0.25328586  9.026277    0.14833692\n",
            "  9.173198   12.366668   -1.0208894   9.427601   11.55578     7.334961\n",
            "  2.0931356  22.160091   -2.8687727  19.519817    8.411126   27.37609\n",
            " 13.766825  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 29 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.1070603   8.799445    0.10756353 -0.3115071  10.095       0.09426761\n",
            " 10.191627   13.56354    -0.98493713 10.235122   12.479057    8.321153\n",
            "  2.2328622  25.237692   -3.2953331  20.305908    9.420757   29.375635\n",
            " 14.995815  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 30 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5286472  11.338618    0.13218096 -0.4360922  13.093566    0.06951758\n",
            " 13.088738   18.203798   -0.94055426 12.803007   15.418936   10.845314\n",
            "  2.7051196  30.270813   -3.8444443  25.54864    12.234701   36.964767\n",
            " 18.61442   ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 31 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.91742694  8.931102    0.10639133 -0.20290932 10.089983    0.03700601\n",
            "  9.599301   13.667231   -0.79523236  9.724295   11.811188    7.876687\n",
            "  1.8711059  25.074003   -2.7793431  19.53841     8.9898205  28.778316\n",
            " 13.967182  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 32 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.3019792  10.203253    0.0610912  -0.42974597 12.006957    0.09439844\n",
            " 12.442951   16.962456   -0.6563726  11.192796   13.359896    9.771444\n",
            "  2.289908   29.822906   -3.271816   23.802824   10.887794   30.326668\n",
            " 16.332088  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 33 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.06005883e+00  1.03624001e+01  1.24319844e-01 -2.88574040e-01\n",
            "  1.15219793e+01  2.40039136e-02  1.23552303e+01  1.52347383e+01\n",
            " -6.89286172e-01  1.05217495e+01  1.27724457e+01  8.75463390e+00\n",
            "  2.40781927e+00  2.85774460e+01 -3.12656021e+00  2.14192524e+01\n",
            "  1.00696650e+01  3.18561573e+01  1.52633848e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 34 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-7.4610949e-01  8.8325796e+00  1.7735013e-01 -2.5050843e-01\n",
            "  1.0087299e+01 -7.7952757e-03  1.1268608e+01  1.3785084e+01\n",
            " -5.1205546e-01  9.2652416e+00  1.1159311e+01  7.4720769e+00\n",
            "  1.7326175e+00  2.6037544e+01 -2.5547810e+00  1.9600153e+01\n",
            "  8.7740097e+00  2.6301306e+01  1.3183551e+01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 35 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-9.0224302e-01  9.2014265e+00  2.1351208e-01 -2.9769748e-01\n",
            "  1.0454017e+01  4.4733277e-03  1.2181092e+01  1.4106109e+01\n",
            " -6.7435753e-01  9.4524231e+00  1.1481067e+01  7.8736839e+00\n",
            "  1.8782600e+00  2.6934114e+01 -2.5445926e+00  2.0735039e+01\n",
            "  9.1019163e+00  2.7073370e+01  1.3534594e+01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 36 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-9.6137708e-01  9.5119009e+00  1.4740628e-01 -2.5207224e-01\n",
            "  1.0987509e+01  1.0434633e-02  1.3056972e+01  1.4661101e+01\n",
            " -6.5072662e-01  9.6835756e+00  1.1777772e+01  8.1456680e+00\n",
            "  1.9091419e+00  2.7844545e+01 -2.6806998e+00  2.2271111e+01\n",
            "  9.4092922e+00  2.7549288e+01  1.3770523e+01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 37 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.31880236e+00  1.10345097e+01  1.60857275e-01 -3.26151639e-01\n",
            "  1.26232452e+01 -2.29611453e-02  1.57995405e+01  1.61235771e+01\n",
            " -1.31346548e+00  1.11427402e+01  1.35291624e+01  9.86079502e+00\n",
            "  2.26640224e+00  3.29715424e+01 -3.42469049e+00  2.51098213e+01\n",
            "  1.08412933e+01  3.20317650e+01  1.59903011e+01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 38 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.45902336e+00  1.16245584e+01  1.90805823e-01 -3.01674455e-01\n",
            "  1.32442160e+01 -1.23067675e-02  1.69636211e+01  1.70182476e+01\n",
            " -1.24527037e+00  1.14922667e+01  1.39591990e+01  9.87852669e+00\n",
            "  2.50907016e+00  3.35161552e+01 -3.31015515e+00  2.85292568e+01\n",
            "  1.14668980e+01  3.41939659e+01  1.64285126e+01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 39 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.8050929   9.840663    0.21334234 -0.2616204  11.108776    0.04778792\n",
            " 14.340479   14.924594   -0.72912747  9.6499405  11.760465    8.097868\n",
            "  1.9278227  28.785358   -2.666663   26.975962    9.602463   27.405796\n",
            " 13.851528  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 40 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-9.0973675e-01  1.0307372e+01  1.3932723e-01 -2.4344650e-01\n",
            "  1.1680428e+01  1.0270347e-02  1.5253753e+01  1.5731951e+01\n",
            " -5.7708335e-01  1.0061410e+01  1.2153123e+01  8.3798275e+00\n",
            "  2.0069950e+00  2.9054440e+01 -2.6766794e+00  2.9318794e+01\n",
            "  9.9568911e+00  2.8099211e+01  1.4321457e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 41 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-8.70642066e-01  1.02846565e+01  1.92935064e-01 -3.02051842e-01\n",
            "  1.18582745e+01 -1.51161235e-02  1.54906359e+01  1.55390301e+01\n",
            " -5.79436243e-01  1.00128183e+01  1.21482563e+01  8.73967457e+00\n",
            "  1.88678789e+00  2.93408813e+01 -2.77967000e+00  3.04818878e+01\n",
            "  9.99034595e+00  2.78522434e+01  1.45158892e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 42 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.1979946e+00  1.2305188e+01  1.1483493e-01 -3.0954581e-01\n",
            "  1.4011668e+01 -2.0241082e-02  1.8928411e+01  1.8063786e+01\n",
            " -1.0288913e+00  1.1749062e+01  1.4137197e+01  1.0478706e+01\n",
            "  2.1465290e+00  3.4291000e+01 -3.5450506e+00  3.4874420e+01\n",
            "  1.1807769e+01  3.2422455e+01  1.6913074e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 43 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.3807114  12.12441     0.11733736 -0.34792858 13.882668    0.11216868\n",
            " 18.711395   17.63655    -0.8677324  11.290781   13.756128   10.331379\n",
            "  2.9624128  32.698082   -3.4743218  35.800327   11.601243   32.205883\n",
            " 16.48245   ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 44 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.9131953  11.000952    0.19765782 -0.29359034 12.364253    0.03452731\n",
            " 16.689693   16.086355   -0.7024852  10.153483   12.378602    8.953116\n",
            "  2.0609033  29.124643   -2.8914623  33.257744   10.280101   28.412151\n",
            " 14.693189  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 45 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-9.65055287e-01  1.05992727e+01  2.35689446e-01 -3.16603392e-01\n",
            "  1.22189617e+01 -1.64187811e-02  1.67206898e+01  1.58181334e+01\n",
            " -7.10396349e-01  1.00546465e+01  1.22478046e+01  9.91251469e+00\n",
            "  1.99143434e+00  2.86536598e+01 -2.72716880e+00  3.33354454e+01\n",
            "  1.01949453e+01  2.78451347e+01  1.46242628e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 46 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.3226179e+00  1.2087284e+01  9.3545020e-02 -3.1838542e-01\n",
            "  1.3814035e+01 -3.3895601e-02  1.9809469e+01  1.7873432e+01\n",
            " -1.1207862e+00  1.1683501e+01  1.3863748e+01  1.2205547e+01\n",
            "  2.1104760e+00  3.2695015e+01 -3.3722477e+00  3.6921959e+01\n",
            "  1.1755392e+01  3.1344215e+01  1.6591890e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 47 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2830652e+00  1.1637009e+01  1.5239280e-01 -3.1831908e-01\n",
            "  1.3575031e+01 -4.9368981e-03  1.9136322e+01  1.7192131e+01\n",
            " -1.0527288e+00  1.1057670e+01  1.3202582e+01  1.2672653e+01\n",
            "  2.4238529e+00  3.0344835e+01 -3.3146257e+00  3.5447998e+01\n",
            "  1.1080009e+01  3.0914614e+01  1.5581576e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 48 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-8.2856154e-01  1.0519349e+01  1.9748360e-01 -2.6546344e-01\n",
            "  1.2079943e+01 -2.6285788e-02  1.6894264e+01  1.5897005e+01\n",
            " -6.1531258e-01  9.9064531e+00  1.1833344e+01  1.1796698e+01\n",
            "  1.9097785e+00  2.7462885e+01 -2.7309513e+00  3.3982948e+01\n",
            "  9.9785738e+00  2.7118101e+01  1.4062481e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 49 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.8902605  10.8935795   0.2383693  -0.29896063 12.494717    0.04033906\n",
            " 17.707518   16.271887   -0.74533904 10.1671505  12.481589   12.993744\n",
            "  2.0083718  28.700264   -2.815346   35.77538    10.51849    28.446068\n",
            " 14.842998  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 50 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.2277181  13.078032    0.10597765 -0.30871347 14.972805    0.1672542\n",
            " 21.966488   19.596325   -0.9284614  12.173128   14.875248   16.518368\n",
            "  2.6411524  34.643696   -3.5410416  42.853424   12.842137   33.491016\n",
            " 17.792673  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 51 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.5126852  12.656529    0.14644031 -0.3311361  14.593749    0.12100584\n",
            " 21.070513   18.173594   -1.1337137  11.699344   14.188399   16.653778\n",
            "  3.030778   31.939035   -3.4646907  40.221462   12.21707    34.293552\n",
            " 16.813055  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 52 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.8437999  11.130502    0.18752892 -0.2632423  12.79569    -0.04386602\n",
            " 18.202782   16.815474   -0.54648674 10.3620405  12.342357   14.651587\n",
            "  1.9497074  28.0244     -2.830091   36.804916   10.486359   29.299244\n",
            " 14.641404  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 53 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-8.8090843e-01  1.1695948e+01  1.7469697e-01 -3.0245075e-01\n",
            "  1.3398084e+01  1.0738033e-02  1.8989777e+01  1.7400354e+01\n",
            " -5.8371633e-01  1.0762854e+01  1.2987191e+01  1.5891318e+01\n",
            "  1.9867657e+00  2.9359190e+01 -2.9702041e+00  3.9038734e+01\n",
            "  1.1048021e+01  3.0776033e+01  1.5599215e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 54 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.35934973e+00  1.35770111e+01  3.76486368e-02 -3.58483762e-01\n",
            "  1.54238272e+01  1.10627875e-01  2.27946873e+01  1.99549370e+01\n",
            " -9.78098869e-01  1.24240313e+01  1.50650845e+01  1.91535053e+01\n",
            "  2.67835617e+00  3.42613029e+01 -3.63458872e+00  4.49421234e+01\n",
            "  1.31612396e+01  3.58267899e+01  1.81268997e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 55 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.3960505  12.870171    0.17522277 -0.3309033  14.9261465   0.09439641\n",
            " 21.760803   18.628443   -1.0413663  11.809909   14.28089    18.856789\n",
            "  2.9750993  31.816559   -3.556884   41.68137    12.353394   36.201305\n",
            " 16.920355  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 56 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.9488618  11.521756    0.22604305 -0.2952196  13.3245735  -0.05791732\n",
            " 19.222929   17.386427   -0.7031538  10.742568   12.713111   16.748384\n",
            "  2.0404587  28.697233   -2.8772516  38.740643   10.891171   32.25448\n",
            " 15.156909  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 57 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.1451025e+00  1.2293892e+01  1.5286365e-01 -2.9379493e-01\n",
            "  1.4324494e+01 -3.6051705e-02  2.0861540e+01  1.8467836e+01\n",
            " -9.0999502e-01  1.1496764e+01  1.3800280e+01  1.8201641e+01\n",
            "  2.1174617e+00  3.0419712e+01 -3.1444924e+00  4.1914062e+01\n",
            "  1.1852326e+01  3.4942974e+01  1.6393887e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 58 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.4135956  14.169062    0.11230235 -0.38132    16.399733    0.12477168\n",
            " 24.595762   21.352726   -0.89565635 13.109498   15.853216   21.622854\n",
            "  2.766342   35.900955   -3.6898816  48.275967   13.82894    39.295628\n",
            " 18.99537   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 59 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.3683407e+00  1.2891948e+01  1.7257173e-01 -3.3406755e-01\n",
            "  1.4995138e+01  3.1921040e-02  2.1835051e+01  1.8723166e+01\n",
            " -9.2753029e-01  1.1747624e+01  1.4109127e+01  1.9808216e+01\n",
            "  2.7665591e+00  3.0654316e+01 -3.4752333e+00  4.2294327e+01\n",
            "  1.2166296e+01  3.7590458e+01  1.6653322e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 60 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-9.75491762e-01  1.17277403e+01  1.74290627e-01 -2.29703769e-01\n",
            "  1.33511190e+01  3.19720507e-02  1.98237820e+01  1.76599159e+01\n",
            " -8.09454024e-01  1.09553356e+01  1.29170885e+01  1.78890266e+01\n",
            "  2.15908694e+00  2.87534847e+01 -2.89398909e+00  3.97050247e+01\n",
            "  1.12227907e+01  3.43400803e+01  1.54007921e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 61 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-8.69707704e-01  1.23209362e+01  2.31945992e-01 -2.83638895e-01\n",
            "  1.41387081e+01 -1.59729680e-03  2.05106392e+01  1.84023590e+01\n",
            " -6.79537356e-01  1.12666311e+01  1.35439463e+01  1.90071964e+01\n",
            "  2.05400896e+00  3.04756756e+01 -3.15204787e+00  4.19174042e+01\n",
            "  1.16709795e+01  3.56043587e+01  1.63009300e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 62 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.4776510e+00  1.4569022e+01  3.1241752e-02 -4.0725029e-01\n",
            "  1.6636148e+01  1.4965843e-01  2.4986555e+01  2.1627943e+01\n",
            " -9.4843024e-01  1.3270657e+01  1.6071594e+01  2.3034641e+01\n",
            "  2.8314755e+00  3.6199158e+01 -3.7972496e+00  4.9734081e+01\n",
            "  1.4084470e+01  4.0583923e+01  1.9364847e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 63 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.63213515e+00  1.34269180e+01  1.80917919e-01 -3.84821117e-01\n",
            "  1.57387705e+01 -3.22347172e-02  2.33269482e+01  1.95555477e+01\n",
            " -1.09369898e+00  1.24160509e+01  1.47552528e+01  2.18256493e+01\n",
            "  2.89365172e+00  3.18106976e+01 -3.62321877e+00  4.40877800e+01\n",
            "  1.28178473e+01  4.01227379e+01  1.74717293e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 64 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.0061034  12.5500965   0.23388964 -0.25959742 14.318438   -0.04573785\n",
            " 20.938381   18.52767    -0.7910123  11.459755   13.5147295  19.738085\n",
            "  2.1759024  30.16694    -3.1777549  42.217808   11.64727    36.516663\n",
            " 16.179705  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 65 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-8.9009804e-01  1.1978753e+01  2.0670949e-01 -3.0802542e-01\n",
            "  1.3976479e+01 -2.8443016e-02  2.0388109e+01  1.8235685e+01\n",
            " -5.4511392e-01  1.1128352e+01  1.3299713e+01  1.9272911e+01\n",
            "  2.0187039e+00  2.9557707e+01 -3.0358777e+00  4.1753998e+01\n",
            "  1.1425813e+01  3.5437420e+01  1.5919222e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 66 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.423719   14.337278    0.07839122 -0.42873475 16.607988    0.11583506\n",
            " 25.13921    21.58033    -0.921137   13.165813   15.93806    23.663038\n",
            "  2.8047001  35.885273   -3.7728298  49.572487   13.966985   40.37571\n",
            " 19.09885   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 67 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.3929574e+00  1.3063624e+01  1.9558206e-01 -3.3903781e-01\n",
            "  1.5037017e+01 -6.4739250e-03  2.2051027e+01  1.8856043e+01\n",
            " -9.0275949e-01  1.1778311e+01  1.4022586e+01  2.1278875e+01\n",
            "  2.6943419e+00  3.0051811e+01 -3.5617156e+00  4.2873634e+01\n",
            "  1.2185481e+01  3.7524593e+01  1.6618368e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 68 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.8998373  12.075963    0.18923925 -0.28859392 13.940847   -0.04462729\n",
            " 20.48111    18.357254   -0.54295355 11.135086   13.1762085  19.646526\n",
            "  2.0512984  29.04068    -3.005997   41.66418    11.374678   35.261635\n",
            " 15.635322  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 69 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.885914   12.522477    0.19339123 -0.2889501  14.419662   -0.04684626\n",
            " 20.933315   18.769632   -0.48119393 11.398416   13.605387   20.295444\n",
            "  2.0248544  30.091206   -3.155774   43.11666    11.732325   36.107613\n",
            " 16.37062   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 70 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.4980444e+00  1.4734356e+01  4.9311448e-02 -3.9856246e-01\n",
            "  1.6976841e+01  1.9016658e-01  2.5906946e+01  2.2032539e+01\n",
            " -1.1368681e+00  1.3554555e+01  1.6419401e+01  2.4651226e+01\n",
            "  2.9531682e+00  3.6791756e+01 -3.7777581e+00  5.0823822e+01\n",
            "  1.4442704e+01  4.1232613e+01  1.9648037e+01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 71 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.5124338  13.510953    0.16780707 -0.33444348 15.612483    0.08516928\n",
            " 23.27366    19.524895   -1.1373168  12.271184   14.773485   22.45053\n",
            "  2.9200196  31.741693   -3.5379088  45.047287   12.88265    39.00938\n",
            " 17.43163   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 72 reward=2 new_state=[1 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-8.9556766e-01  1.2384950e+01  2.2463113e-01 -2.8003576e-01\n",
            "  1.4093768e+01 -1.7987340e-03  2.0804541e+01  1.8543339e+01\n",
            "  5.9183973e-01  1.1256743e+01  1.3380772e+01  2.0309416e+01\n",
            "  2.1008341e+00  2.9783974e+01 -3.1197124e+00  4.2324081e+01\n",
            "  1.1647083e+01  3.6268967e+01  1.5998365e+01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 73 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.1246512e+00  1.2109115e+01  2.1274368e-01 -3.5985684e-01\n",
            "  1.4060493e+01  1.9847717e-02  2.0897850e+01  1.8306877e+01\n",
            "  1.7432580e+00  1.1165252e+01  1.3376733e+01  2.0176065e+01\n",
            "  2.0832393e+00  2.9489607e+01 -2.9112027e+00  4.2305008e+01\n",
            "  1.1664455e+01  3.5946236e+01  1.5988611e+01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 74 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.6901571e+00  1.4086933e+01  1.2389337e-01 -3.9430064e-01\n",
            "  1.6299242e+01 -2.6515566e-02  2.5313324e+01  2.1327089e+01\n",
            "  3.5918071e+00  1.3309512e+01  1.5872923e+01  2.3953825e+01\n",
            "  2.4633291e+00  3.4015373e+01 -2.4586310e+00  4.8012695e+01\n",
            "  1.4093176e+01  4.0267654e+01  1.8567959e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 75 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.837864   14.655147    0.15464728 -0.5787966  17.58713     0.08113569\n",
            " 26.974901   23.463175    4.621893   13.716509   16.140388   25.393126\n",
            "  4.526604   35.282253   -0.63021857 51.290977   14.4769945  39.40012\n",
            " 19.308315  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 76 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.4023495  13.015497    0.12165759 -0.32331753 15.292547    0.12547588\n",
            " 22.570055   19.83094     5.249845   12.100711   14.30987    21.872684\n",
            "  5.039732   33.3155      0.6789611  45.51487    12.678328   35.863365\n",
            " 17.230782  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 77 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.54883897e+00  1.34284782e+01  1.35899544e-01 -3.57879817e-01\n",
            "  1.62913380e+01  2.50781570e-02  2.44381199e+01  2.09925804e+01\n",
            "  5.80175972e+00  1.27640257e+01  1.52961636e+01  2.33684196e+01\n",
            "  6.37494755e+00  3.39095726e+01  1.72654390e+00  4.69360542e+01\n",
            "  1.31898775e+01  3.79968796e+01  1.79388237e+01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 78 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.346575   14.15796     0.1217807  -0.23459037 16.216196    0.12980257\n",
            " 23.552988   20.448889    6.866878   12.607223   15.260359   23.33534\n",
            "  7.751324   34.437206    2.7083504  47.78718    13.113016   38.12736\n",
            " 18.235748  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 79 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8252683  14.6898575   0.09914058 -0.41759798 17.971237    0.09193008\n",
            " 27.436695   23.98294     8.514135   14.158084   16.531551   27.359446\n",
            "  9.949069   36.576298    4.273629   52.56107    14.720519   39.380684\n",
            " 19.761768  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 80 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.4768292 12.917132   0.097459  -0.4035173 15.271995   0.1083246\n",
            " 22.888138  19.54941    7.808631  12.029316  14.340132  23.631357\n",
            "  9.058037  31.598711   4.281273  44.09644   12.5001135 36.24871\n",
            " 16.788862 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 81 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.49475372e+00  1.30791359e+01  9.38437954e-02 -3.34498465e-01\n",
            "  1.56467285e+01  8.59971624e-03  2.34841881e+01  2.01577568e+01\n",
            "  8.41625881e+00  1.24227591e+01  1.47061729e+01  2.48979034e+01\n",
            "  9.84661293e+00  3.22998047e+01  5.24985123e+00  4.52363777e+01\n",
            "  1.27836037e+01  3.77698746e+01  1.73162098e+01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 82 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8648334  14.867041    0.16839188 -0.4440825  18.006273    0.08547617\n",
            " 27.175438   22.419178    9.932249   13.922517   16.80124    29.888481\n",
            " 12.567091   36.840958    6.3290687  50.050686   14.537784   41.59267\n",
            " 19.651535  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 83 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8335555  14.512466    0.11180925 -0.40380785 16.835503    1.7240263\n",
            " 25.465069   21.797066   10.60744    13.070533   15.671366   28.59084\n",
            " 12.903445   33.034035    7.0135627  48.86115    13.99437    39.33774\n",
            " 18.582783  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 84 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2117344e+00  1.3414929e+01  3.7984967e-02 -3.3859035e-01\n",
            "  1.5727213e+01  3.1193933e+00  2.3567207e+01  2.1272095e+01\n",
            "  1.0582343e+01  1.2458912e+01  1.4892240e+01  2.7254057e+01\n",
            "  1.2266327e+01  3.2299595e+01  7.1169634e+00  4.7316570e+01\n",
            "  1.2793282e+01  3.5482513e+01  1.7705759e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 85 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.60092    14.269982    0.12630281 -0.23540942 16.806068    4.507025\n",
            " 25.669409   21.669416   11.011043   13.391857   15.783438   29.37386\n",
            " 13.742582   33.979324    7.858317   48.19912    13.938667   39.242523\n",
            " 18.693314  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 86 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.008021   15.460342    0.08970599 -0.4300836  18.099834    5.979912\n",
            " 27.951683   23.149633   13.176446   14.524328   16.986366   32.640976\n",
            " 15.248683   35.65747     8.88765    51.145855   15.292594   42.872776\n",
            " 20.099972  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 87 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.4490068  14.001089    0.14324507 -0.36771604 16.352392    6.471134\n",
            " 24.076916   20.835684   11.595721   12.632928   15.150121   29.399027\n",
            " 14.312034   32.9793      8.48999    47.765812   13.431024   37.345963\n",
            " 17.918755  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 88 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.5315193  15.089314    0.09828936 -0.41863984 18.25306     8.328\n",
            " 26.82629    24.068207   13.357037   14.195621   16.998709   33.458717\n",
            " 15.906114   36.69215    10.173917   53.32544    14.574793   39.66085\n",
            " 20.189083  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 89 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7361094  16.52307     0.13151346 -0.36715794 19.639025    9.951433\n",
            " 29.529102   25.223152   14.62746    15.4505205  18.298323   36.134346\n",
            " 17.729567   40.63904    10.942739   56.75081    15.988495   42.407993\n",
            " 21.63436   ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 90 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.763863   14.470693    0.076837   -0.43572786 16.715862    9.161542\n",
            " 24.932587   20.827318   12.400343   12.9782715  15.535831   31.066456\n",
            " 15.771603   32.581642    9.495728   47.509968   13.803504   39.07137\n",
            " 18.377037  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 91 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.5729941e+00  1.4520956e+01  4.8945505e-02 -3.5251525e-01\n",
            "  1.7371265e+01  1.0352967e+01  2.5749193e+01  2.2873564e+01\n",
            "  1.3458100e+01  1.3638685e+01  1.6204435e+01  3.2771454e+01\n",
            "  1.6436869e+01  3.5508793e+01  1.0724623e+01  5.1645760e+01\n",
            "  1.4029714e+01  3.7127686e+01  1.9327696e+01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 92 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.6156162  13.995694    1.7677789  -0.40940908 16.51466    10.553436\n",
            " 25.466986   21.518654   13.312979   13.179731   15.77389    32.34066\n",
            " 16.304724   34.37133    10.349009   48.04339    14.034459   37.835823\n",
            " 18.605413  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 93 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.6756094 14.609333   3.2331192 -0.355305  16.867561  11.03325\n",
            " 25.429005  21.316956  13.170357  13.219042  15.697091  32.355843\n",
            " 16.472633  31.983511  10.434842  47.72754   13.936768  39.579803\n",
            " 18.433123 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 94 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7127179 14.901229   4.7310815 -0.4458356 17.451584  12.310041\n",
            " 26.250221  23.265749  14.17131   13.625772  16.14107   34.12137\n",
            " 17.626541  34.791832  11.409878  51.071957  14.191082  36.33134\n",
            " 19.24728  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 95 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.5702066  14.300499    5.884931   -0.39694476 17.520731   12.690249\n",
            " 26.297554   22.837467   14.156699   13.521225   16.302498   34.77535\n",
            " 17.477482   36.13701    11.390894   51.141174   14.241934   37.433754\n",
            " 19.150892  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 96 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2538159  14.658437    6.988483   -0.23563722 16.88243    12.801835\n",
            " 25.720879   22.107515   14.038397   13.422667   15.790252   32.49269\n",
            " 17.329603   32.392723   11.440816   49.277153   14.101339   39.57978\n",
            " 18.531286  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 97 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7609973  16.19787     8.859477   -0.44099602 19.141163   14.9076185\n",
            " 28.780762   24.895834   16.051134   14.768063   17.654587   37.907917\n",
            " 19.561947   36.703754   12.903827   54.338833   15.398744   41.130596\n",
            " 21.035393  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 98 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.6612325 15.021719   9.062629  -0.4939341 17.624252  14.043651\n",
            " 26.51269   22.832811  14.957946  13.986908  16.745523  35.3345\n",
            " 18.48478   37.716976  11.871276  51.87064   14.5957775 38.175472\n",
            " 19.742004 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 99 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2185805  14.6267      9.446017   -0.19697852 16.657303   13.598486\n",
            " 24.881306   21.476126   13.93814    13.139197   15.594118   32.648243\n",
            " 17.242573   33.544647   11.46609    48.698517   13.828494   40.02716\n",
            " 18.477528  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 100 reward=-1 new_state=[1 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.0953696 16.924479  11.783919  -0.509904  19.39104   16.05002\n",
            " 29.598658  25.067059  16.641497  15.314139  18.158926  38.665337\n",
            " 20.57277   38.87968   13.523615  56.519226  16.21559   42.748875\n",
            " 21.982182 ]\n",
            "Epsilon reduced to 0.07290000000000002\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.0395322 16.039503  12.273035  -0.5775457 18.701494  16.21136\n",
            " 29.254581  25.353468  15.547504  15.160815  17.508335  37.353043\n",
            " 20.48284   36.042103  13.70528   55.94619   16.048082  42.370125\n",
            " 21.10253  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 2 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.05778    16.298254   12.270577   -0.56540275 18.6306     15.7141905\n",
            " 27.549433   23.32357    14.577224   14.803806   17.125301   37.329628\n",
            " 19.80604    37.671852   12.936315   53.220665   15.2770405  42.45246\n",
            " 20.828445  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 3 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7013905  14.610563   12.307982   -0.43856132 16.897635   15.2722645\n",
            " 26.291784   23.131163   12.969163   13.801361   15.909535   34.90546\n",
            " 18.743467   35.89943    12.564175   52.087234   14.513967   37.78413\n",
            " 19.139263  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 4 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.5804496  14.704441   12.471164   -0.31821868 16.82452    15.211151\n",
            " 25.5668     22.31232    12.3347435  13.744304   16.355585   35.321568\n",
            " 18.418154   37.774826   12.518664   51.13523    14.081354   38.409386\n",
            " 19.450378  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.5578705 13.8875    12.262431  -0.4395263 15.853878  14.576341\n",
            " 24.223671  21.222118  11.543813  12.900215  15.285257  33.808067\n",
            " 17.667505  36.456543  12.016441  48.848675  13.621812  38.100796\n",
            " 18.26594  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.6632701 14.100427  12.823813  -0.3773185 15.972911  14.930663\n",
            " 24.496101  21.252281  11.505418  14.611657  15.434417  34.481594\n",
            " 18.101488  38.014843  12.275006  49.201614  13.853168  38.972237\n",
            " 18.525656 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.5299677  13.459231   12.381242   -0.35322925 15.054291   13.997304\n",
            " 22.978102   20.011284   10.199127   15.224888   14.459537   32.996372\n",
            " 16.765547   35.82873    11.438225   45.90177    12.893023   38.204823\n",
            " 17.315897  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0167499  12.943773   12.185336   -0.39305934 14.852999   13.475999\n",
            " 22.207466   19.810612   10.183834   15.632339   13.907586   32.072704\n",
            " 16.102835   35.324116   10.791381   45.12264    12.317435   36.585186\n",
            " 16.643871  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.3319647  14.170924   13.821902   -0.44484958 16.224897   15.346837\n",
            " 24.625101   22.056543   11.371143   18.465096   15.343167   35.550373\n",
            " 18.332499   40.61189    12.376358   50.632698   13.742504   38.6717\n",
            " 18.675947  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9668062  15.0221405  14.866634   -0.37167963 17.321806   16.259762\n",
            " 26.122538   22.400757   11.060829   20.89136    16.372097   38.37286\n",
            " 19.316847   42.837852   13.149812   51.0567     14.702739   41.934753\n",
            " 19.520067  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.670011   14.653415   14.79122    -0.58665854 17.10463    15.988421\n",
            " 25.59144    22.519812   11.575193   21.296486   16.093105   37.83827\n",
            " 19.116726   42.89581    12.714729   51.360676   14.402278   41.44133\n",
            " 19.307693  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.4096632  13.7858     14.242277   -0.43441537 16.441343   15.243542\n",
            " 24.49606    21.671364   10.58796    21.021841   15.257976   36.162365\n",
            " 18.0731     41.890636   12.091854   49.487602   13.477771   38.578335\n",
            " 18.260439  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7459649  14.077112   14.656065   -0.37759495 16.389967   15.628078\n",
            " 24.897984   21.25981     9.958421   22.205097   15.4936     36.86078\n",
            " 18.332653   41.769146   12.292156   48.54862    13.636026   39.59087\n",
            " 18.445126  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8608912  16.099154   16.796597   -0.42020947 18.384472   17.837421\n",
            " 28.1111     24.477482   11.341342   25.804918   17.500168   41.463257\n",
            " 21.086641   47.004753   14.338436   55.933266   15.681161   43.97499\n",
            " 21.027397  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.6596606  14.556084   15.3722     -0.41632333 17.04855    16.130117\n",
            " 25.41098    21.984634   10.180567   24.137108   15.936809   38.2875\n",
            " 18.788069   42.867115   12.631319   50.158306   14.187896   41.8269\n",
            " 18.884     ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.1222093  15.726621   16.848753   -0.59825015 18.217476   17.645914\n",
            " 27.669952   23.844425   11.124704   26.919252   17.330843   41.820744\n",
            " 20.741016   45.501835   14.015172   54.243763   15.592801   44.63211\n",
            " 20.851637  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8939105  15.711099   16.844408   -0.42624798 18.135191   17.685966\n",
            " 27.464346   23.630424   10.557085   27.070663   17.047112   41.168266\n",
            " 20.480492   46.142967   13.867706   54.570503   15.253845   43.246418\n",
            " 20.732878  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.5824275 14.621366  15.702212  -0.393475  16.666288  16.18894\n",
            " 25.235619  21.852108   9.429643  25.591528  15.911026  37.86634\n",
            " 18.6681    40.664825  12.75601   49.997902  14.064991  42.300125\n",
            " 18.786072 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.1946461  15.414515   17.001574   -0.45973372 17.950773   17.556002\n",
            " 27.335423   23.699427   10.725947   27.91779    16.96709    41.38232\n",
            " 20.624058   43.596195   13.927733   53.807022   15.304489   42.613716\n",
            " 20.578625  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.5510027 14.47709   15.761566  -0.5634973 16.825546  15.936797\n",
            " 25.075947  21.93352    9.736763  26.15465   15.688537  38.622204\n",
            " 18.578152  38.762493  12.432799  49.307083  14.0058155 42.173138\n",
            " 18.79305  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.0152416  15.229421   16.91501    -0.42182234 17.612003   17.450764\n",
            " 26.952766   23.045      10.018812   28.187304   16.891151   40.91984\n",
            " 20.322527   41.575027   13.694208   52.724796   14.90774    41.776894\n",
            " 20.074406  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.838611   15.726994   17.448582   -0.51521266 18.017052   17.773829\n",
            " 27.633076   24.010141   10.83092    29.223305   17.204264   41.369328\n",
            " 20.883648   41.706123   14.219968   55.055855   15.516211   44.322582\n",
            " 20.790848  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.0248249  14.870742   16.426765   -0.47046238 17.275337   16.744522\n",
            " 25.915201   21.995108    9.372203   27.989075   16.128353   39.662006\n",
            " 19.36286    38.9761     12.978006   50.173454   14.418193   43.01378\n",
            " 19.263567  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.1177692 16.277807  17.968512  -0.5435976 18.682817  18.362555\n",
            " 28.15901   24.641808  10.42933   30.72264   17.453196  43.58198\n",
            " 21.211632  42.569542  14.419556  55.837524  15.798524  44.483738\n",
            " 21.364521 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.3241271  13.804982   15.519304   -0.45908314 15.982243   15.584684\n",
            " 24.28573    21.027416    8.959472   26.384176   15.189032   37.245567\n",
            " 17.937593   37.158424   12.138271   47.814713   13.314225   39.62454\n",
            " 17.9287    ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9344361  15.924794   17.90975    -0.48586717 17.955294   18.108034\n",
            " 27.87328    24.000496   10.475866   30.620756   17.509445   42.368984\n",
            " 21.189598   41.634895   14.463027   54.833458   15.778772   44.821095\n",
            " 20.890335  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8451947  15.608118   17.391338   -0.56622905 18.050852   17.753117\n",
            " 26.971832   23.585493   10.151262   30.128708   17.05529    41.7862\n",
            " 20.148796   41.666294   13.807988   53.988758   15.014571   43.2505\n",
            " 20.47028   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7696556  14.683195   16.405596   -0.56145895 16.809416   16.410173\n",
            " 25.287987   21.844143    9.058493   28.551287   15.967349   39.372047\n",
            " 18.869156   38.959885   12.705491   49.46259    14.166226   42.744442\n",
            " 19.138443  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9031843 16.165424  18.014467  -0.4106362 18.235365  18.237778\n",
            " 27.794338  24.37705   10.179759  31.159264  17.339186  42.917088\n",
            " 21.295582  42.543663  14.3515005 55.82751   15.60647   43.679993\n",
            " 20.98546  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.4771824  14.117812   15.993085   -0.35697585 16.35574    15.939281\n",
            " 24.748716   21.621553    9.037305   27.726328   15.350838   37.91225\n",
            " 18.385124   38.10112    12.524585   48.668583   13.583686   40.52458\n",
            " 18.052258  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.0710652  15.490945   17.565071   -0.49232337 17.91597    17.737791\n",
            " 27.03198    23.382689    9.856177   30.76054    16.915302   42.17873\n",
            " 20.455101   42.119358   13.888752   53.2762     15.241311   43.467415\n",
            " 20.286762  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8231269 15.119228  17.24866   -0.5953251 17.813032  17.27748\n",
            " 26.468763  23.137163  10.218942  30.119108  16.59336   41.158573\n",
            " 19.964016  42.22893   13.51832   52.65562   14.67365   42.82461\n",
            " 19.941435 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.6381398 14.560108  16.559252  -0.4816774 16.531683  16.548159\n",
            " 25.553734  21.720697   9.21199   28.921896  15.970344  39.113457\n",
            " 19.261166  40.504185  12.894464  49.62601   14.138718  41.67753\n",
            " 18.954369 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 1 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9567207  16.741581   19.048199   -0.55879915 18.945856   19.118187\n",
            " 29.387018   25.370068   10.702158   33.266563   18.192314   44.681187\n",
            " 22.375061   46.10492    15.204441   57.381077   16.27923    45.87751\n",
            " 21.982412  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8554711  14.520504   16.6232     -0.44130796 16.96712    16.626783\n",
            " 25.44758    21.949692    8.869811   29.498236   15.89856    39.72891\n",
            " 18.909252   41.043877   12.951989   49.521084   14.299847   43.2187\n",
            " 19.033083  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8009502 16.122812  19.183512  -0.5566193 19.207237  19.356611\n",
            " 29.346308  26.549232  12.214282  33.276283  17.714449  45.63609\n",
            " 21.923542  46.41724   15.381726  59.02515   16.414005  44.920147\n",
            " 21.639458 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7181346  15.170722   17.093838   -0.44776666 17.46343    17.260153\n",
            " 26.168043   23.019276   11.212039   30.538363   16.589025   41.033497\n",
            " 19.63938    45.271587   13.408652   53.020157   14.614016   41.596672\n",
            " 19.841496  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7947248 16.221966  19.13966   -0.5141213 18.823542  19.190525\n",
            " 29.278137  26.60802   13.780805  33.494938  17.656479  46.097286\n",
            " 21.709581  47.91866   15.102497  58.90211   16.207108  43.35396\n",
            " 21.412428 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7960441  16.142363   18.604906   -0.41406044 18.681015   18.682455\n",
            " 28.451769   24.828001   14.629119   32.72859    17.608229   43.814037\n",
            " 21.521696   48.70595    14.807561   56.929253   15.901045   44.262897\n",
            " 21.477852  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7616535  15.927598   19.16404    -0.42366165 18.837767   19.208773\n",
            " 29.363375   26.394753   15.673518   33.72461    17.756868   46.083267\n",
            " 21.720213   48.455864   15.164858   58.386353   16.160803   43.4796\n",
            " 21.285526  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8896202  15.6055     18.137009   -0.41721433 18.133476   18.155455\n",
            " 27.422834   23.894669   15.42953    31.847303   17.143085   42.375072\n",
            " 20.776735   47.887478   14.2421465  55.040936   17.160095   43.753635\n",
            " 20.704935  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7978823  16.731424   19.823557   -0.49321213 19.570208   19.841259\n",
            " 30.154606   27.287445   18.051386   34.474655   18.130352   46.941418\n",
            " 22.712341   49.528503   15.78029    60.691654   20.725687   45.600445\n",
            " 22.319859  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8467667  15.908986   18.328125   -0.41994035 18.338835   18.349152\n",
            " 27.881178   24.154022   16.798073   32.76917    17.513254   43.601532\n",
            " 20.959894   48.642044   14.3894     55.441338   20.584486   43.6889\n",
            " 20.901546  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7851777 16.424416  19.770367  -0.5199827 19.380371  19.852118\n",
            " 30.275604  27.312658  19.51013   34.503014  18.12737   46.904842\n",
            " 22.736347  49.13692   15.675988  60.349884  23.87346   44.99359\n",
            " 22.16675  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 45 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7667271 15.782456  18.23749   -0.3953731 18.150747  18.351267\n",
            " 27.6801    24.067604  17.637762  32.63337   17.493889  43.320606\n",
            " 20.8323    48.121433  14.3157    55.02367   23.199928  43.580334\n",
            " 20.762737 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8364049 17.215515  19.866804  -0.4090422 19.90235   19.983519\n",
            " 30.373775  26.652946  19.836681  36.008934  19.037737  47.869686\n",
            " 22.431648  51.49255   15.661155  60.39251   26.102999  45.991936\n",
            " 22.807667 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9066987  16.730696   19.327883   -0.38083744 19.407322   19.385445\n",
            " 29.280012   25.209179   19.833437   34.465134   18.357252   45.330055\n",
            " 22.04993    50.143814   15.348549   58.303738   26.819956   46.426834\n",
            " 22.3206    ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.859429   16.961216   20.1617     -0.54698277 19.773355   20.183268\n",
            " 30.698605   27.92316    20.92521    35.589855   18.477215   48.47938\n",
            " 22.639957   48.11765    15.905586   61.670776   28.719515   45.70778\n",
            " 22.430502  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8407557 15.950963  18.295727  -0.3748964 18.498373  18.402088\n",
            " 27.797964  24.33518   19.404095  32.921387  17.535496  43.388992\n",
            " 20.95902   47.028996  14.336186  55.84445   26.950565  43.2972\n",
            " 21.058735 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7325846  16.42256    19.744225   -0.49732298 19.36286    19.792017\n",
            " 30.059895   27.255238   21.828688   34.49413    18.042604   46.7566\n",
            " 22.55417    45.200638   15.692282   60.36818    30.113577   45.04667\n",
            " 22.052309  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 51 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.778501   15.699965   18.12259    -0.41350862 18.120552   18.205732\n",
            " 27.518238   24.042067   19.624382   32.54608    17.356874   43.143093\n",
            " 20.683947   45.22735    14.185668   54.94638    28.205957   43.093422\n",
            " 20.613968  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 52 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8994769  17.27568    19.976873   -0.40390006 19.958185   19.974724\n",
            " 30.463573   26.603947   21.956617   36.210136   19.06785    47.919865\n",
            " 22.641182   48.72538    15.711577   60.14128    31.303959   46.121502\n",
            " 22.858812  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 53 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9937137  16.22088    18.758085   -0.41674015 18.858597   18.744894\n",
            " 28.565157   24.475197   21.071842   33.56628    17.890387   44.06263\n",
            " 21.628515   47.158623   14.728236   56.550934   30.509274   44.913612\n",
            " 21.668047  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 54 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8877198 17.087265  20.451025  -0.5427347 19.827677  20.4368\n",
            " 31.163748  28.089693  22.935259  36.131477  18.733358  49.045906\n",
            " 23.127247  46.67385   16.036507  61.600822  33.3905    46.068424\n",
            " 22.69988  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 55 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9065473 16.50488   19.02833   -0.3901764 19.12124   19.06758\n",
            " 28.861656  24.991508  21.342485  34.251106  18.136854  44.832035\n",
            " 21.791151  47.89437   14.976628  57.051136  31.650576  45.649967\n",
            " 21.857847 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 56 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8159184  17.344784   20.11122    -0.38143966 19.938442   20.171087\n",
            " 30.619963   26.751085   22.817049   36.46652    19.21595    48.257057\n",
            " 22.738316   49.145584   15.804966   60.365494   33.598846   46.403095\n",
            " 22.968866  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 57 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7164415  16.04095    18.212727   -0.40200517 18.479847   18.32088\n",
            " 27.58754    24.205097   20.95808    32.64695    17.421064   43.23709\n",
            " 20.864017   45.80237    14.239807   55.907932   31.540226   44.601788\n",
            " 21.023794  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 58 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7370145 17.363007  20.087652  -0.3744844 20.181273  20.221424\n",
            " 30.57932   26.839283  23.629717  36.17992   19.137146  47.731915\n",
            " 22.87498   48.32803   15.936877  61.058514  34.75934   47.034805\n",
            " 23.211258 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 59 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8974506 16.342703  18.536919  -0.3640348 18.713573  18.590126\n",
            " 28.054276  24.40237   21.504992  33.18617   17.705639  43.726517\n",
            " 21.243683  46.86106   14.523522  56.573048  32.50183   44.628864\n",
            " 21.465807 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 60 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8891519 17.377428  20.172318  -0.4263597 20.059235  20.226048\n",
            " 30.883131  26.971745  23.624397  36.68873   19.286263  48.57312\n",
            " 22.923317  49.249397  15.7674055 60.644093  35.029224  46.186672\n",
            " 23.08247  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 61 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8936865  16.103632   18.543625   -0.43135092 18.716646   18.624008\n",
            " 28.290657   24.65053    21.947725   33.47369    17.778917   43.89337\n",
            " 21.366026   47.440727   14.462674   56.29718    32.75812    43.83214\n",
            " 21.373352  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 62 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.912825  17.12632   20.463524  -0.5649297 19.93575   20.427998\n",
            " 31.216434  28.255947  24.137054  36.235126  18.726725  49.18997\n",
            " 23.138641  46.23337   16.015663  61.95584   36.12751   45.911613\n",
            " 22.709581 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 63 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8402444  16.046957   18.410103   -0.40627155 18.62766    18.47962\n",
            " 27.959595   24.48273    21.836102   33.189083   17.6341     43.52049\n",
            " 21.104548   46.901844   14.415085   56.150524   32.90108    43.71163\n",
            " 21.186693  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 64 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9598519  17.03196    20.55591    -0.56911606 20.097765   20.486816\n",
            " 31.28872    28.270075   24.369854   36.390022   18.75313    49.128063\n",
            " 23.056744   46.242435   16.24971    62.1983     36.649265   46.595627\n",
            " 22.694267  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 65 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7262299  15.867026   18.277815   -0.38967887 18.38962    18.356932\n",
            " 27.68708    24.164015   21.65874    32.868084   17.507578   42.91515\n",
            " 20.865023   47.71227    14.298181   55.62962    33.14523    43.99605\n",
            " 21.00943   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 66 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7638631 16.962807  20.580238  -0.54026   20.026945  20.629845\n",
            " 31.319256  28.12338   25.098644  36.062912  18.782776  48.5443\n",
            " 23.53958   46.77413   16.292603  61.982258  37.56031   46.96951\n",
            " 22.888367 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 67 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7186139  15.936556   18.29839    -0.39633057 18.652073   18.385546\n",
            " 27.799557   24.414438   22.013506   32.96424    17.510035   43.160477\n",
            " 20.899834   48.133335   14.345499   56.314434   33.673855   44.507877\n",
            " 21.154888  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 68 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8630098  17.86534    20.641203   -0.40834215 20.66027    20.690332\n",
            " 31.478315   27.682468   24.51118    37.581764   19.699944   49.701996\n",
            " 23.200945   50.887646   16.236406   62.392776   37.358242   47.797386\n",
            " 23.567074  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 69 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.7635586  16.554113   18.805038   -0.43490213 19.126572   18.92769\n",
            " 28.485823   25.122267   22.498009   33.94679    17.970335   44.460358\n",
            " 21.363474   49.87454    14.719801   57.8997     34.50445    45.1584\n",
            " 21.803274  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 70 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8223666  17.320967   20.70368    -0.53293115 20.266397   20.722403\n",
            " 31.508451   28.523039   25.41175    36.33941    18.882982   49.14965\n",
            " 23.670773   47.67053    16.336609   62.869064   38.290627   47.09077\n",
            " 23.188776  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 71 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9832457  17.187557   19.692884   -0.44115987 19.525152   19.742922\n",
            " 29.93494    26.08853    23.94742    35.646603   18.778679   46.928448\n",
            " 22.561544   50.892616   15.37187    59.174393   36.188023   45.88025\n",
            " 22.56715   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 72 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9148494  17.67331    20.539364   -0.43484735 20.409967   20.58232\n",
            " 31.426367   27.459496   24.791906   37.386116   19.611862   49.438038\n",
            " 23.31674    51.313362   16.048546   61.646496   37.549618   47.01692\n",
            " 23.469648  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 73 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8823801 16.44926   18.941988  -0.3887943 19.098583  19.015059\n",
            " 28.721113  25.171028  22.846792  34.18925   18.084614  44.88153\n",
            " 21.62757   50.143555  14.820818  57.53229   34.962845  44.730797\n",
            " 21.711048 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 74 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9365416  17.019758   20.591791   -0.57874995 20.005478   20.552279\n",
            " 31.478628   28.422237   25.044893   36.501522   18.837702   49.39077\n",
            " 23.280499   47.51145    16.14135    62.134796   37.855465   46.09556\n",
            " 22.72568   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 75 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8676332  16.552113   19.12339    -0.37088516 19.16271    19.203133\n",
            " 28.939196   25.255226   23.008417   34.478596   18.246553   45.185284\n",
            " 21.808561   50.34122    14.984642   57.655533   35.45712    45.301014\n",
            " 21.887457  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 76 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8666388  17.070463   20.449825   -0.56584156 19.849737   20.401653\n",
            " 31.08782    27.830637   24.584366   36.16469    18.77545    48.78067\n",
            " 23.006802   47.824837   15.994712   61.350983   37.75087    46.318268\n",
            " 22.572834  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 77 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9596658  16.877956   19.293358   -0.37129915 19.151167   19.37107\n",
            " 29.113398   25.464714   23.411818   34.790512   18.429464   45.602215\n",
            " 22.09705    49.9671     15.109908   57.89864    35.950283   45.629356\n",
            " 22.076359  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 78 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8957869 17.517838  20.28714   -0.4289317 20.277122  20.378485\n",
            " 30.826088  27.14173   24.442158  36.98458   19.41257   48.827465\n",
            " 22.844355  49.936714  15.981153  61.307545  37.457973  47.003983\n",
            " 23.157879 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 79 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9022968 17.127377  19.608448  -0.4753406 19.627506  19.692518\n",
            " 29.798407  26.320253  23.779827  35.713898  18.782455  46.99175\n",
            " 22.236935  49.430843  15.4406    59.497246  36.59281   46.008774\n",
            " 22.50233  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 80 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.940313  17.525562  20.96515   -0.5353781 20.378561  20.852856\n",
            " 31.753748  28.647594  25.21723   37.05848   19.106213  50.039345\n",
            " 23.567434  46.975838  16.509592  62.915596  38.847843  47.55489\n",
            " 23.1837   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 81 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9284707  16.271795   18.823097   -0.39755893 18.927807   18.872086\n",
            " 28.646547   24.947409   22.82269    33.995926   18.000895   44.602947\n",
            " 21.579002   49.269585   14.645493   56.84822    35.232952   44.483932\n",
            " 21.549597  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 82 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.8798461 18.810263  22.537407  -0.5551461 21.703924  22.4894\n",
            " 34.509487  30.973818  27.835802  39.52033   20.464914  52.046913\n",
            " 25.657942  49.721035  17.949892  67.41837   42.447277  51.18361\n",
            " 24.779692 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 83 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9244219  17.670874   20.456232   -0.28029954 20.451466   20.454403\n",
            " 30.991766   26.867731   25.009502   36.26784    19.457552   48.140053\n",
            " 25.282719   49.934982   16.142933   61.556335   38.593338   48.84858\n",
            " 23.282215  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 84 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9062258 17.644222  20.5058    -0.4062154 20.453768  20.538385\n",
            " 31.090809  27.456814  24.820232  37.30764   19.621876  49.305973\n",
            " 26.6237    49.10634   16.096771  61.772964  37.971878  47.039246\n",
            " 23.379591 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 85 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.9058552  16.92021    19.80509    -0.43776453 19.57826    19.730274\n",
            " 29.988518   26.222818   23.957651   35.51775    18.683804   47.146423\n",
            " 27.179407   48.16972    15.503123   58.8284     36.98003    46.567932\n",
            " 22.260614  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 86 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.367639   18.93613    21.407578   -0.65378886 21.91572    21.419914\n",
            " 32.95025    28.394133   26.09032    38.93264    20.45503    52.352886\n",
            " 30.975145   53.069683   16.591      62.58177    40.16988    50.13517\n",
            " 24.525366  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 87 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.1697965 18.138374  21.564976  -0.7275988 21.454666  21.456648\n",
            " 32.863644  28.37169   26.204134  37.98411   22.105957  51.365383\n",
            " 32.1831    50.582024  16.77843   62.224182  40.63406   50.11258\n",
            " 23.974806 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 88 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.323946  18.973343  22.131813  -0.8325537 22.049833  21.993511\n",
            " 34.15283   29.993126  27.136133  39.37048   24.907894  54.101376\n",
            " 34.077114  50.139736  17.247885  64.62158   41.615467  50.99287\n",
            " 24.482033 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 89 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.3787963 19.221466  22.098356  -0.6406064 22.20216   22.059805\n",
            " 33.809532  29.29872   26.724272  40.109627  27.483305  53.82839\n",
            " 35.257835  51.617603  17.2726    64.718445  41.17982   50.543003\n",
            " 25.110771 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 90 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.2776432  19.624561   22.713709   -0.44310498 22.496397   22.659393\n",
            " 34.541073   29.71499    26.667353   41.350304   30.355427   54.525314\n",
            " 37.02231    53.69078    17.61393    65.985176   42.292385   52.37784\n",
            " 25.876234  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 91 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.4438176  19.191751   22.116999   -0.66836953 22.293875   22.033644\n",
            " 34.09154    29.218237   27.015726   40.079964   31.00329    54.102406\n",
            " 37.312603   52.91491    17.114393   63.90386    41.423218   51.001755\n",
            " 24.949987  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 92 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.4341283 20.363848  23.16772   -0.704068  23.5224    23.097258\n",
            " 35.659016  30.553635  28.335583  42.20304   34.13124   56.758015\n",
            " 39.77863   55.309906  17.943924  66.79106   43.432613  53.914585\n",
            " 26.362963 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 93 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.4385731 19.177307  22.602037  -0.7888961 22.378704  22.359402\n",
            " 34.75197   30.169695  27.212023  40.22635   33.869015  54.86865\n",
            " 39.23205   51.272724  17.419743  65.41854   42.233845  51.12324\n",
            " 25.077112 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 94 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.425545   19.678722   23.028671   -0.46626216 22.855307   22.95287\n",
            " 35.046818   30.067724   27.46259    41.977947   36.685402   54.966805\n",
            " 40.4719     52.97068    17.964523   66.55955    42.97358    53.651035\n",
            " 26.13238   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 95 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.3142693  18.389328   21.684616   -0.68752116 21.594046   21.487362\n",
            " 32.973743   28.372643   26.335829   38.220726   34.56909    51.822327\n",
            " 38.64332    49.911064   16.76917    62.40965    40.909443   50.777996\n",
            " 24.022999  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 96 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.3255675  19.306662   23.45148    -0.53247404 22.508545   23.287386\n",
            " 35.777874   31.258251   27.849728   41.69418    38.46041    55.907654\n",
            " 42.008442   49.828136   18.311687   67.592094   43.812534   53.003536\n",
            " 25.724617  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 97 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.3733585  19.953632   23.163074   -0.73986536 23.104113   23.0652\n",
            " 35.76791    31.061436   28.189228   41.4213     39.17822    56.576836\n",
            " 42.519447   51.988705   18.048466   67.469894   43.721386   52.729183\n",
            " 25.891983  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 98 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.476246 21.028973 24.028496 -0.654129 24.290806 23.968485 36.76325\n",
            " 31.579042 29.157469 43.663742 42.06012  58.66131  44.355022 55.557156\n",
            " 18.76119  69.18541  45.08572  56.242344 27.261683]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 99 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.4524226 19.705894  22.675028  -0.6685507 22.931683  22.612495\n",
            " 34.815712  29.996286  27.545935  41.080296  40.398617  55.4093\n",
            " 42.445118  53.498314  17.66374   65.86517   42.533726  52.70566\n",
            " 25.537498 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 100 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.4847763  20.525822   23.844944   -0.47605082 23.649035   23.806583\n",
            " 36.258347   31.248688   28.284433   43.524673   43.440308   57.306637\n",
            " 44.6434     54.527966   18.573507   69.48846    44.48084    55.518864\n",
            " 27.189611  ]\n",
            "Epsilon reduced to 0.06561000000000002\n",
            "Total reward: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ijUbxL8gTas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fcacd9c7-44fb-4952-ffb4-8462f83893ba"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=50, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=8, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_21(env), max_epsilon=0.2, epsilon_decay=0.8)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_4 (Reshape)          (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 187, 32)           128       \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 5984)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 19)                113715    \n",
            "=================================================================\n",
            "Total params: 113,843\n",
            "Trainable params: 113,843\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |████████████----------------------------------------------------------------------------------------| 12.5% \r\n",
            "Taking action 6\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.04379103 -0.11789246  0.08814161 -0.05587201 -0.08640745  0.14598218\n",
            "  0.05284258 -0.04752502  0.05603062  0.02321982 -0.02421732  0.11349384\n",
            " -0.1321032   0.04056481  0.02300414 -0.01324749  0.13802837 -0.20048918\n",
            " -0.08922318]\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 2 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.0875068   0.02546689  0.05122022 -0.1799491  -0.156868    0.12719657\n",
            "  0.03149413  0.13493867 -0.04404335  0.03511471 -0.02054982 -0.13127421\n",
            " -0.22477715  0.0498333   0.00388826 -0.11026134  0.20556942 -0.25747126\n",
            "  0.07943894]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 3 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.12298303 -0.00442346  0.01469268 -0.09263457 -0.10086395 -0.0312927\n",
            " -0.10437789  0.02862515  0.13293539  0.05379698 -0.0097018  -0.02285459\n",
            " -0.10011423 -0.00912775 -0.02166084 -0.02207245  0.16692214 -0.18316108\n",
            "  0.12586728]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 4 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.09082128 -0.01735033  0.13584699 -0.2132099  -0.14829642 -0.07118203\n",
            " -0.13175286 -0.00351873 -0.09262433  0.05878708 -0.18027954  0.04635772\n",
            " -0.05431018  0.00545035 -0.12175893  0.04308992  0.0689314  -0.15176482\n",
            "  0.01078494]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 5 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.08333708 -0.08207367 -0.04630677 -0.14926766 -0.18892044 -0.0744015\n",
            " -0.24237312 -0.05198756  0.02064638 -0.05146137 -0.11362279  0.06886958\n",
            " -0.07276168  0.13695121 -0.21034758 -0.10822573  0.1924378  -0.229098\n",
            " -0.13410681]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 6 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.0570687  -0.04249943  0.1000173  -0.10629836 -0.13260713 -0.17428549\n",
            " -0.16378243  0.15895055 -0.03939977  0.05045847 -0.1653396  -0.1465099\n",
            " -0.09739468  0.05103301 -0.24786057 -0.07819571  0.03486436 -0.0614289\n",
            " -0.0038435 ]\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 7 reward=-3 new_state=[0 0 1 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.06778155 -0.07668373 -0.17382288 -0.11456815 -0.11556837 -0.2214719\n",
            " -0.16836368  0.05228888 -0.01651954 -0.10945668 -0.11354174 -0.07777301\n",
            " -0.20565577 -0.08626186 -0.36365953 -0.06110008  0.1882212  -0.14597711\n",
            "  0.1804323 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 8 reward=-3 new_state=[0 0 1 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.00475107 -0.07722192 -0.21671508 -0.14544947 -0.096034   -0.20825687\n",
            " -0.15778488 -0.03137933 -0.1358434  -0.06593645 -0.09856598  0.00954426\n",
            "  0.0264935  -0.02140599 -0.44494617 -0.02495002  0.08984703  0.04918968\n",
            "  0.05436404]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 9 reward=-3 new_state=[0 0 1 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.14278592  0.00782008 -0.04649527  0.01504496 -0.10156877 -0.2085266\n",
            " -0.09054015 -0.06417077 -0.07761211  0.08813814 -0.21353914 -0.00835656\n",
            " -0.03510922 -0.05743739 -0.21041939 -0.03800995  0.02686189  0.03462334\n",
            " -0.0533738 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 10 reward=-3 new_state=[0 0 1 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.07261144 -0.14524077 -0.1501229  -0.05041573 -0.05774932 -0.24740683\n",
            " -0.27221748 -0.08273021 -0.05219266 -0.09006427 -0.11145926 -0.0418151\n",
            "  0.04735235  0.14603093 -0.3877723   0.01252329 -0.03322455  0.03226047\n",
            " -0.10836211]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 1 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.08936709 -0.03371697 -0.1477407  -0.06162378 -0.04451866 -0.2987396\n",
            " -0.22638752 -0.14144452  0.00572976  0.06823721 -0.22275351 -0.05625901\n",
            " -0.04980373  0.05857039 -0.5578932   0.01393724  0.07978296 -0.17132765\n",
            " -0.20980228]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 1 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.0059429  -0.09146787 -0.23213847 -0.06929619 -0.07723611 -0.37400985\n",
            " -0.27927005 -0.18692228  0.01797966  0.04538814 -0.18546793  0.02509089\n",
            " -0.03843692 -0.06121015 -0.7962801   0.13242967  0.15795991 -0.17176822\n",
            " -0.06567312]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 13 reward=-2 new_state=[0 0 1 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0522405  -0.12924089 -0.4462205  -0.08882084 -0.08577115 -0.36916196\n",
            " -0.3316566  -0.05781793 -0.06122763 -0.00534424 -0.05429695 -0.03901403\n",
            "  0.04701355 -0.1231967  -0.82735157  0.01049367  0.0827954  -0.0705798\n",
            " -0.1368601 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 14 reward=-2 new_state=[0 0 1 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.02237659 -0.15829314 -0.25400135 -0.00228085 -0.11350587 -0.28734803\n",
            " -0.2661075  -0.10641217  0.02378625  0.05709469 -0.20765565 -0.02950058\n",
            "  0.0496874  -0.1419575  -0.5944473  -0.00949159  0.09826486 -0.05582717\n",
            " -0.21214531]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 15 reward=-2 new_state=[0 0 1 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.07681896 -0.06666632 -0.3309613   0.0854345  -0.13909987 -0.39206183\n",
            " -0.33781406 -0.20293753  0.01448715  0.16051762 -0.26708466 -0.07790072\n",
            " -0.04789176 -0.2386954  -0.8765433  -0.02514809  0.05490455 -0.14920427\n",
            " -0.3707514 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.15571643 -0.13299273 -0.3155744   0.00371926 -0.2036936  -0.37239707\n",
            " -0.28282717 -0.17894095  0.00201639 -0.00188446 -0.19264491 -0.09508584\n",
            " -0.09745684 -0.15458249 -0.80750024 -0.01319544  0.01099563 -0.10539544\n",
            " -0.24257503]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 17 reward=-2 new_state=[1 0 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.15305795 -0.06904973 -0.33760014  0.05436036 -0.21296227 -0.2863373\n",
            " -0.30475202 -0.18697539 -0.02350351 -0.00892698 -0.2688549  -0.08825635\n",
            "  0.00193741 -0.15947224 -0.6714771  -0.01132654  0.03420598 -0.18521093\n",
            " -0.26882923]\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 18 reward=-3 new_state=[1 1 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.17372403  0.01673472 -0.371017    0.04495919 -0.13147642 -0.31993917\n",
            " -0.22633538 -0.28447622 -0.03004701 -0.01393015 -0.10785876 -0.00172793\n",
            " -0.08388848 -0.20937881 -0.7815928  -0.07579187  0.11235138 -0.17529468\n",
            " -0.25034985]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 19 reward=-3 new_state=[1 1 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5390538e-01 -3.9590556e-02 -2.7565899e-01  2.9354610e-02\n",
            " -1.7877455e-01 -2.9600009e-01 -1.6160499e-01 -2.1347055e-01\n",
            " -5.3529747e-02  6.1184689e-02 -2.1412639e-01 -1.5004397e-04\n",
            " -1.2837659e-01 -1.3663362e-01 -5.7927048e-01 -2.4984393e-02\n",
            " -2.9490747e-02 -1.5127601e-01 -2.2778626e-01]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 20 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.23227812 -0.13193421 -0.32333243  0.02551628 -0.15260252 -0.2507311\n",
            " -0.21416667 -0.16548344 -0.06590162 -0.03163622 -0.16258912 -0.02906403\n",
            " -0.01111213 -0.16618827 -0.5766814   0.05620119  0.00605149 -0.02168432\n",
            " -0.22118346]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 21 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18031362 -0.2996582  -0.47880718  0.01500907 -0.07757034 -0.38571298\n",
            " -0.3418848  -0.29914972 -0.03152764 -0.10477282 -0.06922241 -0.11990733\n",
            " -0.01457805 -0.3474302  -1.0121492  -0.08108044  0.01011651 -0.15608852\n",
            " -0.3499679 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 22 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2778426  -0.3205456  -0.48246288 -0.15088217 -0.16987324 -0.50804424\n",
            " -0.41217104 -0.42681265  0.07980531 -0.11028153 -0.11945749  0.09050775\n",
            "  0.01385023 -0.3695601  -1.2911193  -0.0505917   0.14192817 -0.4252022\n",
            " -0.5156816 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 23 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.27722982 -0.40295732 -0.6262434  -0.19080982 -0.16587327 -0.450871\n",
            " -0.46977684 -0.3204526  -0.09356505 -0.17320572 -0.10338838 -0.08390244\n",
            " -0.05698372 -0.39145958 -1.430118    0.03196995  0.04725859 -0.3389819\n",
            " -0.43704528]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 24 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.35492304 -0.4962753  -0.76674706 -0.35897192 -0.2785252  -0.5570291\n",
            " -0.50340503 -0.47032964 -0.0959717  -0.20928389 -0.14886461  0.07702405\n",
            "  0.01065848 -0.4944574  -1.8300419   0.05273829  0.2056898  -0.6248734\n",
            " -0.60681283]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 25 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.35885492 -0.5388839  -0.75149757 -0.3756704  -0.15506996 -0.65523946\n",
            " -0.52518004 -0.47618702 -0.15566537 -0.26739818 -0.20856643 -0.16650914\n",
            " -0.10675391 -0.6197833  -1.9857643   0.08446781  0.03784574 -0.639626\n",
            " -0.65181506]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 26 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.4471471  -0.58932567 -0.8029006  -0.44337752 -0.21152833 -0.5155554\n",
            " -0.5381064  -0.55819756 -0.03595495 -0.33318818 -0.14143685  0.02461824\n",
            " -0.00354594 -0.54775333 -2.085059    0.09379642  0.22254547 -0.73755246\n",
            " -0.67857945]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 27 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.50623614 -0.5838195  -0.65302765 -0.44399753 -0.11504781 -0.6690564\n",
            " -0.5642879  -0.5651377  -0.12037081 -0.46425852 -0.14708292  0.00252735\n",
            " -0.10001628 -0.59680265 -2.1070673  -0.02270799  0.10180353 -0.8214898\n",
            " -0.6739632 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 28 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.563004   -0.63513404 -0.62631255 -0.51395977 -0.17052463 -0.6379829\n",
            " -0.6224961  -0.5609347   0.05927618 -0.6509565  -0.15019545  0.05632714\n",
            " -0.02578508 -0.551757   -2.4176097   0.05104577  0.14615105 -0.91103303\n",
            " -0.66718286]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 29 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.4341543  -0.5747268  -0.75571555 -0.5347677  -0.27908093 -0.6053716\n",
            " -0.5355465  -0.4702656  -0.18946466 -0.6650917  -0.21084863 -0.10407855\n",
            " -0.12889294 -0.6014853  -2.4452107   0.09976617  0.0041129  -1.0536078\n",
            " -0.60613275]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 30 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-4.9009070e-01 -7.4411058e-01 -8.1763273e-01 -5.4754728e-01\n",
            " -2.9228431e-01 -7.5191718e-01 -6.3237011e-01 -6.1962330e-01\n",
            " -1.0754656e-01 -7.5575095e-01 -1.7245649e-01  5.9395242e-02\n",
            " -1.2247440e-03 -6.7458224e-01 -2.8590798e+00 -3.4826443e-02\n",
            "  6.2838309e-02 -1.1946642e+00 -7.8092861e-01]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 31 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.60833347 -0.7635855  -0.70136684 -0.6720198  -0.15833655 -0.68095183\n",
            " -0.5637973  -0.604322   -0.17524923 -0.8638244  -0.14461245 -0.21134043\n",
            " -0.14147198 -0.61503965 -2.8343608   0.00716358 -0.05153556 -1.2650077\n",
            " -0.5604741 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 32 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.66265327 -0.85044575 -0.8847626  -0.8169251  -0.15791352 -0.79083854\n",
            " -0.8035378  -0.686084   -0.13748384 -1.0112033  -0.06976467  0.0644504\n",
            " -0.06518917 -0.75917166 -3.4020376   0.02493505  0.25652313 -1.4648349\n",
            " -0.860563  ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 33 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5782214  -0.89987326 -0.87819135 -0.6872057  -0.21032952 -0.76452166\n",
            " -0.69137204 -0.62837297 -0.21864139 -1.0825341  -0.13184774 -0.13217555\n",
            " -0.18415469 -0.78956    -3.3989906   0.03049622  0.15313946 -1.7376826\n",
            " -0.6877753 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 34 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6741189  -0.9357542  -0.93617505 -0.7615875  -0.14866045 -0.79146427\n",
            " -0.73564833 -0.76716566 -0.09612351 -1.1251886  -0.06847739  0.06891391\n",
            " -0.05262797 -0.8115857  -3.857165   -0.01102831  0.23688467 -1.8642703\n",
            " -0.8977169 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 35 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.61485225 -0.8753136  -0.8914509  -0.74690676 -0.28331774 -0.6623684\n",
            " -0.7307588  -0.5839734  -0.18283293 -1.0789752  -0.27095848 -0.08680126\n",
            " -0.09168082 -0.7022606  -3.5525994   0.10200101  0.06461947 -2.0369117\n",
            " -0.77099997]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6393156  -0.9392882  -0.9114993  -0.76340604 -0.24114342 -0.6784532\n",
            " -0.71907085 -0.5515205  -0.06594195 -1.3052769  -0.18587005 -0.10190791\n",
            " -0.01843217 -0.69434047 -3.7694418   0.03714724  0.14760953 -2.066278\n",
            " -0.76752293]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 37 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6427286  -0.99498814 -1.0190196  -0.8294546  -0.23810959 -0.7707534\n",
            " -0.7589261  -0.6137358  -0.19305666 -1.3552452  -0.21053211 -0.14305155\n",
            " -0.16752414 -0.8549789  -4.183241    0.1115935   0.09634905 -2.4529254\n",
            " -0.8453734 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 38 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.69705474 -0.98647034 -0.97676706 -0.8811182  -0.31524515 -0.8254508\n",
            " -0.71912706 -0.76932997 -0.1169031  -1.2720475  -0.18949106  0.13088933\n",
            " -0.08951903 -0.90887314 -4.4689546  -0.01500626  0.2003452  -2.502748\n",
            " -1.0015944 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 39 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.64257866 -1.0118941  -0.8568051  -0.9326897  -0.27554876 -0.8446003\n",
            " -0.60042876 -0.66902626 -0.09343082 -1.4263293  -0.2000489  -0.06028805\n",
            " -0.09331499 -0.81210244 -4.307842    0.08971172  0.07208165 -2.8784454\n",
            " -0.7996049 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 40 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.87099624 -1.1089492  -1.0092359  -1.0457357  -0.21873239 -0.9754018\n",
            " -0.8260406  -0.8840471  -0.12377194 -1.558333   -0.01648143 -0.04922245\n",
            " -0.03676508 -0.90482795 -4.7359147  -0.03846995  0.1654106  -2.9968307\n",
            " -0.96821934]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 41 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8106689  -1.2383218  -0.9586532  -1.1112579  -0.20253247 -0.9079871\n",
            " -0.6818667  -0.74562067 -0.05248157 -1.5786905  -0.13585222  0.01542035\n",
            " -0.05781731 -1.0177947  -4.9466286   0.12321686  0.10568047 -3.5581255\n",
            " -1.0782087 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 42 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.81669647 -1.1777325  -1.121642   -1.1094193  -0.23720121 -0.85136306\n",
            " -0.76903254 -0.92964834 -0.06707613 -1.507911   -0.31326255  0.04509986\n",
            " -0.0141124  -0.9342873  -5.227783   -0.03346697  0.19698058 -3.6617033\n",
            " -1.2164351 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 43 reward=-5 new_state=[1 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7299863  -1.0843741  -0.8392619  -0.93742853 -0.3018342  -0.6997831\n",
            " -0.5959477  -0.5855363  -0.20893127 -1.2480354  -0.6060207  -0.15880544\n",
            " -0.11438069 -0.7658265  -4.0134087   0.11149243  0.04973608 -3.1173844\n",
            " -0.81633985]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 44 reward=-5 new_state=[1 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7904884  -0.8723035  -0.81146395 -0.81398493 -0.17762494 -0.6142841\n",
            " -0.5494058  -0.7467451  -0.15185224 -1.1926825  -0.58442974 -0.08035585\n",
            " -0.21425973 -0.74811685 -3.987345    0.08396178  0.2000293  -2.844953\n",
            " -0.82973695]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 45 reward=-5 new_state=[1 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.85841566 -1.0397012  -0.8560929  -1.1406242  -0.21557233 -0.7323015\n",
            " -0.7230503  -0.7143866  -0.22286247 -1.4069366  -0.7577485   0.00602367\n",
            " -0.3809199  -0.8821638  -4.3596144   0.13707812  0.22110017 -3.3518684\n",
            " -0.94922405]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 46 reward=-5 new_state=[1 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.73052883 -1.0189288  -1.0256853  -1.0811614  -0.22505747 -0.61309946\n",
            " -0.5635723  -0.60939777 -0.20823482 -1.3539085  -0.7649666  -0.05083585\n",
            " -0.40166053 -0.7404842  -4.098808    0.11347529  0.13005023 -3.0404882\n",
            " -0.8737827 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 47 reward=-5 new_state=[1 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.72973055 -1.1353468  -0.93509525 -1.4078217  -0.32451102 -0.8227106\n",
            " -0.5835469  -0.6435869  -0.23534109 -1.4560523  -1.0654634  -0.08819317\n",
            " -0.5821789  -0.8742432  -4.6553245   0.05740727  0.04015309 -3.899847\n",
            " -1.2386214 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 48 reward=-4 new_state=[1 1 1 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.84060174 -0.93975973 -0.8333172  -1.2677842  -0.14668214 -0.6339836\n",
            " -0.5479702  -0.6606579  -0.04713628 -1.2719617  -0.95040816 -0.19454427\n",
            " -0.7049866  -0.74780065 -4.2594457   0.07309932  0.17303288 -3.4109695\n",
            " -1.1795664 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 49 reward=-5 new_state=[1 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8588019  -1.1926951  -1.0485251  -1.5497583  -0.24082582 -0.8150181\n",
            " -0.6469925  -0.62922394 -0.19589274 -1.6277645  -1.0712497  -0.07729103\n",
            " -0.7628824  -0.926902   -4.733023   -0.03361434  0.14316456 -3.911697\n",
            " -1.3302562 ]\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 50 reward=-5 new_state=[1 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8142002  -1.2540734  -0.9286327  -1.8569734  -0.22240469 -0.7878993\n",
            " -0.7074645  -0.57626104 -0.15030839 -1.5334525  -1.2698052  -0.15717797\n",
            " -0.9117125  -0.86915207 -4.884009   -0.10044304  0.16888782 -4.4208364\n",
            " -1.6113044 ]\n",
            "Epsilon reduced to 0.16000000000000003\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0698465  -1.2357872  -0.7891409  -2.2024717  -0.02090214 -0.9480764\n",
            " -0.8420042  -1.0016828   0.04518833 -1.6774766  -1.1641202  -0.00792684\n",
            " -1.1640428  -0.9479652  -5.1799526  -0.6001957   0.22194934 -5.2251215\n",
            " -1.8540453 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.162364   -1.6096137  -0.87600046 -2.3914874  -0.12795147 -0.9667258\n",
            " -0.7567567  -1.1562339   0.12188151 -1.7742236  -1.357535   -0.11428681\n",
            " -1.3316445  -1.1055505  -6.0293827  -0.6519865  -0.1496544  -5.714366\n",
            " -2.2301865 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1379035  -1.250443   -0.90187275 -2.5567617  -0.08147413 -1.0498638\n",
            " -1.0316372  -1.104918   -0.30214977 -1.9074723  -1.4358617  -0.03720188\n",
            " -1.437585   -1.0210669  -5.788027   -0.88740844 -0.16786385 -5.76525\n",
            " -2.2130256 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0232924  -1.4170872  -0.92489064 -2.2868984  -0.0941186  -0.9049916\n",
            " -0.73258793 -1.0130004  -0.4371311  -1.754794   -1.461012    0.0146373\n",
            " -1.2797005  -1.073119   -5.6415925  -0.8015896  -0.41229367 -5.395069\n",
            " -2.1483057 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0867205  -1.3173383  -0.90660715 -2.6939046  -0.22966549 -1.1174421\n",
            " -0.9237453  -0.95837635 -0.7968203  -1.9966302  -1.7079335   0.1485759\n",
            " -1.5655742  -1.1587232  -5.8120785  -1.0730432  -0.6559176  -6.021046\n",
            " -2.4292343 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8802341  -1.2602923  -0.77620864 -2.1986604  -0.13992842 -0.83170164\n",
            " -0.6785795  -0.9287259  -0.808098   -1.6197262  -1.3907444   0.13345805\n",
            " -1.2100134  -0.9368225  -5.179029   -0.98547196 -0.5166124  -4.9034944\n",
            " -1.9788706 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1008686  -1.3511492  -1.0463442  -2.7713842  -0.08711116 -1.0289154\n",
            " -1.0080633  -1.02733    -1.4110681  -2.079032   -1.6571846   0.12207016\n",
            " -1.6400156  -1.0164962  -6.0804796  -1.4268101  -0.87552154 -6.2289844\n",
            " -2.490368  ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1002816  -1.3221685  -1.0318031  -2.7771022  -0.18590099 -1.1496851\n",
            " -0.9753841  -1.055522   -1.3726482  -1.8731623  -1.7924092   0.13528767\n",
            " -1.6579037  -1.0261658  -6.112668   -1.3465327  -0.8584395  -5.952083\n",
            " -2.4212537 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1337804  -1.4325265  -0.94845986 -3.0305655  -0.22365722 -1.2285147\n",
            " -0.9309307  -1.0061226  -1.6632283  -2.1421537  -1.9130188   0.15710229\n",
            " -1.832093   -1.176196   -6.2422953  -1.5118164  -1.1944954  -6.58232\n",
            " -2.8727958 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 10 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9857965  -1.4394274  -0.95187116 -2.6721435  -0.19634381 -1.0156736\n",
            " -0.8372593  -0.925152   -1.6194215  -1.877128   -1.7261903  -0.01840955\n",
            " -1.6573806  -1.0589418  -6.0540786  -1.3951218  -1.1077683  -5.865117\n",
            " -2.5907552 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9420212  -1.3533763  -0.96408844 -3.0403624  -0.20366536 -1.1109287\n",
            " -0.86430854 -0.8820283  -1.9448992  -1.8773316  -1.8855633   0.14639623\n",
            " -1.8050635  -0.88114595 -6.2540593  -1.5274874  -1.3501642  -6.25563\n",
            " -2.7472198 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7579855  -1.1424026  -0.77669096 -2.2635283   0.09531581 -0.7585458\n",
            " -0.64092845 -0.7547143  -1.5040944  -1.5417024  -1.4333066   0.01859152\n",
            " -1.3877426  -0.55841804 -5.125495   -1.2185171  -1.096134   -4.8939614\n",
            " -2.1196105 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9181862  -1.3076975  -0.9318806  -2.92509     0.23757839 -1.0307802\n",
            " -0.81192094 -0.85978925 -2.116626   -1.9315697  -1.8313015   0.39341432\n",
            " -1.7586747  -0.537716   -6.069195   -1.5827662  -1.4198428  -6.1325016\n",
            " -2.713983  ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.87179357 -1.266466   -0.8391278  -2.5999522   0.52095145 -0.92540044\n",
            " -0.72893906 -0.8445602  -1.9852903  -1.7505519  -1.6277514   0.39754957\n",
            " -1.6059763  -0.33978757 -5.6719613  -1.4588274  -1.318587   -5.5037017\n",
            " -2.4361467 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9035581  -1.2668867  -0.89837235 -2.9359465   0.74799365 -1.0071563\n",
            " -0.8155524  -0.8933723  -2.2775795  -1.8635633  -1.8584181   0.8779487\n",
            " -1.7950839  -0.18485953 -5.995335   -1.6628883  -1.5195414  -6.1247745\n",
            " -2.7176356 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9267308  -1.2197146  -0.85831016 -2.6859567   1.1265334  -0.9483682\n",
            " -0.7942809  -0.9824601  -2.2349176  -1.8000635  -1.8144916   0.98379743\n",
            " -1.7453717  -0.11483897 -5.983466   -1.570156   -1.4720843  -5.7087116\n",
            " -2.6028678 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8118644  -1.2575393  -0.92099106 -2.8803248   1.3343687  -0.915027\n",
            " -0.7819059  -0.78867465 -2.4242463  -1.9467714  -1.7611047   1.1754613\n",
            " -1.7602001   0.05176558 -5.8477755  -1.7540209  -1.638888   -6.092466\n",
            " -2.5751202 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8556557  -1.196181   -0.9865587  -2.842811    1.5782905  -0.9600992\n",
            " -0.77433765 -0.91786754 -2.3579514  -1.8345474  -1.7939097   1.6781685\n",
            " -1.7549202   0.1286324  -6.002181   -1.7120525  -1.5586762  -5.8389645\n",
            " -2.5233772 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8985766  -1.2827916  -0.8999163  -3.0459538   1.9286926  -1.0427338\n",
            " -0.8324959  -0.8536991  -2.5687087  -1.8230387  -1.9033444   2.0809858\n",
            " -1.8689058   0.30759552 -6.0209365  -1.7380819  -1.6547073  -6.1595263\n",
            " -2.3703828 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8252951 -1.2193907 -0.7495497 -2.6320658  2.030878  -0.8906395\n",
            " -0.7363542 -0.8539753 -2.2528253 -1.6973784 -1.6857249  1.896154\n",
            " -1.6398534  0.3018156 -5.5332355 -1.5713935 -1.5553471 -5.390417\n",
            " -1.9541333]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9162856  -1.3410479  -0.909068   -3.1913214   2.5012772  -1.127175\n",
            " -0.87969255 -0.8678108  -2.77058    -1.8821659  -1.9933989   2.8204613\n",
            " -1.9918512   0.5284073  -6.2214794  -1.851181   -1.8030775  -6.283835\n",
            " -2.1402318 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9001083  -1.241131   -0.82375926 -2.7845275   2.7564456  -0.92062044\n",
            " -0.75388336 -0.96615815 -2.5827181  -1.7296093  -1.8284695   2.6150665\n",
            " -1.7921714   0.43167618 -5.9779344  -1.6909369  -1.705303   -5.7346787\n",
            " -1.778339  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.79428166 -1.2560004  -0.92685586 -2.9714508   3.0004714  -0.917953\n",
            " -0.7860216  -0.7753663  -2.7610688  -1.9737489  -1.8440604   2.8529742\n",
            " -1.8300962   0.5701279  -5.9193416  -1.8976105  -1.8558952  -6.225136\n",
            " -1.731841  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 24 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.96095514 -1.2311945  -0.9184055  -2.8457503   3.2560349  -0.95133615\n",
            " -0.7878935  -0.93795735 -2.722282   -1.8607179  -1.8733392   3.2600298\n",
            " -1.8497761   0.55475646 -6.107521   -1.7851263  -1.4803662  -5.7203064\n",
            " -1.64824   ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 25 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8288758  -1.0756398  -0.7992055  -2.5611203   2.8363378  -0.72130513\n",
            " -0.6200579  -0.6303032  -2.4321315  -1.5855887  -1.7033184   3.0525603\n",
            " -1.5658735   0.6419852  -5.0360894  -1.5791297  -1.1328052  -5.084219\n",
            " -1.4104906 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 26 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8251923  -0.92794895 -0.75021875 -2.241049    2.7788217  -0.65822214\n",
            " -0.56747997 -0.7002391  -2.1363146  -1.3652418  -1.5467867   2.8956501\n",
            " -1.3897774   0.49532974 -4.76311    -1.3957182  -0.85648626 -4.261826\n",
            " -1.2853261 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.90407246 -1.219118   -0.7969214  -2.9013946   3.4863756  -0.7638734\n",
            " -0.67756194 -0.7067645  -2.6884258  -1.6942937  -1.8828167   3.9043918\n",
            " -1.7802851   0.72623974 -5.5374093  -1.7363728  -0.73577225 -5.320411\n",
            " -1.4419788 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 28 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8560722  -1.1812046  -0.9865875  -2.920996    3.7810905  -0.8226033\n",
            " -0.6763946  -0.7464118  -2.8291945  -1.6499937  -1.7554482   4.088394\n",
            " -1.8103669   0.8699697  -5.8183575  -1.7732098  -0.87998533 -5.1952567\n",
            " -1.4303236 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 29 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.7733944  -0.9602858  -0.8077225  -2.3675547   3.1690886  -0.7674085\n",
            " -0.57408434 -0.6365423  -2.2888124  -1.4015007  -1.2820212   3.7061536\n",
            " -1.5114403   0.64196455 -4.6861687  -1.4841207  -0.5692481  -3.9868724\n",
            " -1.0847148 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 30 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.77272177 -1.102265   -0.8380678  -2.599963    3.5349889  -0.77401525\n",
            " -0.620096   -0.6042889  -2.5680215  -1.5249455  -1.2287703   4.2812624\n",
            " -1.6103874   0.80429    -4.982763   -1.6138021  -0.5353832  -4.258822\n",
            " -1.1264267 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 31 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.80189276 -1.0605258  -0.7971236  -2.4514863   3.5074797  -0.77215004\n",
            " -0.6157939  -0.6564543  -2.4525163  -1.4562277  -1.0716237   4.4348416\n",
            " -1.5723815   0.79882807 -4.8496976  -1.5696349  -0.42907724 -3.9479773\n",
            " -1.0338558 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 32 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.7690251  -1.0907837  -0.8486081  -2.4872348   3.6337318  -0.69625264\n",
            " -0.5828887  -0.56476474 -2.5191827  -1.506745   -0.95726657  4.825762\n",
            " -1.5128064   0.74516106 -4.840467   -1.5374161  -0.30384    -4.0012465\n",
            " -0.99700534]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 33 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.78829795 -1.0164683  -0.8008516  -2.292538    3.495468   -0.7207213\n",
            " -0.5978826  -0.6107128  -2.3563323  -1.4693356  -0.83542365  4.9616423\n",
            " -1.4689056   0.71973175 -4.6522818  -1.4786059  -0.19607083 -3.647913\n",
            " -0.92275596]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 34 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.630811   -0.72684723 -0.49190706 -1.7757995   2.7998247  -0.54322785\n",
            " -0.22250246 -0.46942776 -1.7743337  -1.1412282  -0.56764996  4.0596657\n",
            " -1.0911822   0.55205935 -3.621047   -1.176153   -0.13001835 -2.8377743\n",
            " -0.7498358 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 35 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.7052753  -0.6391749  -0.6024668  -2.0016115   3.158376   -0.6358196\n",
            " -0.18319483 -0.5356421  -1.9780251  -1.2304004  -0.5754176   4.794725\n",
            " -1.2931602   0.71961546 -4.039619   -1.2696419  -0.04735015 -3.0795016\n",
            " -0.7906257 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 36 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8543792  -0.55115294 -0.6044365  -2.2314465   3.6391318  -0.6770953\n",
            " -0.06513846 -0.6477004  -2.3601246  -1.3984482  -0.55651015  5.623515\n",
            " -1.4493773   0.7133565  -4.614379   -1.4141891  -0.05623543 -3.3710089\n",
            " -0.91237634]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 37 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.667518   -0.33346426 -0.6454641  -1.9539292   3.1378925  -0.5830445\n",
            "  0.09481776 -0.46511403 -1.8853711  -1.1695687  -0.41713423  5.0641937\n",
            " -1.1916895   0.6570577  -3.9831076  -1.182612    0.07023202 -2.9096236\n",
            " -0.7888744 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 38 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6716667  -0.2520119  -0.5582291  -1.8543991   3.0673873  -0.5883145\n",
            "  0.22893713 -0.42948955 -1.8750765  -1.1733164  -0.38189524  5.0134687\n",
            " -1.1310222   0.57938623 -3.7704546  -1.1597859   0.07838868 -2.768117\n",
            " -0.6894172 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 39 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.96916664 -0.30230308 -0.8375426  -2.5641463   4.268674   -0.8669473\n",
            "  0.35347348 -0.8935214  -2.5061648  -1.6520824  -0.3477714   7.00265\n",
            " -1.759729    0.9403708  -5.3441257  -1.6951897   0.17952584 -3.8760676\n",
            " -0.8520849 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 40 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8280904  -0.03325016 -0.8064581  -2.6380723   4.175566   -0.7768208\n",
            "  0.57777363 -0.6613339  -2.5774024  -1.6122468  -0.44706467  7.366209\n",
            " -1.7326629   1.0028594  -5.1279864  -1.7208941   0.09498587 -3.7096567\n",
            " -0.82408655]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 41 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.64802474  0.08368463 -0.5133842  -1.9175806   3.2385452  -0.64061105\n",
            "  0.43621135 -0.48799822 -1.8908782  -1.1971595  -0.25081933  5.6843553\n",
            " -1.2035685   0.71765035 -3.8096807  -1.2233601   0.13246892 -2.7120705\n",
            " -0.58127636]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 42 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.77761096  0.16415457 -0.6915972  -2.3933256   3.893257   -0.69438493\n",
            "  0.6594261  -0.5072318  -2.4487405  -1.5228279  -0.34592998  7.0581274\n",
            " -1.4905132   0.93973845 -4.620408   -1.4870746   0.19207884 -3.2565818\n",
            " -0.49184304]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 43 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.960198    0.12720092 -0.9339027  -2.7090466   4.5054755  -0.98153245\n",
            "  0.7091843  -0.7808268  -2.529614   -1.7235819  -0.2970417   7.7981467\n",
            " -1.7740103   0.9907328  -5.302365   -1.7254537   0.27627748 -3.790362\n",
            " -0.2724046 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 44 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.78464025  0.3461111  -0.7778716  -2.4039333   4.0106926  -0.66843915\n",
            "  0.93632084 -0.61677665 -2.4402206  -1.561936   -0.20178181  7.2331\n",
            " -1.5829896   0.95001537 -4.772983   -1.6266211   0.24138857 -3.3238697\n",
            " -0.20672566]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 45 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.7953618   0.42272344 -0.6959568  -2.5228333   4.324553   -0.7408352\n",
            "  0.86833787 -0.6473623  -2.5760412  -1.5344406  -0.26619592  7.4487753\n",
            " -1.5572137   0.9666619  -4.878631   -1.5624193   0.2690894  -3.3969648\n",
            " -0.19832562]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 46 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6619068   0.38716424 -0.5737779  -1.9206498   3.3419275  -0.6155627\n",
            "  0.722461   -0.485828   -1.9599704  -1.2094724  -0.17686415  5.846215\n",
            " -1.2003148   0.6075611  -3.6704025  -1.1994623   0.22109564 -2.6071463\n",
            " -0.13528557]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 47 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0128658   0.3920497  -0.65916866 -2.6473188   4.6244993  -0.875675\n",
            "  0.92233723 -0.914926   -2.7063797  -1.6925571  -0.11744387  7.690613\n",
            " -1.7921677   0.97630864 -5.047345   -1.7544588   0.3881707  -3.744686\n",
            "  0.2158389 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 48 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.70937634  0.4595541  -0.3815544  -2.1732094   3.8188221  -0.7129586\n",
            "  0.88856554 -0.663036   -2.0894876  -1.3850672  -0.07033568  6.550158\n",
            " -1.3224565   0.7804715  -4.020895   -1.469199    0.3458792  -2.9681966\n",
            "  0.21453442]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 49 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.74111325  0.6190758  -0.16948968 -2.4183702   4.125711   -0.64739716\n",
            "  1.0415086  -0.5944588  -2.239389   -1.40925    -0.16767143  7.381199\n",
            " -1.443035    0.9422003  -4.062937   -1.4371676   0.42273793 -3.230501\n",
            "  0.11998708]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 50 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.845775    0.49482092  0.01413888 -2.269476    4.059028   -0.6783892\n",
            "  0.88251776 -0.6422667  -2.0498     -1.4779812  -0.09041584  6.8689117\n",
            " -1.3722484   0.70101535 -4.000415   -1.42063     0.37738672 -3.0019982\n",
            "  0.17062351]\n",
            "Epsilon reduced to 0.12800000000000003\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9529415   0.56173867  0.29727238 -2.6617246   4.745498   -0.8399736\n",
            "  1.3486093  -0.799213   -1.8967016  -1.6317239  -0.04801481  8.031454\n",
            " -1.6977209   0.74342465 -4.6853776  -1.8202938   0.34752935 -3.581057\n",
            "  0.25792676]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.74256325  0.67902625  0.3086109  -2.8069935   4.756077   -0.69501704\n",
            "  1.2445991  -0.80086786 -2.0229127  -1.6474146   0.01909949  7.4986186\n",
            " -1.6152894   0.9854257  -4.2355113  -1.8040937   0.2027593  -3.525936\n",
            "  0.36182532]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.69789153  0.63202584  0.35109684 -2.3646905   4.0920086  -0.58649427\n",
            "  1.1925858  -0.8389221  -1.4532468  -1.2072672   0.03390754  6.1863856\n",
            " -1.4784632   0.7734341  -3.9023168  -1.496921    0.1799135  -3.0051594\n",
            "  0.3523909 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-7.4158704e-01  7.0130646e-01  5.2949351e-01 -2.6572144e+00\n",
            "  4.4505992e+00 -7.2586691e-01  1.1831779e+00 -7.2673011e-01\n",
            " -1.6535614e+00 -1.6840776e+00 -4.6235868e-03  6.6368294e+00\n",
            " -1.5232517e+00  8.1753242e-01 -3.8119068e+00 -1.6579643e+00\n",
            "  2.7729359e-01 -3.3346710e+00  4.2054436e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7489538   0.63652575  0.45284843 -2.2086687   3.9132416  -0.60522413\n",
            "  1.144465   -0.6783383  -1.0039421  -1.3497237  -0.06539848  5.541118\n",
            " -1.3414638   0.63765264 -3.4789236  -1.4355999   0.5259652  -2.911978\n",
            "  0.5120552 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.65718436  0.748235    0.6401397  -2.5058243   4.1536026  -0.746981\n",
            "  1.1391006  -0.7747584  -1.4272771  -1.410075   -0.17033838  5.9948864\n",
            " -1.4813876   0.8526449  -3.565897   -1.5851959   0.3805016  -3.2082865\n",
            "  0.34975025]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-7.3706001e-01  7.4851310e-01  5.0311095e-01 -2.4074383e+00\n",
            "  4.1649294e+00 -6.6182297e-01  1.1723744e+00 -8.2775718e-01\n",
            " -1.1166929e+00 -1.2474793e+00  1.0686195e-03  5.4622474e+00\n",
            " -1.4180274e+00  7.3236144e-01 -3.7836878e+00 -1.5021554e+00\n",
            "  3.0949318e-01 -2.9750907e+00  5.6346130e-01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.66505945  0.71034825  0.7549196  -2.4678      4.1662884  -0.6443109\n",
            "  1.1719401  -0.69141346 -1.2168392  -1.5398335   0.02880122  5.284562\n",
            " -1.418552    0.81799275 -3.3701985  -1.577965    0.23893718 -3.0286062\n",
            "  0.47636092]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6543762   0.7002667   0.64399415 -2.3148658   3.9797177  -0.7274331\n",
            "  1.2776185  -0.8414349  -0.9592397  -1.3131392  -0.04163802  4.991364\n",
            " -1.4144111   0.77614    -3.4879315  -1.5221374   0.26141754 -2.9397783\n",
            "  0.51961356]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6564444   0.65487677  0.7366212  -2.2675571   3.6902823  -0.7261788\n",
            "  1.2570536  -0.6696108  -1.013706   -1.4162073  -0.20339926  4.6882443\n",
            " -1.3917928   0.704779   -3.0935829  -1.4303504   0.41664743 -2.8454688\n",
            "  0.42778906]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7239183   0.6779502   0.740888   -2.1822696   3.7681274  -0.66335976\n",
            "  1.4485105  -0.64162755 -0.7114023  -1.3474008  -0.08129084  4.339472\n",
            " -1.340224    0.6094962  -3.1010106  -1.4356946   0.5126471  -2.7792852\n",
            "  0.5730421 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6041396   0.7072318   0.8356385  -2.3071454   3.8301976  -0.6534049\n",
            "  1.3543117  -0.62826914 -0.94912535 -1.4287503   0.02909784  4.017998\n",
            " -1.3244336   0.6991301  -2.8106878  -1.456169    0.1962797  -2.7775102\n",
            "  0.4703277 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.68000424  0.59286547  0.7909272  -1.9630735   3.4977083  -0.57589036\n",
            "  1.4356494  -0.6806414  -0.5515404  -1.2322958  -0.06703256  3.6651511\n",
            " -1.100209    0.6020949  -2.6464703  -1.2816852   0.5027737  -2.5806587\n",
            "  0.5481753 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6687462   0.6444603   0.79826224 -2.0617821   3.5932693  -0.62995404\n",
            "  1.3113374  -0.74280053 -0.7201026  -1.1333513  -0.06416988  3.560881\n",
            " -1.1055205   0.5971415  -2.7100468  -1.3012316   0.38935626 -2.6697733\n",
            "  0.4327311 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5747573   0.6935971   0.90694535 -2.0570211   3.6954174  -0.57174474\n",
            "  1.6864399  -0.833658   -0.6991907  -1.202651   -0.01790895  3.5493026\n",
            " -0.9928195   0.65187573 -2.6082766  -1.43744     0.24709505 -2.7108483\n",
            "  0.5181649 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5709177   0.68102723  0.968685   -2.1737726   3.5723653  -0.62729436\n",
            "  1.6101277  -0.6693992  -0.8216519  -1.2133023  -0.15935856  3.4503083\n",
            " -0.8950314   0.66410935 -2.461621   -1.3794773   0.3756232  -2.760837\n",
            "  0.45055404]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5279257   0.67893845  0.6975374  -1.7595288   2.9484165  -0.53328353\n",
            "  1.4942981  -0.6104267  -0.30493766 -1.0219151   0.09067776  2.6687386\n",
            " -0.61815864  0.41841748 -2.0915444  -1.0724432   0.23534277 -2.3214698\n",
            "  0.43290457]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.39870098  0.49450558  0.75665575 -1.5277815   2.5180504  -0.49081388\n",
            "  1.1104074  -0.467639   -0.3662012  -1.0099527   0.09571502  1.9860994\n",
            " -0.42344666  0.39871904 -1.6542412  -1.0108438   0.07485706 -1.9615264\n",
            "  0.37301558]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5059573   0.49681056  0.7791253  -1.5719589   2.551125   -0.46372613\n",
            "  1.5259724  -0.57791555 -0.29021916 -0.9585237   0.02536961  2.2389874\n",
            " -0.50334525  0.5886853  -1.7730052  -1.0463802   0.10774315 -2.0461202\n",
            "  0.48532552]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.46861604  0.46543068  0.7987712  -1.6569281   2.7170317  -0.53607947\n",
            "  1.3447173  -0.50598896 -0.36257985 -1.0776198   0.07544401  1.9178241\n",
            " -0.39500055  0.56794274 -1.7903304  -1.081497    0.04043032 -2.1090112\n",
            "  0.4746495 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.53378326  0.5531819   0.8537408  -1.6172255   2.7158697  -0.52547413\n",
            "  1.5057642  -0.5114592  -0.24008173 -1.026508   -0.04027376  2.2461066\n",
            " -0.27170652  0.59947675 -1.7485281  -1.0623674   0.34361362 -2.1150682\n",
            "  0.5146282 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.4840053   0.5069305   0.86368495 -1.5812079   2.5582836  -0.55737245\n",
            "  1.3271661  -0.58089775 -0.3264781  -1.0231912  -0.15646861  2.0899944\n",
            " -0.4069127   0.73646593 -1.7400097  -1.0947231   0.34236908 -2.1290724\n",
            "  0.3886607 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 23 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.46138006  0.43680418  0.6896113  -1.3159671   2.2804358  -0.37520096\n",
            "  1.4511217  -0.5434538  -0.24399598 -0.78634727  0.03711538  1.8532352\n",
            " -0.3113027   0.6922747  -1.3786408  -0.92659914  0.0616747  -1.75453\n",
            "  0.42401424]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 24 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.39784107  0.4067707   0.49145722 -1.099787    1.9916426  -0.38773614\n",
            "  1.0060512  -0.41142863 -0.14642248 -0.71685565 -0.00589011  1.4927928\n",
            " -0.25075284  0.54869276 -1.1883986  -0.78011376  0.21438442 -1.5041015\n",
            "  0.44878727]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.42888477  0.49610382  0.78257436 -1.2444401   2.336998   -0.43863258\n",
            "  1.3045902  -0.56830376 -0.2501823  -0.82029545 -0.05759295  1.6325467\n",
            " -0.3059178   0.81767005 -1.511544   -0.9638367   0.22877155 -1.8908875\n",
            "  0.47732309]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4552462   0.4749053   0.7417949  -1.2007092   2.469393   -0.49627084\n",
            "  1.4630172  -0.5983276  -0.27284515 -0.87432927 -0.06215962  1.733893\n",
            " -0.06161714  0.84309095 -1.3926063  -1.0055647   0.21357256 -1.8648629\n",
            "  0.5128607 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.41558835  0.48890665  0.65577096 -1.2053095   2.314691   -0.47941396\n",
            "  1.2161878  -0.4869875  -0.18542102 -0.86155707  0.13309623  1.9532362\n",
            " -0.25226113  0.8952685  -1.4809864  -0.9863857   0.18215488 -1.7476447\n",
            "  0.49033087]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6139048   0.709505    0.8514368  -1.4541254   2.7077897  -0.67212\n",
            "  1.5837812  -0.7455352  -0.41330576 -1.0460137  -0.17910144  2.344335\n",
            " -0.42581818  1.3163364  -1.9898343  -1.2102247   0.35635123 -2.4574733\n",
            "  0.65968937]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 29 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.48274407  0.47705626  0.48896483 -1.0820683   2.2225463  -0.40156418\n",
            "  1.4469965  -0.5202712  -0.22181314 -0.7716391  -0.05024222  1.6603326\n",
            " -0.11784533  1.0318536  -1.4324847  -0.85832876  0.25341552 -1.8166012\n",
            "  0.65733683]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 30 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7262107   0.741436    1.0208719  -1.3476499   2.922171   -0.70304203\n",
            "  1.7649403  -0.6213407  -0.36762348 -1.2153491  -0.15286049  2.6769667\n",
            " -0.4741355   1.52097    -2.2794673  -1.0938334   0.50590754 -2.6275973\n",
            "  0.7732163 ]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 31 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.51613325  0.7051761   0.84638137 -1.127408    2.4633048  -0.5768773\n",
            "  1.486353   -0.43797842 -0.24905941 -0.94830674 -0.03252019  2.3541353\n",
            " -0.17064714  1.2736913  -1.6540152  -0.90622324  0.28222066 -2.0289702\n",
            "  0.74831104]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 32 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.39738667  0.37251076  0.46403757 -0.631299    1.4090348  -0.3721347\n",
            "  0.8260507  -0.26853165 -0.16476165 -0.5562629  -0.06886441  1.2817119\n",
            "  0.042345    0.6127602  -0.9468138  -0.4812399   0.10553466 -1.0901011\n",
            "  0.45161995]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 33 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.35723242  0.5835885   0.566786   -0.62971604  1.4567238  -0.46530786\n",
            "  0.9757749  -0.35293323 -0.14176822 -0.77347875 -0.0917715   1.3357949\n",
            " -0.14401788  0.83957404 -1.1492031  -0.6087896   0.15590264 -1.429037\n",
            "  0.49967834]\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 34 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5826697   0.6891145   0.64083993 -0.77293485  1.5378923  -0.4394199\n",
            "  1.192362   -0.35084584 -0.18888251 -0.78865993 -0.00508173  2.07245\n",
            " -0.24855481  1.0007557  -1.5141425  -0.5202018   0.35859948 -1.6721956\n",
            "  0.56060326]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 35 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.37900102  0.5857339   0.38907534 -0.59139746  1.1565074  -0.42031795\n",
            "  0.76057607 -0.33390287 -0.10901088 -0.64346194 -0.03764251  1.2990266\n",
            " -0.06929742  0.7095608  -0.99843913 -0.3930851   0.2032942  -1.310259\n",
            "  0.50082904]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 36 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3861616   0.50418115  0.50245476 -0.43140385  0.9866878  -0.42201766\n",
            "  0.8710529  -0.40474334 -0.20636958 -0.70725965 -0.11257122  1.0717134\n",
            " -0.07717859  0.7491273  -1.055306   -0.3994031   0.10992306 -1.3548445\n",
            "  0.41249374]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 37 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.42197192  0.6586577   0.7790777  -0.43678227  1.0910145  -0.35852194\n",
            "  1.3262397  -0.39481    -0.1390129  -0.7215616   0.01790312  1.6095316\n",
            " -0.06638476  1.0419792  -1.2032133  -0.51101935  0.20243017 -1.5146056\n",
            "  0.60084707]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 38 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.49147996  0.6376708   0.6094781  -0.66449744  1.0970591  -0.3544697\n",
            "  1.1838175  -0.3548592  -0.36212108 -0.7743871  -0.11402529  1.8090513\n",
            " -0.10999961  1.1219118  -1.2072034  -0.37557584  0.2599292  -1.6616457\n",
            "  0.46489745]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 39 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.41822997  0.70098996  0.6250438  -0.7435005   1.3096844  -0.50177604\n",
            "  1.0643511  -0.3819364  -0.36954102 -0.88342464  0.01426766  1.3288687\n",
            " -0.04414874  1.0581238  -1.2507687  -0.4595009   0.2427378  -1.6426698\n",
            "  0.7744942 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 40 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.47742546  0.6630728   0.8201005  -0.4964051   0.91212755 -0.42801455\n",
            "  1.245745   -0.38502693 -0.20151113 -0.6938339  -0.10234242  1.4709471\n",
            " -0.04691422  1.0474241  -1.1460633  -0.23389289  0.21913612 -1.6794285\n",
            "  0.70680076]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 41 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6612274   0.8054854   0.93899184 -0.6036242   1.1317292  -0.50138193\n",
            "  1.6370984  -0.54180723 -0.26806933 -1.0785718   0.06749117  1.95336\n",
            " -0.04547739  1.4434922  -1.8614904  -0.45029786  0.4291012  -2.1197789\n",
            "  0.9202715 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 42 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.43908924  0.7050765   0.5785596  -0.5706317   0.90297574 -0.63573545\n",
            "  1.1904382  -0.46048257 -0.36397794 -0.8999799  -0.23971465  1.4762346\n",
            " -0.08582199  1.2607863  -1.4405148  -0.34218314  0.27725297 -1.77968\n",
            "  0.66085726]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 43 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.30420128  0.42233923  0.28981796 -0.1614964   0.31145775 -0.23468669\n",
            "  0.6510553  -0.33569667 -0.13747676 -0.42008463 -0.06232804  0.5328511\n",
            "  0.06522675  0.47456536 -0.64498997 -0.17819767  0.13318975 -0.85683376\n",
            "  0.26848483]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 44 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.443623    0.70339173  0.7903806  -0.5082545   0.59434277 -0.3458258\n",
            "  1.4127645  -0.3904773  -0.26423937 -0.77367204  0.02681364  1.2378427\n",
            " -0.01943983  1.2441921  -1.296834   -0.2997619   0.2191995  -1.7652805\n",
            "  0.6892278 ]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 45 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.49326485  0.83508605  0.7794977  -0.46001813  0.7424846  -0.6071193\n",
            "  1.2621967  -0.56992537 -0.4430049  -0.9636184  -0.25851366  1.2857841\n",
            " -0.22376053  1.4322982  -1.4165992  -0.29639587  0.27584198 -1.9669547\n",
            "  0.56916463]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 46 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.27564925  0.4741519   0.3862161  -0.21444967  0.30834538 -0.29346627\n",
            "  0.6825796  -0.32027057 -0.1025072  -0.40369955 -0.05207451  0.334864\n",
            "  0.08519071  0.6029748  -0.6945195  -0.14084546  0.1735982  -0.93745154\n",
            "  0.44725636]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 47 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.44782716  0.69227254  0.6666064  -0.44214153  0.4783137  -0.29989973\n",
            "  1.0095553  -0.3947877  -0.35448745 -0.7250828  -0.05789882  0.79135275\n",
            " -0.00361291  1.076375   -1.1081735  -0.13154349  0.25532416 -1.6196084\n",
            "  0.5899727 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 48 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5720497   0.9845445   1.0379342  -0.5647373   0.78943133 -0.6141112\n",
            "  1.5615864  -0.54974896 -0.44259506 -1.1148967  -0.17127953  1.2396684\n",
            " -0.17702176  1.7800981  -1.762681   -0.35533687  0.4685617  -2.258788\n",
            "  0.7614325 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 49 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.28674626  0.5036384   0.43613863 -0.18815781  0.22993733 -0.35269412\n",
            "  0.59972084 -0.33381665 -0.10452641 -0.5112757  -0.08958825  0.34147996\n",
            "  0.05862662  0.653785   -0.7788332  -0.1034082   0.16883387 -1.0098509\n",
            "  0.4442404 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 50 reward=-1 new_state=[1 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3854713   0.5378456   0.28296745 -0.27409267  0.23331428 -0.25255007\n",
            "  0.6467945  -0.21130678 -0.16627958 -0.4400577  -0.04833782  0.40933153\n",
            "  0.0613392   0.62389183 -0.7865392   0.00612628  0.2664904  -1.0153888\n",
            "  0.32525945]\n",
            "Epsilon reduced to 0.10240000000000003\n",
            "\n",
            "Taking action 4\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4350638   0.559037    0.53666943 -0.38710874  0.95009494 -0.42574227\n",
            "  0.7742423  -0.36384267 -0.11067929 -0.8497845   0.02729955  1.1353145\n",
            " -0.11400536  0.9915776  -1.3201766  -0.51439935  0.33885726 -1.6437079\n",
            "  0.6232609 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.542142    0.7998599   0.94397855 -0.60823774  1.2591386  -0.6041623\n",
            "  1.384701   -0.45452142 -0.11989348 -1.2256235  -0.08164468  1.4388809\n",
            "  0.10459905  1.3877138  -1.7017134  -0.46957877  0.43740085 -2.3520324\n",
            "  0.88873225]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.41892174  0.42989585  0.58498377 -0.2714456   0.5153342  -0.36534443\n",
            "  0.6410132  -0.3467169  -0.17152023 -0.74488     0.07325079  1.021178\n",
            "  0.05303394  0.8876741  -0.7920472  -0.32277057  0.20036648 -1.3381704\n",
            "  0.4512977 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.47115818  0.59890544  0.78656536 -0.33571497  0.91336405 -0.34616607\n",
            "  1.1641572  -0.35365674 -0.06035508 -0.86869234 -0.07260739  1.1123369\n",
            "  0.04764476  1.0739932  -1.1569799  -0.51614255  0.23742001 -1.8476213\n",
            "  0.59063107]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.43267393  0.5428089   0.647501   -0.29659814  0.443797   -0.35329255\n",
            "  0.7328777  -0.38913098 -0.21343496 -0.7143694   0.03837258  1.0545636\n",
            "  0.01686671  1.053451   -0.9184943  -0.32511544  0.2686488  -1.545496\n",
            "  0.5017129 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 6 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.33443224  0.6359327   0.78044015 -0.3415698   0.7387191  -0.23057842\n",
            "  1.0695597  -0.2968534  -0.13608152 -0.66555905  0.01591783  1.0607651\n",
            "  0.01539124  1.0109721  -1.14893    -0.42706174  0.24829166 -1.5645906\n",
            "  0.47620693]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 7 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2593541   0.4277362   0.4958567  -0.16584681  0.24920574 -0.2481288\n",
            "  0.608611   -0.27507123 -0.16296878 -0.4459014  -0.03552493  0.6602395\n",
            " -0.05680886  0.7680515  -0.8644381  -0.24650039  0.15086292 -1.0778439\n",
            "  0.34615338]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.32586455  0.725148    0.60887814 -0.3384277   0.43392447 -0.31649762\n",
            "  1.1242863  -0.3580729  -0.18405667 -0.5749046  -0.03278776  0.88130546\n",
            " -0.12256475  1.0081564  -1.114444   -0.3019646   0.07974497 -1.3065732\n",
            "  0.3898739 ]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.35839418  0.5260481   0.5689682  -0.16002253  0.24806014 -0.41125628\n",
            "  0.6445392  -0.28371117 -0.16765852 -0.65796274 -0.08594874  0.5429847\n",
            " -0.06432677  0.848127   -1.0377481  -0.12283982  0.15296732 -1.1731347\n",
            "  0.30012375]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.32090932  0.5216154   0.68422544 -0.13328274  0.14771032 -0.25234064\n",
            "  0.6736388  -0.29111972 -0.07660376 -0.5117494  -0.04172397  0.5040718\n",
            " -0.03149612  0.8818434  -0.9112059  -0.21273541  0.19290271 -1.0061911\n",
            "  0.24532983]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.24046269  0.28179705  0.25257346 -0.17453846  0.06725657 -0.24239767\n",
            "  0.31069326 -0.14712285 -0.11536928 -0.32002482 -0.11709608  0.4492255\n",
            "  0.00371913  0.4259072  -0.5053772  -0.03046626  0.11925403 -0.5792547\n",
            "  0.17055447]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.24740684  0.3518804   0.3479153  -0.16284205  0.1127591  -0.26303968\n",
            "  0.43649942 -0.17799687 -0.03896819 -0.36095524  0.02078425  0.49153867\n",
            " -0.02024407  0.6516382  -0.80110955 -0.25257546  0.1392962  -0.66350543\n",
            "  0.10965319]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3832469   0.5163529   0.644644   -0.16510831  0.27463365 -0.34643754\n",
            "  0.5704484  -0.36476442 -0.17904271 -0.58083636 -0.09929968  0.62548435\n",
            "  0.02051573  0.8272058  -0.84384817 -0.21925335  0.19487648 -1.0137535\n",
            "  0.3153344 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3069784   0.47880828  0.5395367  -0.14937003  0.1190336  -0.2849471\n",
            "  0.51096576 -0.26650116 -0.10814659 -0.3256453  -0.01839663  0.52239466\n",
            "  0.00906767  0.74488    -0.7779562  -0.16094768  0.16344124 -0.7484814\n",
            "  0.18723772]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.20224793  0.3293165   0.35293338 -0.14089751  0.03058228 -0.26377907\n",
            "  0.3086401  -0.17115225 -0.02331086 -0.25222147 -0.00229049  0.4661318\n",
            " -0.06070809  0.5794417  -0.5665053  -0.21720892  0.112074   -0.49640036\n",
            "  0.06786488]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.19866195  0.31038025  0.26336902 -0.17956467  0.07376264 -0.20446585\n",
            "  0.24585532 -0.1735763  -0.10124507 -0.22634867 -0.10797112  0.43850932\n",
            " -0.00526389  0.46339655 -0.43185905 -0.11139008  0.12781948 -0.50809485\n",
            "  0.13942896]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5284099   0.72877634  0.8138301  -0.22169517  0.34671897 -0.3828881\n",
            "  0.8795773  -0.39830267 -0.12782499 -0.61339456  0.01066032  0.6400262\n",
            " -0.02798585  1.1161678  -1.2550607  -0.2749498   0.27986002 -0.9813895\n",
            "  0.2686431 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3620532   0.4639868   0.5407464  -0.17509666  0.11821099 -0.25724158\n",
            "  0.5289669  -0.2514843  -0.12600222 -0.27748433 -0.06125446  0.44181454\n",
            "  0.02385511  0.7977595  -0.8971261  -0.08090205  0.19673201 -0.6154494\n",
            "  0.27582014]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31627038  0.54016036  0.5122156  -0.17680825  0.07858999 -0.34331748\n",
            "  0.4778204  -0.22725612 -0.12589249 -0.17976883 -0.05985207  0.6048345\n",
            " -0.04808386  0.86686254 -0.81456286 -0.11297381  0.22121738 -0.54153633\n",
            "  0.14960276]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.410148    0.55626476  0.68051416 -0.20508166  0.15959516 -0.3074542\n",
            "  0.71014    -0.2769637  -0.12406548 -0.34336925 -0.00117687  0.4267935\n",
            "  0.03018899  0.962741   -1.1358598  -0.2130809   0.2002628  -0.43386143\n",
            "  0.23614725]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3735496   0.43085805  0.36358562 -0.21377896  0.2705974  -0.2593035\n",
            "  0.4332698  -0.31433335 -0.16437924 -0.48942593 -0.15000519  0.4492307\n",
            " -0.01379284  0.60557646 -0.7933408  -0.24457626  0.14566505 -0.51960355\n",
            "  0.27145526]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-3.7973300e-01  5.7466382e-01  6.2914276e-01 -2.4873015e-01\n",
            "  1.8802197e-01 -2.4972141e-01  7.0681304e-01 -2.7709520e-01\n",
            " -1.1380967e-01 -3.1375584e-01 -7.2995177e-04  5.2434570e-01\n",
            "  1.1095240e-02  9.9187022e-01 -1.0642844e+00 -2.4783653e-01\n",
            "  2.5048265e-01 -2.2830391e-01  2.0582902e-01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.1990805   0.2802115   0.24965665 -0.12584731  0.01379036 -0.19152525\n",
            "  0.24075319 -0.17198254 -0.09159939 -0.16006592 -0.02390698  0.36619872\n",
            "  0.0490129   0.47353306 -0.50508964 -0.04835355  0.13502046 -0.20839548\n",
            "  0.13762508]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.6580098e-01  6.8256360e-01  6.1261582e-01 -2.9613099e-01\n",
            "  2.2328591e-01 -2.5787631e-01  9.4605386e-01 -3.4890866e-01\n",
            " -2.0424719e-01 -1.8114085e-01  5.6249334e-04  6.7653966e-01\n",
            " -1.6037831e-02  1.0038154e+00 -9.9929142e-01 -2.9139555e-01\n",
            "  1.5046200e-02 -4.9356011e-01  3.1746757e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18064502  0.6371622   0.5657069  -0.27665597  0.21654363 -0.2656648\n",
            "  0.80265725 -0.35310656 -0.1208185  -0.09183002  0.02268388  0.6726319\n",
            " -0.0290276   0.8927539  -0.96370524 -0.31688094 -0.01400229 -0.34566286\n",
            "  0.31762978]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.3707187e-01  6.8048394e-01  6.3704175e-01 -4.0115196e-01\n",
            "  2.9722527e-01 -4.0256563e-01  8.5853058e-01 -3.0766582e-01\n",
            " -8.1584632e-02 -3.0980012e-01  3.4459084e-02  6.7585570e-01\n",
            "  8.5968769e-04  9.7021526e-01 -8.7391579e-01 -2.8064167e-01\n",
            " -1.1331359e-02 -3.8653603e-01  3.5334045e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2276802   0.7316191   0.59087276 -0.31691942  0.1907181  -0.23487987\n",
            "  0.9205499  -0.31791887 -0.09813964 -0.01887838  0.04098689  0.7314458\n",
            " -0.04265241  0.99607986 -0.95193523 -0.2445709   0.04464225 -0.2935171\n",
            "  0.32081795]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.34864604  0.5660132   0.6495585  -0.29243916  0.32224202 -0.20452768\n",
            "  0.8577413  -0.27679363 -0.02588839 -0.07947572 -0.03413584  0.76084375\n",
            "  0.03479932  0.9442949  -0.9972284  -0.3291508   0.26780373 -0.26715052\n",
            "  0.2859685 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2251677   0.6776741   0.5295199  -0.30063236  0.1836699  -0.2664148\n",
            "  0.80806106 -0.31337634 -0.04262612  0.01715404  0.0140052   0.61182094\n",
            " -0.01370322  0.9618928  -0.9476735  -0.22447655  0.0495602   0.0475795\n",
            "  0.36861622]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.25659895  0.6834479   0.6047213  -0.29843086  0.21172391 -0.28266326\n",
            "  0.92844087 -0.34627074 -0.09783898 -0.10524248 -0.00766108  0.67066324\n",
            " -0.00373222  1.0127438  -1.0407209  -0.32212403  0.00190549  0.18008713\n",
            "  0.32988882]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18963028  0.60983026  0.51664    -0.22165929  0.15063028 -0.21059935\n",
            "  0.69786847 -0.32075164  0.00860604 -0.05728608  0.01328195  0.5427473\n",
            "  0.02722407  0.8461565  -0.82177657 -0.25723505  0.03423736  0.3434039\n",
            "  0.29122946]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.4766415e-01  6.6567159e-01  6.3719970e-01 -4.0351358e-01\n",
            "  2.8498176e-01 -4.1027662e-01  8.5214412e-01 -2.8927857e-01\n",
            " -3.3327364e-04 -3.1442583e-01  2.2464434e-02  6.4588815e-01\n",
            "  4.7702651e-02  9.6983254e-01 -9.1065413e-01 -2.6765841e-01\n",
            " -1.1393887e-03  2.8972653e-01  3.8932809e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.20640358  0.6519352   0.5887309  -0.22134323  0.13248506 -0.1850466\n",
            "  0.7869233  -0.31028274 -0.01993953 -0.04223642  0.03355102  0.60947996\n",
            "  0.01873076  0.8493938  -0.8463429  -0.28049207  0.00585043  0.5354127\n",
            "  0.277665  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2431884   0.6808127   0.5133414  -0.23012733  0.12332138 -0.2091989\n",
            "  0.85727406 -0.3300045   0.01797876 -0.01632216  0.03488355  0.58025587\n",
            "  0.00782294  0.93198067 -0.9133189  -0.21848081  0.0212698   0.7038435\n",
            "  0.26286376]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.26479802  0.6866745   0.5751699  -0.25473267  0.15558256 -0.18375462\n",
            "  0.9279665  -0.3080803   0.02083772 -0.06893033  0.03841488  0.6257424\n",
            " -0.0036271   0.97578585 -0.9795696  -0.28725758  0.06551701  0.81999886\n",
            "  0.2845116 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2352739   0.6723336   0.58805746 -0.2698597   0.17961837 -0.23013794\n",
            "  0.87856776 -0.34789953 -0.00604701 -0.061799    0.01737528  0.640247\n",
            "  0.00163429  0.96190256 -0.89450544 -0.2855802   0.00241851  0.9641195\n",
            "  0.26011372]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 37 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.16603255  0.51093036  0.46072382 -0.14056647  0.0550646  -0.20940074\n",
            "  0.47060785 -0.25948292  0.03619544  0.07233305 -0.02395112  0.39188144\n",
            "  0.06707174  0.7718658  -0.7240369  -0.23699348  0.06276574  0.9452176\n",
            "  0.20415552]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 38 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.24057227  0.4559822   0.29343325 -0.21410793  0.07121414 -0.20489614\n",
            "  0.41178888 -0.16079237  0.03104737  0.02959603 -0.01651643  0.4738872\n",
            " -0.00820704  0.6938734  -0.80608267 -0.20074448  0.17872353  0.7446997\n",
            "  0.11770213]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 39 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.17342815  0.34012553  0.2683454  -0.13225603 -0.04928143 -0.19335414\n",
            "  0.33228576 -0.12927783  0.0971918   0.11595027  0.00672873  0.34299994\n",
            " -0.02400748  0.5554808  -0.5288912  -0.06179506  0.10272423  0.6388231\n",
            "  0.13032845]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 40 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.22365962  0.3800076   0.3776175  -0.23056437  0.06962531 -0.1886049\n",
            "  0.42735335 -0.16790679 -0.01631254  0.06019561 -0.0350663   0.45264268\n",
            "  0.03454174  0.6986074  -0.7875169  -0.09934679  0.11046761  0.8546295\n",
            "  0.21866807]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 41 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.17684673  0.2993371   0.30963334 -0.1204325  -0.03161699 -0.21629104\n",
            "  0.3052013  -0.13235505  0.09541786  0.02036546 -0.01773574  0.3270004\n",
            "  0.02269246  0.5602728  -0.6267972  -0.01487138  0.08818419  0.9117787\n",
            "  0.13767232]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 42 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.21715786  0.3954739   0.32333127 -0.16926092  0.04647566 -0.14207327\n",
            "  0.38573596 -0.1524118   0.06724106  0.0334726   0.02296847  0.45091742\n",
            "  0.02711309  0.6429743  -0.6983833   0.12381743  0.13809091  0.8690479\n",
            "  0.1270702 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 43 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.17571415  0.30335307  0.31898862 -0.20940585  0.04653953 -0.19902311\n",
            "  0.28432998 -0.16022523  0.06412175 -0.03763945  0.0199725   0.33329672\n",
            "  0.06370007  0.5787368  -0.65833724  0.22234543  0.10872489  0.8103462\n",
            "  0.13642868]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 44 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.22364487  0.39355215  0.30946794 -0.17509341  0.04550989 -0.14255838\n",
            "  0.363785   -0.17290772  0.05470172  0.00472914 -0.00692697  0.4097297\n",
            "  0.03945988  0.6341916  -0.7145933   0.42861265  0.15223971  0.96833825\n",
            "  0.10495582]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 45 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.16776825  0.29457828  0.3091693  -0.14745876  0.02122606 -0.18866508\n",
            "  0.26384652 -0.1436985   0.05234648  0.03914373 -0.02200395  0.37387842\n",
            "  0.04896824  0.55478483 -0.6150776   0.48794332  0.11121132  1.0002706\n",
            "  0.16618562]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 46 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.19781305  0.38085166  0.3734775  -0.3366038   0.19553727 -0.35432795\n",
            "  0.36367267 -0.10997739  0.10541213 -0.19801164  0.00829737  0.41409203\n",
            "  0.09527978  0.6624307  -0.8266659   0.4214994   0.12339348  0.7791954\n",
            "  0.1851691 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 47 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.1384889   0.28641516  0.26211628 -0.06635381 -0.06267049 -0.17221469\n",
            "  0.23879328 -0.14271213  0.12246679  0.05966284 -0.00489469  0.3004926\n",
            "  0.00116643  0.47282693 -0.47594798  0.63173807  0.07673556  0.8917943\n",
            "  0.07995697]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 48 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.206259    0.42038298  0.32170737 -0.19089293  0.04840986 -0.17521366\n",
            "  0.36594987 -0.15015581  0.05013192  0.03538804 -0.02203015  0.46887589\n",
            " -0.00299592  0.6365608  -0.638313    0.97272843  0.16297114  1.1195877\n",
            "  0.08406988]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 49 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.182431    0.36519802  0.31211355 -0.21096997  0.04384581 -0.22714756\n",
            "  0.34118834 -0.14643143  0.04903333  0.17954245 -0.00473901  0.44095504\n",
            " -0.00939002  0.6530555  -0.67296815  1.0946229   0.11415479  1.1441637\n",
            "  0.19519368]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 50 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.19332363  0.28539366  0.26113975 -0.20451231  0.05268224 -0.23462513\n",
            "  0.24020912 -0.07314429  0.02278758 -0.05719596 -0.09667737  0.38242874\n",
            "  0.10720447  0.5118862  -0.45870167  0.8776014   0.09973566  0.43657565\n",
            "  0.20579115]\n",
            "Epsilon reduced to 0.08192000000000003\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.27125385  0.76877224  0.65611976 -0.2206019   0.25501093 -0.39542437\n",
            "  0.7436573  -0.41631955  0.19356419 -0.36817688  0.08319816  0.31850132\n",
            "  0.09157799  1.0003607  -1.0332769   0.91530716 -0.00890665  1.2194515\n",
            "  0.6004435 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 2 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.3023077   0.5549419   0.6525253  -0.39780024  0.35990307 -0.5560358\n",
            "  0.6392435  -0.33205274  0.04656721 -0.6654485  -0.07220746  0.65915024\n",
            "  0.06611797  1.0417908  -1.2376417   0.81361973  0.2363449   0.5279822\n",
            "  0.51472694]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 3 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2992681   0.37338635  0.4313405  -0.15904267  0.17078196 -0.3163884\n",
            "  0.2699145  -0.2067584   0.1803888  -0.30908749  0.0218454   0.6531646\n",
            "  0.04707866  0.69784987 -0.5372928   0.6916421   0.08884333  0.6049022\n",
            "  0.21519077]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 4 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.29982015  0.31429824  0.3853833  -0.2909028   0.11932693 -0.32891694\n",
            "  0.23340003 -0.16122298  0.08289922 -0.35741717 -0.05339039  0.52081144\n",
            "  0.09732135  0.7685805  -0.6534234   0.94781053  0.18692046  0.6552012\n",
            "  0.27203244]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 5 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.32327595  0.2980202   0.2529183  -0.23676646  0.08780653 -0.21640767\n",
            "  0.21074285 -0.18984361 -0.04092894 -0.31043354  0.06142613  0.58137536\n",
            "  0.14863542  0.6818476  -0.5314392   0.7232865   0.27610645  0.23160288\n",
            "  0.27610078]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 6 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.27584842  0.21021403  0.14370394 -0.13864331  0.03785386 -0.17411779\n",
            "  0.2222035  -0.12095374  0.06669114 -0.17505082  0.01851226  0.25452268\n",
            "  0.13712384  0.5998427  -0.43591547  1.0274793   0.19915931  0.3358191\n",
            "  0.17111996]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 7 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.36691108  0.4336021   0.34055224 -0.31925535  0.29614702 -0.2803616\n",
            "  0.37918556 -0.15081118  0.03629997 -0.2822235  -0.01203717  0.69896585\n",
            "  0.13865198  0.7828271  -0.57045734  1.1428438   0.1761389   0.23891664\n",
            "  0.36332408]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 8 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.2674201   0.15866432  0.22816628 -0.1536424   0.05084654 -0.18121031\n",
            "  0.14944865 -0.15689014 -0.03966983 -0.24784534 -0.04536784  0.33236185\n",
            "  0.18246661  0.60548437 -0.43494642  1.0363057   0.19821429  0.35907423\n",
            "  0.22974238]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 9 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.31829894  0.36077535  0.39229885 -0.28756762  0.2917147  -0.25828016\n",
            "  0.3195523  -0.16665693  0.0095122  -0.2754022  -0.02915025  0.68949074\n",
            "  0.15069418  0.7401771  -0.46069753  1.1646614   0.11079571  0.24031359\n",
            "  0.36459643]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 10 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.2260745   0.16467294  0.20301402 -0.12929864  0.03208669 -0.18742338\n",
            "  0.14832155 -0.16256766 -0.04167577 -0.15601657 -0.02508761  0.2965167\n",
            "  0.15377025  0.6715262  -0.45174214  1.1821808   0.18632995  0.4558727\n",
            "  0.20929788]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 11 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.28415555  0.37543085  0.3611266  -0.24956264  0.3015432  -0.28773892\n",
            "  0.2542551  -0.1786499   0.0251715  -0.2669328  -0.03602768  0.72390145\n",
            "  0.10486276  0.7591348  -0.4373073   1.319023    0.10060326  0.352346\n",
            "  0.28394172]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 12 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.2694399   0.22940451  0.2073333  -0.1812559   0.07127228 -0.23152688\n",
            "  0.17569667 -0.15633217 -0.03383787 -0.16949955 -0.03827975  0.40673926\n",
            "  0.1170443   0.78936225 -0.46823987  1.5234103   0.21617547  0.48602846\n",
            "  0.20140499]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 13 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.28974184  0.36727387  0.36175817 -0.24006574  0.28903937 -0.25574392\n",
            "  0.24555773 -0.17492214  0.03999498 -0.29889208 -0.02573952  0.70859504\n",
            "  0.12231784  0.7227106  -0.37149122  1.5427668   0.11490986  0.27459228\n",
            "  0.26334995]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 14 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.23363036  0.13956551  0.20011033 -0.10755061  0.03461079 -0.15572831\n",
            "  0.16682732 -0.14086193  0.04142334 -0.16942307  0.00206184  0.24815983\n",
            "  0.1535498   0.6296471  -0.33614203  1.5288718   0.13702042  0.37515104\n",
            "  0.1752435 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 15 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.29370305  0.37137747  0.3661923  -0.24261329  0.29343015 -0.25845134\n",
            "  0.24850895 -0.17702335  0.04022374 -0.3006896  -0.02527266  0.71785545\n",
            "  0.12343852  0.75132555 -0.37678126  1.8206875   0.11644489  0.28545216\n",
            "  0.2664499 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 16 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-3.1379724e-01  2.4872178e-01  1.7994741e-01 -1.8047324e-01\n",
            "  6.0543258e-02 -1.9621815e-01  2.0070972e-01 -1.5945797e-01\n",
            "  3.1802442e-04 -2.2033630e-01 -1.1016195e-02  3.6896279e-01\n",
            "  1.5449715e-01  8.0233133e-01 -5.0832272e-01  2.0834599e+00\n",
            "  2.6193064e-01  4.1351259e-01  1.9488385e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 17 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.36825424  0.46042585  0.35120696 -0.32164645  0.31216994 -0.27622277\n",
            "  0.37751663 -0.16964212  0.05135566 -0.24593069  0.00699037  0.7468808\n",
            "  0.12026542  0.966894   -0.5377227   2.458885    0.1737754   0.28547966\n",
            "  0.33565623]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 18 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-3.59188706e-01  2.59752095e-01  2.41851225e-01 -2.47296244e-01\n",
            "  9.68205184e-02 -2.34072357e-01  3.00886482e-01 -1.68770313e-01\n",
            " -1.83393396e-02 -1.66742921e-01 -1.66117842e-03  4.70345527e-01\n",
            "  1.13896035e-01  1.01514888e+00 -6.50472701e-01  2.51809645e+00\n",
            "  2.46553227e-01  5.10239542e-01  3.75283450e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 19 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.32814     0.29191574  0.31923229 -0.25419688  0.1076824  -0.25192592\n",
            "  0.21990524 -0.18893212 -0.07963773 -0.35092032  0.03427505  0.6433583\n",
            "  0.16437341  0.86625344 -0.5415258   2.3725984   0.2762152   0.38175744\n",
            "  0.3321344 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 20 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.31483543  0.16104819  0.27451152 -0.17269641  0.07242128 -0.20233782\n",
            "  0.18875982 -0.16609845 -0.0298229  -0.30183968 -0.03974649  0.42881387\n",
            "  0.16126977  0.77592766 -0.49713784  2.4554465   0.21864516  0.4570747\n",
            "  0.336406  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 21 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.33535972  0.2929296   0.3464645  -0.28244945  0.16238016 -0.2639144\n",
            "  0.25398186 -0.202578   -0.05263632 -0.34827888  0.02728571  0.6454202\n",
            "  0.13088472  0.99260455 -0.6510261   2.665208    0.26753706  0.4823336\n",
            "  0.3866849 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 22 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.27283934  0.19107123  0.2448384  -0.16254573  0.07874512 -0.21671556\n",
            "  0.15641841 -0.18899491 -0.04415659 -0.18728767 -0.0387435   0.39638633\n",
            "  0.15648806  0.84768885 -0.485557    2.6503997   0.1872195   0.52650446\n",
            "  0.22803685]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 23 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.31372935  0.4070071   0.39422202 -0.27017766  0.337158   -0.31104127\n",
            "  0.27783182 -0.19482195  0.02580572 -0.2785559  -0.03342973  0.79506594\n",
            "  0.11217187  0.93905    -0.48073956  3.1111274   0.11066691  0.42211068\n",
            "  0.3099659 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 24 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.280763    0.18129888  0.24585278 -0.15182132  0.06263673 -0.18251114\n",
            "  0.14805394 -0.18654783 -0.0298042  -0.22413369 -0.02724451  0.37789944\n",
            "  0.17618395  0.78147614 -0.42025873  2.8429964   0.20416152  0.43630177\n",
            "  0.20609325]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 25 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.34289953  0.39678365  0.41071713 -0.27536997  0.33056337 -0.29467627\n",
            "  0.29506385 -0.18287499  0.02545624 -0.36029184 -0.0387499   0.7799491\n",
            "  0.15532506  0.8873885  -0.48566645  3.462605    0.13961704  0.3512363\n",
            "  0.33901557]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 26 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.34800237  0.21728575  0.2509233  -0.18806644  0.1019194  -0.22871816\n",
            "  0.22578584 -0.1975867  -0.00961976 -0.25461456 -0.01703351  0.46111804\n",
            "  0.14705707  0.95074356 -0.6395808   3.4973507   0.23089997  0.5633982\n",
            "  0.3435622 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 27 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.3570543   0.3573996   0.3332339  -0.29974937  0.15952949 -0.25372314\n",
            "  0.27210006 -0.19769834 -0.01922391 -0.34458235  0.06185681  0.7060682\n",
            "  0.09879515  1.0536981  -0.56266683  3.585521    0.3249658   0.42021862\n",
            "  0.3215923 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 28 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.42359856  0.39929688  0.3406303  -0.29182497  0.13697548 -0.26115352\n",
            "  0.382686   -0.18086082 -0.01086738 -0.24435851  0.08221779  0.54854393\n",
            "  0.16876319  1.0695274  -0.6919713   3.9305964   0.27144057  0.4147627\n",
            "  0.46878085]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 29 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.34579545  0.35040078  0.30063346 -0.27378413  0.12066843 -0.24154937\n",
            "  0.26212943 -0.20115875 -0.04778532 -0.31418565  0.05913025  0.67773265\n",
            "  0.12056122  1.0342271  -0.5379764   3.8808627   0.31096548  0.46551263\n",
            "  0.30925688]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 30 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.3593261   0.21983545  0.24542193 -0.18180242  0.08260264 -0.20514102\n",
            "  0.22425523 -0.19521543 -0.02555617 -0.3006528  -0.02575421  0.3723948\n",
            "  0.22989751  0.89203006 -0.63750076  4.067742    0.25642753  0.48157066\n",
            "  0.28466037]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 31 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.35668397  0.3154678   0.36969608 -0.27996176  0.16850246 -0.23411553\n",
            "  0.25100568 -0.22758718 -0.02650619 -0.37001395  0.0636194   0.6943209\n",
            "  0.13605069  1.0156753  -0.5724877   4.120857    0.29344207  0.40732554\n",
            "  0.34574535]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 32 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.33760092  0.18869703  0.29998267 -0.1784676   0.09236606 -0.20950758\n",
            "  0.19992824 -0.20211326 -0.01854741 -0.29210886 -0.02169256  0.49560377\n",
            "  0.1541271   0.8987315  -0.49681073  4.164481    0.2289546   0.49479765\n",
            "  0.32286435]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 33 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.33975917  0.40236115  0.39864546 -0.26471388  0.270214   -0.3322608\n",
            "  0.29126534 -0.21140786  0.0079698  -0.30210513 -0.02866312  0.6663174\n",
            "  0.12226172  1.093425   -0.51190454  4.768853    0.11050081  0.38075545\n",
            "  0.35779124]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 34 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.35760337  0.289375    0.34530917 -0.2086735   0.11280522 -0.22651717\n",
            "  0.23131761 -0.21700257 -0.01724239 -0.3287554   0.02985584  0.5202324\n",
            "  0.18754302  0.9218521  -0.5467505   4.547129    0.24059905  0.4956424\n",
            "  0.39022344]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 35 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.4373072   0.37685746  0.39871752 -0.35029325  0.20142981 -0.22666559\n",
            "  0.3882312  -0.2521042  -0.02784835 -0.33306986  0.10237883  0.71295285\n",
            "  0.17119268  1.2181944  -0.7787387   5.2444315   0.32358736  0.38401324\n",
            "  0.4535619 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 36 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-4.0967008e-01  3.3438745e-01  3.2832122e-01 -2.2581732e-01\n",
            "  1.2934624e-01 -2.2671203e-01  2.8642532e-01 -2.3041706e-01\n",
            "  3.0169436e-03 -3.5836098e-01  4.8134428e-02  5.1110470e-01\n",
            "  2.1258360e-01  9.9073541e-01 -6.9111562e-01  5.1446409e+00\n",
            "  2.7630550e-01  4.9710754e-01  4.1613677e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 37 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.43551925  0.35587323  0.46409813 -0.3680894   0.21631    -0.2578417\n",
            "  0.39814857 -0.24627234 -0.07454561 -0.37613302  0.06742229  0.76094264\n",
            "  0.18124005  1.2377223  -0.7711368   5.6296816   0.313393    0.41917756\n",
            "  0.5104147 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 38 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.36746338  0.29348758  0.29245427 -0.22266804  0.11250038 -0.25127465\n",
            "  0.25944358 -0.21198589 -0.03838662 -0.26378915 -0.02076521  0.51407796\n",
            "  0.16900507  1.0688032  -0.5711087   5.2448344   0.2772313   0.5297592\n",
            "  0.24945135]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 39 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.38393542  0.4499708   0.3978753  -0.28663757  0.25158298 -0.30371985\n",
            "  0.36283994 -0.22426735  0.03043514 -0.3546788  -0.01564121  0.636736\n",
            "  0.10676321  1.1780916  -0.51989466  5.6968784   0.17561458  0.35470763\n",
            "  0.35809752]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 40 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.4002974   0.33241415  0.39580974 -0.23650563  0.15192197 -0.24869224\n",
            "  0.2857211  -0.24374333 -0.0292171  -0.36685172  0.03342348  0.58348966\n",
            "  0.20244989  1.0293989  -0.6334046   5.707579    0.25656763  0.5176774\n",
            "  0.43478405]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 41 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.40296212  0.36801475  0.4058592  -0.29178894  0.18678659 -0.2528855\n",
            "  0.31950727 -0.26961544 -0.07454031 -0.39530158  0.06882586  0.7608159\n",
            "  0.1763206   1.1437678  -0.67237264  6.0149293   0.3031862   0.48181894\n",
            "  0.39235488]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 42 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.41565412  0.34913352  0.4150128  -0.24655698  0.16753215 -0.2565707\n",
            "  0.30650613 -0.25338277 -0.03405989 -0.3803649   0.03497752  0.6068552\n",
            "  0.20741671  1.0679959  -0.66543436  6.114162    0.2617253   0.5243925\n",
            "  0.4519289 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 43 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.44404298  0.40454403  0.47144613 -0.34041896  0.25663298 -0.28100997\n",
            "  0.36260313 -0.27820796 -0.04942396 -0.44918048  0.07763688  0.84513026\n",
            "  0.15993705  1.2370613  -0.74969107  6.570371    0.33098942  0.4466885\n",
            "  0.43168652]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 44 reward=2 new_state=[0 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.44569066  0.39185047  0.47775117 -0.31824297  0.21623378 -0.29336208\n",
            "  0.4210875  -0.27801287 -0.07948963 -0.26702523  0.04303845  0.6735286\n",
            "  0.20350806  1.3305887  -0.8299014   6.907518    0.2363543   0.6190712\n",
            "  0.5829326 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 45 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.5582742   0.6914549   0.5340129  -0.4779028   0.71562326 -0.31401074\n",
            "  0.6635648  -0.24515967  0.02182719 -0.45746115 -0.00790635  1.0813167\n",
            "  0.14117102  1.4388188  -0.99879354  6.8836813   0.18339297  0.36256918\n",
            "  0.4929887 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.48082072  0.5154933   0.3667698  -0.27290156  0.40716502 -0.23058005\n",
            "  0.5188659  -0.26349154 -0.02075441 -0.29029956 -0.04908938  0.58162665\n",
            "  0.22452262  1.3176931  -0.99706185  6.4062552   0.2387092   0.6498556\n",
            "  0.39829203]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.45520532  0.19135858  0.23307827 -0.27434266  0.16197088 -0.2557787\n",
            "  0.3978519  -0.2619403   0.06045533 -0.21635376 -0.03614879  0.46775955\n",
            "  0.11568177  1.1329265  -0.9521071   6.084708    0.18576355  0.2617056\n",
            "  0.30714276]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.37399396  0.17203715  0.2763126  -0.21857505  0.11087408 -0.23888348\n",
            "  0.38015497 -0.19884713  0.03835353 -0.13232258 -0.07103854  0.3575395\n",
            "  0.16275765  1.0308197  -0.8255783   5.3691797   0.16539368  0.3546589\n",
            "  0.33020294]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.37043473  0.7300531   0.43071914 -0.2915586   0.4784634  -0.33371595\n",
            "  0.62561077 -0.17708118  0.01038465  0.07619968 -0.09564002  0.5520138\n",
            "  0.19747822  1.3109994  -0.8973596   5.125342    0.2031798   0.8308131\n",
            "  0.32540676]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.5104935   0.8991688   0.7137535  -0.44984213  0.58167267 -0.62270206\n",
            "  0.9538043  -0.3408178   0.02280574 -0.05777611 -0.12226539  0.5332918\n",
            "  0.37643832  1.9697887  -1.5418952   6.4504795   0.04088374  1.1556671\n",
            "  0.67465705]\n",
            "Epsilon reduced to 0.06553600000000002\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.54536456  0.94574076  0.7685523  -0.47096428  0.5709164  -0.5230972\n",
            "  0.9369051  -0.51639974 -0.00928218  0.21006952  0.1292471   1.3846653\n",
            " -0.08562204  2.0134137  -1.4289559   7.959209    0.31766006  0.9646002\n",
            "  0.60638833]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-5.2855873e-01  6.8149906e-01  9.0913570e-01 -8.1313097e-01\n",
            "  7.9512429e-01 -5.8073890e-01  9.9689782e-01 -3.4735355e-01\n",
            "  7.9545565e-03  5.9122532e-03  7.8281663e-02  1.2755291e+00\n",
            "  5.1451862e-02  2.0280988e+00 -1.6380248e+00  6.8600512e+00\n",
            "  3.0318129e-01  1.0867704e+00  7.7170342e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.622266    0.8265771   0.7474311  -0.62422687  0.46910152 -0.47803164\n",
            "  0.7887804  -0.45784858  0.05627138  0.5985095   0.10889199  1.3833169\n",
            "  0.07354603  2.2080996  -1.5660042   8.731727    0.39378753  1.4484391\n",
            "  0.5927572 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6074842   0.8813729   0.8463643  -0.9569093   0.812584   -0.6056021\n",
            "  0.98187786 -0.35122105  0.08221605  0.25835824  0.11845133  1.3063083\n",
            "  0.25106892  2.204239   -1.773564    7.3297777   0.34081766  1.0269307\n",
            "  0.79074717]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-5.6726277e-01  6.5729582e-01  6.6383183e-01 -4.3977717e-01\n",
            "  3.6887476e-01 -4.6441785e-01  6.4731693e-01 -4.1400531e-01\n",
            " -3.6706224e-02  5.7880551e-01  3.8534652e-02  1.2078781e+00\n",
            " -3.7067700e-03  1.8374826e+00 -1.3564751e+00  8.0777025e+00\n",
            "  3.3555132e-01  1.2727461e+00  4.7497112e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.57323927  0.74285376  0.7488674  -0.88255745  0.71095586 -0.70492095\n",
            "  0.8805056  -0.29873955  0.07206459  0.24947388  0.107246    1.2534151\n",
            "  0.14573741  1.995334   -1.5430453   6.9339952   0.25116026  0.98332894\n",
            "  0.62587225]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4655884   0.6282625   0.5840178  -0.46219155  0.37598658 -0.498339\n",
            "  0.49861428 -0.37982693 -0.05031199  0.5478278   0.01714816  1.1787224\n",
            "  0.09104141  1.5641814  -1.1730405   7.076212    0.30717576  0.93514097\n",
            "  0.43360713]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.48359185  0.7788651   0.68200254 -0.61441207  0.4712995  -0.4508966\n",
            "  0.80060434 -0.3281604  -0.02546434  0.570035    0.01842078  1.1494292\n",
            "  0.10844105  2.0591836  -1.3394377   6.3363857   0.32730836  1.5071895\n",
            "  0.48475724]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5025344   0.7145804   0.6319339  -0.5545343   0.4120586  -0.45898655\n",
            "  0.5436707  -0.37731197  0.01163069  0.7136231   0.08475194  1.3032212\n",
            "  0.03377194  1.8375026  -1.2114862   6.9040074   0.3188327   1.2756555\n",
            "  0.37881598]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.47456342  0.58743453  0.81170285 -0.75092113  0.6971645  -0.5345769\n",
            "  0.880212   -0.3105319   0.01225448  0.35745603  0.06870157  1.1439774\n",
            "  0.04961746  1.8388305  -1.4797802   5.4435725   0.28314564  1.0204802\n",
            "  0.6864575 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5021193   0.6489343   0.61533576 -0.5218499   0.34441754 -0.46339443\n",
            "  0.56217563 -0.39160064 -0.00815414  0.7234045   0.02696911  1.2219999\n",
            "  0.05243802  1.8043802  -1.2518171   6.5338745   0.320067    1.3146868\n",
            "  0.43596092]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5354485   0.5579808   0.7782812  -0.79204124  0.8031835  -0.588167\n",
            "  0.9935337  -0.31419078  0.02021723  0.40177208  0.03397027  1.1614852\n",
            "  0.00803346  1.9117502  -1.5967994   5.6821485   0.21959026  0.73879355\n",
            "  0.7456994 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.55685484  0.8280612   0.74244773 -0.5176846   0.61870736 -0.50037515\n",
            "  0.9182088  -0.44574088  0.08976898  0.72880614  0.081945    1.4119521\n",
            " -0.03644611  1.9849305  -1.3191214   5.874337    0.2108752   1.1390762\n",
            "  0.49012542]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4534011   0.7310805   0.67224604 -0.6842693   0.7039466  -0.55707085\n",
            "  0.67048144 -0.2697743   0.15825401  0.33991277  0.10771616  1.189799\n",
            "  0.14282584  1.6015009  -1.2446367   3.792843    0.26174408  1.0766832\n",
            "  0.49138013]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-4.3696848e-01  5.0037158e-01  5.8371603e-01 -3.7582713e-01\n",
            "  2.1001238e-01 -3.7252170e-01  4.6963000e-01 -3.3290878e-01\n",
            " -6.5827102e-04  7.5877666e-01  2.2288462e-02  9.4815487e-01\n",
            "  2.3116086e-02  1.6012704e+00 -1.0480309e+00  5.4206624e+00\n",
            "  2.8223968e-01  1.2988914e+00  3.9520553e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.43011248  0.6066665   0.6782795  -0.75770295  0.5565617  -0.5312487\n",
            "  0.6746898  -0.27806768  0.01443507  0.6144004   0.1043761   1.0184194\n",
            "  0.19936469  1.6796372  -1.2440956   4.1600313   0.20830448  0.8548924\n",
            "  0.5442077 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.43100798  0.5376354   0.6100142  -0.39392668  0.27698037 -0.40156922\n",
            "  0.5731429  -0.3483465  -0.01495015  0.79088736  0.03464789  1.0759538\n",
            " -0.04092622  1.6378475  -1.0599179   4.973473    0.27129436  1.2132294\n",
            "  0.39426252]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.37021998  0.5206718   0.5577922  -0.47634447  0.43842226 -0.319604\n",
            "  0.7951369  -0.29203987 -0.03829683  0.6117752   0.02328246  0.9342868\n",
            " -0.12093468  1.7819288  -1.1433233   3.6067278   0.23623845  1.2322614\n",
            "  0.42216468]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4691216   0.64783394  0.61448777 -0.44504535  0.28161678 -0.4357637\n",
            "  0.65918726 -0.34962842  0.00902549  0.8425843   0.04407481  1.1097208\n",
            " -0.04743168  1.868035   -1.1080546   4.803671    0.31766155  1.375521\n",
            "  0.3608187 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.53105515  0.5984753   0.55082905 -0.58536434  0.4468234  -0.31083283\n",
            "  0.68724525 -0.35255376  0.01976208  0.84074783  0.11797827  1.0237902\n",
            "  0.10850508  1.816185   -1.2384338   4.073826    0.21934482  1.1729667\n",
            "  0.4166066 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.41610214  0.5768208   0.56293035 -0.530325    0.35664067 -0.3881409\n",
            "  0.44810545 -0.37441832 -0.04627174  0.8591356   0.04177051  1.1589483\n",
            "  0.04592339  1.769005   -1.076824    4.259038    0.30137715  1.381694\n",
            "  0.3734645 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4058915   0.5239003   0.5414056  -0.5093131   0.36446118 -0.3546689\n",
            "  0.5905927  -0.31170404 -0.03919022  0.638022    0.00736928  0.9263335\n",
            "  0.10728034  1.6449245  -1.1212491   3.4554482   0.25416616  1.2009335\n",
            "  0.3922341 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4475076   0.47532243  0.53499347 -0.41504714  0.15562528 -0.3434894\n",
            "  0.5472566  -0.29815888 -0.00604738  0.8278656   0.04569891  0.80738264\n",
            "  0.0439442   1.6261361  -1.1636534   4.293575    0.25918078  1.1939337\n",
            "  0.4512188 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.46467948  0.49758518  0.58592117 -0.5635879   0.3992943  -0.3634568\n",
            "  0.7026426  -0.31813332  0.033771    0.72737885  0.0556463   0.92740744\n",
            "  0.07952178  1.7701782  -1.1921574   3.1255283   0.21654718  1.166614\n",
            "  0.48245966]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4418308   0.5540932   0.57024914 -0.48647428  0.23893641 -0.3348886\n",
            "  0.47188893 -0.3404086   0.01252182  0.94055766  0.03893853  0.9877315\n",
            "  0.05023365  1.7535     -1.0391088   3.6162696   0.28923965  1.3485636\n",
            "  0.43170753]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.44668704  0.5625495   0.6480659  -0.7261566   0.6057713  -0.5173014\n",
            "  0.5660454  -0.25219446  0.09170523  0.48776844  0.11166799  1.1007116\n",
            "  0.09979632  1.4977353  -1.156583    2.7233317   0.23595068  0.8553759\n",
            "  0.48441246]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.38267568  0.5782382   0.5922809  -0.4576255   0.43508184 -0.39455387\n",
            "  0.43538898 -0.31493157  0.06184047  0.68242157 -0.02026824  1.0421584\n",
            " -0.01733026  1.6169312  -0.92622083  3.6260438   0.10306785  1.2704917\n",
            "  0.3313408 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.45475903  0.49201086  0.5420318  -0.54807746  0.45218694 -0.3091897\n",
            "  0.58762276 -0.32372114 -0.00399497  0.7880666   0.04738319  1.0021107\n",
            "  0.05055486  1.6415495  -1.019939    2.917342    0.17011715  1.0706421\n",
            "  0.34608003]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3874308   0.58413184  0.5537787  -0.36432844  0.4319591  -0.45016408\n",
            "  0.61433524 -0.29011378  0.09864724  0.642803   -0.02586772  1.1083493\n",
            " -0.06054883  1.463972   -0.8999981   3.0295038   0.16165937  1.0247167\n",
            "  0.26727915]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4017964   0.47147146  0.48347536 -0.47649583  0.32610926 -0.3125738\n",
            "  0.5808679  -0.29816768  0.03434502  0.7051549   0.106208    0.92182344\n",
            "  0.04275127  1.5543181  -0.9713891   2.2772918   0.14396013  0.9910576\n",
            "  0.3552673 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.7350932e-01  5.5700356e-01  5.6921494e-01 -4.7534171e-01\n",
            "  3.9967778e-01 -4.3125689e-01  5.5263978e-01 -3.2974845e-01\n",
            "  8.7047424e-03  5.1867390e-01  1.2408851e-03  9.5663106e-01\n",
            "  5.7662643e-02  1.5783631e+00 -1.0687070e+00  2.9531190e+00\n",
            "  1.3397820e-01  1.1260324e+00  2.5191641e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.48412558  0.56090915  0.4686247  -0.5900055   0.47807813 -0.323845\n",
            "  0.66536456 -0.31949174  0.04756002  0.5511697   0.04228627  1.090249\n",
            "  0.08222719  1.5962443  -1.2389877   2.3025088   0.23650527  1.049609\n",
            "  0.41588157]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.410463    0.50797486  0.42697018 -0.41962123  0.1896931  -0.30799317\n",
            "  0.38879693 -0.31981936  0.00892231  0.77831686  0.02716521  0.87302554\n",
            "  0.04443679  1.5278534  -0.9878555   2.6890023   0.2786637   1.1998761\n",
            "  0.31477347]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 34 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.37177262  0.746904    0.7545782  -0.5362559   0.37065017 -0.5002069\n",
            "  0.93851346 -0.41856214 -0.0625475   0.64123094 -0.05857167  1.0325809\n",
            " -0.02202291  1.9588383  -1.4271272   1.9284649   0.33165964  0.86759734\n",
            "  0.60652405]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3943156   0.6845242   0.61327994 -0.42693135  0.4427562  -0.49256125\n",
            "  0.66681564 -0.38238254  0.03191372  0.53122175 -0.01150205  0.8184987\n",
            "  0.21113856  1.5055981  -1.0091046   1.827678    0.23773922  0.7341444\n",
            "  0.58254504]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.22064959  0.6304136   0.5066755  -0.23339313  0.22824146 -0.34282646\n",
            "  0.536794   -0.32508957 -0.00523639  0.55942124 -0.08677679  0.62989074\n",
            " -0.0377718   1.4447746  -0.8619304   1.4627525   0.12291437  1.2765238\n",
            "  0.2205573 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2467097   0.5332072   0.32213634 -0.29655042  0.2889487  -0.23652618\n",
            "  0.29166687 -0.19128382  0.03924304  0.5598832  -0.02864974  0.7824615\n",
            "  0.03651246  1.1528519  -0.5296307   1.0834036   0.12488064  0.9084358\n",
            "  0.1484123 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.26204246  0.49850088  0.47648722 -0.42237842  0.38171357 -0.4666495\n",
            "  0.43133646 -0.15984048  0.06389099  0.35504222 -0.02444666  0.7317425\n",
            "  0.04192685  1.1903493  -0.84580326  1.4883641   0.15128008  1.0070028\n",
            "  0.22519106]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.16927852  0.4229066   0.28008232 -0.25108382  0.285163   -0.2087507\n",
            "  0.25565833 -0.21256831 -0.02967606  0.5479202  -0.06459083  0.815793\n",
            "  0.0548525   0.922112   -0.4401128   0.68365157  0.04840427  0.5976165\n",
            "  0.13942772]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.20605804  0.46933845  0.42648613 -0.2369905   0.21007136 -0.24362193\n",
            "  0.39485344 -0.21462902  0.0074307   0.58675396 -0.02906937  0.91035837\n",
            " -0.04236346  1.2767432  -0.6094072   1.3552153   0.09900144  1.3038876\n",
            "  0.16560195]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.24315083  0.5560255   0.36104384 -0.39337486  0.3973717  -0.33102614\n",
            "  0.33586183 -0.1738071   0.05237751  0.36556923 -0.04008298  1.0095928\n",
            "  0.11049522  1.0243928  -0.6361847   0.8653014   0.11945362  0.67203265\n",
            "  0.22576377]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 42 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.22020707  0.47036496  0.40067294 -0.21464771  0.21267718 -0.20570797\n",
            "  0.4311095  -0.20666522  0.03867003  0.5561609  -0.02499112  0.8983861\n",
            "  0.04334465  1.2918     -0.710127    1.9731212   0.10799545  1.3033032\n",
            "  0.20193203]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 43 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.19964102  0.32554576  0.40715286 -0.28431985  0.21194762 -0.2663101\n",
            "  0.288228   -0.12747538  0.06042837  0.33461654 -0.05328384  0.51700175\n",
            "  0.0583153   1.0981082  -0.65727353  1.6364319   0.08831912  0.93123263\n",
            "  0.25870797]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 44 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.20825925  0.33864355  0.41586816 -0.21746844  0.13583629 -0.1285161\n",
            "  0.39971283 -0.13480288  0.00850825  0.5083267  -0.05265103  0.7434123\n",
            "  0.02584058  1.3344144  -0.678153    1.9355239   0.09383626  1.34176\n",
            "  0.25849238]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 45 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.21780296  0.27907282  0.32734516 -0.13905649  0.08394714 -0.11402857\n",
            "  0.28379193 -0.15786237  0.06756245  0.54814816 -0.04114738  0.5853454\n",
            "  0.01646911  1.249476   -0.5059329   1.6912377   0.07417207  1.1674302\n",
            "  0.17679256]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 46 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.56981841e-01  3.18392783e-01  3.38590026e-01 -1.62170693e-01\n",
            "  1.06026672e-01 -1.17894605e-01  3.08917254e-01 -1.40537158e-01\n",
            "  1.15692313e-03  4.41998988e-01 -5.76735362e-02  7.85535395e-01\n",
            " -1.53479446e-02  1.25003743e+00 -5.29782236e-01  1.93761015e+00\n",
            "  6.40286207e-02  1.27276039e+00  1.23828195e-01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 47 reward=1 new_state=[0 1 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.4809851e-01  2.0021671e-01  1.7666864e-01 -3.2553274e-02\n",
            " -1.2578592e-03 -1.5113580e-01  3.7955739e-02 -1.2055900e-01\n",
            "  1.4819865e-02  1.8542320e-01 -1.1136303e-01  3.0600861e-01\n",
            "  4.6751432e-02  7.0554680e-01 -3.5135019e-01  1.3241735e+00\n",
            "  5.5926993e-02  5.5074203e-01 -1.3256157e-02]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 48 reward=1 new_state=[0 1 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.1489189   0.24114223  0.1899768   0.00137339  0.01622324 -0.16972904\n",
            "  0.05777124 -0.13401744  0.05351452  0.21670707 -0.09981518  0.33029965\n",
            "  0.04267097  0.81930006 -0.40039107  1.2912532   0.0490314   0.6658975\n",
            " -0.04538195]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 49 reward=1 new_state=[0 1 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.16101865  0.3462065   0.29935145 -0.02372103  0.07482622 -0.12581466\n",
            "  0.25139275 -0.1423253   0.00192685  0.36978272 -0.09407628  0.79440314\n",
            " -0.00819801  1.2357019  -0.51655626  1.7862945   0.07680744  1.1936884\n",
            "  0.08128989]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 50 reward=1 new_state=[0 1 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.17076996  0.23147471  0.1962772  -0.00137209  0.04432878 -0.14162688\n",
            "  0.05297327 -0.1148058   0.02256694  0.17064184 -0.10693965  0.34452417\n",
            "  0.0865632   0.7179418  -0.36902007  1.3417704   0.08583333  0.4963095\n",
            "  0.06301855]\n",
            "Epsilon reduced to 0.052428800000000025\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.43039578  0.7630279   0.76258224 -0.4301817   0.5951625  -0.58955467\n",
            "  0.7919268  -0.41824192 -0.02714884  0.11771974  0.01306208  1.0850713\n",
            " -0.01470912  2.1696784  -1.1312543   2.4734974   0.16393268  0.65409195\n",
            "  0.58351105]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.43090886  0.511615    0.39856237 -0.27198893  0.03428661 -0.3830582\n",
            "  0.50593305 -0.38997898 -0.12090822  0.21518584  0.05658529  0.2683584\n",
            "  0.11701564  1.3981436  -1.0459813   2.6805396   0.29473436  0.05722744\n",
            "  0.6681613 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.41888767  0.6023489   0.5756121  -0.38360244  0.20172438 -0.4594731\n",
            "  0.66827387 -0.4677673  -0.31417513  0.23523203 -0.14096789  1.0115689\n",
            " -0.04907895  1.595261   -0.8736314   2.483228    0.3789685   0.07165386\n",
            "  0.3241957 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.37877792  0.52302593  0.49513906 -0.33774036  0.1351024  -0.3297673\n",
            "  0.66280055 -0.42055386 -0.30437228  0.1852867  -0.13331757  0.6772493\n",
            "  0.0086421   1.5511209  -0.80552816  2.3176882   0.33131573  0.10226949\n",
            "  0.21078146]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39719304  0.7008304   0.6024663  -0.4710461   0.6128794  -0.53788894\n",
            "  0.6933503  -0.35022298 -0.18916708 -0.04646196 -0.00360992  0.80663025\n",
            "  0.05983034  1.6391008  -0.90556663  2.6373875   0.20149368  0.05960431\n",
            "  0.5003667 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3836129   0.64409477  0.52639437 -0.3701505   0.2702942  -0.45599627\n",
            "  0.7686221  -0.39555538 -0.21709764  0.11108677 -0.16817504  0.7645839\n",
            " -0.07675174  1.6721894  -0.84740657  2.6306047   0.2692448   0.16359927\n",
            "  0.23474267]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.38541824  0.5552835   0.5693713  -0.40985107  0.31833687 -0.45268452\n",
            "  0.610535   -0.45267883 -0.3089185   0.2263599  -0.09516672  1.0242403\n",
            " -0.04084722  1.5662091  -0.7046305   2.3918238   0.35512632  0.502811\n",
            "  0.2986787 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40958568  0.5858229   0.42563695 -0.0992438   0.01626128 -0.42844227\n",
            "  0.6398019  -0.46098563 -0.11457729  0.20719892 -0.14458482  0.48220375\n",
            " -0.0179935   1.4594386  -1.0221683   2.6504807   0.29705906  0.5438564\n",
            "  0.29518747]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.44756132  0.61467385  0.68007153 -0.53743935  0.27563596 -0.4250511\n",
            "  0.7762195  -0.4157433  -0.309341    0.3191264  -0.08159205  1.0927341\n",
            "  0.02115509  1.8291057  -0.8586663   2.5154278   0.39938566  0.7812365\n",
            "  0.43300134]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3939129   0.6925428   0.65476483 -0.43885273  0.38196507 -0.45984077\n",
            "  0.81209016 -0.43495116 -0.26311454  0.30098933 -0.168197    0.87625355\n",
            " -0.08305212  1.8801222  -0.95093024  2.5799541   0.23264472  0.84734964\n",
            "  0.40100467]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3729338   0.60106367  0.5037392  -0.42869025  0.33418217 -0.4282673\n",
            "  0.5839419  -0.39206618 -0.288033    0.34347808  0.06499479  0.83831966\n",
            "  0.07753843  1.7268298  -0.81900483  2.4965172   0.232693    0.99796355\n",
            "  0.61302876]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.388218    0.59750396  0.5473958  -0.4034243   0.2330273  -0.44059193\n",
            "  0.70703    -0.41998217 -0.38360834  0.2730338  -0.15798895  0.9315468\n",
            " -0.06097171  1.7478366  -0.95565945  2.513213    0.3576088   1.2732365\n",
            "  0.36737314]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.38425496  0.6067179   0.5511533  -0.40180475  0.4939697  -0.5045563\n",
            "  0.6444042  -0.42033076 -0.18485576  0.10993177 -0.1237938   1.0970118\n",
            "  0.03687599  1.4583828  -0.67382234  2.3975852   0.21098119  1.2644066\n",
            "  0.26764116]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.43757296  0.47228664  0.4215538  -0.12931378  0.0477722  -0.36186865\n",
            "  0.5125832  -0.41511613 -0.07506857  0.252805    0.07159676  0.24483119\n",
            "  0.13564654  1.4274433  -0.96875554  2.7094564   0.23410365  1.2322232\n",
            "  0.6371895 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.36904764  0.6993835   0.6377657  -0.45844975  0.5818397  -0.44347438\n",
            "  0.69008625 -0.35088956 -0.18921408 -0.02082388  0.02293293  0.8162276\n",
            "  0.07511922  1.689332   -0.7959219   2.4564803   0.16031647  1.0908267\n",
            "  0.50309867]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.34946042  0.4888926   0.40866837 -0.3248347   0.08759694 -0.38835174\n",
            "  0.54760426 -0.37600568 -0.13997386  0.18998995  0.0231556   0.49850243\n",
            "  0.07083281  1.5374645  -0.8069462   2.4141996   0.18164873  1.3942547\n",
            "  0.47444457]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40054917  0.72273594  0.5905317  -0.486226    0.56733733 -0.49012813\n",
            "  0.7153128  -0.3363559  -0.18336242 -0.02086734  0.02046647  0.90773153\n",
            "  0.08494631  1.7349873  -0.84552985  2.652857    0.20523669  1.4546862\n",
            "  0.4998088 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3426459   0.5706143   0.4862386  -0.23265502  0.17611316 -0.39219216\n",
            "  0.7056636  -0.41467384 -0.19721092  0.11549963 -0.19530828  0.69644225\n",
            " -0.04163631  1.597913   -0.80573684  2.3981502   0.23276348  1.8091623\n",
            "  0.20958091]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39001736  0.67157596  0.6004125  -0.32949862  0.4762678  -0.49593124\n",
            "  0.6678395  -0.42804447 -0.13056798  0.14902376 -0.13661827  1.12578\n",
            " -0.01104174  1.5279002  -0.7582557   2.4226549   0.25964916  2.2078118\n",
            "  0.31746852]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.38823915  0.5442305   0.5513099  -0.3942336   0.11480806 -0.38531435\n",
            "  0.7796931  -0.44493076 -0.33728004  0.32616732 -0.11704841  0.8418072\n",
            " -0.06201161  1.7864075  -0.8733361   2.474645    0.33489525  2.1281517\n",
            "  0.38789606]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40840277  0.65036345  0.6499169  -0.46169993  0.3697037  -0.4391034\n",
            "  0.7123585  -0.45665535 -0.32314467  0.4313409  -0.08278755  1.0753268\n",
            "  0.01761236  1.8600353  -0.8310387   2.6533425   0.30928394  2.300861\n",
            "  0.41653904]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3633833   0.493549    0.45865166 -0.1871011   0.08492697 -0.38188693\n",
            "  0.6991721  -0.40783808 -0.15621953  0.23616077 -0.20399737  0.52027047\n",
            "  0.02428379  1.5517578  -0.86298007  2.2743974   0.21272789  2.1036098\n",
            "  0.2951666 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.48502633  0.6087332   0.52326983 -0.25974646  0.10103084 -0.45345888\n",
            "  0.59697556 -0.43100968 -0.22599542  0.2583529  -0.00753487  0.7363319\n",
            "  0.08788533  1.4680983  -0.8984527   2.6150649   0.36033148  2.2890773\n",
            "  0.5127069 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40565372  0.5346217   0.4265523   0.00412414 -0.01001769 -0.37615478\n",
            "  0.6813942  -0.45073548 -0.10034633  0.21357523 -0.13962539  0.38968796\n",
            "  0.02242918  1.4432958  -0.9885565   2.4027073   0.27638182  2.3007886\n",
            "  0.29225782]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4658681   0.66744155  0.616      -0.25791907  0.4031678  -0.5019909\n",
            "  0.62459254 -0.41622227 -0.10529208  0.00840348  0.06240767  0.57269\n",
            "  0.17767309  1.485412   -1.0317317   2.6637018   0.17191608  2.079484\n",
            "  0.6012384 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-4.1328365e-01  5.4246801e-01  3.9581075e-01 -5.8015287e-02\n",
            " -1.1469252e-01 -4.0017048e-01  6.6360456e-01 -4.7016540e-01\n",
            " -1.2731707e-01  3.7907499e-01 -6.7306988e-02  5.1632190e-01\n",
            "  1.2780682e-03  1.5537677e+00 -1.0228871e+00  2.7958193e+00\n",
            "  3.9430118e-01  2.4753921e+00  3.3937016e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40667465  0.6592332   0.55353    -0.47391823  0.26895005 -0.41949657\n",
            "  0.6646955  -0.37193495 -0.2929423   0.29489762 -0.07724464  1.1206648\n",
            " -0.03768319  1.74648    -0.65131795  2.6170533   0.3987104   2.5066326\n",
            "  0.34814483]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.34800822  0.546962    0.40048102 -0.15231946  0.07657154 -0.40408856\n",
            "  0.65379596 -0.39070353 -0.11982473  0.18748374 -0.19889349  0.5681095\n",
            " -0.03649316  1.5514494  -0.7801209   2.2820003   0.2536313   2.3201818\n",
            "  0.17861019]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.37505952  0.7233598   0.5834415  -0.32726544  0.58065975 -0.46509206\n",
            "  0.640042   -0.36879513 -0.09940438  0.08476414  0.02508124  0.81146896\n",
            "  0.08650081  1.6945117  -0.79615927  2.6846392   0.18704747  2.2290673\n",
            "  0.49872336]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.34880078  0.5444577   0.51893705 -0.05327201  0.12032884 -0.38571638\n",
            "  0.5365799  -0.38888615 -0.0560916   0.3434152  -0.16712089  0.54130816\n",
            "  0.00520194  1.4472358  -0.688845    2.6962595   0.16712667  2.7028222\n",
            "  0.3279355 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3397033   0.4813323   0.45294833 -0.12302937  0.09036021 -0.39538482\n",
            "  0.47692823 -0.38499793 -0.17814384  0.49837622 -0.06781599  0.83473015\n",
            "  0.05140908  1.3995074  -0.55460644  2.5715044   0.2843532   2.6527524\n",
            "  0.30925646]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.36381197  0.4635833   0.37069863 -0.1207111   0.16967879 -0.38964275\n",
            "  0.4651845  -0.27256593 -0.01889039  0.3438475  -0.03210705  0.13313453\n",
            "  0.16917501  1.4494865  -0.73278767  2.7708733   0.04536807  2.163897\n",
            "  0.5153417 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.34149864  0.51317227  0.46887708 -0.16894248  0.21795827 -0.30419624\n",
            "  0.4420632  -0.313775   -0.15158702  0.3986959   0.122519    0.4610327\n",
            "  0.160912    1.406471   -0.5422399   2.6898284   0.19417155  2.0032716\n",
            "  0.6148941 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.33936614  0.41665536  0.38830742  0.02029181 -0.071474   -0.27828857\n",
            "  0.44233087 -0.37804705 -0.14472614  0.3582443  -0.12108734  0.41323918\n",
            "  0.09035932  1.2290549  -0.604286    2.6135232   0.25158116  2.3196397\n",
            "  0.21299283]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.2982259   0.5541534   0.39302573 -0.10811301  0.34388983 -0.38188416\n",
            "  0.42576367 -0.30480126 -0.00537425  0.15554675  0.06224793  0.5957618\n",
            "  0.18387897  1.1928104  -0.5038467   2.580642    0.06965829  2.174776\n",
            "  0.42051724]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-3.2706910e-01  4.8677859e-01  4.4083214e-01  2.3780162e-03\n",
            "  6.3044935e-02 -3.7827325e-01  5.1955253e-01 -3.7312073e-01\n",
            " -6.1598282e-02  2.5865966e-01 -1.6002536e-01  4.5047748e-01\n",
            "  3.1143665e-02  1.2672980e+00 -6.3891751e-01  2.8376055e+00\n",
            "  1.4998004e-01  2.6169975e+00  2.4803251e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.34669378  0.47503877  0.45398697 -0.10946955  0.07543799 -0.3653819\n",
            "  0.4711036  -0.38211244 -0.16454953  0.41543573 -0.05785     0.81378293\n",
            "  0.06813423  1.3165643  -0.504195    2.759155    0.30141515  2.6980717\n",
            "  0.29086766]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.37144545  0.50212586  0.43366173 -0.07543937 -0.08204465 -0.37528846\n",
            "  0.5490624  -0.3722552  -0.11219741  0.3975722  -0.10843609  0.5880126\n",
            " -0.00687993  1.4442718  -0.7026088   2.865151    0.3386922   2.83239\n",
            "  0.34740585]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.37064952  0.5017194   0.49873665 -0.11849205  0.09586324 -0.37098512\n",
            "  0.49281338 -0.38337144 -0.12720422  0.41106877 -0.0504223   0.8344189\n",
            "  0.08005404  1.372544   -0.54750043  2.7771995   0.3361651   2.9341717\n",
            "  0.3273809 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.37283817  0.5666892   0.46108288 -0.13823706  0.25850523 -0.38344735\n",
            "  0.42697874 -0.29659313  0.04032892  0.29722193 -0.0056639   0.27279004\n",
            "  0.1176074   1.4960327  -0.6545642   3.0162911   0.08362547  2.7922368\n",
            "  0.53725797]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3460948   0.51084626  0.40128854 -0.11428931  0.09287401 -0.41451028\n",
            "  0.49645665 -0.3840119  -0.18120913  0.4909211  -0.02292033  0.8273497\n",
            "  0.05956348  1.4311495  -0.56634665  2.7812078   0.27508166  2.9195745\n",
            "  0.3442327 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4222888   0.5996106   0.60135376 -0.10750271  0.17951986 -0.3935897\n",
            "  0.65764576 -0.4126446  -0.09183591  0.4634722  -0.18340115  0.62953293\n",
            "  0.02208015  1.6157694  -0.83642286  3.203337    0.19428454  3.3056953\n",
            "  0.39879295]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.37912738  0.48568553  0.4621695  -0.16970919  0.10425188 -0.3486188\n",
            "  0.49869433 -0.34950215 -0.17271648  0.511771   -0.05477157  0.7819431\n",
            "  0.11613952  1.3335059  -0.5313304   3.0535455   0.2900145   2.9442143\n",
            "  0.46141604]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.34231105  0.4223696   0.38546437  0.03291148  0.00842768 -0.33675405\n",
            "  0.53489697 -0.36258784 -0.07109101  0.2524334  -0.18727742  0.3653595\n",
            "  0.05442569  1.2711507  -0.6128358   2.875531    0.14404713  2.8403761\n",
            "  0.192183  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 45 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3992592   0.5553393   0.5272014  -0.10437899  0.05864797 -0.38168588\n",
            "  0.5379583  -0.41001225 -0.14108244  0.4152197  -0.0831698   0.83781624\n",
            "  0.05249346  1.4434675  -0.62787604  3.0826135   0.35534516  3.3032656\n",
            "  0.33037734]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3408415   0.40189046  0.36178336 -0.00484268  0.11544772 -0.2828975\n",
            "  0.39698386 -0.27027726  0.06600846  0.14656438 -0.01370854 -0.04743217\n",
            "  0.23200221  1.2245548  -0.58632123  2.4642916   0.06946335  2.4438214\n",
            "  0.43451253]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3729185   0.60012656  0.49096373 -0.09902078  0.308471   -0.46831745\n",
            "  0.6011024  -0.3815444  -0.01851453  0.25265694 -0.06122413  0.89064026\n",
            "  0.0990001   1.3471018  -0.63900745  3.031593    0.20561773  3.375423\n",
            "  0.29813772]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-3.7102106e-01  4.5695210e-01  3.9848962e-01 -1.4080694e-03\n",
            " -8.6787492e-03 -4.2381242e-01  5.5961484e-01 -3.9067724e-01\n",
            " -7.3648751e-02  3.1171152e-01 -1.5420039e-01  4.8218751e-01\n",
            " -2.4407679e-02  1.4240631e+00 -7.1419036e-01  3.3708663e+00\n",
            "  2.3247407e-01  3.1629834e+00  2.3208894e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3378184   0.567791    0.35295305 -0.15313707  0.15623106 -0.3521669\n",
            "  0.39475134 -0.3016484  -0.11116095  0.3496208   0.126892    0.61243474\n",
            "  0.14751495  1.3969934  -0.4717685   3.1055934   0.23974332  2.819904\n",
            "  0.49994862]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.42310312  0.48781583  0.46817228 -0.05155996 -0.07567427 -0.4084619\n",
            "  0.60549766 -0.37873456 -0.10551263  0.36337802 -0.13766241  0.54171085\n",
            "  0.05021463  1.4901947  -0.813202    3.5322406   0.3410313   3.5171168\n",
            "  0.41844496]\n",
            "Epsilon reduced to 0.04194304000000002\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5455933   0.71124375  0.6013594  -0.33822283  0.36154854 -0.4054495\n",
            "  0.82898974 -0.37884533 -0.01974454  0.7828755  -0.01146477  1.3412733\n",
            "  0.0873827   1.9834288  -0.99660254  3.259875    0.5167421   3.4953678\n",
            "  0.64666575]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.44605306  0.70506006  0.6946386  -0.34978303  0.28636596 -0.36590406\n",
            "  0.914745   -0.4258878  -0.07962348  0.35763428 -0.10165463  0.86016715\n",
            " -0.01585553  1.9053501  -0.7782753   3.1094122   0.03301069  3.5614665\n",
            "  0.41911936]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.48957077  0.9095689   0.7080765  -0.39967453  0.40246335 -0.40914994\n",
            "  1.0032057  -0.42693684 -0.04933495  0.41980484 -0.02018473  1.1548486\n",
            "  0.11117998  1.847589   -0.9966716   3.5134997   0.01466248  3.951937\n",
            "  0.7243864 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.41974866  0.7134493   0.6072587  -0.394073    0.22668026 -0.24510984\n",
            "  0.79598194 -0.37121016 -0.17314729  0.3972856  -0.03990377  0.8877242\n",
            "  0.13607255  1.8912568  -0.7170586   3.0832431   0.20060928  3.1915069\n",
            "  0.46878892]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4233954   0.7729384   0.4414694  -0.21281159  0.17322768 -0.29135868\n",
            "  0.78105265 -0.32298073  0.00690207  0.47792262  0.02711169  1.0588295\n",
            "  0.11654216  1.6808169  -0.70766205  3.14398     0.28451166  3.4504883\n",
            "  0.5841587 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40129554  0.7175096   0.591978   -0.37843224  0.22108091 -0.22693568\n",
            "  0.7717403  -0.37860832 -0.15779072  0.4390032  -0.02174398  0.88886493\n",
            "  0.11243091  1.8810906  -0.6449994   2.9961634   0.1886762   3.0151093\n",
            "  0.41960368]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 7 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.43657884  0.46468696  0.365734   -0.09531273  0.06080984 -0.30184764\n",
            "  0.44492745 -0.30522448 -0.02087562  0.62294203  0.07313353  0.90635335\n",
            "  0.13874593  1.4549147  -0.6336626   3.9632025   0.36054596  2.5862617\n",
            "  0.47985074]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 8 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.45216012  0.50183064  0.5419481  -0.2142125   0.14615084 -0.2743485\n",
            "  0.54845464 -0.28890657 -0.054399    0.46848953  0.00730426  0.6633401\n",
            "  0.16984029  1.5750335  -0.70224637  3.6191046   0.15489697  2.4488487\n",
            "  0.56236744]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-3.9718217e-01  4.0655646e-01  3.3076048e-01 -5.7797320e-02\n",
            "  9.1508338e-03 -2.4621259e-01  3.6555749e-01 -2.7594334e-01\n",
            " -1.7362572e-03  6.0064316e-01  4.7474779e-02  8.9610028e-01\n",
            "  1.8015011e-01  1.3184187e+00 -5.7635629e-01  3.7855225e+00\n",
            "  2.8489619e-01  2.3874233e+00  4.9546322e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 10 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.43968707  0.48579615  0.6239738  -0.47892955  0.24996254 -0.29956907\n",
            "  0.5517317  -0.27551058 -0.18337442  0.48783764  0.02600072  1.5095061\n",
            "  0.06629267  1.8637203  -0.5490963   3.5737095   0.29331738  2.4137216\n",
            "  0.4792532 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.37784284  0.40378422  0.36520427 -0.05666825  0.07545383 -0.2748856\n",
            "  0.38953975 -0.27948937  0.00732441  0.70958704  0.06411921  1.3121179\n",
            "  0.19844007  1.4463589  -0.56661385  3.751486    0.29773274  2.5456698\n",
            "  0.46556756]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.41972488  0.491645    0.57140553 -0.471883    0.26277623 -0.26619762\n",
            "  0.5687455  -0.2687134  -0.21201125  0.52037555  0.01846666  1.8046297\n",
            "  0.07354771  1.8010774  -0.5281268   3.5094552   0.296944    2.2544515\n",
            "  0.48542228]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.44444865  0.4425104   0.41558835 -0.07483605  0.07971412 -0.2810358\n",
            "  0.4317484  -0.33270505 -0.06705633  0.5705966   0.0587002   1.723285\n",
            "  0.15988833  1.4067886  -0.65242225  3.8289068   0.31922546  2.4490638\n",
            "  0.50536835]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39295533  0.56984115  0.53131914 -0.42450154  0.3215603  -0.37597758\n",
            "  0.58320904 -0.22525026 -0.16112421  0.3735096  -0.10792439  1.8690479\n",
            "  0.03704695  1.8514711  -0.62290126  3.7627158   0.17708184  2.7865343\n",
            "  0.4577227 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-3.8954321e-01  4.5603764e-01  4.0655410e-01 -2.7573556e-01\n",
            "  2.1239382e-01 -3.0518699e-01  4.9688175e-01 -2.7495185e-01\n",
            " -1.8410300e-01  4.4602093e-01 -1.9720921e-03  2.2880366e+00\n",
            "  1.1931520e-01  1.6107513e+00 -5.9680545e-01  3.6295009e+00\n",
            "  3.3860403e-01  2.5270090e+00  4.1162562e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.38701752  0.4053362   0.45852953 -0.32929397  0.13287604 -0.23865895\n",
            "  0.47028124 -0.2343529  -0.22469935  0.34951535 -0.05401852  2.0654018\n",
            "  0.17194065  1.5936261  -0.55848104  3.3294895   0.25939226  2.061687\n",
            "  0.38214555]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.41889453  0.61427724  0.48545364 -0.33409292  0.43400833 -0.3937387\n",
            "  0.629669   -0.23879565 -0.03658842  0.3114597  -0.02156841  2.7084002\n",
            "  0.09635449  1.761267   -0.64089537  3.8407807   0.261961    2.9136841\n",
            "  0.3071581 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39886224  0.3852253   0.51865816 -0.4445874   0.16894394 -0.23347381\n",
            "  0.48669255 -0.23246911 -0.22016196  0.45600918 -0.01440563  2.7568378\n",
            "  0.04657316  1.7330067  -0.58898866  3.1613994   0.25746536  2.0914502\n",
            "  0.46165866]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.42991045  0.4632339   0.40580222 -0.1182415   0.04154701 -0.31528306\n",
            "  0.4151854  -0.3307587  -0.07850567  0.6121676   0.08617219  3.0660162\n",
            "  0.1329221   1.5108865  -0.6119695   3.7884202   0.3731938   2.3566868\n",
            "  0.4766226 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.36849213  0.49927387  0.5125486  -0.3499541   0.28621465 -0.32930818\n",
            "  0.54368716 -0.24084443 -0.12449379  0.29479137 -0.07784357  2.8275244\n",
            "  0.08606553  1.6643524  -0.53761905  3.5430205   0.14587033  2.495465\n",
            "  0.38056448]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.38982365  0.49929783  0.35062662 -0.15679704  0.12420316 -0.32344815\n",
            "  0.44672924 -0.2710374  -0.03628995  0.6709671   0.03348146  3.4196553\n",
            "  0.16242142  1.6191813  -0.592247    3.7218301   0.36721286  2.469695\n",
            "  0.49030378]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4087238   0.53661007  0.48870853 -0.43785203  0.19471443 -0.3353773\n",
            "  0.5339405  -0.24510635 -0.2526641   0.5736028  -0.04118878  3.706026\n",
            "  0.03977587  1.907461   -0.5976561   3.676533    0.31884184  2.3813097\n",
            "  0.44700775]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 23 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.32072574  0.3468687   0.3854565  -0.16842689  0.20159839 -0.26456437\n",
            "  0.3790942  -0.26065448 -0.15730412  0.43271944 -0.07595116  2.933828\n",
            "  0.12303513  1.5017205  -0.5301111   3.623079    0.25088698  1.8698289\n",
            "  0.28808025]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 24 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.25037742  0.4260888   0.42927927 -0.26165187  0.26959768 -0.23541746\n",
            "  0.4429169  -0.14717306 -0.19857231  0.15246555 -0.10538445  2.5226784\n",
            "  0.07749335  1.5545357  -0.59930575  2.6296446   0.15988411  1.1456091\n",
            "  0.23643717]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 25 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.22878197  0.40504292  0.38158953 -0.22283453  0.2514148  -0.19304402\n",
            "  0.47665972 -0.15300463 -0.18424654  0.33959317 -0.07569331  2.4953787\n",
            "  0.08811653  1.6076412  -0.5887025   2.7663794   0.16036087  1.186345\n",
            "  0.28394258]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 26 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.23600756  0.43822655  0.42102143 -0.24988873  0.269834   -0.22185078\n",
            "  0.42538786 -0.1585807  -0.18590574  0.20143521 -0.08753872  2.72648\n",
            "  0.05340561  1.7043446  -0.53476024  2.5700774   0.1502927   1.0682544\n",
            "  0.18902394]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 27 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.7461660e-01  3.8454854e-01  3.3785865e-01  5.8184016e-02\n",
            "  7.5274497e-02 -2.9596245e-01  3.2250929e-01 -2.4169064e-01\n",
            "  9.5165446e-03  4.6193573e-01  5.9731526e-04  2.5859942e+00\n",
            "  7.7052236e-02  1.6165731e+00 -7.7943933e-01  3.0108280e+00\n",
            "  1.8556194e-01  1.3857012e+00  2.8320110e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 28 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2993092   0.50385356  0.5292362  -0.29753032  0.24386594 -0.29914737\n",
            "  0.5257514  -0.16440223 -0.13779442  0.35948917 -0.05207209  3.2302337\n",
            " -0.01556018  2.2978926  -0.6518931   2.9647007   0.18482788  1.4220982\n",
            "  0.19809261]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 29 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.31939352  0.44379526  0.35320315 -0.00890386  0.06654399 -0.28000358\n",
            "  0.42228293 -0.22470553  0.01201247  0.5408251   0.03511011  2.8930445\n",
            "  0.06915148  1.8617104  -0.7953741   3.3270826   0.23555467  1.2762359\n",
            "  0.32133418]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 30 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.26233032  0.46187404  0.4703183  -0.30957285  0.2907101  -0.21697989\n",
            "  0.50938493 -0.17182387 -0.21214281  0.3534704  -0.07054625  3.3085847\n",
            "  0.06218644  2.2713     -0.6006729   2.7268543   0.14278238  1.0747976\n",
            "  0.26817656]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 31 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.20292151  0.37706658  0.31100363 -0.0794936   0.18440436 -0.23474507\n",
            "  0.2873994  -0.19220133 -0.08305849  0.30943465 -0.08679937  2.7638004\n",
            "  0.06194929  1.931545   -0.48041272  2.734563    0.13682263  1.1451896\n",
            "  0.15591232]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 32 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2401966   0.42823187  0.43151686 -0.29587984  0.3142723  -0.2675877\n",
            "  0.4557261  -0.18442361 -0.2265145   0.33371708 -0.07760738  3.2280543\n",
            "  0.06034146  2.45101    -0.59399956  2.6513534   0.15482801  1.2435137\n",
            "  0.19678573]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 33 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-3.0390671e-01  4.1305181e-01  3.4357801e-01  1.4847827e-03\n",
            "  1.1158127e-01 -2.7701849e-01  4.5643941e-01 -2.4552478e-01\n",
            " -5.8272589e-02  6.2200165e-01 -6.1107958e-03  3.3333478e+00\n",
            "  1.1962000e-01  2.2847438e+00 -9.2219019e-01  3.2482152e+00\n",
            "  1.9615752e-01  1.3738289e+00  4.3063849e-01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 34 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2567375   0.4762651   0.45555347 -0.27291727  0.29865718 -0.24332093\n",
            "  0.46343446 -0.17637277 -0.20269874  0.21498194 -0.09020446  3.6623533\n",
            "  0.05375294  2.9073713  -0.58910686  2.74569     0.16221964  1.1205374\n",
            "  0.2038334 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 35 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2555057   0.37601447  0.284388    0.09579222  0.07926064 -0.21554892\n",
            "  0.34268346 -0.17602459  0.03625021  0.3974517  -0.00457622  3.0428061\n",
            "  0.18247736  2.2386527  -0.70508635  3.1785934   0.22191913  1.2545168\n",
            "  0.2740764 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 36 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2840446   0.48406443  0.4837594  -0.29905444  0.31377453 -0.26888815\n",
            "  0.50489914 -0.17406708 -0.22602125  0.16818158 -0.11158583  3.9307015\n",
            "  0.07978531  3.3864822  -0.6884491   2.9021158   0.17939357  1.2270025\n",
            "  0.26315436]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 37 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.3240597   0.41840032  0.35686162  0.01214833  0.09723964 -0.24879411\n",
            "  0.46938604 -0.2537152  -0.04542135  0.5332578   0.01055864  3.6782577\n",
            "  0.13845547  2.894388   -0.9047798   3.383678    0.22278969  1.2533886\n",
            "  0.4182131 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 38 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.29699647  0.49865887  0.54663956 -0.30148655  0.31115696 -0.30064234\n",
            "  0.47674388 -0.20225589 -0.1822937   0.18780205 -0.08248392  4.2177687\n",
            "  0.04443977  3.9502566  -0.66445917  2.9530761   0.16971429  1.3047217\n",
            "  0.21158388]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 39 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.29384685  0.407262    0.391676    0.04564133  0.08971525 -0.23723945\n",
            "  0.39821512 -0.21805254  0.06519849  0.5916387   0.04906409  3.6447933\n",
            "  0.15803665  3.2228599  -0.75164086  3.383073    0.20561236  1.3461623\n",
            "  0.3107663 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 40 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.31784272  0.59382904  0.5179069  -0.39043617  0.35090062 -0.28982246\n",
            "  0.6319424  -0.18448828 -0.13719685  0.42860866 -0.06995439  4.5354633\n",
            "  0.0223008   4.70635    -0.7287065   3.1989717   0.2118431   1.225441\n",
            "  0.29779148]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 41 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.22537164  0.2982894   0.3344574   0.1365522   0.07126356 -0.22740659\n",
            "  0.15777932 -0.14113125  0.10511596  0.4132209  -0.04105118  2.6000652\n",
            "  0.14229727  2.6662383  -0.36479118  3.1674595   0.09084262  1.3395473\n",
            "  0.17499708]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 42 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.20530882  0.31216782  0.2772102   0.14747502  0.09038401 -0.19163671\n",
            "  0.18566136 -0.13243946  0.08781387  0.45887402 -0.05035971  2.5319922\n",
            "  0.15166093  2.7016695  -0.34226498  3.1187282   0.09536185  1.2096988\n",
            "  0.18749326]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 43 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.20961612  0.34272826  0.29558754  0.1275395   0.13500902 -0.24349034\n",
            "  0.21793635 -0.14604974  0.09948327  0.5914572  -0.06381421  2.7571912\n",
            "  0.13447483  3.079494   -0.43061432  3.1637087   0.08066148  1.4057169\n",
            "  0.23208058]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 44 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.21978882  0.36049926  0.3139879   0.12235723  0.15573117 -0.25208262\n",
            "  0.24039759 -0.15336439  0.12693176  0.6035836  -0.06123096  2.9035852\n",
            "  0.1349879   3.2422593  -0.45431444  3.22172     0.08612212  1.4312199\n",
            "  0.24697116]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 45 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.2813855   0.35259497  0.41624582  0.10006749  0.14514796 -0.27475592\n",
            "  0.26094294 -0.15553994  0.22638571  0.39433676 -0.05338175  3.2043016\n",
            "  0.17061217  3.366553   -0.5302197   3.5204697   0.12494393  1.525581\n",
            "  0.28105712]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 46 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.26585177  0.37503168  0.36619177  0.10755581  0.1747792  -0.24165305\n",
            "  0.30158427 -0.14947933  0.1746873   0.4434633  -0.06162512  3.1797793\n",
            "  0.18015334  3.3908815  -0.51661754  3.5174723   0.1320429   1.4007243\n",
            "  0.30126086]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 47 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.27830833  0.39762542  0.38951588  0.0995912   0.20157677 -0.2516907\n",
            "  0.3300418  -0.15736541  0.19843127  0.45374587 -0.05812145  3.3616328\n",
            "  0.18000665  3.555769   -0.54571515  3.6173558   0.13910252  1.4317265\n",
            "  0.32004076]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 48 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.2959417   0.4252689   0.46906325  0.09590808  0.22040753 -0.28695524\n",
            "  0.31296468 -0.18932167  0.33562636  0.47971463 -0.020279    3.651276\n",
            "  0.14190753  3.7890205  -0.5313823   3.705437    0.1308532   1.5266323\n",
            "  0.2785797 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 49 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.2999765   0.46443447  0.46749657  0.04493959  0.26900595 -0.23674636\n",
            "  0.44638956 -0.1925889   0.24060498  0.7079815  -0.00433961  3.8305678\n",
            "  0.15968463  4.076594   -0.5749808   3.8396819   0.12636161  1.3920792\n",
            "  0.3922201 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 50 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-3.3483258e-01  5.6001514e-01  4.7304302e-01 -6.6243378e-03\n",
            "  3.0545676e-01 -2.8528962e-01  5.2741605e-01 -1.8295482e-01\n",
            "  2.6221496e-01  7.5925481e-01  1.7901517e-03  4.1385126e+00\n",
            "  1.1920630e-01  4.4683170e+00 -6.3526851e-01  4.2254558e+00\n",
            "  1.8430193e-01  1.4876060e+00  4.0688911e-01]\n",
            "Epsilon reduced to 0.033554432000000016\n",
            "Total reward: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlHDI_Nl_Zjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_conv_24(env):\n",
        "    input_shape = env.observation_shape()\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape + (1, ), input_shape=input_shape))\n",
        "    model.add(Conv1D(8, kernel_size=3, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(env.action_space.n))\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vw3eC0XzpsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbc2c162-bc52-4107-9710-d8951d90a853"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=50, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=8, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_24(env), max_epsilon=0.2, epsilon_decay=0.8)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_2 (Reshape)          (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 187, 8)            32        \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1496)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 19)                28443     \n",
            "=================================================================\n",
            "Total params: 28,475\n",
            "Trainable params: 28,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |████████████----------------------------------------------------------------------------------------| 12.5% \r\n",
            "Taking action 6\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.08598629 -0.08291756  0.3093452  -0.13864309 -0.3233019  -0.17588997\n",
            "  0.23635313  0.06962009  0.3346591  -0.06938405 -0.01930345  0.10968348\n",
            " -0.10786492  0.29605734  0.1618903  -0.13215914  0.20804518  0.27268395\n",
            " -0.1119948 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.35467005 -0.00619521 -0.12808652  0.01042968 -0.12395432 -0.23899199\n",
            "  0.1059985   0.17808862 -0.04439265  0.22913843  0.33140525  0.21225783\n",
            " -0.203549   -0.10412697  0.20989141  0.05585035  0.17772198  0.6128315\n",
            "  0.4380833 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.09331004  0.15538192  0.03781972 -0.06436893 -0.03312624 -0.23273255\n",
            "  0.06119836  0.04871247  0.26748553 -0.2655907   0.07784741  0.24882331\n",
            " -0.24188747  0.291875   -0.05406126  0.00952192  0.08647013  0.34684807\n",
            " -0.12760396]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.06906599  0.2226961   0.08534548  0.12181712 -0.26134175  0.09402192\n",
            " -0.0363057  -0.14511746  0.15270965  0.42078957 -0.0964934   0.2546033\n",
            " -0.22773138  0.13139479  0.06797726  0.04994018 -0.11888772 -0.16166298\n",
            "  0.19983079]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.17669284  0.14845625  0.208867    0.1715266  -0.22692256 -0.04106303\n",
            " -0.24302913  0.05369949  0.00510686  0.21816434  0.3179075   0.03208284\n",
            "  0.03342646  0.02682977  0.11632992  0.10560426 -0.21150933  0.20793413\n",
            "  0.15701544]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.22011769  0.32528308  0.40921083  0.0872398   0.11366457 -0.12051835\n",
            " -0.2270072   0.24804538  0.17220175  0.27068305  0.05443596  0.0131066\n",
            " -0.00998882  0.3030275   0.305999    0.07556296 -0.22507322  0.22314693\n",
            "  0.09209967]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.27973557  0.14244856  0.04223034 -0.0540951  -0.11949065 -0.16864033\n",
            " -0.33967677 -0.11282568  0.2028097   0.08276817  0.24882008  0.05718345\n",
            "  0.01035545  0.16678774  0.02809311  0.01245098 -0.3129422   0.20964517\n",
            "  0.16407374]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.17847508  0.0679896   0.07511746 -0.01031331 -0.07392992  0.02319853\n",
            " -0.40530607  0.10159763  0.14863272  0.26411667  0.14633797  0.11411361\n",
            "  0.07586571  0.10265668  0.13076918  0.18098703 -0.24950017  0.03610808\n",
            "  0.11825696]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.7895195e-01 -1.1658804e-02 -3.0141588e-02 -6.7137938e-05\n",
            " -1.3790530e-01 -3.7466568e-01 -2.1176898e-01 -9.8687664e-02\n",
            "  7.8756839e-02  1.2089270e-01  2.2886465e-01  1.3800079e-01\n",
            " -4.2874731e-02  8.2918465e-02  1.4132188e-01 -6.7293704e-03\n",
            " -1.5418573e-01  3.2169437e-01 -7.8946054e-02]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 10 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.01412616  0.00707604 -0.06586271  0.06110667 -0.10672662 -0.47689247\n",
            " -0.04410077 -0.08558963 -0.05893727  0.05294023  0.18200693  0.15563159\n",
            " -0.11530858  0.16398135  0.20200107  0.03835212 -0.36652765  0.22434026\n",
            " -0.13640781]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 11 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.32001412  0.18730263  0.16175415 -0.03079651  0.00195728 -0.11907266\n",
            " -0.154508   -0.00322891  0.29672968 -0.05881754  0.06244345  0.1425713\n",
            "  0.00650068  0.30179042  0.1660884  -0.08023809 -0.0739119   0.2515544\n",
            "  0.09450872]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 1 0 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.32153317 -0.05022127 -0.01559733 -0.05181194  0.09943397 -0.19829144\n",
            "  0.06835585  0.02938097  0.19650126 -0.0542136   0.07898384  0.22417048\n",
            " -0.00815932  0.24218546 -0.0325214  -0.00744768 -0.16482148  0.14964418\n",
            " -0.00817491]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [ 0.09092872  0.10815125 -0.03111356  0.08024977 -0.13724771 -0.3863645\n",
            " -0.12103451 -0.10508929  0.08228164  0.04556855  0.12211395  0.18243594\n",
            " -0.18051101  0.22714081  0.15455279 -0.21756417 -0.23785876  0.16398297\n",
            " -0.07385588]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.12732197  0.13387746  0.17102921 -0.23297983 -0.06391802 -0.4169749\n",
            " -0.28504965 -0.14213155  0.35799518 -0.02332977  0.11436629  0.07278803\n",
            " -0.11250012  0.2270411   0.32438663 -0.0880476  -0.39706817  0.21968202\n",
            "  0.34988406]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.33146295  0.00878505  0.32541853 -0.16048211  0.10906605 -0.32589918\n",
            " -0.0987702  -0.06365602  0.36161903 -0.11389086  0.12822405  0.13400549\n",
            " -0.13055438  0.33812925  0.19920252 -0.24798259 -0.4527276   0.08864266\n",
            "  0.06256972]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.01496094 -0.08899824  0.0507023  -0.16764855  0.04701403 -0.4668811\n",
            "  0.05433774 -0.06804977  0.3291944  -0.27935898 -0.11571477  0.11363521\n",
            " -0.4637002   0.20967397  0.18996727 -0.34440157 -0.50504804  0.12759243\n",
            "  0.14056775]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.1548857   0.04744725  0.0825276   0.02662759 -0.04438388 -0.25195378\n",
            " -0.46219814  0.02976481  0.38555494  0.0339995   0.01197323  0.11070201\n",
            " -0.04143744  0.05071495  0.25770956 -0.17554909 -0.2605624  -0.0789028\n",
            "  0.14398928]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.02394156e-01  8.61923993e-02  1.48496137e-03 -2.65716983e-04\n",
            " -2.01109037e-01 -3.14943492e-01 -4.55194086e-01 -1.14140980e-01\n",
            "  3.89497191e-01  1.52513748e-02  1.30906299e-01 -2.79399077e-03\n",
            " -5.53325824e-02  2.88723293e-03  2.08005816e-01 -2.27160275e-01\n",
            " -5.27193904e-01 -1.88206658e-01  4.45207953e-01]\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [ 0.09785824 -0.09730304 -0.05839365 -0.12400471 -0.11236559 -0.5070361\n",
            " -0.09109285 -0.29381245  0.38581175 -0.05723428 -0.1199966   0.10994166\n",
            " -0.32987133  0.05828731  0.12047722 -0.484926   -0.41437244  0.03588211\n",
            "  0.09465936]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 20 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.05013748  0.08744391  0.15207365 -0.01093473 -0.15301408 -0.6294737\n",
            " -0.11577109 -0.08242805  0.18724656 -0.07145135 -0.01664186  0.03065296\n",
            " -0.34747845  0.15038884  0.41139004 -0.49927774 -0.72497606 -0.09529419\n",
            "  0.13476336]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 21 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.10163964  0.16536905  0.17041247 -0.07192364  0.0337893  -0.45763767\n",
            " -0.11377115 -0.09802384  0.57737285 -0.11741544  0.06061623  0.09856761\n",
            " -0.10506998  0.27375752  0.18743734 -0.4030217  -0.57717407  0.08817941\n",
            "  0.26023057]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 22 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.04343073  0.00859465 -0.02434441  0.07624855 -0.24641863 -0.33932054\n",
            "  0.06591977  0.17798261  0.25839075 -0.13323179 -0.08401096 -0.10217565\n",
            " -0.02660322 -0.22431257  0.1832753  -0.13302955 -0.5592001   0.11321317\n",
            " -0.03157948]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 23 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.18709642  0.10529898  0.13861144 -0.08561199 -0.26038045 -0.3179295\n",
            " -0.01937015  0.04042373  0.09893782  0.1475394   0.03413484  0.20113304\n",
            "  0.02029844 -0.10562121  0.15666677 -0.35650927 -0.59297544  0.09597392\n",
            " -0.28472948]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 24 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [ 0.07275764  0.07747794  0.07615713 -0.24225675 -0.24092695 -0.51509535\n",
            " -0.19892049 -0.12298264  0.4355119  -0.07158113  0.03243562  0.10491358\n",
            " -0.16775262  0.26535147  0.2370904  -0.46849635 -0.70205295  0.25082576\n",
            "  0.30138543]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 25 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.16413437  0.0827779   0.34376785 -0.2127503  -0.06162328 -0.50016075\n",
            " -0.11554434  0.07623413  0.22043583 -0.12708011 -0.0319482   0.04549679\n",
            " -0.07349418  0.09356073  0.17397821 -0.5189487  -0.8178393   0.40200284\n",
            " -0.14963615]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 26 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.30334464 -0.18658927  0.08849675 -0.23589256 -0.27411774 -0.6777523\n",
            "  0.10879777 -0.10223165  0.03287718 -0.25686762 -0.11631092  0.2205034\n",
            " -0.19468836  0.01635831  0.26795062 -0.5790189  -0.8257779  -0.07254411\n",
            " -0.33879435]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 27 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.55106920e-01  1.22837298e-01  6.78470433e-02 -4.31961179e-01\n",
            "  4.07713000e-04 -4.97299224e-01 -2.16817588e-01 -4.20906365e-01\n",
            "  3.79701197e-01  4.72811842e-03 -1.72577411e-01  1.84135944e-01\n",
            " -1.75654426e-01  2.39662286e-02  4.42374647e-01 -6.02453530e-01\n",
            " -7.75394499e-01  1.92240134e-01  1.23094395e-01]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 28 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.09347723  0.15432692  0.17928012 -0.17771718 -0.19207436 -0.68152\n",
            " -0.2109678   0.11197747  0.23938896 -0.12118914 -0.21245627 -0.03143327\n",
            " -0.06728761 -0.23248334  0.3693229  -0.40980732 -0.9543279  -0.02134511\n",
            " -0.20478438]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 29 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.48160598 -0.1306186   0.18525454 -0.3522233  -0.23209819 -0.4925389\n",
            "  0.03924881 -0.07678427  0.32245606 -0.22052906 -0.45092386  0.14026693\n",
            " -0.30672356 -0.05994343  0.23030017 -0.517876   -1.0746337   0.07446251\n",
            " -0.37033677]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 30 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.2578922   0.05770311  0.12830307 -0.3321313  -0.26481152 -0.3415451\n",
            " -0.32813314 -0.00944888  0.66233426 -0.27498376 -0.07792922  0.17098357\n",
            " -0.2583373  -0.00303297  0.20489636 -0.56490356 -0.85467464  0.08295307\n",
            "  0.16925377]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 31 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.11003347  0.01923609 -0.03171172 -0.36878648 -0.24337883 -0.8815125\n",
            " -0.24629886 -0.03964     0.40381694 -0.54161453 -0.29928154 -0.02799904\n",
            " -0.02328013 -0.30344498  0.24746385 -0.48739716 -1.1608241   0.07509553\n",
            " -0.2154165 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 32 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.2162619e-01  8.5643157e-03  2.4503724e-01 -2.8786382e-01\n",
            " -4.9148370e-02 -5.8523279e-01 -1.3979243e-01  1.4762107e-02\n",
            "  3.5749838e-01 -5.1928341e-01 -2.8213191e-01  1.4616844e-01\n",
            "  1.6972213e-04 -1.7032586e-01  3.6010626e-01 -2.6872373e-01\n",
            " -1.1074380e+00  2.2249770e-01 -2.2055148e-01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 33 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.3400518   0.0420918   0.00868277 -0.45515475 -0.13105512 -0.65957844\n",
            "  0.00476469 -0.25713542  0.2348268  -0.23702492 -0.32156852 -0.01027994\n",
            " -0.25127092  0.04646567  0.25640106 -0.6215588  -0.8774668   0.20009303\n",
            "  0.17021413]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 34 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.12718315 -0.12903813  0.21767071 -0.6276425  -0.18594187 -0.98697233\n",
            " -0.30106664 -0.2073861   0.22658885 -0.5391229  -0.6436276   0.13133669\n",
            " -0.20573187 -0.3215771   0.33811998 -0.5260132  -1.3907603   0.37435767\n",
            " -0.18238647]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 35 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.00839156 -0.24738662  0.28720704 -0.4401823  -0.16740227 -0.9738487\n",
            " -0.17802294 -0.30282435  0.20971671 -0.6770797  -0.620293    0.34470668\n",
            " -0.06724997 -0.33173728  0.49272147 -0.59747845 -1.5452752   0.1757112\n",
            " -0.33285832]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 36 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.36011997  0.09187731  0.19941221 -0.25829124 -0.31332585 -0.71624106\n",
            " -0.2690528  -0.22857228  0.3893772  -0.32590893 -0.36950076  0.12033618\n",
            " -0.19554569 -0.14826952  0.21574464 -0.44643593 -0.8501712  -0.00320879\n",
            " -0.04573792]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 37 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [ 1.2285081e-02  7.5486902e-04  2.0843051e-01 -5.9102643e-01\n",
            " -3.4003176e-02 -8.2339835e-01 -7.0570908e-03  7.9786293e-02\n",
            "  3.3131105e-01 -6.4432418e-01 -6.4783317e-01  8.7910667e-02\n",
            " -2.7071121e-01 -4.2859280e-01  2.9365584e-01 -7.5114942e-01\n",
            " -1.5867718e+00  3.2230115e-01 -3.1187922e-01]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 38 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.19293681  0.05835317  0.1644925  -0.43926805 -0.27316266 -0.6493838\n",
            " -0.06778858 -0.04952722  0.60228384 -0.55516285 -0.70507133  0.13661475\n",
            " -0.10896894 -0.289991    0.2595873  -0.9261538  -1.4090731   0.10783269\n",
            " -0.28195438]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 39 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.14943501  0.13120522  0.19699854 -0.5069262  -0.15634269 -0.65871686\n",
            " -0.21690664 -0.17876084  0.43137866 -0.3356576  -0.49771246  0.11145619\n",
            " -0.0848773  -0.17426592  0.10987791 -0.8222952  -1.2152009   0.16385372\n",
            " -0.04493479]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 40 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.19035815 -0.09086733 -0.11874828 -0.55731004 -0.31175166 -0.87372214\n",
            "  0.0016754  -0.01391793  0.06714727 -0.6043877  -0.87471074 -0.06767046\n",
            " -0.37806195 -0.6094656   0.22981401 -0.77544814 -1.5476115   0.3200174\n",
            " -0.21603057]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 41 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.40166137  0.19756562  0.18716119 -0.7043101  -0.35782573 -0.9798605\n",
            " -0.28795835 -0.06482094  0.6063564  -0.5349768  -0.9262084   0.04322175\n",
            " -0.24409498 -0.5893663   0.25596708 -0.97766453 -1.7279346   0.00629138\n",
            " -0.39462817]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 42 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.22823723  0.3318209   0.2834097  -0.41570115 -0.11034709 -0.85969937\n",
            " -0.25380415 -0.16367716  0.8068196  -0.59936154 -0.7040003   0.16996488\n",
            " -0.2666552  -0.25538555  0.25238737 -0.9439962  -1.5171651   0.24947423\n",
            "  0.10906967]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 43 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.3365023e-01  2.3383707e-02  7.9111964e-02 -4.8098132e-01\n",
            " -4.2031965e-01 -9.0495282e-01  2.8428137e-02 -1.2341407e-01\n",
            "  2.3718961e-01 -6.9580251e-01 -1.2422915e+00 -1.2445860e-03\n",
            " -2.7833083e-01 -7.6533622e-01  2.0721291e-01 -7.3502773e-01\n",
            " -1.4951036e+00  1.0688046e-01 -4.2782632e-01]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 44 reward=-2 new_state=[0 1 1 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.54781324  0.30736002  0.33560804 -0.5715407  -0.15349886 -1.087571\n",
            "  0.0578234   0.014907    0.3958625  -0.8161032  -1.1615129   0.2206728\n",
            " -0.28480962 -0.5157857   0.39672622 -1.0269003  -1.738807    0.1883444\n",
            " -0.3806647 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 45 reward=-3 new_state=[0 1 1 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.11781282  0.02316644  0.06150384 -0.47158358 -0.26842523 -0.9517485\n",
            " -0.12107346  0.0051232   0.61808056 -0.6898923  -0.86827946  0.20824076\n",
            " -0.37295777 -0.45628816  0.24570699 -1.0308394  -1.6895313   0.23823534\n",
            " -0.05901095]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 46 reward=-3 new_state=[0 1 1 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.33705515  0.04495059  0.20337233 -0.5396938  -0.28599882 -0.93475133\n",
            " -0.14342372 -0.09340295  0.23282216 -0.52105916 -0.9273717   0.15508492\n",
            " -0.33697507 -0.87987894  0.23276852 -0.97426844 -1.7191099  -0.04612171\n",
            " -0.512157  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 47 reward=-2 new_state=[0 1 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.19646826  0.19204795  0.21887395 -0.5698145  -0.2501153  -0.73838377\n",
            " -0.41922563 -0.17997462  0.39411286 -0.4091266  -0.70735747  0.03591952\n",
            " -0.36828315 -0.47016194  0.16412    -1.0651011  -1.4813619   0.14644974\n",
            " -0.0805659 ]\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 48 reward=-2 new_state=[0 1 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.37897113  0.17863727  0.19486961 -0.5751719  -0.45483667 -1.0485579\n",
            " -0.01651416  0.05278671  0.5690402  -0.509193   -1.027069   -0.05533458\n",
            " -0.422286   -0.70208925  0.29152074 -1.0252928  -1.6343114   0.6629626\n",
            " -0.43148205]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 1 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.36125723  0.10156272  0.05090939 -0.4967805  -0.15087403 -0.8508296\n",
            " -0.24282978 -0.1241447   0.04379114 -0.4857583  -0.77782243 -0.03731973\n",
            " -0.38029295 -0.73921055  0.29491192 -0.93625164 -1.8715392  -0.00881008\n",
            " -0.3248233 ]\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 1 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.2693106   0.21410196  0.11257458 -0.5830241  -0.2135964  -0.83930284\n",
            " -0.3631253  -0.30581132  0.34971216 -0.37567392 -1.0582918   0.05087368\n",
            " -0.40804705 -0.4690671   0.18640028 -0.971933   -1.4437855   0.27474484\n",
            "  0.00876901]\n",
            "Epsilon reduced to 0.16000000000000003\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.30624387  0.06258179  0.09581364 -0.8433111  -0.28624076 -0.9553077\n",
            " -0.1967776   0.0655217   0.04554818 -1.1392515  -1.3955423   0.29095656\n",
            " -0.531298   -0.882351   -0.11869463 -0.9740093  -1.615342    0.4294517\n",
            " -0.56432575]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.51585406  0.15538949  0.07088432 -0.43075162 -0.59047383 -0.72733295\n",
            " -0.24619293 -0.13803226  0.20182614 -0.5344563  -1.2248833   0.24842413\n",
            " -0.44717124 -0.5270123   0.123286   -0.67875534 -1.1829923   0.17453027\n",
            " -0.17389466]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 3 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.30564696 -0.04630025  0.25090414 -0.41794017 -0.27746174 -0.6744662\n",
            " -0.38278857  0.08592746  0.05063064 -0.6409109  -1.2597929   0.2558076\n",
            " -0.48359573 -0.92649305 -0.10174751 -0.7332262  -1.2492617  -0.00372003\n",
            " -0.27635932]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 4 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3863466   0.23845251  0.18003869 -0.40972304 -0.47595063 -0.7130512\n",
            " -0.35751414  0.0042965   0.03373384 -0.32745752 -1.1006436   0.25864333\n",
            " -0.5379736  -0.63026154  0.09086134 -0.79761755 -1.3665103  -0.02701802\n",
            " -0.34377724]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 5 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.25941312  0.03469552  0.36990505 -0.5722089  -0.05607055 -0.84057367\n",
            " -0.3321783   0.04643241  0.23073894 -0.64403373 -1.1072196   0.30391264\n",
            " -0.6197265  -0.6956058   0.0381473  -0.5211079  -1.5155014  -0.24456792\n",
            " -0.3208303 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.61526763 -0.02104774  0.10435621 -0.49665737 -0.540746   -0.7747076\n",
            " -0.44034246 -0.05815405  0.05952811 -0.3219818  -1.3280585   0.39019236\n",
            " -0.73099107 -0.8632282   0.12183158 -0.70771646 -1.642537   -0.13787422\n",
            " -0.38398823]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3134797  -0.07909953  0.26573563 -0.62049055 -0.06967899 -1.1784216\n",
            " -0.23316169  0.08095339  0.12013002 -0.8128712  -1.6048738   0.43187052\n",
            " -0.7352597  -1.0082122   0.12289753 -0.79048026 -1.6172484  -0.22179501\n",
            " -0.6830832 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5253944   0.00800919  0.11099441 -0.38225538 -0.53487664 -1.1269612\n",
            " -0.13650912  0.07314821 -0.26032814 -0.43178707 -1.482423    0.6354115\n",
            " -0.7197914  -0.99072796  0.06230722 -0.94255555 -1.7000349   0.01724223\n",
            " -0.56892395]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.21295476  0.06098686  0.18476892 -0.6815615  -0.12871377 -1.1374625\n",
            " -0.15187044  0.04448448 -0.11303175 -0.831116   -1.5363184   0.58024716\n",
            " -0.85096186 -0.8011578  -0.01480514 -0.77782285 -1.5384896  -0.22311826\n",
            " -0.40565917]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.44555172 -0.0640723   0.12775892 -0.38028738 -0.35279253 -1.1406425\n",
            " -0.13390425  0.14020932 -0.11832479 -0.61095273 -1.4638602   0.96201336\n",
            " -0.7496046  -0.9400616   0.13304663 -0.7748722  -1.5076447  -0.30002856\n",
            " -0.5496766 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.29383743 -0.09718947  0.11752035 -0.7159802  -0.2847118  -1.0215473\n",
            " -0.08315736 -0.11182809 -0.4325468  -0.58940923 -1.1929975   0.311074\n",
            " -0.62495536 -0.83906096 -0.03452761 -0.94109905 -1.5551796   0.01629142\n",
            " -0.37519586]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3443959  -0.1869623  -0.04723123 -0.49711826 -0.42290142 -0.86479306\n",
            " -0.20062934 -0.09561338 -0.07045931 -0.68902576 -1.5074767   1.1269403\n",
            " -0.7800633  -0.87601113  0.0722894  -0.70648116 -1.4741547  -0.34841356\n",
            " -0.46221983]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.22200365  0.07186195  0.1903126  -0.6834609  -0.08019187 -1.050149\n",
            " -0.20731902  0.06814997 -0.14686511 -0.78144693 -1.5340676   0.7454248\n",
            " -0.79306406 -0.80037403  0.07130542 -0.8491225  -1.3966569  -0.28404722\n",
            " -0.40069002]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.39236498 -0.06862738  0.32138142 -0.5071947  -0.3857403  -1.1338897\n",
            " -0.14368586  0.01106504 -0.3577643  -0.357564   -1.3481969   0.66721654\n",
            " -0.708337   -1.2222861   0.13906293 -1.0443176  -1.6153362   0.07420679\n",
            " -0.49897164]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1842375  -0.26653317  0.4017879  -0.78543377 -0.33292928 -0.9563122\n",
            " -0.3306243  -0.07073076 -0.19836402 -0.4384448  -1.5134108   0.4959347\n",
            " -0.6924716  -0.9737917   0.15637143 -1.0833278  -1.6085674  -0.15737565\n",
            " -0.57284343]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.37980273 -0.2277982   0.11268823 -0.44183168 -0.42066306 -0.9245922\n",
            " -0.2707168  -0.08103402 -0.09197212 -0.6530132  -1.6206168   1.1000894\n",
            " -0.88609815 -1.0552949   0.08442896 -0.7108012  -1.4129055  -0.51372814\n",
            " -0.50906324]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.28079316  0.11472496  0.3939574  -0.42440382 -0.10195667 -1.214186\n",
            " -0.14963503  0.13374683 -0.15544291 -0.75896513 -1.368989    0.68800735\n",
            " -0.78215766 -0.8523662   0.12883885 -0.6317794  -1.3764035  -0.40126297\n",
            " -0.36544603]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.16387288 -0.01870357  0.25494266 -0.7247292  -0.26769558 -0.96809685\n",
            " -0.23703451 -0.10622133 -0.18437983 -0.34199184 -1.1747913   0.9776181\n",
            " -0.8730949  -0.6409469   0.15480804 -1.1738908  -1.3862593   0.00740167\n",
            " -0.28877923]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.29625812  0.12506403  0.27326316 -0.60940176 -0.43419248 -1.2015609\n",
            " -0.18411697  0.00721311 -0.4425697  -0.50183725 -1.0463058   1.1210784\n",
            " -0.9338162  -1.1514918   0.14617485 -1.1092882  -1.457317    0.1281401\n",
            " -0.37539646]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.29675293  0.24493866  0.45921868 -0.7924323  -0.3247827  -1.3164854\n",
            " -0.29004666 -0.04585587 -0.11386549 -0.7456795  -1.7270547   0.5552477\n",
            " -0.87964135 -0.74153125  0.07568523 -0.948559   -1.3858917  -0.39728266\n",
            " -0.34056407]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.1824738e-01 -1.3781493e-03  4.0422630e-01 -7.0771950e-01\n",
            " -1.2850973e-01 -1.0687095e+00 -1.6558410e-01 -3.4798309e-02\n",
            " -3.8292322e-01 -8.5377651e-01 -1.2295854e+00  5.5948091e-01\n",
            " -7.8006470e-01 -9.0021729e-01  1.4487717e-01 -7.6557422e-01\n",
            " -1.5382230e+00 -3.3296174e-01 -3.8831320e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.33611128 -0.0651525   0.30067968 -0.811543   -0.4950806  -0.9800772\n",
            " -0.11384896 -0.13498846 -0.34425613 -0.43883657 -1.2754635   0.76385707\n",
            " -0.96193826 -0.82645965 -0.01251918 -1.0958285  -1.4446796  -0.03486492\n",
            " -0.18914454]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.28261012  0.09972067  0.58425635 -0.7500175  -0.44965026 -1.1481102\n",
            " -0.10865531 -0.07087103 -0.4079693  -0.4859482  -0.98841727  0.7732038\n",
            " -0.94876736 -1.0142149  -0.0701925  -1.1617098  -1.5602787  -0.01704939\n",
            " -0.31573436]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.20247158 -0.13718116  0.8985202  -0.7950443  -0.24089855 -0.94985694\n",
            " -0.10356456  0.13070334 -0.3207329  -0.6191192  -1.4615788   0.34369096\n",
            " -0.724622   -0.97618914  0.11940728 -1.076111   -1.5773039  -0.10907885\n",
            " -0.69803053]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.48284099  0.01699527  0.6868558  -0.5106461  -0.5150032  -0.89324796\n",
            " -0.16952944 -0.06450138 -0.44490469 -0.3493789  -1.4381832   0.3580579\n",
            " -0.8262186  -0.8073486   0.03483805 -0.8399485  -1.3409886  -0.25683716\n",
            " -0.49263763]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.27868485 -0.05755865  0.43487018 -0.67372185 -0.12672088 -0.81586725\n",
            " -0.16696128  0.07966137 -0.21784942 -0.77726805 -1.3479403   0.2115529\n",
            " -0.8025159  -0.7342286   0.06215373 -0.694897   -1.0896593  -0.32056394\n",
            " -0.43822512]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4489778   0.03086884  0.6123103  -0.8095025  -0.4283923  -0.9071556\n",
            " -0.36299017  0.04738961 -0.5729758  -1.0735534  -2.1230466   0.5469304\n",
            " -1.2492222  -1.4186091   0.10576462 -0.8203947  -1.1723173  -0.51465666\n",
            " -0.7093746 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 28 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1344055  -0.21791753  0.319004   -0.81190294 -0.12784562 -0.7817059\n",
            " -0.31313545 -0.04046853 -0.33412892 -0.8702177  -1.7071652   0.2728515\n",
            " -0.9337709  -1.1036171   0.11975801 -0.9618971  -1.2318385  -0.31832334\n",
            " -0.6248055 ]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 29 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.13779876 -0.03743749  0.19879279 -0.6678174  -0.28017625 -0.9467188\n",
            " -0.17457373 -0.03570859 -0.42478624 -0.5576435  -1.3865523   0.10479249\n",
            " -0.8068562  -1.0751929   0.21243544 -1.0675508  -1.2099451  -0.14976409\n",
            " -0.7552353 ]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 30 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.15515307 -0.16532142  0.02559355 -0.5335382  -0.26329583 -0.7023822\n",
            " -0.22518131 -0.07144149 -0.4164798  -0.45687598 -1.0983412   0.23810533\n",
            " -0.5842436  -1.0225828   0.1804361  -1.0452316  -1.1373153  -0.15571748\n",
            " -0.5799717 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 31 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.13050403  0.18369734  0.17278829 -0.24137323 -0.1336273  -0.46059024\n",
            " -0.23816486 -0.02207279 -0.02796289 -0.14967509 -0.54992986  0.08355351\n",
            " -0.23635949 -0.33287683  0.13297737 -0.58827233 -0.55181575 -0.05740547\n",
            " -0.28748822]\n",
            "\n",
            "Taking action 1\n",
            "\n",
            "Step 32 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.24071415  0.11728601  0.18798563 -0.5630927  -0.20624591 -0.60600084\n",
            " -0.16399989 -0.18107314  0.0165699  -0.38451004 -0.81555474 -0.00925549\n",
            " -0.45176873 -0.31422397  0.03950997 -0.71646196 -0.82488704 -0.02640446\n",
            " -0.22527778]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 33 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.46747008e-01 -9.45884967e-05  1.70310244e-01 -4.19083446e-01\n",
            " -1.15265958e-01 -5.06025374e-01 -3.00463140e-01 -4.66515124e-02\n",
            " -9.38233268e-03 -3.51478249e-01 -7.99478829e-01 -7.41952704e-03\n",
            " -4.27965701e-01 -3.25117141e-01  4.96578217e-02 -6.96833313e-01\n",
            " -9.50024128e-01 -5.08843511e-02 -1.20415695e-01]\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 34 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 2.7550675e-02 -3.0063307e-01 -1.1654189e-02 -3.3969688e-01\n",
            " -1.5098846e-01 -7.5871634e-01 -2.8551245e-01  1.2650311e-01\n",
            " -2.4680206e-01 -5.6228673e-01 -7.9307324e-01 -1.0068694e-01\n",
            " -4.2971006e-01 -6.9804817e-01 -7.1479345e-04 -6.0533756e-01\n",
            " -1.2264861e+00 -3.3091325e-01 -5.3495187e-01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 35 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.04438758 -0.3172142  -0.02534822 -0.4633675  -0.13608555 -0.94972193\n",
            " -0.39292014 -0.03165207 -0.14920004 -0.50677633 -0.7626624  -0.02880687\n",
            " -0.6011266  -0.7880065   0.01204803 -0.8484444  -0.9266272  -0.14152095\n",
            " -0.4850643 ]\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.1016759  -0.19249861 -0.02271035 -0.44403067 -0.19926137 -0.5798194\n",
            " -0.52565634  0.08824265 -0.15690558 -0.4150759  -0.552133   -0.05642857\n",
            " -0.358391   -0.5895035  -0.0473646  -0.5939455  -0.9168698  -0.22625454\n",
            " -0.36535487]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 37 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.22082405 -0.43221295 -0.18821326 -0.7617311  -0.4647627  -1.0916513\n",
            " -0.47622257 -0.23659167 -0.21355683 -0.7047489  -0.96867627 -0.12363324\n",
            " -0.81428385 -0.88068086 -0.07015045 -0.9603693  -1.2426636  -0.21938972\n",
            " -0.5562697 ]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 38 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.18567052 -0.43620065 -0.37816137 -0.7694904  -0.30944073 -1.0360142\n",
            " -0.48676285 -0.30396605 -0.19894168 -0.7226124  -1.310186   -0.2168364\n",
            " -0.70037407 -1.0961194   0.13423036 -0.9305859  -1.327115   -0.2716812\n",
            " -0.7995982 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 39 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.258669   -0.48408952 -0.48354462 -0.67095995 -0.2156978  -0.60151535\n",
            " -0.6082627   0.0342518  -0.22378735 -0.57308024 -0.99395055 -0.32040238\n",
            " -0.5788291  -0.63977885  0.0172998  -0.7006348  -1.2970871  -0.32726544\n",
            " -0.47401172]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5\n",
            "\n",
            "Step 40 reward=-2 new_state=[0 0 1 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.24836169 -0.35068268 -0.41332993 -0.7655622  -0.29762912 -1.1031133\n",
            " -0.5548765  -0.20659412 -0.2981722  -0.78004867 -0.8976697  -0.3238454\n",
            " -0.7804046  -0.8768547  -0.04813599 -0.989982   -1.309655   -0.18619746\n",
            " -0.5545762 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 41 reward=-2 new_state=[0 0 1 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.479605   -0.4819424  -0.4907623  -0.50047594 -0.23634748 -0.82345515\n",
            " -0.48638088  0.06112214 -0.33495587 -0.83601034 -0.8103372  -0.10765421\n",
            " -0.69455904 -1.0010004  -0.0375147  -1.0282398  -1.3937296  -0.13104478\n",
            " -0.65508175]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 42 reward=-2 new_state=[0 0 1 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.56621695 -0.615361   -0.71304315 -0.74809945 -0.32934573 -1.0622493\n",
            " -0.6884069  -0.32306534 -0.41164842 -0.85538304 -1.2842635  -0.40878698\n",
            " -0.8486331  -0.91135234  0.08384087 -1.0839938  -1.4255316  -0.5434339\n",
            " -0.59419096]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 43 reward=-2 new_state=[0 0 1 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.69133353 -0.66028523 -0.8833677  -0.7883921  -0.26163313 -1.016342\n",
            " -0.47723198 -0.3302068  -0.48118582 -1.0374751  -1.1913836  -0.6109349\n",
            " -0.8535278  -1.1026982   0.09136552 -0.96918696 -1.5902699  -0.54430145\n",
            " -0.7930993 ]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 44 reward=-2 new_state=[0 0 1 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.36939552 -0.49396878 -0.46358317 -0.47609973 -0.28441888 -0.90165323\n",
            " -0.5126481  -0.11185981 -0.24929798 -0.7462525  -0.8466306  -0.32271034\n",
            " -0.7241258  -0.73431647 -0.00344506 -0.8122694  -1.2319939  -0.12776789\n",
            " -0.4275477 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 45 reward=-2 new_state=[0 0 1 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.51316863 -0.6781022  -0.9456864  -0.8075992  -0.32057464 -1.2084913\n",
            " -0.8574951  -0.29617128 -0.5752123  -1.050667   -1.2485315  -0.54162335\n",
            " -1.0224074  -1.0712193   0.10217766 -1.2082931  -1.4772955  -0.50685316\n",
            " -0.6985316 ]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 46 reward=-2 new_state=[0 0 1 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6046983  -0.7081823  -0.73096865 -0.7513664  -0.50250006 -1.3896089\n",
            " -0.9235951  -0.25715733 -0.4359567  -1.0083342  -1.1543156  -0.7105482\n",
            " -1.0002998  -0.86546963  0.0772708  -1.0186182  -1.4228219  -0.18095064\n",
            " -0.6463617 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 47 reward=-1 new_state=[0 0 1 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5464539  -0.49288356 -0.6465424  -0.7030925  -0.2847239  -1.1153014\n",
            " -0.76798373 -0.12472187 -0.44157648 -1.2638338  -0.87283474 -0.5155116\n",
            " -0.7823409  -1.1518397  -0.13454242 -0.9351223  -1.4007703  -0.16723461\n",
            " -0.78679097]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 0 1 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.73780614 -0.56180525 -0.95751154 -0.61761844 -0.35352165 -1.112961\n",
            " -1.2193027  -0.29991266 -0.50719553 -1.2504212  -1.1752523  -0.3550177\n",
            " -1.1023633  -1.2999144   0.32231218 -1.1473496  -1.4778743  -0.49629438\n",
            " -0.80444705]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 1 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.70099545 -0.8622567  -0.9646215  -0.5712493  -0.19257444 -1.1080034\n",
            " -1.2980062  -0.00437083 -0.4088099  -1.2534217  -1.3604064  -0.5155898\n",
            " -0.8907068  -1.2512043   0.12892157 -1.2096429  -1.6055137  -0.528894\n",
            " -0.847193  ]\n",
            "\n",
            "Taking action 6\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 0 1 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6714189  -0.60306144 -0.87171656 -0.7196442  -0.26191476 -1.3251572\n",
            " -1.2009858   0.04331303 -0.4486933  -1.3063022  -1.0830141  -0.7728783\n",
            " -1.0461364  -1.2229061   0.04599756 -1.0367129  -1.5124297  -0.2918343\n",
            " -0.8667114 ]\n",
            "Epsilon reduced to 0.12800000000000003\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.982478   -0.7611961  -0.85239744 -0.9190093  -0.33479905 -1.4057913\n",
            " -1.1810515  -0.51237535 -0.42459044 -1.1905922  -1.2933875  -0.5891469\n",
            " -1.0465969  -0.9056947   0.1959684  -1.185548   -1.0230923  -0.3340373\n",
            " -0.8809319 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.4074194  -0.74029195 -0.9993695  -1.009411   -0.61686486 -1.3193368\n",
            " -1.7453113  -0.2682697  -0.9649932  -1.7724847  -1.8082663  -0.57865673\n",
            " -1.8003905  -1.4966872   0.1321235  -1.4739895  -0.9923785  -0.53281957\n",
            " -1.1023448 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9289839  -0.779166   -1.1830587  -1.0066522  -0.02818802 -1.4901516\n",
            " -1.4990395  -0.22683537 -0.7277715  -1.5190744  -1.6662831  -0.72449857\n",
            " -1.1988566  -1.4228568   0.05233668 -1.2086004  -1.2179977  -0.3092803\n",
            " -0.8827531 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.3118759  -0.9003521  -1.1798388  -0.8081748  -0.4959481  -1.4075204\n",
            " -1.8825213  -0.12411971 -0.7607623  -1.5782491  -1.3987684  -0.8739657\n",
            " -1.6006868  -1.4661806   0.47140676 -1.5386251  -0.9526174  -0.43620744\n",
            " -0.9794315 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9438566  -0.73154503 -1.2327799  -0.99768335 -0.15450273 -1.4451905\n",
            " -1.5609505  -0.18100733 -0.644338   -1.8795093  -1.7654377  -0.7104937\n",
            " -1.5087883  -1.2801392   0.0349677  -1.3961699  -1.1796935  -0.4042257\n",
            " -1.1393254 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1675696  -1.0382915  -1.1793373  -0.8709343  -0.32848194 -1.4605196\n",
            " -1.6011989  -0.05524596 -0.9849436  -1.6744802  -1.6310742  -1.0058041\n",
            " -1.566949   -1.4766456   0.25462186 -1.4022647  -1.1174847  -0.33572719\n",
            " -1.0808158 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7680465  -0.8302709  -1.2731929  -0.9200962  -0.28452516 -1.4505544\n",
            " -1.5503615  -0.2574528  -0.7287862  -1.578859   -1.5193301  -0.6752289\n",
            " -1.2991837  -1.425202   -0.02541461 -1.052808   -1.1981603  -0.3990085\n",
            " -0.83872455]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.6154885  -0.78372073 -1.2906448  -0.80490357 -0.6736674  -1.5172719\n",
            " -1.9036745  -0.3601253  -0.8789452  -1.7438388  -1.7517643  -0.7614826\n",
            " -1.7859726  -1.3036221   0.30554405 -1.4052571  -1.0000771  -0.7133901\n",
            " -0.849333  ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0914043  -0.74660957 -1.2974818  -1.0364864  -0.18605372 -1.4740152\n",
            " -1.5403382  -0.23539098 -0.77928317 -1.9711773  -1.6615283  -0.77635205\n",
            " -1.5905045  -1.2354932  -0.15455827 -1.407756   -1.2007484  -0.31991667\n",
            " -1.0579041 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14\n",
            "\n",
            "Step 10 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.4514955  -0.902455   -1.2655406  -0.98803073 -0.55746543 -1.5024933\n",
            " -2.001413   -0.16250306 -1.3066474  -1.721572   -1.5497823  -0.87807804\n",
            " -1.8096683  -1.3965185   0.20347567 -1.6200973  -0.9601492  -0.3541517\n",
            " -0.95283705]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 11 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0688095  -0.73736554 -1.3064455  -0.9939136  -0.20537469 -1.5186737\n",
            " -1.6218997  -0.20168255 -0.91866887 -1.9147834  -1.6393481  -0.8331658\n",
            " -1.6206311  -1.2618355  -0.26480138 -1.4131776  -1.1155596  -0.36130828\n",
            " -1.0760162 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.2287118  -0.9202675  -1.202922   -0.8424516  -0.60656834 -1.6325564\n",
            " -1.8926765  -0.19669035 -0.9181326  -1.5121617  -1.2754152  -0.8689126\n",
            " -1.6984881  -1.5794369   0.10061429 -1.4412732  -1.2215042  -0.37639445\n",
            " -1.0093799 ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.91826403 -0.8964492  -1.05458    -0.81115735 -0.25837964 -1.5297836\n",
            " -1.6835697  -0.22480638 -0.67676824 -1.3355203  -1.183469   -0.9036901\n",
            " -1.267888   -1.1263378  -0.24398582 -1.2899836  -1.0878788  -0.03983017\n",
            " -0.70248586]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2462473  -0.42978474 -0.99468064 -0.52645975 -0.5679529  -1.5786633\n",
            " -1.5203055  -0.21146888 -0.39773285 -1.5496159  -1.1874803  -0.4578135\n",
            " -1.2767533  -0.9423599  -0.09565227 -1.1583965  -0.9813795  -0.50800836\n",
            " -0.855552  ]\n",
            "\n",
            "Taking action 8\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6911022  -0.55699575 -0.8306361  -0.7032822  -0.37111428 -1.0896572\n",
            " -1.4411725  -0.21062872 -0.3878593  -0.967589   -0.87104833 -0.4532724\n",
            " -0.9827139  -0.84440136 -0.2475191  -1.1508243  -0.7700729  -0.00822797\n",
            " -0.46970135]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9368641  -0.38409403 -0.7574877  -0.5565339  -0.58687866 -1.0948764\n",
            " -1.5074329  -0.10510497 -0.40519655 -1.4563869  -1.2215221  -0.23798557\n",
            " -1.2682301  -0.95124364 -0.37496808 -1.2384661  -0.7789645  -0.07695764\n",
            " -0.82610613]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.59770936 -0.5434469  -0.98410153 -0.62340134 -0.49209043 -0.95949656\n",
            " -1.3565376  -0.2114199  -0.20790257 -0.91874444 -0.8775071  -0.46563697\n",
            " -1.02475    -0.79831284 -0.2678117  -1.015503   -0.8697082   0.09291301\n",
            " -0.431312  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.85617983 -0.4416871  -0.78410816 -0.6171172  -0.66981995 -1.1004459\n",
            " -1.5490266  -0.10997338 -0.26207057 -1.4281042  -1.1928294  -0.22067153\n",
            " -1.334442   -0.98505086 -0.50401247 -1.1730139  -0.8250318   0.10640341\n",
            " -0.874591  ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.75984055 -0.43636432 -0.8616272  -0.46020472 -0.32077387 -0.9698782\n",
            " -1.226726   -0.14430042 -0.03321848 -1.188881   -1.039181   -0.28483915\n",
            " -1.1200677  -0.6817942  -0.46081614 -1.1581248  -0.8437399   0.34948945\n",
            " -0.48728356]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1507667  -0.40317902 -0.76652575 -0.6040393  -0.62681264 -1.3261772\n",
            " -1.5293789  -0.23867176  0.21213591 -1.2728841  -1.0697912  -0.23217697\n",
            " -1.1408067  -0.8977436  -0.35604218 -1.050584   -0.8215987   0.14755408\n",
            " -0.70254165]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6801151  -0.35520846 -0.57836175 -0.47100827 -0.31309426 -0.95905256\n",
            " -1.1432136  -0.0370877   0.05357499 -0.8039672  -0.7160471  -0.27839696\n",
            " -0.75576717 -0.5033669  -0.26244774 -0.94167304 -0.82120025  0.3170967\n",
            " -0.30840024]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9166632  -0.5853332  -0.8224773  -0.7532939  -0.27732503 -1.2114761\n",
            " -1.5888511  -0.11675953  0.1126193  -1.0524648  -0.9577682  -0.4225574\n",
            " -1.1016464  -0.7661768  -0.471729   -1.2047398  -0.88608056  0.60234195\n",
            " -0.5696837 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.76403177 -0.40157458 -0.52363163 -0.35147572 -0.26841348 -1.0578352\n",
            " -1.2829509   0.04661014  0.08142481 -0.82836336 -0.53948057 -0.29586762\n",
            " -0.8875005  -0.6651951  -0.32493508 -0.942627   -0.74515915  0.5148912\n",
            " -0.34177205]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9454966  -0.6095778  -0.6566714  -0.49234414 -0.43260133 -1.355384\n",
            " -1.4626914  -0.03029159  0.03585565 -1.046      -0.76483166 -0.16093528\n",
            " -1.0442213  -0.8584328  -0.33715078 -1.0993237  -0.6932701   0.4819103\n",
            " -0.6928003 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.72962856 -0.44986734 -0.8488904  -0.7885516  -0.38592082 -1.2182077\n",
            " -1.407219    0.02338835  0.17635246 -1.0732298  -0.9234265  -0.28274003\n",
            " -1.0056791  -0.5922758  -0.5610965  -1.052313   -1.0372837   0.6411231\n",
            " -0.5585239 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6149162  -0.56333804 -0.90629274 -0.6235295  -0.30974805 -1.1754969\n",
            " -1.551098    0.12384054  0.24639577 -1.0882595  -1.0938522  -0.40315485\n",
            " -1.0844356  -0.7520399  -0.48156044 -1.1700386  -0.9657923   0.73621905\n",
            " -0.59383184]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6040972  -0.49864525 -0.65949017 -0.54318476 -0.5517412  -1.3327526\n",
            " -1.2469238   0.16197099  0.3769447  -1.2591102  -0.86928266 -0.27286923\n",
            " -1.1139269  -0.79153293 -0.52458155 -1.0583067  -0.8787241   0.79032797\n",
            " -0.4969648 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9205484  -0.78574777 -1.1174756  -0.6399659  -0.39946467 -1.8238783\n",
            " -1.5263283   0.10506672  0.23307925 -1.5490284  -1.1350719  -0.25801972\n",
            " -1.2328159  -1.0979239  -0.6318743  -1.1208762  -1.2589865   0.5121759\n",
            " -0.9574227 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.65924484 -0.6104645  -0.98667866 -0.90879214 -0.5170483  -1.4596825\n",
            " -1.7678723   0.32152846  0.24199097 -1.2412282  -1.1257088  -0.32159057\n",
            " -1.2640905  -0.780415   -0.68673116 -1.0591155  -1.2490836   1.0585092\n",
            " -0.6166177 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.32394657 -0.62748986 -0.7495835  -0.6882829  -0.51598793 -1.2602787\n",
            " -1.5156486   0.27926436  0.1654533  -1.2501795  -0.93135625 -0.14394844\n",
            " -1.0768931  -0.77233493 -0.63530195 -1.1941626  -0.94520676  0.97440165\n",
            " -0.6265884 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.37295318 -0.5220771  -0.8508305  -0.6668559  -0.3618665  -1.199366\n",
            " -1.4080559   0.20581481  0.24056244 -1.0977125  -0.77093714 -0.3798731\n",
            " -1.0357897  -0.6999117  -0.5674237  -0.9966359  -1.2823465   0.52175885\n",
            " -0.49793854]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7671778  -0.6772901  -1.2539957  -1.0671885  -0.32369858 -1.9325962\n",
            " -2.0255845   0.60028154 -0.1781673  -1.7682393  -1.3368336  -0.38992143\n",
            " -1.5415912  -1.4265009  -1.0593853  -1.3602115  -1.3591416   1.2915066\n",
            " -1.0729294 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 33 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.29547992 -0.3428099  -0.5787438  -0.4926497  -0.34524286 -0.8570966\n",
            " -1.0536805   0.13142957  0.42413795 -0.81106174 -0.6583955  -0.14971602\n",
            " -0.88034457 -0.5121726  -0.3040199  -0.63772607 -0.7301117   0.5927778\n",
            " -0.44208106]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 34 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66100603 -0.6332985  -0.9964565  -0.94657594 -0.2571726  -1.3484874\n",
            " -2.0209098   0.45698366  0.16464561 -1.3524243  -1.0662084  -0.2616652\n",
            " -1.4352671  -0.9117222  -0.611202   -1.225055   -0.99543124  1.1548678\n",
            " -0.7743522 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 35 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39441124 -0.87233937 -0.97628856 -0.6398383  -0.39380375 -1.4384309\n",
            " -1.6148779   0.41895604  0.19102193 -1.3306788  -0.84746385 -0.16163842\n",
            " -1.3478106  -0.84397656 -0.539719   -1.2287306  -1.0431383   0.83941627\n",
            " -0.6197098 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.19830136 -0.7777036  -0.881811   -1.0082849  -0.4167497  -1.5468417\n",
            " -1.4287137   0.29886898  0.04869157 -1.4498731  -0.8150364  -0.27215773\n",
            " -1.3274003  -0.6904562  -0.685605   -1.2345687  -1.2292941   0.78442377\n",
            " -0.71432877]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 37 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40104318 -0.59830135 -0.7978936  -0.836124   -0.26514614 -1.4817578\n",
            " -1.9950482   0.6752659   0.23648328 -1.6718063  -1.1588221  -0.05080773\n",
            " -1.4195478  -1.175019   -0.7379494  -1.1722984  -0.90971833  0.83079815\n",
            " -0.74349254]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 38 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.48250332 -0.48975217 -0.804359   -0.6972922  -0.43441287 -1.2656512\n",
            " -1.4867632   0.3207192   0.22549589 -1.190103   -0.9623698  -0.03371271\n",
            " -1.052378   -0.63928235 -0.48052546 -1.0610353  -0.82828677  0.5491191\n",
            " -0.82817197]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 39 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.37239227 -0.77357996 -0.9392333  -1.0887134  -0.43339506 -1.3892792\n",
            " -1.6455895   0.48790535  0.25603867 -1.5164379  -0.8902322  -0.24896863\n",
            " -1.449479   -0.74606264 -0.7438491  -1.2534959  -1.1953039   0.9727533\n",
            " -0.5430445 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 40 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.54328984 -0.807109   -0.9470254  -0.78276765 -0.32108906 -1.4737825\n",
            " -1.4380442   0.51774687  0.24445589 -1.5925193  -0.8366463  -0.16618863\n",
            " -1.2354696  -0.93742204 -0.72968036 -1.330731   -0.98327076  0.48499632\n",
            " -0.5664374 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39712182 -0.558419   -1.0388302  -0.77845067 -0.39840516 -1.3252145\n",
            " -1.7251072   0.44668615  0.08732379 -1.118363   -0.7432923  -0.26737925\n",
            " -1.1437589  -0.73485374 -0.72405374 -1.1288791  -0.9767056   0.07014925\n",
            " -0.7520262 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.14152221 -0.7041078  -0.879717   -0.86190385 -0.46553367 -1.3177013\n",
            " -1.9183818   0.46642214  0.3075893  -1.2189679  -0.71267354 -0.12154768\n",
            " -1.1549034  -0.9245682  -0.78126115 -1.0801511  -0.8674188   0.23287791\n",
            " -0.66832477]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39396012 -0.5559835  -0.943653   -0.56931967 -0.40728873 -1.2213373\n",
            " -1.1198432   0.27343154  0.3104031  -0.9678899  -0.6181532  -0.21568286\n",
            " -1.0334225  -0.79235244 -0.49800617 -0.98056775 -0.9792816   0.30737805\n",
            " -0.6961832 ]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.47591007 -0.5021798  -0.8484753  -0.6852614  -0.376871   -1.660196\n",
            " -1.5060997   0.60526574  0.23448053 -1.3761413  -0.63240224 -0.4010123\n",
            " -1.2614464  -1.1395676  -0.8479215  -1.036122   -1.0609766   0.15478581\n",
            " -0.4794286 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.30402008 -0.3941788  -0.3756933  -0.27477238 -0.53650665 -1.0544835\n",
            " -1.0363927   0.32488313  0.2516712  -0.78868794 -0.50776416 -0.10953732\n",
            " -0.7419607  -0.56733555 -0.41760874 -0.91893125 -0.6019098   0.10621253\n",
            " -0.4110335 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.20512159 -0.47797439 -0.68897563 -0.7313943  -0.33025536 -1.3294871\n",
            " -1.5527778   0.5380886   0.43403146 -1.0438089  -0.7572435  -0.1419497\n",
            " -0.9613464  -0.57361966 -0.54009426 -0.9700241  -0.9668315  -0.23061733\n",
            " -0.51811016]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.15987845 -0.886536   -1.1720424  -0.80851585 -0.41147745 -1.5802602\n",
            " -1.996333    0.5717602   0.25103435 -1.3477316  -0.605976    0.01683193\n",
            " -1.2887344  -0.9922481  -0.6096303  -1.2557743  -1.2345886  -0.35137516\n",
            " -0.86182296]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3798484  -0.47994626 -0.9254571  -0.52092594 -0.39476395 -1.3625529\n",
            " -1.2254034   0.32634643  0.4534787  -1.0665016  -0.74253917 -0.18259035\n",
            " -0.9638691  -0.6974909  -0.42161357 -1.1284716  -0.8874525  -0.04050316\n",
            " -0.76787955]\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3220386  -0.34737238 -0.8798262  -0.56818753 -0.37128463 -1.1802244\n",
            " -1.4459236   0.45127538  0.4457468  -1.0218687  -0.5946784  -0.00250665\n",
            " -0.9007882  -0.7989467  -0.61051136 -1.0067195  -0.8070534  -0.15337652\n",
            " -0.34269136]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2623022  -0.6803812  -0.6290405  -0.46758482 -0.39411318 -1.1760747\n",
            " -1.5163957   0.5572802   0.22571665 -1.0515454  -0.6252897   0.34779668\n",
            " -0.99258167 -0.93640834 -0.5922783  -1.1000651  -0.78500485 -0.29106355\n",
            " -0.47243306]\n",
            "Epsilon reduced to 0.10240000000000003\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.42189458 -0.9780784  -0.7025925  -0.4404854  -0.18732917 -1.1343205\n",
            " -1.3329855   0.80850154  0.1439266  -1.1983223  -0.91826713 -0.12577121\n",
            " -0.85893697 -1.0341228  -0.44117525 -0.9322048  -0.9191922  -0.36319762\n",
            " -0.92414176]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.28805766 -0.73341435 -1.0168124  -0.38661033 -0.06209318 -1.1134679\n",
            " -1.3015572   1.0229763  -0.01083755 -1.355718   -0.8683329   0.04461956\n",
            " -0.9112205  -1.2070177  -0.7646263  -1.0088656  -0.998619   -0.01716488\n",
            " -0.9264685 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.3565936  -0.78227425 -0.8853792  -0.50793904 -0.14164944 -1.0333706\n",
            " -1.2200614   0.8655051  -0.07728963 -1.4296774  -1.0889875   0.15410247\n",
            " -0.88397807 -0.8621238  -0.8172155  -0.8368165  -0.87682515 -0.08012351\n",
            " -0.7189942 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.24361067 -0.7633764  -0.85548466 -0.4045051  -0.09314643 -0.9246966\n",
            " -1.2189001   0.95752084  0.02143223 -1.3026098  -0.81509113  0.12217264\n",
            " -0.7950601  -0.85655427 -0.7771988  -1.019362   -0.9886851  -0.05291484\n",
            " -0.8977071 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.42436212 -0.70307314 -0.8915255  -0.46725953 -0.16052589 -1.145291\n",
            " -1.2539754   0.8183325   0.14900878 -1.4340498  -0.9343852   0.03025989\n",
            " -0.7854015  -0.48281276 -0.6250872  -0.7821999  -0.9435952  -0.35696843\n",
            " -0.63856256]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.33693436 -0.78515357 -0.9524011  -0.46323845 -0.252352   -0.9780268\n",
            " -1.3980162   0.97653604  0.04648221 -1.3393735  -0.8417953   0.03663807\n",
            " -0.79918975 -0.6621321  -0.7195706  -1.055601   -0.95898473 -0.14077505\n",
            " -0.88287145]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.40733787 -0.6223082  -0.80949855 -0.52187896 -0.25038877 -1.0865835\n",
            " -1.2432686   0.7362545   0.009983   -1.223068   -0.871578    0.01952653\n",
            " -0.6008941  -0.11201234 -0.5138417  -0.80436295 -0.8488949  -0.266854\n",
            " -0.6525654 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.21311052 -0.5517032  -0.9044007  -0.38211516 -0.06592051 -0.89106625\n",
            " -1.1603848   0.8805009  -0.02679359 -1.282879   -0.61580384  0.27954096\n",
            " -0.6701219  -0.2677796  -0.76131386 -0.86601776 -0.7803539  -0.06352715\n",
            " -0.73380727]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.3256598  -0.51124644 -0.78560156 -0.3414594  -0.25700346 -1.0741352\n",
            " -1.2075129   0.79763246 -0.11394403 -0.8984641  -0.771485   -0.09973469\n",
            " -0.6072984  -0.09636459 -0.60480833 -0.9762487  -0.9159069  -0.1443943\n",
            " -0.60605055]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.25842825 -0.6222777  -0.8091483  -0.41172332 -0.12320481 -1.080505\n",
            " -1.2842273   0.7100506   0.25915748 -1.060118   -0.66100323  0.0114612\n",
            " -0.55056596  0.00353845 -0.44589883 -0.9275934  -0.9333363  -0.55403715\n",
            " -0.80650306]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.4562677  -0.49579594 -0.81292236 -0.37072212 -0.1645682  -1.1603504\n",
            " -1.3853582   0.6680669   0.03391056 -0.9087315  -0.6228942   0.12461122\n",
            " -0.51896536 -0.11708094 -0.686539   -0.86976105 -1.0685467  -0.35500315\n",
            " -0.5855679 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.2358802  -0.28099746 -0.6660763  -0.4658346  -0.10210877 -0.87987113\n",
            " -1.2607461   0.56206334  0.36904585 -0.94194186 -0.5242945   0.25227994\n",
            " -0.40470645  0.16859025 -0.44173884 -0.71877736 -0.8731938  -0.5169912\n",
            " -0.52529055]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.22262272 -0.34278238 -0.6113517  -0.24225397 -0.14681885 -0.82022196\n",
            " -1.1200523   0.7447654   0.3726494  -0.9808684  -0.52458787  0.44319883\n",
            " -0.48048997  0.356756   -0.48984867 -0.55011517 -0.73852426 -0.54609257\n",
            " -0.38930267]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10041527 -0.31951827 -0.5323062  -0.29876655 -0.10383453 -0.6603415\n",
            " -1.107234    0.756004    0.05688578 -0.8791177  -0.4713584   0.38724676\n",
            " -0.4975308   0.36444724 -0.68396926 -0.74766016 -0.61626923 -0.13538398\n",
            " -0.5472336 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.21903144 -0.336525   -0.56876767 -0.38388023 -0.2585666  -0.7563264\n",
            " -1.2024505   0.816498    0.05768526 -1.055077   -0.6138404   0.3933313\n",
            " -0.5712104   0.37204948 -0.68455684 -0.6932512  -0.5707608  -0.22311714\n",
            " -0.56128883]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.05089202 -0.3155828  -0.61992085 -0.37756574 -0.15990022 -0.71817416\n",
            " -1.2337501   0.62366515 -0.03835337 -0.91154355 -0.57318044  0.3376189\n",
            " -0.35299554  0.41723448 -0.5450896  -0.72578734 -0.7232025  -0.15009044\n",
            " -0.425005  ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.24586609 -0.29540396 -0.61370903 -0.31656042 -0.21771489 -0.84476525\n",
            " -1.3981557   0.66595334  0.12904993 -0.561684   -0.5876316   0.3269842\n",
            " -0.30049425  0.55278236 -0.38553455 -0.80355763 -0.8711934  -0.48150715\n",
            " -0.35088316]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.02215607 -0.27842894 -0.58909863 -0.40274727 -0.12936765 -0.6933706\n",
            " -1.2135639   0.6807288   0.0065617  -0.9301023  -0.52585375  0.43718386\n",
            " -0.37671053  0.7378535  -0.51293063 -0.8393618  -0.70212823 -0.14140102\n",
            " -0.5428382 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 19 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0055212  -0.18098268 -0.42224634 -0.2960941  -0.1698599  -0.6392997\n",
            " -1.0516171   0.5423233   0.04853568 -0.89155453 -0.3955993   0.5143506\n",
            " -0.3816655   0.64194554 -0.6090813  -0.802331   -0.4934803  -0.11170749\n",
            " -0.35585213]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 20 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.17336343 -0.29429317 -0.3283228  -0.14896923 -0.11764471 -0.5699292\n",
            " -0.9638117   0.38995746  0.05088359 -0.5001295  -0.48588735  0.5205297\n",
            " -0.29241437  0.4387296  -0.3793221  -0.73729247 -0.35922638  0.03300524\n",
            " -0.19710238]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 21 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.17238794 -0.04438275 -0.15154503 -0.16570152 -0.03510888 -0.5341656\n",
            " -0.6480193   0.30942407  0.20572326 -0.42796198 -0.3829292   0.20862128\n",
            " -0.18603341  0.50590444 -0.14981426 -0.6050385  -0.54048884 -0.06456564\n",
            " -0.1343331 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 22 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.22788303 -0.03403236 -0.16682835 -0.1584919  -0.05304205 -0.50522405\n",
            " -0.5745933   0.25057143  0.18634848 -0.40695506 -0.2952192   0.16273738\n",
            " -0.16980708  0.4518998  -0.21873768 -0.45859167 -0.5495594   0.00685444\n",
            " -0.08664495]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 23 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.15839973  0.02674296 -0.18997642 -0.10958313 -0.08662847 -0.44820547\n",
            " -0.5166431   0.28009132  0.20873453 -0.3960158  -0.30156547  0.1739729\n",
            " -0.09493782  0.51386696 -0.2287187  -0.38925117 -0.524631    0.02928916\n",
            " -0.07637635]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 24 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.11245529  0.00554121 -0.18713447 -0.21164247 -0.09784225 -0.50639385\n",
            " -0.67883486  0.31742975  0.13926913 -0.36731574 -0.3464069   0.15839905\n",
            " -0.05497605  0.5737068  -0.08978119 -0.4475301  -0.5230499   0.06181629\n",
            " -0.09206977]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 25 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.11638331 -0.048057   -0.15526463 -0.17089894  0.02146443 -0.5861135\n",
            " -0.6805577   0.29062694  0.14873989 -0.332321   -0.29214066  0.12053116\n",
            " -0.14339721  0.3985166  -0.12591948 -0.44799992 -0.5458827   0.09898125\n",
            " -0.17086336]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 26 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.13651761  0.01531947 -0.22845718 -0.13307104 -0.08301985 -0.51872206\n",
            " -0.64197415  0.29195637  0.21502338 -0.37392837 -0.3561913   0.1527986\n",
            " -0.06966785  0.5514     -0.11540528 -0.33212575 -0.45916244  0.08046313\n",
            " -0.15636958]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 27 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.17045625  0.01321295 -0.249803   -0.08213432 -0.12300213 -0.48649782\n",
            " -0.6039134   0.27074707  0.19037694 -0.36367878 -0.3228972   0.12317842\n",
            " -0.09253366  0.5365491  -0.19342205 -0.24235313 -0.44990394  0.1377264\n",
            " -0.09563436]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 28 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.12970683  0.0489272  -0.17002304 -0.07017238 -0.10405986 -0.46747085\n",
            " -0.55523455  0.28579843  0.1567348  -0.310476   -0.27700314  0.15185522\n",
            " -0.0849542   0.54698414 -0.1930955  -0.23234552 -0.43262085  0.22064428\n",
            " -0.09244891]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 29 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.16267249 -0.01034724 -0.12122528 -0.17015705 -0.02887318 -0.55487335\n",
            " -0.6474444   0.27645403  0.15498675 -0.32561523 -0.30316177  0.18303053\n",
            " -0.13175085  0.5634708  -0.10289389 -0.30140278 -0.46127498  0.29361382\n",
            " -0.16504051]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 30 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.15293442  0.03124318 -0.1829654  -0.10833026 -0.08236139 -0.44605333\n",
            " -0.50904554  0.28151593  0.19756843 -0.38332078 -0.29308194  0.19411749\n",
            " -0.08395033  0.6369163  -0.22809413 -0.19174412 -0.51481825  0.34040034\n",
            " -0.07926393]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 31 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.12951112  0.04966139 -0.16886234 -0.07063038 -0.10268832 -0.4692145\n",
            " -0.5563721   0.2878895   0.15372086 -0.3072387  -0.27585396  0.15821365\n",
            " -0.08139125  0.58622724 -0.19351785 -0.17636156 -0.4310823   0.37403372\n",
            " -0.0936897 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 32 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.1967578  -0.01271526 -0.1415741  -0.11963976 -0.06848536 -0.5268236\n",
            " -0.6135239   0.25657636  0.13085765 -0.31250557 -0.27147916  0.15599616\n",
            " -0.1545784   0.5664972  -0.18085529 -0.17974946 -0.45226708  0.4605796\n",
            " -0.10587577]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 33 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.23401266 -0.07219224 -0.1900594  -0.25137278 -0.08046363 -0.5126569\n",
            " -0.6518637   0.26228943  0.12358392 -0.4283583  -0.32225934  0.18276142\n",
            " -0.13962922  0.7135772  -0.18967792 -0.20476577 -0.6163519   0.581972\n",
            " -0.03410127]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 34 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.23435014 -0.07199416 -0.18955998 -0.25331855 -0.08011968 -0.5156764\n",
            " -0.6543791   0.2635953   0.12295447 -0.42795876 -0.32302073  0.18511853\n",
            " -0.13898146  0.72571325 -0.19023871 -0.19041488 -0.6181767   0.6447378\n",
            " -0.03526626]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16\n",
            "\n",
            "Step 35 reward=2 new_state=[0 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.16870451 -0.05245855 -0.16705212 -0.2601489  -0.12624663 -0.53749233\n",
            " -0.6528917   0.30148545  0.16822252 -0.44382167 -0.37530762  0.18056066\n",
            " -0.1384702   0.9300656  -0.25860447 -0.180815   -0.76071763  0.51775664\n",
            " -0.0015687 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.14777459 -0.08743655 -0.31824562 -0.22808844 -0.1383672  -0.64155686\n",
            " -0.82234997  0.37194228  0.11715981 -0.64805686 -0.29531175  0.22479995\n",
            " -0.07930419  1.0423902  -0.24204901 -0.11518513 -0.66404396  0.5171621\n",
            " -0.17062627]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 37 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.17433168  0.01386005 -0.25172165 -0.08668953 -0.11947815 -0.49881575\n",
            " -0.6166474   0.28126526  0.18446298 -0.358101   -0.32323676  0.14330313\n",
            " -0.08305075  0.6612361  -0.19933663 -0.0919432  -0.41098058  0.6839551\n",
            " -0.09741928]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 38 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.17515066  0.01395601 -0.25254098 -0.08752174 -0.11931633 -0.5012791\n",
            " -0.6196807   0.2830585   0.18453448 -0.3585663  -0.3241842   0.14506939\n",
            " -0.08276159  0.67058057 -0.20038325 -0.08332549 -0.39552748  0.739155\n",
            " -0.09774636]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 39 reward=3 new_state=[0 0 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.15342708  0.03602596 -0.18054673 -0.11746304 -0.07638425 -0.46615335\n",
            " -0.5227092   0.29779923  0.19486456 -0.38061336 -0.29433733  0.23320751\n",
            " -0.07996839  0.7326332  -0.23712786 -0.0786822  -0.44649604  0.8508958\n",
            " -0.08808344]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 40 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.14776742  0.06759791 -0.24503534 -0.16630897 -0.06797606 -0.49957252\n",
            " -0.53220665  0.34403843  0.21852016 -0.38009587 -0.30501035  0.23582669\n",
            " -0.08545405  0.7863913  -0.21037568 -0.11779698 -0.48692834  0.9161156\n",
            " -0.08581101]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 41 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10338678  0.10525967 -0.1814172  -0.05919335 -0.07280502 -0.42560586\n",
            " -0.46480054  0.330336    0.14724855 -0.2702298  -0.28486538  0.17309904\n",
            " -0.06311955  0.5835263  -0.1035169  -0.07189755 -0.34602144  0.7341681\n",
            " -0.10688481]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 42 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.06762927  0.10569978 -0.15723169 -0.09726192 -0.0279965  -0.4585995\n",
            " -0.50385594  0.35685202  0.165601   -0.2857833  -0.31996134  0.24395959\n",
            " -0.03551893  0.63109833 -0.02058097 -0.14197306 -0.34896788  0.7990906\n",
            " -0.17440131]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 43 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.04971389  0.01202449 -0.23293737 -0.2084087   0.01415449 -0.4826458\n",
            " -0.5738186   0.38075498  0.13855965 -0.40451193 -0.35589406  0.25855532\n",
            " -0.0206742   0.67323613 -0.06105277 -0.21490763 -0.57574517  0.97600526\n",
            " -0.11098117]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 44 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.18489294 -0.11854766 -0.32837817 -0.2284636  -0.03584716 -0.65454966\n",
            " -0.54906225  0.43542343  0.09586088 -0.5787534  -0.4401607   0.32710642\n",
            " -0.040176    0.94390404 -0.13331488 -0.29311222 -0.6562144   0.89139426\n",
            " -0.22679591]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 45 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.2033165  -0.2429277  -0.5492124  -0.12909463  0.00367736 -0.8548123\n",
            " -0.85110635  0.5259258   0.0502657  -0.6537318  -0.4332245   0.422485\n",
            " -0.28730458  0.7567832  -0.21982704 -0.2604679  -0.7509088   0.6977555\n",
            " -0.27951768]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 46 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.12955005 -0.17659797 -0.54227245 -0.01718329 -0.12455579 -0.7749399\n",
            " -1.0214485   0.54959494  0.01039502 -0.610471   -0.25901458  0.5306195\n",
            " -0.2379618   0.77746236 -0.31005418 -0.11449084 -0.6425803   0.55797577\n",
            " -0.45917773]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 47 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.03811364 -0.01192442 -0.18440777 -0.04636328 -0.05547979 -0.78749615\n",
            " -0.8558802   0.37942764  0.19522852 -0.41364446 -0.41140366  0.4702359\n",
            " -0.24919353  0.9333291  -0.11930568 -0.22999196 -0.5119292   1.062075\n",
            " -0.24756333]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 48 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.31534985 -0.16238688 -0.60455537 -0.07475416 -0.20887479 -0.8039464\n",
            " -0.85300785  0.51711047  0.05916138 -0.62268376 -0.52563864  0.7050239\n",
            " -0.12482592  1.2326509  -0.35289896 -0.302673   -0.524534    1.1553818\n",
            " -0.3454976 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 49 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.37916118 -0.37750703 -0.59902364 -0.13885517  0.04037607 -1.0602624\n",
            " -1.1216314   0.5023028   0.02169567 -0.90692556 -0.49552113  0.6006223\n",
            " -0.39267963  0.82257223 -0.24675375 -0.26532948 -0.870793    0.9589522\n",
            " -0.36543676]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 50 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.20638876 -0.02084439 -0.30623883 -0.02840106  0.04663045 -0.70230263\n",
            " -0.95843005  0.48126978  0.13323244 -0.5508424  -0.25278977  0.76326704\n",
            " -0.35901132  0.709381   -0.21983528 -0.09419814 -0.496034    1.0148425\n",
            " -0.24949276]\n",
            "Epsilon reduced to 0.08192000000000003\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.27088857 -0.36567602 -0.7234641  -0.5586215  -0.21355663 -1.077194\n",
            " -1.3172736   0.55020946 -0.31694892 -1.1497608  -0.30019832  1.0418035\n",
            " -0.4613006   1.5562438  -0.89477235 -0.5913175  -0.51905835  1.5177995\n",
            " -0.638059  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.47216138 -0.87459815 -0.83748275 -0.514361   -0.25902382 -0.7088536\n",
            " -1.3266653   0.68378896 -0.33796984 -1.0509452  -0.71014786  0.95622194\n",
            " -0.7079505   1.2907594  -0.433758   -0.8625635  -0.6979614   1.1888144\n",
            " -0.641587  ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4677322  -0.80643183 -1.002492   -0.5606034  -0.2558801  -1.2720155\n",
            " -1.6869271   0.9953693  -0.4565931  -1.5685369  -0.58032006  1.3550788\n",
            " -0.7507964   2.0948877  -0.84722555 -0.9089314  -0.98305064  1.5708627\n",
            " -0.99765724]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.553223   -0.53685653 -0.76202935 -0.48885027 -0.2626048  -1.2467642\n",
            " -1.4346383   0.7073486  -0.2378171  -1.1269782  -0.5047556   1.1732005\n",
            " -0.7398128   1.677009   -0.5590278  -0.877831   -0.5379673   1.0192537\n",
            " -0.60854316]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1791977  -0.64904773 -0.93021166 -0.39136493 -0.17345056 -1.1219567\n",
            " -1.3117805   0.80140054 -0.27907878 -1.2267187  -0.45939496  1.1996906\n",
            " -0.65515244  1.5136529  -0.95831597 -0.49508035 -0.6290217   1.4918593\n",
            " -0.8258797 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 6 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.44328728 -0.34619033 -0.96877694 -0.44946948 -0.19121774 -1.0751985\n",
            " -1.4514637   0.810259   -0.34130237 -1.3024248  -0.45764774  1.3555806\n",
            " -0.84111845  1.6488615  -0.9834524  -0.6071588  -0.5126708   1.6564307\n",
            " -0.70505804]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 7 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.46903214 -0.3645944  -0.74364007 -0.17144114 -0.25191057 -0.72043633\n",
            " -1.3863142   0.5594379  -0.19681628 -0.9537241  -0.36396354  1.6070817\n",
            " -0.66880924  1.3179759  -0.85275435 -0.5207745  -0.38111985  1.2316877\n",
            " -0.35619432]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 8 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.3544936  -0.13440824 -0.6258818  -0.0935964   0.02609346 -0.84514767\n",
            " -1.2256848   0.5882642   0.12173357 -0.73262584 -0.25613326  1.5761021\n",
            " -0.4256887   1.5356623  -0.50754553 -0.36327428 -0.46434206  1.064771\n",
            " -0.23956181]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 9 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.43137398 -0.45321542 -0.53880274 -0.15397337 -0.27330142 -0.6672216\n",
            " -1.3417883   0.626571   -0.31212607 -0.8117126  -0.34775203  1.6901779\n",
            " -0.5570209   1.276841   -0.6486768  -0.4559804  -0.2423813   1.3432401\n",
            " -0.39609963]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 10 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.27002788 -0.03770874 -0.38744673 -0.08338368 -0.04411478 -0.6955285\n",
            " -0.94185126  0.49343824 -0.04361467 -0.5490206  -0.15128112  1.1646533\n",
            " -0.31806383  0.95805585 -0.40576333 -0.28252044 -0.35149375  1.2956269\n",
            " -0.27679262]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 11 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.29289192 -0.3855736  -0.5596037  -0.38713768  0.043323   -0.8947116\n",
            " -1.1185244   0.52099365 -0.10009446 -0.8074166  -0.40831816  1.3116872\n",
            " -0.35027185  1.2324945  -0.31847546 -0.2767468  -0.6128789   1.4570351\n",
            " -0.37633425]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 12 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.35488275 -0.21811016 -0.3886437  -0.2951065  -0.08026681 -0.7397966\n",
            " -1.0966405   0.4689067  -0.02617302 -0.6490345  -0.16127749  1.4454509\n",
            " -0.32396653  1.2398462  -0.4092629  -0.21296668 -0.55059975  1.6513244\n",
            " -0.23896632]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 13 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.21964565 -0.2805427  -0.54262304 -0.24761032 -0.01057757 -0.7968957\n",
            " -0.9890117   0.5726241  -0.13076136 -0.74351615 -0.30667818  1.2901033\n",
            " -0.30170223  1.369608   -0.43492228 -0.21258189 -0.6020989   1.4539539\n",
            " -0.3055615 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 14 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.19992687 -0.12836196 -0.342358    0.05468582 -0.10194141 -0.76452446\n",
            " -1.005621    0.45303252  0.10868814 -0.60788965 -0.01168883  1.4451917\n",
            " -0.295227    1.1449524  -0.37769735 -0.12421359 -0.3575385   1.5041234\n",
            " -0.30746445]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 15 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.0286064e-01 -2.3170212e-01 -5.1122969e-01 -1.3048317e-01\n",
            "  8.3849428e-04 -8.2731616e-01 -9.7702563e-01  5.7720488e-01\n",
            " -1.1517003e-01 -6.4469278e-01 -2.4507764e-01  1.3035917e+00\n",
            " -3.3943972e-01  1.4635392e+00 -4.3013650e-01 -1.8840033e-01\n",
            " -4.7790536e-01  1.3767899e+00 -3.9299163e-01]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 16 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-2.2266069e-01 -9.9989451e-02 -4.0402451e-01 -6.8198800e-02\n",
            " -8.3987445e-02 -7.3939717e-01 -1.0154715e+00  5.2915996e-01\n",
            "  5.5163167e-04 -5.5771518e-01 -1.1980136e-02  1.4611888e+00\n",
            " -3.1382713e-01  1.5183178e+00 -4.6316814e-01 -2.5315920e-01\n",
            " -4.1760778e-01  1.6950136e+00 -2.9676279e-01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 17 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5244366  -0.28153068 -0.53888816 -0.10906919 -0.21418932 -0.70824087\n",
            " -1.1556      0.4941224  -0.20979236 -0.742846   -0.102786    1.792882\n",
            " -0.51963747  1.6661053  -0.4663793  -0.39151523 -0.37613866  1.6226548\n",
            " -0.31864876]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 18 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2054932  -0.16171257 -0.36556348 -0.04601019 -0.1119117  -0.79897594\n",
            " -1.057434    0.4781374   0.07670125 -0.6017609   0.02001948  1.6131984\n",
            " -0.33691755  1.676556   -0.40177345 -0.26185784 -0.45369548  1.7634463\n",
            " -0.27541038]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 19 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.28654838 -0.30610356 -0.65266186 -0.17003556 -0.02466159 -0.8965389\n",
            " -1.094912    0.60851014 -0.08283991 -0.7559827  -0.2635047   1.6636744\n",
            " -0.37881976  2.1021137  -0.4749499  -0.2089126  -0.5330366   1.6264961\n",
            " -0.4228361 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 20 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2099944  -0.12476592 -0.41834992 -0.16588308 -0.03477317 -0.83241487\n",
            " -1.1297264   0.60062706  0.02736678 -0.6402784  -0.01339607  1.8438921\n",
            " -0.31647715  2.1647477  -0.40479982 -0.37990016 -0.48388556  2.0522096\n",
            " -0.4019247 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 21 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.53837067 -0.34491366 -0.42496789 -0.25964862 -0.10871558 -0.8723344\n",
            " -1.2981914   0.5430893  -0.24934272 -0.78495544 -0.07039982  2.2350438\n",
            " -0.6035313   2.3334198  -0.38252386 -0.53204477 -0.42951724  2.141012\n",
            " -0.45744127]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 22 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.35073552 -0.26600552 -0.47896656 -0.28329185 -0.0956215  -0.9156675\n",
            " -1.3026167   0.5739459  -0.08038945 -0.698027   -0.01220408  2.009614\n",
            " -0.42893532  2.5131478  -0.47947657 -0.35841662 -0.5623395   2.3567739\n",
            " -0.30874568]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 23 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.3659682  -0.45241785 -0.6181364  -0.38138744  0.03317616 -1.0261724\n",
            " -1.2392907   0.6638079  -0.165981   -0.9058709  -0.24978603  2.2757983\n",
            " -0.4657848   2.9309773  -0.5168067  -0.28925896 -0.7129407   2.2509744\n",
            " -0.42120695]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 24 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.26699644 -0.23589306 -0.44712842 -0.10079883 -0.07275018 -0.8783633\n",
            " -1.1018974   0.56107014  0.13045762 -0.77362543  0.0730968   2.3578176\n",
            " -0.3969409   2.6728508  -0.51526845 -0.30525634 -0.6079271   2.3502717\n",
            " -0.3217348 ]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 25 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.29899213 -0.3469183  -0.63928884 -0.21936908  0.04866585 -0.96988523\n",
            " -1.0954535   0.7188141  -0.07972588 -0.88802755 -0.18672435  2.614939\n",
            " -0.426373    3.1227946  -0.5857021  -0.26088423 -0.6689318   2.1940093\n",
            " -0.48188555]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 26 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.2979356  -0.16824651 -0.5243959  -0.13303378 -0.08163161 -0.8911234\n",
            " -1.1981398   0.650394    0.01307233 -0.7083558   0.07147059  2.6675467\n",
            " -0.41251805  2.9849315  -0.5708576  -0.35322684 -0.54950076  2.4675264\n",
            " -0.38772824]\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 27 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.62359285 -0.35903412 -0.67465293 -0.1833201  -0.22257835 -0.86732084\n",
            " -1.3556963   0.62151647 -0.21591298 -0.9056303  -0.02821953  3.2916646\n",
            " -0.6376358   3.2095535  -0.5772683  -0.50266623 -0.51601744  2.531363\n",
            " -0.41868106]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 28 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5194973  -0.28796008 -0.7443688  -0.34762338 -0.26941    -0.9507467\n",
            " -1.4330491   0.6445113  -0.0266819  -0.9158552   0.00379219  3.3586648\n",
            " -0.57141954  3.3857374  -0.5178015  -0.4910044  -0.6180919   2.7018597\n",
            " -0.36124283]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 29 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6463949  -0.37693155 -0.70598704 -0.20201102 -0.22506253 -0.90420073\n",
            " -1.4022514   0.65067106 -0.21831807 -0.944339   -0.01722711  3.7834864\n",
            " -0.66666377  3.4841793  -0.6017586  -0.5278694  -0.5475223   2.757676\n",
            " -0.44211528]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 30 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.3342865  -0.20216534 -0.58476824 -0.16825926 -0.0827402  -0.9653855\n",
            " -1.2888415   0.7095843   0.01654561 -0.78701276  0.09563169  3.560569\n",
            " -0.46468645  3.5187974  -0.621172   -0.40159062 -0.6121202   2.8991802\n",
            " -0.43353122]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 31 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.30191755 -0.38275015 -0.6927192  -0.353055    0.05741337 -1.1403093\n",
            " -1.3645273   0.829241   -0.10929161 -0.9819385  -0.21178773  4.0494823\n",
            " -0.4700702   3.902204   -0.49706116 -0.43828788 -0.7047569   2.843625\n",
            " -0.63564837]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 32 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.39210913 -0.30489197 -0.55325145 -0.34288326 -0.00999111 -1.1464614\n",
            " -1.4790936   0.7292924   0.01525993 -0.8812271   0.05202944  4.1753035\n",
            " -0.5433369   3.8663723  -0.5249281  -0.5248139  -0.6650374   3.4756887\n",
            " -0.56850064]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 33 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.70975304 -0.4650027  -0.6405711  -0.30767316 -0.17263632 -1.0774319\n",
            " -1.5445666   0.6960331  -0.2928214  -0.999678    0.03108414  4.7012076\n",
            " -0.8121737   3.977608   -0.63066494 -0.59040135 -0.60653365  3.7212882\n",
            " -0.5368173 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 34 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6190368  -0.43648058 -0.7248523  -0.4959264  -0.17598793 -1.1644953\n",
            " -1.5634153   0.7398929  -0.0539342  -1.1032791   0.062454    4.8349504\n",
            " -0.74487484  4.281628   -0.6347173  -0.5664669  -0.79131395  4.1657515\n",
            " -0.4807203 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 35 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-6.8136483e-01 -5.0017369e-01 -7.7132630e-01 -4.0072009e-01\n",
            " -2.0157744e-01 -1.0327560e+00 -1.5825820e+00  8.1815618e-01\n",
            " -2.8954631e-01 -1.1854837e+00 -5.7194941e-04  5.2235556e+00\n",
            " -7.4293411e-01  4.4982104e+00 -7.1344638e-01 -6.6455412e-01\n",
            " -8.1159592e-01  4.4883332e+00 -4.7112158e-01]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 36 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5474746  -0.34544352 -0.7963044  -0.43937266 -0.25603268 -1.1281548\n",
            " -1.6244073   0.8019926  -0.0709733  -1.0787721   0.06830207  5.0377345\n",
            " -0.70968354  4.381631   -0.6204599  -0.61766386 -0.7659095   4.584255\n",
            " -0.48618835]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 37 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6837943  -0.43936902 -0.7647558  -0.29185885 -0.21027263 -1.0829527\n",
            " -1.5963348   0.81970704 -0.2713799  -1.1093321   0.0432462   5.449148\n",
            " -0.81181806  4.467606   -0.710594   -0.6628095  -0.6945458   4.8994284\n",
            " -0.5728506 ]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 38 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5886356  -0.31323597 -0.9170566  -0.30858707 -0.07310267 -1.1681889\n",
            " -1.5812926   0.8667177   0.01464737 -1.109879   -0.01292525  5.271825\n",
            " -0.6605635   4.604232   -0.75334394 -0.59786934 -0.7694265   5.1706176\n",
            " -0.57415515]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 39 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.73773605 -0.51107365 -0.8925795  -0.42173842 -0.20389709 -1.1908025\n",
            " -1.7804307   0.87370145 -0.20683017 -1.2594349  -0.04973901  6.0318565\n",
            " -0.8236607   4.843921   -0.63829803 -0.7942065  -0.7658939   5.7959\n",
            " -0.67924607]\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 40 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6163204  -0.3848973  -0.93435425 -0.33557895 -0.15461856 -1.180302\n",
            " -1.6714061   0.877643    0.00674251 -1.1854019   0.02464831  5.731992\n",
            " -0.7505494   4.7847915  -0.6701327  -0.6060894  -0.8163723   5.8513904\n",
            " -0.5315767 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 41 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8196673  -0.5898681  -0.8183361  -0.42744875 -0.19569089 -1.2939384\n",
            " -1.818384    0.86289924 -0.31554404 -1.2421404   0.03190269  6.268174\n",
            " -0.97818375  5.117918   -0.76260436 -0.7281961  -0.78237784  6.465187\n",
            " -0.6731483 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 42 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6339653  -0.49103713 -0.8510111  -0.7090859  -0.02189071 -1.4379908\n",
            " -1.9634829   0.8930644  -0.14572759 -1.3478116  -0.03867143  6.3867273\n",
            " -0.80867857  5.450422   -0.5479766  -0.8024805  -0.917383    7.0470567\n",
            " -0.6442858 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 43 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8573741  -0.68196297 -0.9131826  -0.5850333  -0.24842288 -1.3635111\n",
            " -2.0266783   0.9299699  -0.39282835 -1.3753135  -0.02269973  6.6716986\n",
            " -0.9998707   5.8583403  -0.7736489  -0.8208834  -0.9255186   7.3414364\n",
            " -0.64868873]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 44 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6780031  -0.5257855  -1.0013161  -0.58415383 -0.23476337 -1.3488507\n",
            " -1.8223392   0.99570996 -0.04474554 -1.4254955   0.06213877  6.6226416\n",
            " -0.8595753   6.2212358  -0.8226978  -0.7558412  -1.0232702   7.4679027\n",
            " -0.62980294]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 45 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.7947337  -0.57554716 -0.95734346 -0.417134   -0.23256987 -1.3088182\n",
            " -1.8733408   0.9968802  -0.29478735 -1.3687943   0.03200915  6.8754196\n",
            " -0.97296226  6.465687   -0.85183674 -0.8148106  -0.88334036  7.6772704\n",
            " -0.7184162 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5132201  -0.53568643 -1.0146048  -0.5298283  -0.01019303 -1.4374655\n",
            " -1.5983827   0.98661697 -0.07062016 -1.3909258   0.09282026  6.3527374\n",
            " -0.770629    6.580108   -0.93302125 -0.7729301  -1.0405233   7.3627644\n",
            " -0.7940158 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.61941725 -0.48359534 -1.1059471  -0.40322933  0.12985647 -1.4383638\n",
            " -1.7494824   1.0515312  -0.0860481  -1.12136    -0.12551638  6.4589095\n",
            " -0.80719113  6.748446   -0.76093614 -0.6505531  -0.97940904  7.1840973\n",
            " -0.9472663 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.49557948 -0.6993654  -0.8910325  -0.41503552  0.07229154 -1.4963723\n",
            " -1.8010688   0.9615648   0.01942137 -0.9530855  -0.07901081  6.2464204\n",
            " -0.919575    6.537263   -0.6169949  -0.49939492 -1.041042    7.1085706\n",
            " -0.79107237]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.34157607 -0.6068308  -0.89508367 -0.46676403 -0.09484605 -1.3128934\n",
            " -1.6840794   0.93582296 -0.01966839 -0.85742354 -0.14474227  5.850532\n",
            " -0.65540576  6.879082   -0.4953375  -0.65507627 -1.1601951   7.2196445\n",
            " -0.6330899 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5291595  -0.82860446 -1.0117735  -0.5313844   0.04654297 -1.6116881\n",
            " -1.9865148   0.9882222   0.16967197 -0.79073465 -0.21652097  6.3817544\n",
            " -0.9841969   7.361322   -0.56098616 -0.5790753  -1.2887912   7.435774\n",
            " -0.6765293 ]\n",
            "Epsilon reduced to 0.06553600000000002\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.2676096  -1.0351379  -1.6883279  -1.1542696  -0.48551    -2.2024262\n",
            " -2.398566    1.380749   -0.46535456 -1.1825895  -0.12300056  7.9335437\n",
            " -1.3303826   9.347235   -1.1074259  -1.5719223  -1.6013811  10.343013\n",
            " -1.0069008 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 2 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.0295683  -1.0743331  -1.4732829  -0.90286607 -0.2636055  -2.0252821\n",
            " -2.312962    1.374319   -0.3727458  -0.8958994  -0.34638733  8.053295\n",
            " -1.3482105   8.995017   -1.3439356  -1.2696135  -1.6744721   9.999313\n",
            " -1.1330303 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 3 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.95376146 -0.5960492  -1.4192861  -0.7800717  -0.7033622  -1.5961353\n",
            " -1.9404091   1.3113787  -0.11332139 -0.5683391  -0.24069045  8.027014\n",
            " -1.277708    8.637849   -0.9913218  -1.1083735  -1.3143516   9.852088\n",
            " -0.7772492 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 4 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5070392  -0.6377983  -1.1546837  -0.56960285 -0.02665634 -1.6124223\n",
            " -2.0935526   1.2668291   0.02470722 -0.1410537  -0.30764955  7.4120526\n",
            " -1.0493385   7.537914   -0.86742604 -0.25786233 -1.456442    8.369916\n",
            " -0.8082971 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 5 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.52956617 -0.6607828  -1.3794516  -0.27954483 -0.17450179 -1.5018806\n",
            " -2.1732676   1.4279623  -0.31022406  0.03565271 -0.22756292  8.406794\n",
            " -1.2217171   8.572911   -0.8890905  -0.18293722 -1.3453614  10.03507\n",
            " -0.47198564]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 6 reward=1 new_state=[0 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.51758987 -0.7313126  -1.2480557  -0.26541966 -0.17600681 -1.577574\n",
            " -2.2849796   1.2820363  -0.2686158   0.2603884  -0.22078249  8.192252\n",
            " -1.1364739   8.067063   -1.0314311   0.30782795 -1.4941127   9.584991\n",
            " -0.5825311 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3\n",
            "\n",
            "Step 7 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2897697  -0.5527929  -1.3942649  -0.38161448 -0.02282603 -1.416884\n",
            " -1.7451967   1.4481157   0.02588853  0.18187374 -0.17016867  8.018604\n",
            " -1.060252    8.195838   -0.957682    0.79351354 -1.4172572   9.611371\n",
            " -0.489226  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 8 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.68943894 -0.58504975 -0.8509889  -0.22456284  0.04803571 -1.5243412\n",
            " -1.8154017   0.9307064   0.14057726  0.06502911 -0.31635624  6.7350483\n",
            " -0.8936457   6.995585   -0.65528077  1.2067639  -1.2131609   8.147015\n",
            " -0.41323203]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 9 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7423968  -0.8820601  -1.5061631  -0.21967018 -0.14296107 -1.8250087\n",
            " -2.4495425   1.3727224   0.14538378  0.3856466  -0.33734784  8.390958\n",
            " -1.1739414   8.376984   -1.0364051   1.7578123  -1.7574538   9.485032\n",
            " -1.0436653 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 10 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-4.4156086e-01 -1.0653462e+00 -1.8288561e+00  1.8002260e-02\n",
            " -1.2483032e-01 -2.1130018e+00 -2.5356562e+00  1.6084630e+00\n",
            " -6.8005025e-01  8.6236671e-03 -5.5984670e-01  8.7964172e+00\n",
            " -1.4103293e+00  8.8121519e+00 -9.3252707e-01  2.4928493e+00\n",
            " -1.9974889e+00  1.0533185e+01 -7.2367448e-01]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 11 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.43003675 -0.8437916  -1.4741228   0.5140507   0.03097386 -2.00073\n",
            " -2.2349584   1.7257079   0.03693471  0.26028252 -0.41030577  7.8005624\n",
            " -1.2911038   7.9815416  -0.7517471   2.3436534  -1.9627743   9.815987\n",
            " -0.59612364]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 12 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6804734  -0.51243114 -0.891456    0.43846697 -0.11463316 -1.5183489\n",
            " -1.9265274   1.0120553   0.03171887  0.1829654  -0.16484712  7.3829274\n",
            " -1.0135332   7.3473086  -0.5155283   2.606566   -1.3674693   8.34936\n",
            " -0.8067214 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 13 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5382502  -0.51079094 -1.1497827   0.40779608  0.04885393 -1.633128\n",
            " -2.1130044   1.3481994  -0.02459521  0.47302613 -0.23254962  8.133273\n",
            " -1.1369423   7.7654357  -0.8349511   3.2884736  -1.4964699   9.286426\n",
            " -0.8273304 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 14 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.0268363  -0.65670705 -1.6715169   0.6075258  -0.49551657 -1.7845598\n",
            " -2.120209    1.4340581  -0.33027396  0.16993853 -0.4721379   8.939924\n",
            " -1.2366265   8.82859    -0.86457556  3.4833498  -1.5721234  10.233494\n",
            " -0.886769  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 15 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5292559  -1.0388803  -1.4481467   0.9728089  -0.176101   -1.8520173\n",
            " -2.0099516   1.3899438  -0.30183262  0.2011881  -0.39037016  6.913532\n",
            " -1.0623835   7.218417   -0.90316516  4.0367045  -1.8279097   8.965531\n",
            " -0.9228637 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 16 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7399406  -0.46155402 -1.3958176   1.0005498  -0.37615412 -1.6892505\n",
            " -2.080711    1.5359991   0.17517556  0.35992432 -0.347934    8.54095\n",
            " -1.1465276   8.544967   -0.9084574   4.4571037  -1.6744043   9.9091215\n",
            " -0.86058116]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 17 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.31485513 -0.686809   -1.4505897   1.2252482   0.12180685 -1.7497182\n",
            " -2.069056    1.6385925  -0.27804837  0.66501206 -0.3491542   9.347671\n",
            " -1.241884    8.784858   -0.9544519   5.590061   -1.4498835  10.735722\n",
            " -0.693915  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 18 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6994386  -0.44302154 -1.3288428   1.2498338  -0.21147835 -1.5943769\n",
            " -2.0828834   1.329631   -0.23181006  0.6787158  -0.06818842  8.351815\n",
            " -1.0968109   8.6536     -0.9879509   5.0308614  -1.3180642  10.102974\n",
            " -0.5684471 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 19 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.042866   -1.1981938  -1.9949285   1.1322968  -0.56346226 -2.3378718\n",
            " -2.3496478   1.6240174  -0.2871171   0.03100686 -0.22632778  9.371789\n",
            " -1.389521    9.43051    -1.3075078   6.6328216  -2.2320642  11.630617\n",
            " -1.2875468 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 20 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6976108  -0.8091592  -1.4275894   1.4065031  -0.01958784 -1.644015\n",
            " -2.2276783   1.5272255  -0.4181056   0.53924596 -0.68747896  9.001508\n",
            " -1.2243478   8.548174   -1.0200413   6.5256248  -1.8637925  10.158033\n",
            " -0.8670364 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 21 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5216753  -0.37661391 -0.9289824   1.172325   -0.3711318  -1.4109162\n",
            " -1.8290247   1.2961872   0.08553052  0.65779835 -0.3136307   8.020051\n",
            " -0.8957307   7.6580257  -0.67114276  5.9645915  -1.2991571  10.086306\n",
            " -0.6281925 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17\n",
            "\n",
            "Step 22 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.26187953 -0.91208416 -1.6943369   1.7409282  -0.1052739  -1.9606346\n",
            " -2.4179685   1.7897215   0.1980857   0.65143764 -0.3471125   9.297232\n",
            " -1.3288941   8.723356   -0.8533732   7.634662   -1.9998109  10.621072\n",
            " -0.55555934]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 23 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6569545  -0.8426597  -1.4853542   1.4565681  -0.5113166  -1.9949665\n",
            " -2.2688384   1.6045846  -0.0592179   0.38231787 -0.38554767  8.336601\n",
            " -0.9494345   8.377613   -1.0595369   7.458446   -1.768835   10.635586\n",
            " -1.16055   ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 24 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.68002933 -0.4255559  -1.2526367   1.481193   -0.55214983 -1.7220075\n",
            " -1.8296301   1.4929898   0.28545758  0.48954865 -0.34919634  8.756625\n",
            " -1.1240125   8.3121395  -0.79353374  7.3718624  -1.5339122  10.554251\n",
            " -0.7408521 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 25 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.64408654 -0.52028346 -1.078595    1.635358   -0.24094439 -1.6155113\n",
            " -2.2266846   1.385992    0.12695828  0.6494108  -0.06094307  8.945414\n",
            " -1.0090718   8.49304    -0.69298244  7.616114   -1.2539314  10.02669\n",
            " -0.956054  ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 26 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.68858075 -0.4610049  -1.3498365   1.7464893  -0.5884235  -1.896031\n",
            " -2.082463    1.6586676   0.2688694   0.50869316 -0.24602905  8.885454\n",
            " -1.1044364   8.649713   -0.8467098   8.176759   -1.6453298  10.980919\n",
            " -0.69018245]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 27 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.594696   -1.1158736  -2.0082893   2.3484147  -0.43358722 -2.18958\n",
            " -2.3362935   2.0000818  -0.15570949  0.47636864 -0.29645035  8.902114\n",
            " -1.0678478   8.923586   -1.2368221   9.646286   -2.0965252  11.179106\n",
            " -0.9236806 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 28 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6110227  -0.49353445 -1.0811627   1.538416   -0.37656248 -1.4599457\n",
            " -1.8714746   1.2406707   0.32100973  0.7053256  -0.4017704   8.833106\n",
            " -0.74215233  8.282708   -0.79659003  8.242883   -1.2387872  10.378321\n",
            " -0.7263854 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 29 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3636554  -0.68169576 -1.2592822   2.3574038   0.03205213 -1.799149\n",
            " -2.0727549   1.4787359   0.2425052   0.9394135  -0.46098584  9.432703\n",
            " -1.0159265   8.566541   -0.741875    9.790595   -1.4969074  11.107712\n",
            " -0.49688375]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 30 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.70215297 -0.79018164 -1.1795688   1.7005897  -0.2247922  -2.0009499\n",
            " -2.1786163   1.3724962   0.14868599  0.7208535  -0.36406252  8.786907\n",
            " -1.0148612   8.74502    -0.8646538   9.391018   -1.3939106  10.570591\n",
            " -0.8818882 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 31 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6644271  -1.2343211  -2.3078828   2.5997598  -0.11500122 -2.6470609\n",
            " -2.9532661   2.1822224  -0.2762728   0.7414148  -0.35964993 10.883485\n",
            " -1.5859035  10.374524   -1.2645481  12.376061   -2.2899542  12.496267\n",
            " -1.4334333 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 32 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.56373835 -0.64711267 -1.2886323   1.8608462  -0.22860739 -1.8001351\n",
            " -2.5438747   1.4349773   0.12493236  1.0590615  -0.48996133  9.51502\n",
            " -1.2031705   8.826474   -0.76537883  9.9005785  -1.730273   11.242868\n",
            " -0.5128863 ]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 33 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.5366774  -0.47275734 -1.0438731   1.9303156  -0.20018397 -1.5045962\n",
            " -1.7955476   1.3060683  -0.0597079   0.8812167  -0.02876291  8.164258\n",
            " -0.8830559   7.9054704  -0.7258072   9.003138   -1.194724    9.882047\n",
            " -0.63631463]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 34 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.7031127  -0.83975494 -1.3380593   1.9763098  -0.16900778 -1.9448067\n",
            " -2.056221    1.4316859  -0.08795039  0.57842666 -0.3011628   8.40859\n",
            " -1.2268329   8.657841   -1.022344   10.360512   -1.6598687  10.2684555\n",
            " -0.9497549 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 35 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.93980265 -1.1529211  -1.9544537   2.228145   -0.11292679 -2.3937907\n",
            " -2.669944    1.825145   -0.36515224  0.5132964  -0.16023675  9.705126\n",
            " -1.3801292   9.723986   -1.2524151  11.953045   -1.9569033  11.246626\n",
            " -1.5668007 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 36 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.56837314 -0.5241335  -1.0290731   1.9406964  -0.16024332 -1.5775982\n",
            " -1.9558185   1.3908693   0.05009279  0.8213867  -0.08957642  8.488213\n",
            " -1.0483173   8.382218   -0.82133424  9.700219   -1.2849724  10.147188\n",
            " -0.68564636]\n",
            "\n",
            "Taking action 15\n",
            "\n",
            "Step 37 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.60643804 -0.5693115  -0.8868036   1.9170568  -0.08931231 -1.4789436\n",
            " -1.7708156   1.1022187  -0.0519213   0.8415811  -0.22192214  8.208783\n",
            " -0.89092827  8.083693   -0.6058634   9.712724   -1.1967416   9.760831\n",
            " -0.7548004 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 38 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-6.94019973e-01 -9.97764468e-01 -1.52883029e+00  2.03048897e+00\n",
            " -2.48244569e-01 -2.03352642e+00 -2.06187057e+00  1.54631627e+00\n",
            "  8.90952442e-03  6.25953317e-01 -3.04566920e-01  8.84882164e+00\n",
            " -1.13932586e+00  9.54621124e+00 -1.17994809e+00  1.09567995e+01\n",
            " -1.68540251e+00  1.05167751e+01 -9.80634928e-01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 39 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.093875   -1.2917471  -2.155231    2.345408   -0.10717574 -2.459967\n",
            " -2.716998    1.9089849  -0.30566844  0.4591857  -0.22935344 10.261047\n",
            " -1.4262903  11.043549   -1.372477   13.191543   -2.071828   11.798746\n",
            " -1.6119801 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 40 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.71017563 -0.665907   -1.263978    2.0876534  -0.0521694  -1.8105232\n",
            " -2.0752017   1.4032048   0.14571351  0.7988987  -0.23797138  9.048656\n",
            " -0.97022545  9.586124   -0.9440525  10.719752   -1.3390021  10.543828\n",
            " -0.7162456 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 41 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.6034091  -0.5019582  -1.0070322   2.0973678  -0.14217435 -1.6777124\n",
            " -2.0185804   1.3908389   0.02218527  0.9301533  -0.13303764  8.683328\n",
            " -0.9328252   9.389832   -0.8120996  10.449863   -1.2257643  10.328605\n",
            " -0.7295448 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 42 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.6712575  -0.9778165  -1.3608403   2.10723    -0.2688164  -1.9695443\n",
            " -2.255978    1.3859868  -0.01244763  0.7300161  -0.378284    8.830019\n",
            " -1.310031   10.334127   -0.9443971  11.662955   -1.7005589  10.519584\n",
            " -0.9150118 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 43 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.9166333  -1.3246318  -2.0809445   2.551358   -0.18655011 -2.7323194\n",
            " -3.21711     1.9247155  -0.08636013  0.64438504 -0.47948244 10.937531\n",
            " -1.4473536  12.3074465  -1.3071188  14.114014   -2.3463693  12.315529\n",
            " -1.7473103 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 44 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.5645632  -0.6239061  -1.2909015   2.4206223  -0.19911313 -1.8947957\n",
            " -2.6332438   1.7838197   0.35734245  1.0055732  -0.39363757 10.072155\n",
            " -1.1731454  11.111761   -0.8678104  12.198591   -1.8185432  11.446519\n",
            " -0.77113   ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 45 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-5.9630507e-01 -5.6772768e-01 -1.1369618e+00  2.0779645e+00\n",
            " -1.8290918e-01 -1.5335501e+00 -1.8696656e+00  1.3321079e+00\n",
            "  6.3911900e-03  9.4640446e-01 -1.8418650e-01  8.8108568e+00\n",
            " -8.6970359e-01  1.0290707e+01 -7.8026515e-01  1.0708053e+01\n",
            " -1.1769341e+00  1.0367379e+01 -7.9782206e-01]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 46 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.7668988  -1.0742105  -1.4409084   2.2113163  -0.2320814  -2.1802227\n",
            " -2.2929256   1.4557055  -0.08787435  0.69188696 -0.38052255  9.0765505\n",
            " -1.3135164  11.717878   -1.052124   12.316308   -1.799683   10.82249\n",
            " -1.057465  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 47 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-5.4712588e-01 -1.3020768e+00 -2.3353465e+00  3.0465367e+00\n",
            " -1.7182754e-01 -2.6436803e+00 -2.9698153e+00  2.2142160e+00\n",
            "  5.9924359e-03  9.6998405e-01 -4.2170891e-01  1.1473259e+01\n",
            " -1.7160854e+00  1.4073675e+01 -1.2622175e+00  1.4847810e+01\n",
            " -2.4199879e+00  1.2753324e+01 -1.3747965e+00]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 48 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.57275516 -0.6672337  -1.0542802   2.361849   -0.15164635 -1.5746486\n",
            " -2.0794046   1.367516    0.02449861  1.0164474  -0.47730765  9.44891\n",
            " -0.7145531  11.460909   -0.6897427  11.778619   -1.3908911  11.043766\n",
            " -0.70758164]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 49 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.70931154 -0.62671477 -1.2912296   2.1339638  -0.22233233 -1.5552553\n",
            " -1.9511975   1.343396    0.04231692  0.936063   -0.24243341  9.054362\n",
            " -0.3426623  11.155472   -0.79983246 11.240814   -1.1849347  10.623385\n",
            " -0.8076898 ]\n",
            "\n",
            "Taking action 13\n",
            "\n",
            "Step 50 reward=2 new_state=[0 1 0 0 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.986783   -1.4392703  -2.2745051   2.6381695  -0.22479962 -2.842902\n",
            " -3.2206953   2.181167   -0.43967086  0.58379495 -0.24180847 10.58211\n",
            " -0.3797834  13.977798   -1.4704369  14.885794   -2.479024   12.550175\n",
            " -1.4741143 ]\n",
            "Epsilon reduced to 0.052428800000000025\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 1 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9500812  -1.7488791  -1.6694887   2.5646138  -0.2509301  -2.3504736\n",
            " -2.820073    1.9746114  -0.5002141   0.69208777 -1.0230991  11.396848\n",
            " -0.4659543  14.096347   -1.2187623  16.441244   -1.8918194  12.296662\n",
            " -2.0234358 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 2 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9272198  -1.4819738  -2.2726443   1.9708915  -0.33598033 -2.258795\n",
            " -2.658909    1.5416578  -0.60925543  0.94770116 -0.21388128  9.65935\n",
            "  0.04570494 12.582662   -1.4477774  13.180139   -2.060806   11.241594\n",
            " -1.4008349 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 3 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8961525  -1.5822825  -2.3683355   1.9916767  -0.22860974 -2.613218\n",
            " -3.1184006   1.7684042  -0.6190683   1.3160248  -0.47797382  9.706164\n",
            "  0.20421939 12.513604   -1.7519277  14.692902   -2.1433012  11.421347\n",
            " -1.8452337 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 4 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.93324846 -1.2145851  -1.6404235   1.8568914  -0.16519639 -1.7257103\n",
            " -2.3399158   1.3089333  -0.2406823   1.7906392  -0.29387796  9.363967\n",
            "  0.20713344 10.865491   -0.89485    11.648841   -1.559555   10.131205\n",
            " -1.0324786 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 5 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.78634065 -0.77795494 -1.0974766   2.0022712  -0.1458949  -1.4054993\n",
            " -1.9942117   1.0959526   0.01380212  2.432002   -0.25849667  8.196837\n",
            "  0.3925486   9.37642    -0.7187171  10.567285   -0.9409181   9.592655\n",
            " -0.6554187 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 6 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7745608  -1.7352642  -2.3371055   1.8303336  -0.10598341 -2.4695601\n",
            " -2.8309498   1.798722   -0.34331363  2.9938128  -0.6741552   9.703846\n",
            "  0.78766805 11.916641   -1.1864126  14.084899   -2.2637746  11.237865\n",
            " -1.6492685 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 7 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1297227  -1.3274685  -2.2436657   2.068035   -0.312813   -2.6008046\n",
            " -3.1414301   1.5209266  -0.23611914  3.977609   -0.6094185  10.378708\n",
            "  0.9000002  12.482364   -1.424735   14.597484   -1.8596288  11.925845\n",
            " -1.682162  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 8 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6285932  -1.2916249  -1.6654122   2.2430384  -0.09637764 -1.7020617\n",
            " -2.4708843   1.5282794   0.08275883  4.0947003  -0.53561527  9.453201\n",
            "  0.73409426 10.587591   -1.0789309  12.4890585  -1.5008337  10.477352\n",
            " -0.60523826]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 9 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.50846934 -1.0109154  -1.5566179   2.2840626  -0.2299151  -1.5404755\n",
            " -2.3570187   1.5181042  -0.22911897  4.5919504  -0.4319035   8.529699\n",
            "  1.0970786   9.576525   -0.8987067  11.835668   -1.2225428   9.840426\n",
            " -0.8730328 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 10 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.85585874 -1.4348087  -1.8026842   2.0699332   0.16222152 -2.7901475\n",
            " -2.9893308   1.4729898  -0.23981275  5.527681   -0.52811223  9.736607\n",
            "  1.145746   11.012849   -1.0712785  13.539792   -2.1475701  11.186515\n",
            " -1.499788  ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9915626  -1.5931697  -2.2967625   1.9187136  -0.13488182 -2.4783344\n",
            " -3.1857285   1.8446577  -0.5078857   6.5189295  -0.6478795  10.655829\n",
            "  1.4991109  12.242352   -1.217622   14.59806    -2.191755   11.976967\n",
            " -1.6861994 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.69934475 -1.134971   -1.5305755   2.138853   -0.3332723  -1.5549085\n",
            " -1.9956965   1.3754514  -0.12740512  5.938067   -0.42695874  8.546503\n",
            "  1.2284956   9.702558   -0.9603567  11.488333   -1.292147    9.817077\n",
            " -0.8051946 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 13 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.96873677 -1.358799   -2.0471916   2.0438654  -0.07447162 -2.2288864\n",
            " -2.8771935   1.8513972  -0.50372714  7.4302588  -0.3379464  10.062647\n",
            "  1.4195472  11.3587675  -1.402108   13.690338   -2.006361   11.6823635\n",
            " -1.6214573 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 14 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6295424  -1.3625572  -2.0113966   1.9485372  -0.34171182 -2.1569197\n",
            " -2.969754    1.4484556  -0.12776753  7.764835   -0.49318555  9.534936\n",
            "  1.4341046  10.888892   -1.1275477  14.023459   -1.9331495  10.8349905\n",
            " -1.272118  ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 15 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7837587  -1.2801646  -1.8050835   2.3706458  -0.11174688 -2.104721\n",
            " -2.6344817   1.7912745  -0.2607963   8.189063   -0.390381   10.097014\n",
            "  1.3141994  10.87239    -1.2503262  13.462619   -1.8390001  11.392879\n",
            " -1.333743  ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 16 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9656284  -1.5331829  -2.4440064   1.9772463  -0.1985913  -2.5203323\n",
            " -3.0620291   1.7805709  -0.4703014   9.316502   -0.47029474 10.337518\n",
            "  1.910228   11.901806   -1.4445273  14.937233   -2.1622005  11.556372\n",
            " -1.8982613 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 17 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.58042574 -1.3849368  -1.8911141   2.134041   -0.34438434 -2.1467767\n",
            " -2.4577825   1.4736607  -0.20949027  8.735694   -0.71156365  9.426082\n",
            "  1.5487381  10.766197   -1.11347    13.111866   -1.9163806  10.892541\n",
            " -0.9904687 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 18 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0275667  -1.5095911  -2.2758586   2.3261704   0.11851577 -2.1840568\n",
            " -2.7657518   1.9808547  -0.5743464  10.2943125  -0.49758935 10.593907\n",
            "  1.7583264  11.618417   -1.6170235  14.448927   -2.2142062  12.225731\n",
            " -1.5812113 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 19 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6591224  -1.6050655  -2.2990227   2.3641815  -0.2854115  -2.1397016\n",
            " -3.0648553   1.6713015  -0.14816198 10.397064   -0.68494356 10.49101\n",
            "  1.7652057  11.270112   -1.3590999  15.221255   -2.1305008  11.5026\n",
            " -1.2341442 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 20 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1470797  -1.3461165  -1.694691    2.0534794   0.14784677 -2.3033884\n",
            " -2.9655864   1.6565813  -0.2056264   9.94868    -0.519414   10.655028\n",
            "  1.7356439  10.922476   -0.90795326 13.412048   -1.9184408  12.086706\n",
            " -1.0651897 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 21 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1700438  -1.796092   -2.67354     2.3463264   0.11291675 -2.8605216\n",
            " -3.3304646   1.8102704  -0.5594408  12.235076   -0.6260891  11.013696\n",
            "  2.2695606  12.310691   -1.6588287  16.300303   -2.2391179  12.293562\n",
            " -1.787764  ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 22 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7951553  -1.4002904  -1.7239901   1.7750667  -0.24628125 -2.3317194\n",
            " -2.5968587   1.2695776  -0.1380957  10.108577   -0.54143494  9.388588\n",
            "  1.7780218  10.753467   -1.0590085  13.096907   -1.8168646  10.971563\n",
            " -1.1616486 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 23 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.034176   -1.206997   -1.9829981   2.2477832  -0.05902174 -2.2161405\n",
            " -2.954913    1.8790629  -0.45241746 11.121321   -0.49767905 10.600507\n",
            "  1.9374024  11.626314   -1.3039182  14.106846   -1.9011595  12.259003\n",
            " -1.5670171 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 24 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.53045475 -1.6093403  -2.146527    2.2487977  -0.41977593 -2.0922709\n",
            " -3.012521    1.6416316  -0.20899688 11.095308   -0.70387524  9.913872\n",
            "  1.9179522  11.046314   -1.2246116  14.605837   -1.9780295  11.291238\n",
            " -1.1347036 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 25 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9143943  -1.2345685  -1.5955572   2.0295055  -0.03905812 -2.008829\n",
            " -2.501512    1.5486137  -0.296302   10.096659   -0.21931548  9.537394\n",
            "  1.8199404  10.050909   -0.88543355 12.566447   -1.7838814  11.224889\n",
            " -1.1749781 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 26 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0951223  -1.5212135  -2.5168524   2.0946867   0.05453113 -2.688338\n",
            " -3.1640315   1.8853592  -0.5948704  11.793433   -0.6732463  10.643694\n",
            "  2.3626266  11.857922   -1.3597732  15.220312   -2.1936736  12.020102\n",
            " -1.9639132 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 27 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6779482  -1.4404408  -1.8069444   1.7717817  -0.2667206  -2.3475447\n",
            " -2.8732936   1.4554967  -0.04621639 10.580868   -0.59020853  9.616511\n",
            "  1.9641579  10.809442   -1.2337536  13.448959   -1.9569188  11.167324\n",
            " -1.0085135 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 28 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.97113997 -1.2704483  -2.0693603   2.345554   -0.09625404 -2.070085\n",
            " -2.799045    1.9213493  -0.5733011  11.089266   -0.5984526  10.521351\n",
            "  2.0099382  11.35802    -1.4586164  14.258375   -1.9644605  12.2286825\n",
            " -1.4463291 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 29 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6622244 -1.4587861 -2.1597645  2.1298282 -0.3659637 -2.032996\n",
            " -2.93403    1.7893969 -0.185456  10.909358  -0.5360865 10.258784\n",
            "  1.9075441 11.198863  -1.2893089 15.055031  -1.9524462 11.419472\n",
            " -1.1477488]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 30 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9234574  -1.2895652  -1.6610672   2.0746188   0.1635871  -2.1399293\n",
            " -2.4878476   1.6140434  -0.25353116  9.840757   -0.16121805  9.644957\n",
            "  1.9499637   9.90656    -1.0508163  12.789856   -2.0555792  11.2373295\n",
            " -1.3365272 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 31 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-9.1386849e-01 -1.5297551e+00 -2.4300413e+00  2.4594841e+00\n",
            "  6.7881318e-03 -2.1854544e+00 -2.7861917e+00  1.8960769e+00\n",
            " -5.5719775e-01  1.0911540e+01 -6.7956334e-01  1.0659367e+01\n",
            "  2.3108852e+00  1.1458773e+01 -1.3600813e+00  1.4760617e+01\n",
            " -2.3091736e+00  1.1810526e+01 -1.5611198e+00]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 32 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.72175354 -1.3105009  -1.8594105   1.6314796  -0.3682995  -2.0894427\n",
            " -2.6689796   1.2671337  -0.05347152  9.653641   -0.48300636  9.136166\n",
            "  1.9469814  10.293001   -1.2128781  12.902389   -1.7756819  10.9006\n",
            " -0.9682515 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 33 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.219333   -1.4258666  -2.3072567   2.293164   -0.11733463 -2.1500773\n",
            " -2.8313036   1.9149817  -0.44976926 10.627863   -0.52897084 10.276974\n",
            "  2.1202297  10.871774   -1.5953305  14.459431   -2.0476131  12.042696\n",
            " -1.5968447 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 34 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.854479   -1.5009739  -2.1951602   2.0988562  -0.3858629  -2.254949\n",
            " -3.1981692   1.704494   -0.15288332 10.071024   -0.5195225   9.742015\n",
            "  2.052559   10.944066   -1.2582916  14.744761   -1.8276889  11.081046\n",
            " -1.1957086 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 35 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.91354007 -1.2348915  -1.5603216   2.0666156  -0.0399693  -2.1564136\n",
            " -2.6594298   1.4549942  -0.2278837   9.332014   -0.4895879   9.467111\n",
            "  1.8670406  10.03757    -0.8144665  12.622134   -1.7816486  11.173791\n",
            " -1.1105788 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 36 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.88734883 -1.5043815  -2.1608508   2.1782162  -0.16356847 -2.1194775\n",
            " -2.8723905   1.6828831  -0.4983286   9.658911   -0.5963845  10.183864\n",
            "  2.2065682  11.029737   -1.248883   14.036335   -1.9525023  11.255468\n",
            " -1.3796685 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 37 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.56188434 -1.298613   -1.7505732   1.9254421  -0.3882126  -1.8548503\n",
            " -2.2984552   1.2777424  -0.21795516  8.704762   -0.5415486   8.691652\n",
            "  1.9985188   9.771758   -1.2037808  12.397743   -1.5938727  10.256057\n",
            " -0.8990957 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 38 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-9.8515546e-01 -1.0395700e+00 -1.7664682e+00  1.8806599e+00\n",
            "  9.9299867e-03 -2.0407674e+00 -2.4338841e+00  1.6187345e+00\n",
            " -3.4737119e-01  8.8406849e+00 -1.5323743e-01  9.2383976e+00\n",
            "  1.9911312e+00  9.7122421e+00 -1.2995733e+00  1.2605622e+01\n",
            " -1.7359507e+00  1.0592134e+01 -1.5964870e+00]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 39 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.72676873 -1.4143485  -2.0111926   2.0459704  -0.3403938  -2.2003393\n",
            " -3.0784848   1.6721436  -0.18037859  9.015354   -0.44646683  9.433086\n",
            "  1.9942737  10.594678   -1.2179401  14.192141   -1.7923218  10.720621\n",
            " -1.162617  ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 40 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.97867143 -1.1902813  -1.5341799   2.0677316   0.04653571 -2.0434632\n",
            " -2.3591938   1.450206   -0.10334778  8.174048   -0.5466107   9.010081\n",
            "  1.8348855   9.708568   -1.0013968  12.004015   -1.7488495  10.489831\n",
            " -0.75260437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12\n",
            "\n",
            "Step 41 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.98079586 -1.3124093  -2.1968498   2.0187669  -0.03173892 -2.2498555\n",
            " -2.9454212   1.7532774  -0.5676361   8.94647    -0.63564926 10.11236\n",
            "  2.3523695  10.914851   -1.1845665  13.81432    -2.0445929  11.579983\n",
            " -1.5713558 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11\n",
            "\n",
            "Step 42 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4170152  -0.8164748  -1.3141532   1.813207   -0.28772223 -1.7489231\n",
            " -2.3261015   1.1604043   0.04120361  7.6531415  -0.3496267   8.863482\n",
            "  1.9803637   9.360615   -0.7957907  11.582935   -1.395726   10.404987\n",
            " -0.71873635]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 43 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7856016  -1.0871408  -1.9391583   2.2411494  -0.0889901  -1.6706054\n",
            " -2.5106418   1.4801536  -0.32265136  8.196686   -0.33837146  9.730346\n",
            "  2.218725   10.067165   -1.1960881  12.751426   -1.5893967  10.947445\n",
            " -1.1067071 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 44 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.451588   -1.3411142  -1.9352503   2.2174447  -0.37871835 -2.1546118\n",
            " -3.0059168   1.5146747  -0.13014705  8.479171   -0.51574934  9.14685\n",
            "  2.4209507  10.175798   -1.0717151  14.032674   -1.7483149  11.029816\n",
            " -1.0576049 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 45 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6928626  -0.7823713  -1.3139048   2.1824028  -0.19036238 -1.6516913\n",
            " -2.3762093   1.2781098  -0.09433421  7.3527117  -0.12353896  9.005083\n",
            "  2.209088    9.469013   -0.86593497 11.540486   -1.2921221  10.448005\n",
            " -0.87459743]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 46 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5892878  -1.0014213  -1.9078908   2.1256526  -0.31469738 -2.0919533\n",
            " -2.8896444   1.4715466  -0.31903046  8.019441   -0.298705    8.987807\n",
            "  2.817992   10.383785   -1.3463976  13.403054   -1.5965147  10.802626\n",
            " -1.3789178 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 47 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.37528577 -0.902351   -1.4305583   1.7177439  -0.30219495 -1.9145191\n",
            " -2.3002064   1.0941528  -0.04549585  7.4563937  -0.3680513   8.023941\n",
            "  2.5293663   9.396091   -0.889134   11.623223   -1.435578   10.099499\n",
            " -0.9308161 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.63623005 -0.91418487 -1.7422146   2.1133358  -0.23207167 -1.6565086\n",
            " -2.6320834   1.5188313  -0.2925394   8.211613   -0.42740533  9.404811\n",
            "  2.7513926  10.293661   -1.216175   12.722168   -1.6240464  11.324132\n",
            " -1.0385295 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.44113305 -1.2003103  -1.7834255   2.0119083  -0.31642196 -2.0286698\n",
            " -2.6844463   1.3878073  -0.05116137  8.03591    -0.3512864   7.9689684\n",
            "  2.8907204   9.536989   -1.1450272  13.11266    -1.6942012  10.313546\n",
            " -0.8511525 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5085411  -0.9241282  -1.2735864   2.2833354  -0.04353219 -1.6956843\n",
            " -2.3243313   1.446758   -0.07121761  7.6587143  -0.31510988  8.750926\n",
            "  2.7430873   9.656779   -0.9078118  11.865004   -1.4671347  10.598797\n",
            " -0.6815549 ]\n",
            "Epsilon reduced to 0.04194304000000002\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 1 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7246068 -1.4107546 -2.1012363  1.8992077 -0.5175208 -2.2454872\n",
            " -3.0996797  1.8697989 -0.3676911  9.099086  -0.8059516  9.140064\n",
            "  3.56419   11.195989  -1.7044547 14.277221  -1.8777342 12.622563\n",
            " -1.5328199]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 2 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7331223  -1.509197   -1.9746922   2.0220106  -0.41495657 -2.2539713\n",
            " -2.8900485   1.5936083  -0.43373188  8.332748   -0.533349    7.804684\n",
            "  3.407351   10.411813   -1.2823728  13.717456   -1.7835449  10.840083\n",
            " -1.1130879 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 3 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-8.5468262e-01 -1.3128597e+00 -1.6413517e+00  1.7533350e+00\n",
            "  1.2669233e-02 -2.1544352e+00 -2.7461202e+00  1.6268868e+00\n",
            " -3.7825522e-01  8.6781416e+00 -3.7086621e-01  8.0303125e+00\n",
            "  3.4095876e+00  1.0067417e+01 -1.2467492e+00  1.3125226e+01\n",
            " -1.6770053e+00  1.1465021e+01 -1.3229611e+00]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 4 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.57261294 -1.5190306  -1.9376543   1.9204551  -0.21026812 -2.1663373\n",
            " -2.7013514   1.5255721  -0.29537413  7.8239098  -0.5644806   7.5341125\n",
            "  3.381698    9.969928   -1.1992064  13.160659   -1.8196235  10.51246\n",
            " -1.1527839 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 5 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.4277412e-01 -9.1309547e-01 -1.3926594e+00  1.5216111e+00\n",
            " -1.8062125e-01 -1.7018373e+00 -2.1517439e+00  1.0135022e+00\n",
            " -2.4188110e-03  6.9476676e+00 -3.1666186e-01  7.1408715e+00\n",
            "  2.8594875e+00  8.8641262e+00 -8.0305129e-01  1.1052337e+01\n",
            " -1.4155358e+00  9.5958462e+00 -8.4803921e-01]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 6 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6535464  -1.42559    -1.88781     2.2832587  -0.30214038 -1.8062278\n",
            " -2.521031    1.7247182  -0.45320016  7.866896   -0.36333543  7.785765\n",
            "  3.4878597   9.858227   -1.1571776  12.658928   -1.6921692  10.683507\n",
            " -0.9735991 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 7 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7336755  -1.3391975  -2.021769    1.9884804  -0.36550033 -2.383307\n",
            " -3.0089836   1.7565751  -0.5266543   8.261003   -0.4166764   7.1093082\n",
            "  3.8045473  10.166761   -1.464658   13.758026   -1.8595501  10.994704\n",
            " -1.3425267 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 8 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6791196  -0.8531881  -1.2284511   1.7492847  -0.1948319  -1.4504926\n",
            " -2.1484761   1.239943    0.09054841  6.7401514  -0.17191923  7.1877527\n",
            "  2.6519873   8.759195   -0.7748351  10.842236   -1.4857798   9.579913\n",
            " -0.46144876]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 9 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.35009864 -1.3252444  -1.7811462   1.8547761  -0.1397336  -1.7043003\n",
            " -2.4580402   1.5502204  -0.1667153   7.306093   -0.65457547  7.6422977\n",
            "  3.402032    9.684475   -0.94400865 12.650047   -1.6652478  10.153295\n",
            " -0.8807683 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 10 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.67960423 -1.4856981  -1.958422    1.9652511  -0.12720242 -2.2523193\n",
            " -2.8060234   1.5703965  -0.1688357   8.342226   -0.56743306  7.7429748\n",
            "  3.9172766  10.384199   -1.3224709  13.316659   -1.8172667  10.769594\n",
            " -1.1623209 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.98171806 -1.5088009  -1.8965569   1.9520104  -0.14153863 -2.230964\n",
            " -2.6880274   1.4542423  -0.42485636  7.7644773  -0.38539585  7.310932\n",
            "  3.7352107   9.55485    -1.0863618  12.64037    -1.8987517  10.618734\n",
            " -1.2945778 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5863011  -1.2258887  -1.9033083   1.7710965  -0.51444775 -2.0261235\n",
            " -3.1445446   1.5245028  -0.08055225  8.104888   -0.6683053   7.8450794\n",
            "  3.9692693  10.132501   -1.163603   13.808492   -1.6321831  10.903169\n",
            " -1.0491487 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 13 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.66000015 -1.1927518  -1.6338162   2.210217   -0.16183609 -1.5255945\n",
            " -2.342156    1.4917346  -0.08692418  7.6512747  -0.43921986  7.694656\n",
            "  3.3983483   9.492721   -0.9808148  11.904517   -1.4110117  10.571204\n",
            " -0.6204822 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 14 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.756235   -1.4560541  -1.9259194   1.7815342  -0.24204431 -2.50701\n",
            " -3.0281372   1.6813496  -0.53360635  8.093433   -0.4630107   7.0651474\n",
            "  4.044004   10.223441   -1.5185525  13.758554   -1.8960528  11.096147\n",
            " -1.5292989 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 15 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.65009195 -1.3619817  -1.8221105   1.7547345  -0.20732263 -2.0551903\n",
            " -2.7093291   1.2360033  -0.07299897  8.161549   -0.5620737   7.6515465\n",
            "  3.6009402  10.185357   -1.0355703  12.816118   -1.549704   10.538928\n",
            " -1.055797  ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 16 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8609496  -1.2538772  -1.7659078   1.9427433  -0.12158857 -2.109746\n",
            " -2.5146942   1.4675153  -0.368561    7.5451713  -0.3893066   6.8763003\n",
            "  3.682806    9.362333   -1.0012304  12.078208   -1.7839051  10.280983\n",
            " -1.0997679 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 17 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7121184  -1.4673369  -2.1301947   2.1033084  -0.38830948 -2.2410417\n",
            " -2.8406527   1.7009914  -0.5030071   8.073763   -0.5450635   7.0830374\n",
            "  4.0325756  10.144697   -1.3224411  13.713655   -1.8564594  10.857093\n",
            " -1.1733001 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 18 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.75336254 -1.0493908  -1.4222975   1.8715198  -0.14385176 -1.5928258\n",
            " -2.2758164   1.368041   -0.11509445  7.625224   -0.24131732  7.3271346\n",
            "  3.403744    9.4502325  -0.9000027  11.440106   -1.337006   10.3502445\n",
            " -0.8019627 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 19 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-6.1379910e-01 -1.3787881e+00 -1.9203166e+00  1.7277217e+00\n",
            "  2.5042160e-03 -2.4497950e+00 -2.7385871e+00  1.7495565e+00\n",
            " -4.2165506e-01  8.0983438e+00 -5.3510177e-01  6.9856529e+00\n",
            "  4.1165628e+00  1.0124341e+01 -1.2390451e+00  1.3213991e+01\n",
            " -2.0086746e+00  1.0832873e+01 -1.4949313e+00]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 20 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-7.6209068e-01 -1.5779225e+00 -2.0206220e+00  1.8763654e+00\n",
            " -1.7935769e-01 -2.1810460e+00 -2.8455563e+00  1.4306570e+00\n",
            " -1.3339527e-02  8.6109457e+00 -6.9660604e-01  7.8113465e+00\n",
            "  4.1450286e+00  1.0810608e+01 -1.4832140e+00  1.3950466e+01\n",
            " -1.8078179e+00  1.0754935e+01 -1.0246346e+00]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 21 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7898835  -1.529512   -1.8876042   2.1311812  -0.15964456 -2.0061092\n",
            " -2.5361612   1.6934292  -0.38010216  7.9959316  -0.5457325   7.5253763\n",
            "  3.9516056   9.94527    -1.169435   12.799533   -1.9520657  10.59448\n",
            " -0.9843746 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 22 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.89931923 -1.6111406  -2.2597933   1.935576   -0.22677729 -2.465373\n",
            " -3.0580602   1.6603109  -0.36231175  8.39641    -0.50168854  7.0822806\n",
            "  4.281803   10.097155   -1.2559398  14.102853   -2.0777502  10.792336\n",
            " -1.4018205 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 23 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7493471  -0.9075265  -1.4173371   1.9016589  -0.20529984 -1.6131026\n",
            " -2.4529638   1.3880053  -0.02608737  7.8904915  -0.34455317  7.7304745\n",
            "  3.4704301   9.651723   -0.8823917  11.701663   -1.3285984  10.826707\n",
            " -0.63058597]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 24 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.76266754 -1.3677702  -2.037329    2.0144541  -0.09362505 -2.434589\n",
            " -2.8941016   1.7846122  -0.49769464  8.40716    -0.62694865  6.88886\n",
            "  4.2226324  10.105035   -1.3735071  13.72872    -1.9513446  11.075328\n",
            " -1.3278564 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 25 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5818862  -1.581494   -1.9197855   1.9621061  -0.21915296 -2.0849905\n",
            " -2.7123933   1.5013894  -0.24862528  8.558357   -0.6680554   7.5727377\n",
            "  3.8157673  10.344607   -1.2588238  13.117581   -1.7150425  10.740167\n",
            " -1.0438342 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 26 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8082575  -1.241542   -1.5852066   2.1528997  -0.18389216 -1.8943484\n",
            " -2.2883744   1.4724499  -0.29085842  7.8847528  -0.21503222  6.910458\n",
            "  3.4678853   9.324113   -1.1952444  11.869158   -1.5315288  10.17812\n",
            " -0.9927022 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 27 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8776003  -1.4539368  -2.054429    1.8568664  -0.385195   -2.5368803\n",
            " -3.1216652   1.6184124  -0.53902423  8.851421   -0.40964523  6.950422\n",
            "  4.376201   10.403045   -1.3115897  14.138543   -1.7963885  11.197884\n",
            " -1.5587871 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 28 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6692816  -0.9639856  -1.522373    1.8426832  -0.1722557  -1.453373\n",
            " -2.3952193   1.4075412  -0.11086617  8.089028   -0.4708889   7.7982664\n",
            "  3.420326    9.621365   -0.9474719  11.728178   -1.4001424  10.661159\n",
            " -0.603189  ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 29 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.46527898 -1.3501935  -2.0194643   1.8395172  -0.16595645 -2.401537\n",
            " -2.9725592   1.8186361  -0.41047513  8.71912    -0.7724537   7.2779703\n",
            "  4.180759   10.204599   -1.3380209  13.594489   -2.0905051  11.237582\n",
            " -1.3223059 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 30 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6373535  -1.3830948  -1.8361999   1.9283596  -0.23164134 -2.0256543\n",
            " -2.525761    1.4399202  -0.08898577  8.420161   -0.5279196   7.5025787\n",
            "  3.912815   10.433877   -1.3150195  12.839468   -1.6470978  10.604594\n",
            " -1.0224211 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 31 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8717319  -1.4626381  -1.6729659   1.8515551  -0.06376024 -2.0157135\n",
            " -2.4477131   1.557957   -0.28037497  8.259298   -0.30065092  7.274325\n",
            "  3.8368535   9.790385   -1.1172208  12.303833   -1.872792   10.480367\n",
            " -1.0818857 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 32 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7249653  -1.6399264  -2.2822587   2.0182235  -0.35692334 -2.3383799\n",
            " -3.0570316   1.9263109  -0.45163426  9.010655   -0.61678857  7.4789524\n",
            "  4.625247   10.839418   -1.4322238  14.733301   -1.9089419  11.349151\n",
            " -1.3216482 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 33 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.668814   -1.0707574  -1.642306    1.9361185  -0.19561863 -1.5152878\n",
            " -2.5564585   1.3536917   0.03308747  8.450185   -0.66492736  8.102344\n",
            "  3.658016    9.608893   -0.9761945  12.258853   -1.4315501  10.799164\n",
            " -0.5752068 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 34 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5975458  -1.377597   -2.0002654   1.6961005  -0.18293647 -2.3473203\n",
            " -2.9050894   1.7960094  -0.41675353  8.461239   -0.68904245  7.068985\n",
            "  4.1635385  10.202002   -1.1466597  13.324822   -1.9608991  11.240425\n",
            " -1.4106958 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 35 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6787459  -1.3725396  -1.8765861   1.9377823  -0.34396636 -2.0798872\n",
            " -2.640538    1.3606017  -0.19749063  8.316538   -0.35894322  7.1136746\n",
            "  3.8970945  10.116986   -1.4457725  12.847903   -1.5696962  10.518248\n",
            " -1.0142486 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 36 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9137256  -1.407618   -1.6301364   1.9537412  -0.07858086 -2.0063434\n",
            " -2.242601    1.429539   -0.35511374  8.288471   -0.21062058  7.27462\n",
            "  3.5933418   9.762727   -1.1264223  12.232079   -1.7453995  10.458958\n",
            " -1.183007  ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 37 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7631476 -1.522833  -2.1733682  1.9910167 -0.3403689 -2.3640285\n",
            " -3.0173542  1.6261152 -0.5670685  8.567872  -0.4400531  6.9526596\n",
            "  4.3593307 10.180134  -1.4217429 13.829434  -1.7929578 10.9793625\n",
            " -1.4092877]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 38 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.55194056 -0.9762391  -1.4589785   2.1060233  -0.17202954 -1.4117627\n",
            " -2.2470145   1.3313237  -0.14049883  8.019272   -0.46481457  7.5805144\n",
            "  3.4687111   9.331442   -0.9179822  11.615339   -1.1867646  10.453024\n",
            " -0.569234  ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 39 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6702896  -1.264651   -1.8703893   1.9589041  -0.18884878 -2.0808575\n",
            " -2.684705    1.6624085  -0.46570694  8.146609   -0.48207846  6.6067357\n",
            "  3.9378347   9.664136   -1.2114539  12.761486   -1.8579952  10.690776\n",
            " -1.2050703 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 40 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.73225534 -1.5268462  -2.103564    1.8805233  -0.16009717 -2.275979\n",
            " -2.8660986   1.4381748  -0.14532354  8.720024   -0.5103022   7.4230103\n",
            "  4.1208014  10.284758   -1.2813096  13.529529   -1.8716191  10.571435\n",
            " -1.2237312 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 41 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.64455986 -1.3104739  -1.5052263   2.1687238  -0.0772404  -1.7758865\n",
            " -2.1530674   1.4912293  -0.01966944  7.9999895  -0.33100745  7.2742286\n",
            "  3.54105     9.379896   -1.0852408  11.809931   -1.6015563   9.683203\n",
            " -0.79656595]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 42 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.81761855 -1.6817336  -2.2043452   1.7817309  -0.1562861  -2.4746964\n",
            " -3.0249426   1.6788526  -0.47250918  8.683226   -0.47672868  7.2528625\n",
            "  4.576678   10.449853   -1.3586453  14.154777   -2.0270758  11.013085\n",
            " -1.5445647 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 43 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.59753853 -1.2014148  -1.666837    1.8079939  -0.23019387 -1.7524501\n",
            " -2.604325    1.4007982   0.03010171  8.350974   -0.49215972  7.4722276\n",
            "  3.7831953   9.990067   -0.9864169  12.425488   -1.5274377  10.606173\n",
            " -0.85099566]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 44 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-6.0110545e-01 -1.3153030e+00 -1.8363863e+00  1.8531680e+00\n",
            " -6.4316681e-03 -2.4791539e+00 -2.7441566e+00  1.7031252e+00\n",
            " -4.2044425e-01  8.2905216e+00 -5.2251410e-01  6.8175693e+00\n",
            "  4.1845951e+00  1.0044141e+01 -1.2430297e+00  1.3122998e+01\n",
            " -2.0569468e+00  1.0812636e+01 -1.4622552e+00]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 45 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.69530493 -1.6277688  -2.278501    2.1409905  -0.25691643 -2.1537855\n",
            " -2.903319    1.540544   -0.14170237  8.703937   -0.7144373   7.7011886\n",
            "  4.1626015  10.459856   -1.3507816  13.883102   -1.8446007  10.781201\n",
            " -1.0038568 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 46 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7360273  -1.0955856  -1.381806    2.0967352  -0.2629045  -1.7683324\n",
            " -2.171484    1.3364389  -0.12301227  7.664627   -0.18492089  6.984404\n",
            "  3.3359485   9.467393   -1.0984377  11.464753   -1.3537859   9.88966\n",
            " -0.83611035]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 47 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7072382  -1.5062268  -2.131213    1.9049243  -0.29765302 -2.3619003\n",
            " -3.0264933   1.727269   -0.5669682   8.445589   -0.50063026  6.9433203\n",
            "  4.3137674  10.246242   -1.4996485  13.857886   -1.8431339  11.06268\n",
            " -1.4467642 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 48 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-6.0965335e-01 -9.1340792e-01 -1.4830670e+00  2.0161753e+00\n",
            " -1.3530469e-01 -1.3847555e+00 -2.2326484e+00  1.3651092e+00\n",
            " -6.3458802e-03  7.7865686e+00 -4.9420509e-01  7.4070845e+00\n",
            "  3.3852286e+00  9.2643290e+00 -9.4081211e-01  1.1669625e+01\n",
            " -1.4204085e+00  1.0153791e+01 -5.7215089e-01]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 49 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.47791874 -1.355388   -1.9695395   2.1685545  -0.1145147  -2.255028\n",
            " -2.8837824   1.739293   -0.39314038  8.50333    -0.687588    7.035676\n",
            "  4.1977224   9.66842    -1.3174342  13.338458   -2.1004758  10.910381\n",
            " -1.1840035 ]\n",
            "\n",
            "Taking action 9\n",
            "\n",
            "Step 50 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.64672977 -1.4632454  -1.9377089   1.9233634  -0.20646869 -2.1949925\n",
            " -2.7364264   1.5321513  -0.14087148  8.401611   -0.48392206  7.4471292\n",
            "  4.177006   10.490517   -1.3786991  13.3478     -1.8299116  10.886288\n",
            " -1.226041  ]\n",
            "Epsilon reduced to 0.033554432000000016\n",
            "Total reward: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxQgudduzxwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_conv_25(env):\n",
        "    input_shape = env.observation_shape()\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape + (1, ), input_shape=input_shape))\n",
        "    model.add(Conv1D(8, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(env.action_space.n))\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maD-zpQBCZOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52e3c7fc-b871-45f5-eb75-c797a1f3650e"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=50, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=8, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_25(env), max_epsilon=0.15, epsilon_decay=0.9)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0826 09:12:52.056147 140404775921536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_4 (Reshape)          (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 187, 8)            32        \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 93, 8)             0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 744)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 19)                14155     \n",
            "=================================================================\n",
            "Total params: 14,187\n",
            "Trainable params: 14,187\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |████████████----------------------------------------------------------------------------------------| 12.5% \r\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 1 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10927111  0.10214259  0.01947924  0.32402724 -0.01271004 -0.4852782\n",
            "  0.32603797  0.06826238 -0.3882689  -0.16868186  0.46661213 -0.22127527\n",
            "  0.2424545   0.1992521  -0.05087483 -0.23414287  0.05933524 -0.205421\n",
            " -0.20228197]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 2 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.11395409  0.23520657 -0.18071596  0.05250244 -0.2649498  -0.21999118\n",
            " -0.04742978 -0.1027374   0.16825278  0.03736768  0.07367875 -0.1413156\n",
            "  0.14374529  0.03967611 -0.03692313  0.01192668 -0.24609038 -0.02088452\n",
            " -0.13673939]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 3 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.28319117  0.31933138  0.11602418  0.01314325 -0.34675205 -0.49469632\n",
            "  0.03838953 -0.11267354  0.01556628 -0.02630266 -0.17579587 -0.40049633\n",
            "  0.2098468   0.22712466  0.03644596 -0.15294121 -0.25064456 -0.07060218\n",
            " -0.20457059]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 4 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.2437698  -0.29904246  0.11871579  0.2991786  -0.02001867 -0.2983902\n",
            " -0.13359034  0.2317495  -0.19760548  0.01801356  0.5197167  -0.4125924\n",
            "  0.22062719 -0.09529828  0.00762841 -0.5030987   0.02213915 -0.38259876\n",
            " -0.282293  ]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 5 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.20450187  0.20224498  0.04569894  0.04387301 -0.21539919 -0.6606915\n",
            " -0.17777319 -0.22829986 -0.25984657  0.00914883  0.27038598 -0.33448768\n",
            "  0.0200664   0.19551286  0.06273942 -0.0580209  -0.17250901 -0.20754546\n",
            " -0.12769727]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 6 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.17896307  0.35909536 -0.17520703  0.04895309 -0.3384105  -0.5172731\n",
            "  0.1263535  -0.1253441   0.31354487  0.05124736  0.08952905 -0.2208547\n",
            "  0.13841136  0.1830682  -0.3247805   0.01855199 -0.29927674  0.04529754\n",
            " -0.22849946]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 7 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.17784604 -0.07226627  0.14943117  0.21814153 -0.00467958 -0.5002835\n",
            "  0.11602248  0.0481791  -0.15273456 -0.2463297   0.3528828  -0.20905092\n",
            "  0.2094793   0.09844634 -0.10401567 -0.23933253 -0.4522589  -0.19944404\n",
            " -0.18591186]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 8 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.2370439   0.11981437  0.12018928 -0.10656022 -0.33142662 -0.3854126\n",
            "  0.0920345  -0.20350228  0.10388841 -0.04908152 -0.02391103 -0.18386309\n",
            "  0.1891649   0.28336245 -0.23821148 -0.07967421 -0.10607394 -0.04991356\n",
            " -0.29940966]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 9 reward=0 new_state=[1 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.3623576   0.01835405 -0.23872611 -0.02924522 -0.37821844 -0.19045639\n",
            "  0.4368952  -0.20799059  0.11320928 -0.3384566  -0.04955489  0.17918032\n",
            "  0.1915886  -0.0657941  -0.02012509  0.04267769  0.00434026 -0.15029733\n",
            " -0.2975786 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 10 reward=0 new_state=[1 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.23159456  0.12742941 -0.05951991  0.00075116 -0.49032733 -0.5944042\n",
            "  0.1626083  -0.15885156  0.11137152 -0.10045924  0.11386423 -0.09263588\n",
            "  0.09383609 -0.09051792 -0.31078193  0.04992399 -0.25933185 -0.02781055\n",
            " -0.47515646]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 11 reward=0 new_state=[1 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.17077124  0.07388491 -0.34819755  0.1565782  -0.50054646 -0.38587323\n",
            "  0.244435   -0.07902873  0.19040047 -0.16524115  0.23127857  0.03607599\n",
            "  0.07342187 -0.11294144 -0.5670341   0.15639183 -0.19479406 -0.00750433\n",
            " -0.4064083 ]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 12 reward=0 new_state=[1 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.19208966  0.21927638 -0.33964598  0.03645829 -0.26585722 -0.809949\n",
            "  0.25577182 -0.16396686 -0.22418094 -0.11786718  0.2245626  -0.1326463\n",
            "  0.04085701 -0.13470411 -0.32426775  0.30159426 -0.23774731  0.15384944\n",
            " -0.29410997]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 13 reward=1 new_state=[1 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.2054104   0.1270275  -0.2570314   0.19891462 -0.413141   -0.25612295\n",
            "  0.13751693 -0.00769458  0.0675173   0.02423825  0.05051788 -0.18808952\n",
            "  0.2158628   0.05466552 -0.29157138  0.15237887 -0.25560814 -0.07738759\n",
            " -0.2749624 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 14\n",
            "\n",
            "Step 14 reward=0 new_state=[1 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.21119691 -0.04617815 -0.2053937  -0.12439626 -0.09049337 -0.67099893\n",
            "  0.19861506 -0.21191387 -0.2845116  -0.23323916  0.2537597  -0.04726611\n",
            "  0.12899943 -0.0593922  -0.21623765  0.16931686 -0.2062688  -0.06003203\n",
            " -0.32367018]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 15 reward=0 new_state=[1 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.27377483  0.12760773 -0.14853129 -0.07774154 -0.2802589  -0.37611857\n",
            "  0.25487408 -0.06516152  0.15451756  0.11847793  0.15490514 -0.04608998\n",
            "  0.12004514 -0.02404879 -0.3802222   0.28819305 -0.22193003 -0.03576212\n",
            " -0.32640883]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 16 reward=1 new_state=[1 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.30252412  0.13916785 -0.360415   -0.02631134 -0.31642237 -0.62008125\n",
            "  0.38454986 -0.02892009 -0.268375   -0.12554693  0.28624412 -0.18777339\n",
            "  0.13467433 -0.1628131  -0.08045039  0.27304804 -0.20912206 -0.06777456\n",
            " -0.22932017]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 17 reward=1 new_state=[1 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.2390572   0.01525575 -0.28967106 -0.14461318 -0.3360266  -0.27134722\n",
            "  0.28775877 -0.18756561 -0.19350287 -0.20547736  0.1850696  -0.006756\n",
            "  0.18527634 -0.0451983  -0.332671    0.2147372  -0.05531665 -0.05311226\n",
            " -0.47325867]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 18 reward=1 new_state=[1 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.2674101   0.10875973 -0.22697993 -0.1120166  -0.43823233 -0.39440373\n",
            "  0.29632866 -0.10514594 -0.06563775 -0.12539601  0.23056236 -0.08394776\n",
            "  0.10449717  0.05125394 -0.34548128  0.2440057  -0.10881443 -0.18073061\n",
            " -0.4841187 ]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 19 reward=1 new_state=[1 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.2797429  -0.09942003 -0.32906148 -0.10141794 -0.13659431 -0.6464228\n",
            "  0.47525167 -0.2189871  -0.5094284  -0.00670106  0.44340628 -0.01484978\n",
            "  0.2779684  -0.03483741 -0.3269814   0.26103333 -0.05021566 -0.10288403\n",
            " -0.34724453]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 20 reward=1 new_state=[1 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.06215318 -0.15802386 -0.16224182 -0.0259974  -0.1266499  -0.0997607\n",
            "  0.38200676  0.01959041 -0.06175862  0.12467664  0.3129768   0.16753441\n",
            "  0.308887    0.02247553 -0.47281426  0.17725615  0.01884883 -0.17267656\n",
            " -0.29116943]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 21 reward=1 new_state=[1 0 0 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.27716413 -0.06010493 -0.43308046 -0.03793376 -0.27408063 -0.32718143\n",
            "  0.6694391   0.05065403 -0.05330409 -0.31736508  0.08227079  0.32178697\n",
            "  0.05367127 -0.02377037 -0.3231674   0.3272799   0.02308777 -0.16201077\n",
            " -0.19974937]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 22 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.00051831 -0.01178002 -0.30129978 -0.11455843 -0.18620889 -0.30076212\n",
            "  0.34814513  0.01084977 -0.13914587 -0.11528362  0.2036618   0.18895572\n",
            "  0.10658204  0.00650146 -0.44167036  0.30662522 -0.06858576 -0.0895716\n",
            " -0.2092561 ]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 23 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.18875578 -0.22081389 -0.24813683 -0.13006608 -0.05416858 -0.31978044\n",
            "  0.29792437 -0.01861532 -0.38825804 -0.03862241  0.47063354  0.10998372\n",
            "  0.15299152  0.08885433 -0.38990813  0.21202345  0.09506585 -0.19283834\n",
            " -0.27720672]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 24 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.40925258 -0.23056838 -0.33770847 -0.07929957 -0.21102408 -0.2979813\n",
            "  0.7609659  -0.2243592  -0.18496075 -0.07388403  0.37770978  0.20182914\n",
            "  0.2899483   0.15987442 -0.36131644  0.30277228  0.26324117 -0.2422798\n",
            " -0.22281681]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 25 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.1722913  -0.05373782 -0.28681335  0.14204463 -0.3664571  -0.6268655\n",
            "  0.8060698   0.2225733  -0.3921825   0.0324067   0.3656388   0.04546764\n",
            "  0.27597526 -0.06012464 -0.413222    0.30550757 -0.26708904 -0.09561904\n",
            " -0.34380674]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 26 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.12682453 -0.2953244  -0.24608694  0.03831983 -0.17630585 -0.48141834\n",
            "  0.72686106 -0.06939036 -0.5788393  -0.22894986  0.33388042  0.1179904\n",
            "  0.37712935  0.0565752  -0.23027605  0.2821292  -0.12351002 -0.07445845\n",
            " -0.16520135]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 27 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.14764607 -0.26948637 -0.04776958  0.08761955 -0.19603124  0.02483657\n",
            "  0.48384172 -0.15775862 -0.23305358  0.05087114  0.4998991   0.02841058\n",
            "  0.40552038  0.11141714 -0.5249246   0.06670701  0.01446698 -0.35424048\n",
            " -0.24619335]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 28 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.3944411  -0.26408887 -0.31853476 -0.14298971 -0.38884842 -0.42095426\n",
            "  0.9379503  -0.02226625 -0.7040839   0.00855011  0.5041486   0.15840238\n",
            "  0.37551612 -0.01416388 -0.4691888   0.2328563   0.04874656 -0.24309327\n",
            " -0.25960192]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 29 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.19510046 -0.13162953 -0.29493415  0.09847776 -0.17762838 -0.6085848\n",
            "  0.73719585 -0.10081393 -0.38859665 -0.09129135  0.40524808  0.06873658\n",
            "  0.24155268  0.07059498 -0.1232587   0.40096998 -0.12242869 -0.10714986\n",
            " -0.2529036 ]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 30 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.13916086 -0.07378306 -0.23783103  0.13059509 -0.29543963 -0.5701176\n",
            "  0.7256172   0.13833909 -0.30526197  0.11208765  0.29739788  0.22971892\n",
            "  0.36523277 -0.04369631 -0.28755638  0.16290921 -0.19978078 -0.1226901\n",
            " -0.39453822]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 31 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.01889328 -0.34520066 -0.23716441  0.00466253  0.08597672 -0.25525087\n",
            "  0.6979671  -0.06471983 -0.7322623   0.00291673  0.47205213  0.06860652\n",
            "  0.36052132  0.01008417 -0.22316965  0.24955043  0.10573696 -0.1715389\n",
            " -0.15572797]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 32 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.1235351  -0.10200945 -0.2499506  -0.02091873 -0.2627348  -0.4620547\n",
            "  0.7733347   0.05594432 -0.489958   -0.05204935  0.40555698  0.02094484\n",
            "  0.35863575  0.15487926 -0.43020394  0.13643771 -0.05914029 -0.23238651\n",
            " -0.44193956]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 33 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.34893897 -0.205371   -0.15168895  0.15008794 -0.26037598 -0.43766218\n",
            "  0.67543775  0.04588783 -0.10400034 -0.16302665  0.5384      0.11391465\n",
            "  0.04107773 -0.05760202 -0.58509004  0.3302365  -0.18741363 -0.1397132\n",
            " -0.43453875]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 34 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.1100935  -0.147902   -0.29551163  0.09187555 -0.16759095 -0.56130403\n",
            "  0.65037155 -0.08929141 -0.33430535 -0.11678034  0.3894694   0.24792054\n",
            "  0.2200169  -0.12760824 -0.08547597  0.35508266 -0.18802677 -0.14903508\n",
            " -0.189362  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 35 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.101289   -0.3611579   0.13799164  0.19884871  0.09007633  0.04612716\n",
            "  0.3763656   0.03787522 -0.38703132  0.13216002  0.31439233 -0.06279618\n",
            "  0.4042561  -0.20149711 -0.47060898 -0.08487939 -0.07276119 -0.2198533\n",
            " -0.31676954]\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 36 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.04973225 -0.24941738 -0.23858948 -0.06344448  0.03434484 -0.54470825\n",
            "  0.7611685  -0.10551673 -0.6687716  -0.23907694  0.47923633  0.03434655\n",
            "  0.18526764  0.05595613 -0.32889515  0.34074476  0.10393648 -0.20668453\n",
            " -0.25697413]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 37 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.33960083 -0.00472477 -0.28781313 -0.11955357 -0.40264446 -0.5065626\n",
            "  0.68536663  0.04963816 -0.07218185 -0.2292087   0.29152212  0.13437255\n",
            "  0.243696    0.39420158 -0.30622938  0.37344345 -0.05433905 -0.18479721\n",
            " -0.45662868]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 38 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.18106757 -0.05861666 -0.32008615  0.05348425 -0.24388294 -0.6658545\n",
            "  0.7233634   0.12366251 -0.61366457 -0.12977141  0.48166433  0.17062916\n",
            "  0.25998536 -0.11982316 -0.19450706  0.56732136 -0.20632963  0.00386388\n",
            " -0.18491915]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 39 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.39258525 -0.1419579  -0.31247005 -0.12132163 -0.22486597 -0.20717725\n",
            "  0.79143983 -0.24235786 -0.04446409 -0.06449358  0.44232324  0.3206027\n",
            "  0.36648375  0.17228341 -0.15074687  0.39405492  0.2507817  -0.23330088\n",
            " -0.06117346]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 40 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09478097 -0.2317926   0.10368     0.09973992  0.00106191 -0.06862048\n",
            "  0.50227344  0.07383595 -0.2905178   0.14809361  0.44186366  0.01628579\n",
            "  0.39125675 -0.08389468 -0.4252134   0.00695663 -0.13686131 -0.23964098\n",
            " -0.3437823 ]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 41 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.41694963 -0.11080541 -0.290269    0.12017308 -0.3895688  -0.27356404\n",
            "  0.8588543   0.10397919  0.03787965 -0.2340227   0.19018762  0.21171376\n",
            "  0.15638396  0.1971305  -0.07937915  0.41258985 -0.01268436 -0.14009385\n",
            " -0.10922964]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 42 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.24498828 -0.06723103 -0.31762394 -0.05608703 -0.2935502  -0.35163352\n",
            "  0.5413638   0.00166856 -0.17012943 -0.27421197  0.11829087  0.17778318\n",
            "  0.35419896  0.33985457 -0.14016706  0.23966807 -0.02104978 -0.09024695\n",
            " -0.3316159 ]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 43 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.22906716 -0.31877008 -0.233472   -0.08321545 -0.16407365 -0.5079443\n",
            "  0.71128553  0.18670206 -0.5967668   0.01443812  0.60054964  0.13455316\n",
            "  0.2802333   0.11353082 -0.20764709  0.14698163  0.09761526 -0.19463192\n",
            " -0.07312911]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 44 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.3009643  -0.14181659 -0.2874745   0.02280171 -0.17188895 -0.25861666\n",
            "  0.65413713 -0.17196149 -0.06337807 -0.01235851  0.29613087  0.30745095\n",
            "  0.17266828  0.07090651  0.04807197  0.22801298  0.11035939 -0.31114948\n",
            "  0.05702595]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 45 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.21752629 -0.03364842 -0.2742068   0.06135748 -0.2711029  -0.55488896\n",
            "  0.5325092   0.2643847  -0.20071752  0.05165228  0.35284606  0.04636257\n",
            "  0.40218288  0.20524229 -0.12311395  0.2207661  -0.2581862  -0.13846369\n",
            " -0.11661051]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 46 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.47702798 -0.21399604 -0.2804306   0.265434   -0.36181775 -0.12550202\n",
            "  0.5725598   0.00519819  0.00731816 -0.24874774  0.18168135  0.29900277\n",
            "  0.24752052  0.14791949  0.06700722  0.42243233 -0.02582864 -0.20021766\n",
            "  0.00338181]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 47 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08499142 -0.2180983  -0.19465198 -0.06414644 -0.03191649 -0.29951608\n",
            "  0.56219447 -0.06751419 -0.20891649 -0.14054653  0.33611646  0.16326277\n",
            "  0.1986874   0.41318154 -0.29315192  0.21272399  0.2745306  -0.14262612\n",
            " -0.10834797]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.36179554 -0.06883725 -0.3889119  -0.06401977 -0.24779795 -0.4619262\n",
            "  0.71909195 -0.06096666 -0.40119383 -0.2764324   0.16019571  0.2862189\n",
            "  0.33091444  0.24783957  0.07747699  0.286184    0.08363712 -0.1856016\n",
            "  0.13041344]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.2798785e-01 -8.0275416e-02 -3.7342703e-01  1.9411316e-05\n",
            " -1.6562042e-01 -4.2043740e-01  3.3329368e-01  1.0580737e-01\n",
            " -5.5464780e-01  5.4811571e-02  4.1192967e-01  1.5120266e-01\n",
            "  4.1431776e-01  3.9391637e-02 -1.3040592e-01  2.0532979e-01\n",
            " -7.8236312e-02 -1.0802244e-01  7.5011835e-02]\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.24216084 -0.13593444 -0.38947746 -0.06589426 -0.21716927 -0.37757722\n",
            "  0.6360709  -0.00200637 -0.41447398 -0.33771408  0.12915507  0.37062386\n",
            "  0.2761844   0.09336876  0.05796994  0.31987655  0.06357671 -0.20272654\n",
            "  0.15682818]\n",
            "Epsilon reduced to 0.135\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.2457053e-01 -1.4668986e-01 -2.8100181e-01  1.2844829e-01\n",
            " -3.9645243e-01 -5.4376346e-01  5.5646491e-01  1.6024797e-01\n",
            " -4.2315352e-01 -3.7964087e-02  6.6374034e-01  3.4842727e-01\n",
            "  4.6909940e-01  1.4951184e-02  1.9483779e-01 -5.8851484e-04\n",
            "  2.4990165e-01 -2.0854375e-01  1.7594957e-01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 2 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.02314024 -0.42068705 -0.21974139  0.02473535 -0.25782767 -0.41853935\n",
            "  0.4125815   0.45013425  0.3202594  -0.22220393  0.5958544   0.31955832\n",
            "  0.62919647 -0.15248552 -0.1827856  -0.12353181  0.35252255 -0.22292802\n",
            "  0.02138821]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 3 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.15360093 -0.20496824  0.20261192  0.07685127 -0.15489186 -0.09585847\n",
            "  0.1770719  -0.24255948  0.01151441 -0.0540403   0.38705274  0.02112522\n",
            "  0.32965836  0.21535338 -0.23935711  0.09944181  0.05839807 -0.01289928\n",
            " -0.11776087]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 4 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.03331584 -0.1515101  -0.14511682  0.14734346 -0.31877217 -0.4495617\n",
            "  0.48355097  0.14285253 -0.43243626 -0.1055099   0.46874857 -0.08633663\n",
            "  0.3637225   0.2386858  -0.00700022  0.07082684  0.0473276  -0.15787728\n",
            "  0.00761964]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 5 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.12960476 -0.49703538  0.03849468  0.09585167 -0.22296752 -0.32194436\n",
            "  0.6481111   0.00639359  0.09328148 -0.46877417  0.14988543  0.3796157\n",
            "  0.68479115  0.08313044  0.20032538 -0.17759845  0.38237628 -0.34356746\n",
            "  0.18625213]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 6 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.10419559 -0.15171728 -0.13100527 -0.03070635 -0.13550007 -0.6584938\n",
            "  0.42353243  0.21358956 -0.01178246 -0.32936963  0.11713546  0.15887007\n",
            "  0.5672914   0.27639705 -0.01851499  0.13478589 -0.13770233 -0.15900496\n",
            " -0.10083605]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 7 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.28880882 -0.39517644 -0.3485391   0.022569   -0.33386254 -0.5409244\n",
            "  0.52894765  0.24035737 -0.35399485 -0.4455414   0.40648735  0.08994401\n",
            "  0.19113041  0.19028756  0.18698007  0.12786745  0.19548804 -0.3125779\n",
            "  0.08786076]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 8 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.01187052 -0.12748533 -0.03122443 -0.01935037 -0.2649419  -0.36537912\n",
            "  0.33783025 -0.23720695 -0.2710163  -0.08213146  0.25934824  0.23109533\n",
            "  0.1871074   0.42600974 -0.00714155  0.38883793 -0.05335833 -0.10173588\n",
            "  0.3846767 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 9 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.0130239  -0.435034    0.18715736  0.28986618 -0.28243098 -0.5403191\n",
            "  0.51793224  0.25813618 -0.48982555 -0.3985157   0.21835688  0.34353065\n",
            "  0.34994137 -0.14234021 -0.00549651 -0.0374409   0.20129342 -0.29338518\n",
            "  0.16840501]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 10 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.01259383 -0.32029462  0.097009   -0.03544832 -0.10109678 -0.66628903\n",
            "  0.64320153 -0.17507708 -0.4133115  -0.16576153  0.46293014  0.4329862\n",
            " -0.02568081 -0.00626144  0.1561915   0.23766461  0.00950019 -0.34899428\n",
            "  0.0786991 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 11 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.138977   -0.19399172 -0.09354665 -0.02328226 -0.16849218 -0.3725831\n",
            "  0.26975814  0.02341998  0.06643562 -0.15183614  0.2146385   0.28075916\n",
            "  0.256839   -0.14629391 -0.23528813  0.30020782  0.00409466 -0.05223766\n",
            " -0.03807652]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 12 reward=-2 new_state=[1 0 0 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10399431 -0.3463195  -0.18203333  0.0464748  -0.19251953 -0.33299977\n",
            "  0.7590075  -0.0030807  -0.20462817 -0.2033544   0.07904633  0.6152869\n",
            " -0.05834955  0.14238636  0.12922916  0.26026115  0.23201971 -0.31548542\n",
            "  0.16645563]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 13 reward=-2 new_state=[1 0 0 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.26116762 -0.1199671  -0.33511037 -0.11272256 -0.17826878 -0.3443429\n",
            "  0.29854065 -0.06215269 -0.05209484 -0.10901926  0.0250882   0.45280227\n",
            " -0.10674644  0.15522866 -0.1588963   0.28553116  0.09303012 -0.00253426\n",
            "  0.04568657]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 14 reward=-1 new_state=[1 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.10220437 -0.46873423 -0.46452484  0.07214091 -0.07203589 -0.08069829\n",
            "  0.46614352 -0.23463972 -0.31130582 -0.16696468  0.15253383  0.29027355\n",
            " -0.00586341  0.4211624  -0.07168067  0.33212206  0.3349601  -0.08352336\n",
            " -0.0178207 ]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 15 reward=-1 new_state=[1 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.08639746 -0.3588525  -0.34409922  0.15472825 -0.08086249 -0.15982723\n",
            "  0.31486136 -0.12767078 -0.46636474 -0.08263752  0.17766513 -0.00649337\n",
            "  0.13392214  0.33194202 -0.13911498  0.14065701  0.18184261 -0.20229045\n",
            "  0.04650718]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 16 reward=-1 new_state=[1 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.03582858 -0.6088438  -0.18840502  0.08284023 -0.13388094  0.02691584\n",
            "  0.5548859   0.03035983 -0.4559274  -0.11117423 -0.03990543 -0.01364032\n",
            "  0.0873019   0.3018651   0.0100997   0.18183887  0.3614132  -0.2684596\n",
            "  0.0142028 ]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 17 reward=-1 new_state=[1 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.02383337 -0.1386329  -0.19366468 -0.0681944  -0.06928415 -0.11624734\n",
            "  0.3981146  -0.00586979 -0.12257568 -0.02201314 -0.05218158  0.20960528\n",
            "  0.1428996   0.26679528 -0.0009095   0.4273829   0.0649818  -0.06424296\n",
            "  0.0666099 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 18 reward=-1 new_state=[1 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.02812096 -0.5436705  -0.4629686   0.02416055 -0.14867526  0.11046029\n",
            "  0.5044114  -0.12960464 -0.6605216  -0.19356261  0.04293946  0.08540072\n",
            "  0.06491729  0.2907945   0.0266439   0.1808138   0.30225578 -0.12886807\n",
            "  0.09417262]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 19 reward=-1 new_state=[1 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.04124479 -0.4202496  -0.49191713  0.11071883 -0.02973194 -0.1890252\n",
            "  0.40701452 -0.19413048 -0.705403   -0.28169292  0.1170822   0.01696075\n",
            "  0.21270679  0.08982067  0.2232822   0.3121903   0.072474   -0.24945906\n",
            "  0.17446926]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 20 reward=-1 new_state=[1 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.00415496 -0.46735767 -0.21072131  0.11972564 -0.00948128  0.04554022\n",
            "  0.55148095 -0.11797098 -0.22026713  0.05421611  0.02684565  0.02966559\n",
            "  0.15776017  0.25270393 -0.06603242  0.22832803  0.32071564 -0.12566085\n",
            " -0.12461688]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 21 reward=-1 new_state=[1 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.07931069 -0.4101826  -0.41230917 -0.08152608 -0.01169235 -0.23484541\n",
            "  0.35986578 -0.1470227  -0.41221052 -0.22567898 -0.00303921  0.13746892\n",
            " -0.02675783  0.38628897 -0.1594351   0.35572666  0.26594406 -0.05479132\n",
            "  0.05715314]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 22 reward=-1 new_state=[1 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.21902111 -0.5956818  -0.74719036  0.04680537  0.06664927 -0.34700838\n",
            "  0.38286656 -0.20470653 -0.74177927 -0.2629964   0.08854003  0.055923\n",
            " -0.07208793 -0.00509911  0.29190424  0.3412873   0.14079988 -0.18334183\n",
            "  0.08139871]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 23 reward=-1 new_state=[1 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.01747154 -0.49469057 -0.31173572  0.14397624 -0.0490493  -0.06610213\n",
            "  0.5829998  -0.11166192 -0.5816056  -0.1273094  -0.15109177  0.13584547\n",
            " -0.10027409 -0.0082473   0.2522108   0.1571965   0.22856814 -0.22154957\n",
            " -0.2926566 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02257582 -0.28636163 -0.57217145  0.02025955 -0.38934225 -0.06520534\n",
            "  0.466447   -0.27843696 -0.35156143 -0.30435783  0.00059715  0.16141248\n",
            "  0.05197424  0.0776381  -0.02368154  0.40679905  0.0547277   0.09649138\n",
            "  0.15164113]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 25 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.04529187 -0.47049266 -0.8127294   0.00506645 -0.23907985 -0.15360285\n",
            "  0.50394523 -0.34254712 -0.62201566 -0.44839498 -0.15033239  0.24674349\n",
            " -0.11998202 -0.12642013  0.17432548  0.46207342 -0.05119884  0.03532883\n",
            " -0.04728227]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 26 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.15309091 -0.47715023 -0.53712606 -0.06136302 -0.2756811  -0.06937938\n",
            "  0.38393933 -0.07794389 -0.33598006 -0.16932566 -0.25103384  0.17949344\n",
            " -0.02310205 -0.15232985  0.00955014  0.5150355   0.12514181  0.05022955\n",
            " -0.13063423]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 27 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.22366479 -0.41339436 -0.7281331   0.12163316 -0.38284397 -0.11415637\n",
            "  0.4609506  -0.27928016 -0.29578808 -0.18970229 -0.07111949  0.34426397\n",
            " -0.1152896  -0.26828945 -0.03192364  0.3509361   0.0628447   0.10954902\n",
            " -0.02827416]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 28 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-7.7127911e-02 -3.2590225e-01 -7.2676146e-01  3.2316685e-02\n",
            " -1.4253162e-01 -1.4118448e-01  5.4098922e-01 -3.7792698e-01\n",
            " -5.8376253e-01 -4.7170126e-01 -8.7962799e-02  1.7626543e-01\n",
            " -4.2972185e-02 -7.0092335e-02  8.3880797e-02  5.1158375e-01\n",
            "  4.1046826e-04  1.9621719e-01  1.4798968e-01]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 29 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03768192 -0.5188008  -0.3676303   0.18763062 -0.28692934 -0.11935353\n",
            "  0.14820781 -0.03098778 -0.39407882 -0.0019273  -0.2712877   0.07349008\n",
            "  0.21771437 -0.33816615  0.08395705  0.31699994 -0.11286739  0.13419242\n",
            "  0.03489482]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 30 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2220264  -0.6848126  -0.70383656 -0.00711876 -0.28292543 -0.10453181\n",
            "  0.3277285  -0.09762684 -0.42741725 -0.23004887 -0.14454871  0.20925546\n",
            " -0.17479244 -0.19629627 -0.12697361  0.39889926  0.16341713  0.1171328\n",
            "  0.14639005]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 31 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.16249652 -0.45853552 -0.86396307  0.09583018 -0.12443697 -0.31217697\n",
            "  0.56187373 -0.35142925 -0.68151623 -0.1454287  -0.09593836  0.1300131\n",
            " -0.20420183 -0.25398618  0.11938631  0.5289952   0.07918057  0.22153147\n",
            "  0.11750232]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 32 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02942742 -0.6763491  -0.6215397   0.20639095 -0.19612747 -0.11089054\n",
            "  0.4289526   0.07878913 -0.5472329  -0.0014171  -0.18855184  0.08470759\n",
            "  0.00594966 -0.5560367   0.28758708  0.23762697 -0.04169534  0.2700661\n",
            " -0.32682   ]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 33 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1081441  -0.77535987 -0.62987876  0.13016629 -0.38450408 -0.09750392\n",
            "  0.5165258   0.01319901 -0.4806163  -0.3658183  -0.01916027  0.07201165\n",
            " -0.02978764 -0.39712885 -0.14919828  0.34157443  0.15776676  0.06632522\n",
            " -0.14047962]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 34 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.10141838 -0.32030043 -0.9391722  -0.11171979 -0.2590019  -0.2947673\n",
            "  0.54628897 -0.08946639 -0.12609266 -0.31149456 -0.01350028  0.42984554\n",
            " -0.14296404 -0.13470039  0.18196782  0.58097464 -0.03442826  0.1202369\n",
            "  0.22959185]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 35 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.00450092 -0.4269069  -0.22415511 -0.06000076 -0.18035673 -0.08571769\n",
            "  0.3652046   0.11321561 -0.06185139 -0.12097371 -0.16412127  0.12495662\n",
            " -0.02396874  0.07439007 -0.02515256  0.3519018   0.20531139  0.18932684\n",
            " -0.05110487]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 36 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.10052742 -0.6797709  -0.59411955  0.23399131 -0.2686894  -0.22999112\n",
            "  0.3302596   0.10844979 -0.46934423 -0.14268503 -0.09411778 -0.04573004\n",
            " -0.07632058 -0.49510643  0.05698733  0.2765457  -0.00305049  0.25258592\n",
            " -0.11438198]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 37 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.01274177 -0.8316574  -0.74869925  0.07831839 -0.18579207 -0.16795635\n",
            "  0.6541764   0.19803447 -0.6166017  -0.46001583 -0.04008561  0.13214998\n",
            " -0.12342593 -0.5505663   0.11265955  0.34416306  0.11724117  0.16843572\n",
            " -0.1950578 ]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 38 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.14186718 -0.40218693 -0.21243124  0.1316577  -0.3037546  -0.04459805\n",
            "  0.52223206 -0.11293793 -0.14409006 -0.11187285 -0.17274043  0.12526736\n",
            "  0.0842763  -0.22383744  0.02166231  0.3163899   0.17179938  0.15705618\n",
            " -0.26815504]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 39 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.22218911 -0.518054   -0.5545682   0.00366489 -0.2512046  -0.30361605\n",
            "  0.45920724  0.263459   -0.29189163 -0.02077282 -0.08303612 -0.01694405\n",
            " -0.24729998 -0.18520847  0.06854313  0.5010137  -0.02919695  0.32265332\n",
            "  0.23804423]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 40 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.00538336 -0.6553702  -0.5743834   0.06598583 -0.24883771 -0.16525903\n",
            "  0.55848455  0.4119411  -0.22589551 -0.42015016 -0.03800794 -0.04790712\n",
            " -0.16252798 -0.5217297   0.06030371  0.4012359  -0.04346503  0.28438044\n",
            "  0.08410081]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 41 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.12603751 -0.52563745 -0.23768112  0.05620554 -0.31562632  0.09012461\n",
            "  0.5161788   0.10708068  0.1170509   0.02017497 -0.1613046   0.20217206\n",
            "  0.15833665 -0.25431412  0.08787742  0.3656844   0.29728064  0.31965193\n",
            " -0.15024176]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 42 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.04406831 -0.53049165 -0.3002419   0.07790588 -0.30027074 -0.14649859\n",
            "  0.4524571   0.06604873 -0.14351386 -0.30363193 -0.18310057  0.06561282\n",
            " -0.13302203 -0.14093566 -0.12868029  0.2225597   0.12193578  0.25237605\n",
            " -0.02827985]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 43 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.06610668 -0.6357683  -0.50410455  0.06510486 -0.12267239 -0.3651651\n",
            "  0.46350488  0.5450808  -0.23128071 -0.13556327  0.02619779  0.0108047\n",
            " -0.18139617 -0.5014772  -0.00990296  0.4469763  -0.00850853  0.18917455\n",
            "  0.20128033]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 44 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02217941 -0.69833064 -0.26275524  0.13673334 -0.28330532 -0.07150877\n",
            "  0.49123025  0.49888694 -0.02503633 -0.20500311 -0.25324696 -0.07860217\n",
            " -0.0528739  -0.5475876   0.05597601  0.14373565  0.13435504  0.3413416\n",
            " -0.3391547 ]\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 45 reward=-1 new_state=[1 0 0 1 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08346219 -0.49677774 -0.39341274  0.04548161 -0.18923666 -0.01733499\n",
            "  0.39418027  0.06139648  0.00659114 -0.3274626  -0.14134476  0.05174788\n",
            " -0.17040415  0.04172563  0.0010087   0.3450034   0.11140345  0.1461564\n",
            "  0.24842612]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 46 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.14698339 -0.38286054 -0.43035734  0.12700121 -0.26728272 -0.08077163\n",
            "  0.28879988  0.2274759   0.13750444 -0.18563572 -0.13389827  0.08071367\n",
            "  0.09239472 -0.37293413  0.13737784  0.3496665  -0.01664544  0.26633358\n",
            " -0.15443969]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 47 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.04112014 -0.7298835  -0.2690634  -0.0575329  -0.23850505  0.0839609\n",
            "  0.56471425  0.46224022 -0.03806054 -0.45683253 -0.11340363 -0.0333261\n",
            "  0.11224545 -0.4083766   0.03963972  0.10212059  0.2663172   0.28251788\n",
            " -0.3680689 ]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 48 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.19713485 -0.2963674  -0.28022632  0.00336578 -0.1879511  -0.02407544\n",
            "  0.3293045   0.06337266  0.21677451 -0.05729491 -0.04822015  0.15479991\n",
            "  0.03511299 -0.14041452 -0.02175602  0.380587    0.10110454  0.10727957\n",
            "  0.03172889]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 49 reward=0 new_state=[1 0 0 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.01030813 -0.47632676 -0.336433   -0.03546167 -0.43957725 -0.11341945\n",
            "  0.4183268   0.2622116   0.24228834 -0.43101087 -0.20301312  0.12260251\n",
            " -0.12735419 -0.12042969  0.0195327   0.43315175 -0.09739856  0.22338498\n",
            "  0.1696719 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 50 reward=-1 new_state=[1 0 0 1 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.0634648  -0.6280933  -0.4873487   0.21075164 -0.18145888 -0.22744797\n",
            "  0.6221708   0.71569115  0.28285563 -0.8678838  -0.07095068  0.00670265\n",
            " -0.24862115 -0.34043333  0.13164432  0.07610039  0.13476785  0.01788966\n",
            "  0.05331337]\n",
            "Epsilon reduced to 0.12150000000000001\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10176904 -0.6698667  -0.2592205  -0.04549022 -0.41896144 -0.28685576\n",
            "  0.77441174  0.13500865 -0.04093425 -0.37971893  0.08135518  0.21335062\n",
            " -0.10604643 -0.5125743   0.13095373  0.30964333  0.31790707 -0.07655859\n",
            " -0.2418155 ]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.3803755  -0.61257935 -0.34383598 -0.05832217 -0.25621858 -0.12606972\n",
            "  0.5110594   0.5826471   0.19569014 -0.41302204 -0.0980177  -0.13624515\n",
            " -0.17230998 -0.39890736  0.01823049 -0.2664414   0.41344887  0.13536109\n",
            " -0.30924222]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05085785 -0.5322999  -0.403168   -0.01365077 -0.33122867 -0.24430819\n",
            "  0.7335006   0.79587215  0.11536618 -0.74626917 -0.16934097  0.1477141\n",
            " -0.15661776 -0.30324847  0.31244817  0.1382817   0.42611775  0.06588593\n",
            " -0.26197684]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.11658069 -0.44301802 -0.16301359  0.12816632 -0.30401903 -0.39553386\n",
            "  0.5748265   0.40872604  0.15020238 -0.37642035 -0.16029924  0.2509046\n",
            "  0.1142009  -0.07852434  0.05734424 -0.07488062  0.33019167 -0.06202935\n",
            " -0.29350796]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 1 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1705676  -0.50709075 -0.40601093  0.37982276 -0.26525322 -0.05049277\n",
            "  0.48936206  0.49882945  0.16195707 -0.60007554 -0.28280392  0.20448361\n",
            " -0.43184918 -0.11972862  0.255265    0.3474163   0.24170533  0.12193067\n",
            " -0.02130052]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 1 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.11468329 -0.36473528 -0.25642335  0.32842034 -0.23838699 -0.03619731\n",
            "  0.38152307  0.4032509   0.32354358 -0.28062734 -0.17636064  0.17113039\n",
            " -0.1260897   0.04672171  0.07750124  0.13466029  0.21758036  0.17489101\n",
            " -0.05048677]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 1 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.07895923 -0.5903624  -0.20787258  0.05733597 -0.10551052 -0.3482199\n",
            "  0.7767854   0.67531914 -0.33352926 -0.71636164 -0.19819121  0.23463559\n",
            "  0.09901755 -0.3340524   0.20198469  0.13434808  0.3307535   0.21144605\n",
            "  0.06693894]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 8 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.03099617 -0.28348893 -0.44760725  0.00223849 -0.2664494  -0.08545157\n",
            "  0.44474792  0.00215841  0.1192946  -0.6137582  -0.06143463  0.08375023\n",
            " -0.29918247  0.05409811  0.15651453  0.24536304 -0.02192672  0.38231027\n",
            "  0.18793935]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 9 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.01064592 -0.76823384 -0.37938303  0.10057396 -0.17038345 -0.22953144\n",
            "  0.5038299   0.74445915  0.443049   -0.90606976 -0.51785606  0.1261456\n",
            " -0.07409174 -0.25258     0.17153054  0.06121606  0.34286058  0.22166677\n",
            "  0.02070909]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 10 reward=-1 new_state=[0 0 1 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.15880084 -0.663885   -0.15286024  0.05094178 -0.18444765 -0.48354116\n",
            "  0.74103636  0.65917534 -0.26870042 -0.70838505 -0.30351436  0.22987714\n",
            " -0.16192493 -0.34932867  0.17279664  0.11305394  0.22564605  0.4136499\n",
            "  0.09903266]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.25535282 -0.39041257 -0.41138873  0.02842133 -0.27052245 -0.73209\n",
            "  0.64070207  0.25957742  0.06824164 -0.8541868  -0.2847376   0.11967425\n",
            " -0.12176197 -0.3738024  -0.01847501  0.20100453  0.3932884   0.06602222\n",
            " -0.19865431]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 12 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.07008817 -0.35050195 -0.31557545 -0.14072159 -0.2437499  -0.3673274\n",
            "  0.34831303  0.3887142   0.36419854 -0.67459595 -0.47972667  0.12853847\n",
            " -0.00513582 -0.12166253  0.15364316  0.2931863   0.23697004  0.10851912\n",
            "  0.06666084]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.33456725 -0.222588   -0.46501026  0.04529497 -0.01692838 -0.8874237\n",
            "  0.47699937  0.50197256  0.294266   -1.0141125  -0.5898989   0.05221123\n",
            " -0.05832213 -0.13541208  0.08862662  0.20451185  0.16209526  0.17658736\n",
            "  0.15337437]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.23918135 -0.3130925  -0.35553885  0.09118555 -0.19832364 -0.46068716\n",
            "  0.21819308  0.46831065  0.59166265 -0.86036956 -0.6276424   0.24784434\n",
            " -0.04277115  0.01309551 -0.06843296  0.14301382  0.17521149 -0.04869886\n",
            "  0.1199632 ]\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1986215  -0.1639672  -0.44864997  0.03861538 -0.18150112 -0.598578\n",
            "  0.40158015  0.39155623  0.10909853 -0.80106384 -0.42571092  0.13520047\n",
            " -0.3384951  -0.00522837  0.23365414  0.4155861  -0.00675814  0.22256802\n",
            "  0.22760956]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.31942743 -0.18923718 -0.42538014 -0.14046776 -0.15342744 -0.34924296\n",
            "  0.26128513  0.41536292  0.56712395 -0.7947257  -0.5368763   0.31331936\n",
            "  0.02735123  0.202797   -0.13314229  0.1240508   0.18725203 -0.01598139\n",
            "  0.26769477]\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.13371593 -0.25151786 -0.57868665 -0.13414085 -0.18602067 -0.31604132\n",
            "  0.3112408   0.3215875   0.37615404 -0.72930455 -0.3747529   0.19781937\n",
            " -0.29040176  0.23528805 -0.01766245  0.33420813  0.06689773  0.16830389\n",
            "  0.45797592]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1970002  -0.37479317 -0.39513442 -0.1276832  -0.22514679 -0.38974765\n",
            "  0.2214018   0.47095284  0.46305165 -0.7809522  -0.50940883  0.1688901\n",
            " -0.04752512  0.17035674 -0.2149872   0.273811    0.33520406 -0.1712413\n",
            "  0.3646772 ]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.22298251 -0.29843456 -0.27844772 -0.03253836  0.0498327  -0.05563536\n",
            "  0.22214116  0.28749448  0.40238592 -0.56464016 -0.3011159   0.02845462\n",
            " -0.03561858  0.1534752  -0.15542734  0.23213887  0.32348102  0.2112638\n",
            "  0.09273433]\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03700978 -0.32350802 -0.34264594 -0.05453279 -0.02945404 -0.13711588\n",
            "  0.34519684  0.42460498  0.16366065 -0.64614666 -0.14712423  0.13995227\n",
            " -0.12732333  0.12547214 -0.1532326   0.159604    0.40371925  0.09180135\n",
            "  0.03534157]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.04006191 -0.33640358 -0.29845607 -0.20802952 -0.14250337 -0.3859354\n",
            "  0.29005957  0.41662064  0.21359134 -0.53996843 -0.46217114  0.1479219\n",
            " -0.17625526  0.23329695 -0.10041372  0.3672653   0.14607808 -0.10119389\n",
            "  0.40009156]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.14351802 -0.35164335 -0.38873804 -0.04959475 -0.21809971 -0.42954758\n",
            "  0.26631826  0.54300755  0.43260062 -0.7233597  -0.54032826  0.06867351\n",
            " -0.15688054  0.1416829  -0.22668082  0.30544096  0.2519413  -0.11769591\n",
            "  0.1777154 ]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.10513352 -0.45918208 -0.19905488 -0.20248944 -0.15076497 -0.22073008\n",
            "  0.54844487  0.5561987   0.27426717 -0.66038203 -0.82643694 -0.10210332\n",
            "  0.02265408  0.04429866  0.10867338  0.01500803  0.45286795  0.1637232\n",
            " -0.22776586]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.21267185 -0.32679805 -0.09309832 -0.24289362 -0.15007477 -0.21293409\n",
            "  0.53610826  0.33506277  0.21538273 -0.5913374  -0.58942413 -0.24844219\n",
            "  0.06148097  0.05417498 -0.11905346  0.20039494  0.26713905 -0.01117911\n",
            " -0.1618074 ]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 25 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.11292263 -0.28501928 -0.3564896  -0.25014707 -0.21226467 -0.50063676\n",
            "  0.4632206   0.53542054  0.27739424 -0.6032252  -0.7098144  -0.12288134\n",
            " -0.2947846   0.06106287  0.02996205  0.341942    0.1297973  -0.05869792\n",
            "  0.22025856]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 26 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.20882854 -0.54233044 -0.37280053 -0.14683901 -0.12657969 -0.58010733\n",
            "  0.3836732   0.68637323  0.42043483 -0.7916663  -0.8053788  -0.09237052\n",
            " -0.23729612  0.09884058 -0.20588355  0.18202336  0.5403526  -0.21819001\n",
            "  0.01130558]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.21111785 -0.63102835 -0.3101402  -0.2664093  -0.21946819 -0.42894322\n",
            "  0.47401276  0.5119026   0.422652   -0.7078833  -0.8495242  -0.14648801\n",
            " -0.26452053 -0.00336299 -0.01350435  0.16275291  0.36118183 -0.05583755\n",
            " -0.1268839 ]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.05393003 -0.25450906 -0.56588703 -0.06829859 -0.29209992 -0.425519\n",
            "  0.5248872   0.38585275  0.5046314  -0.7858465  -0.5127039  -0.15732448\n",
            " -0.31759757  0.23498333  0.00632662  0.43045372 -0.02276141 -0.118604\n",
            "  0.02706574]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 29 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1883762  -0.43349302 -0.3271953  -0.21551876 -0.12833145 -0.5369775\n",
            "  0.5593828   0.3670052   0.528815   -0.78401595 -0.7544861  -0.36142316\n",
            " -0.32464105 -0.0311166  -0.3127799   0.25596452  0.48981833 -0.187746\n",
            " -0.3773714 ]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 30 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1833342  -0.22240947 -0.06385054  0.02861229 -0.26405448 -0.632711\n",
            "  0.44420984  0.56935805  0.15570164 -0.8846871  -1.0805304  -0.41156924\n",
            " -0.42557853 -0.22026275  0.08736243  0.08779801  0.29872257 -0.10554257\n",
            " -0.3495121 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 31 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.00251551 -0.31886566 -0.46405566 -0.15147927 -0.1807212  -0.6736043\n",
            "  0.53127056  0.43148476  0.4839331  -0.75130945 -1.028867   -0.22112998\n",
            " -0.4408401   0.04612872 -0.08476018  0.32753968  0.09759365  0.00483721\n",
            "  0.08914344]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 32 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.22189216 -0.37948054 -0.31021243 -0.31613645 -0.16036277 -0.6260073\n",
            "  0.3714222   0.5877992   0.58362395 -0.77885187 -1.0843824  -0.35611704\n",
            " -0.34705392  0.14730535 -0.12072311  0.2359652   0.31967437 -0.23851717\n",
            " -0.00284833]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 33 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02441036 -0.42060524 -0.4590095   0.11720468 -0.27440703 -0.4638534\n",
            "  0.3520923   0.5740286   0.19595861 -0.97437245 -0.81135845 -0.49507785\n",
            " -0.4615636  -0.05305585  0.00649316  0.29249725  0.21082373 -0.14024054\n",
            " -0.08004685]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 34 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.00897783 -0.53724295 -0.37474284  0.01177681 -0.33225897 -0.65410084\n",
            "  0.2864567   0.69482857  0.5550846  -0.7183892  -0.99796367 -0.47529116\n",
            " -0.40407056  0.17041528 -0.07723835  0.3232263   0.10511595 -0.39104363\n",
            "  0.14517239]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 35 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.18865064 -0.13740283 -0.39143178 -0.13563211 -0.29057187 -0.38970274\n",
            "  0.16371     0.4552953   0.6098186  -0.55148697 -0.786732   -0.22365391\n",
            " -0.3211213   0.30007678 -0.10839264  0.33710515  0.11315355 -0.05859357\n",
            "  0.03616766]\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.07129227 -0.4052823  -0.44642252 -0.09122742 -0.23788683 -0.36975813\n",
            "  0.24535203  0.55338264  0.38467935 -0.58653367 -1.002046   -0.40910697\n",
            " -0.5263441   0.37133825  0.01574495  0.19312981  0.360775   -0.2848768\n",
            "  0.09912344]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 1 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.09767557 -0.5278353  -0.5130724   0.03277832 -0.33606926 -0.5262722\n",
            "  0.2821868   0.5564001   0.42033067 -0.651115   -1.075155   -0.5368393\n",
            " -0.29432616 -0.00311315 -0.12278906  0.28915176  0.22343507 -0.2992386\n",
            " -0.07120743]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 1 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.07965384 -0.20886607 -0.47753477 -0.14514731 -0.24316446 -0.21769847\n",
            "  0.32779786  0.35937563  0.3280607  -0.5768269  -1.0043676  -0.3528985\n",
            " -0.04053366 -0.03336574  0.00128283  0.16439074  0.21349354  0.15328524\n",
            " -0.18418762]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 1 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.13168538 -0.44325927 -0.34216332 -0.09103833 -0.12789927 -0.12864539\n",
            "  0.2438853   0.33587068  0.375351   -0.5944154  -1.0299437  -0.5257221\n",
            " -0.22334562 -0.16777033 -0.03776233  0.23237759  0.27305967 -0.09700141\n",
            " -0.29760763]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 1 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.04552252 -0.55030686 -0.54118514 -0.01329628 -0.27588597 -0.3984009\n",
            "  0.4932344   0.6517972   0.3470751  -0.6992636  -1.3554206  -0.6576769\n",
            " -0.23200646 -0.18759799  0.12002049  0.05887181  0.3535418  -0.19497885\n",
            " -0.46798298]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 1 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.05480301 -0.3404313  -0.59035754 -0.1926866  -0.2836045  -0.33328575\n",
            "  0.25789532  0.63922423  0.39596227 -0.4796679  -1.0115216  -0.39817667\n",
            " -0.1120276   0.0845773  -0.03737256  0.28135267  0.240649   -0.03832402\n",
            "  0.13547197]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 1 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.01161805 -0.13897565 -0.5512216  -0.11393189 -0.33165905 -0.22623155\n",
            "  0.20892508  0.2857879   0.4356718  -0.49452412 -0.720474   -0.18955031\n",
            " -0.3971384   0.32815713 -0.04060599  0.31675705  0.078811   -0.10040823\n",
            "  0.14367855]\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 43 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.0870319  -0.49060285 -0.76478565 -0.07601731 -0.17092787 -0.7079006\n",
            "  0.12427964  0.8767755   0.3901389  -1.1114506  -1.8305397  -0.5563832\n",
            " -0.67625606 -0.2809027   0.13584976 -0.05356832  0.32066733 -0.38353002\n",
            " -0.21049155]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 44 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.26506725 -0.48285392 -0.5094478  -0.15985923 -0.29335973 -0.5712874\n",
            "  0.22142416  0.8552391   0.49475017 -0.3137604  -1.3012127  -0.33074424\n",
            " -0.6701844   0.17286584 -0.02371119  0.3329401   0.46202636 -0.06134567\n",
            " -0.01990311]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 45 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.00437928 -0.30020684 -0.6001377   0.08101886 -0.35489926 -0.44911352\n",
            "  0.01650117  0.2940376   0.45852077 -0.6572471  -1.0950499  -0.22717407\n",
            " -0.49193597  0.13768873 -0.02216161  0.41788065 -0.01521634 -0.21313842\n",
            "  0.10755827]\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 46 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.23758565 -0.46989074 -0.6557718  -0.29318377 -0.19982274 -0.48899943\n",
            "  0.03968889  0.7793559   0.5989962  -0.63531643 -1.3182306  -0.20864794\n",
            " -0.3448112   0.00550768 -0.17654592  0.28373748  0.3955938  -0.3826875\n",
            " -0.16117938]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 10\n",
            "\n",
            "Step 47 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.32961294 -0.7204799  -0.938287   -0.09578768 -0.09927124 -0.55593437\n",
            "  0.16636994  0.80831796  0.5337752  -0.74136984 -1.3565894  -0.0429502\n",
            " -0.67859036  0.10442408 -0.02705012  0.16648813  0.70703584 -0.32521245\n",
            " -0.40169972]\n",
            "\n",
            "Taking action 11 from 7\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.15122464 -0.11301665 -0.4407312  -0.11010256 -0.22617537 -0.16462463\n",
            "  0.06675495  0.30305582  0.35496524 -0.33378446 -0.7595563   0.02570056\n",
            " -0.38248402  0.25150856 -0.06804425  0.4234978   0.171882    0.06419057\n",
            "  0.01485244]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.22738259 -0.33281943 -0.5777971   0.10147147 -0.24724905 -0.6512009\n",
            "  0.06749804  0.4566474   0.6064984  -0.80655855 -1.3162415  -0.28526166\n",
            " -0.33749557 -0.0429087  -0.14336064  0.24251013  0.27447078 -0.22361836\n",
            " -0.22075215]\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 0 1 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03300215 -0.41117603 -0.87659425 -0.16654555 -0.08119213 -0.7206546\n",
            "  0.25613922  0.56470287  0.07207149 -0.94508094 -1.7653166  -0.40220433\n",
            " -0.7747731  -0.05818918  0.05651795 -0.07513691  0.30092928 -0.10870197\n",
            " -0.25135952]\n",
            "Epsilon reduced to 0.10935000000000002\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.04607693 -0.33247337 -0.41444576  0.09787185 -0.2651695  -0.3546751\n",
            "  0.08001628  0.40776396  0.34689578 -0.60760134 -1.0858935  -0.2287327\n",
            " -0.39508495 -0.09955785 -0.04033505  0.11451713  0.10912546 -0.17207113\n",
            " -0.15649693]\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.19303498 -0.67217386 -0.5017421   0.14341062 -0.10077046 -0.69015795\n",
            " -0.1071545   0.94261116  0.4652315  -0.84133846 -1.1557077  -0.12629862\n",
            " -0.26198462 -0.30504388 -0.22000387 -0.22012931  0.12368283 -0.4156775\n",
            " -0.19208199]\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.0191933  -0.54215384 -0.55444074  0.21586682 -0.2285639  -0.36012048\n",
            " -0.04793778  0.542924    0.29880306 -0.7941843  -1.001112   -0.16128957\n",
            " -0.42046624 -0.01099044 -0.13618246  0.12361283  0.03540833 -0.22735983\n",
            " -0.21055198]\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.09548774 -0.609235   -0.268887    0.31432983 -0.04004919 -0.76690865\n",
            "  0.05850767  0.7200803   0.3762373  -0.7943414  -1.2756212  -0.06254195\n",
            " -0.2926961  -0.03422013 -0.24245557 -0.40965232  0.06980547 -0.40058264\n",
            " -0.3051184 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.01740526 -0.17697687 -0.45283931  0.03668812 -0.13171852 -0.66649145\n",
            "  0.0985295   0.5078589   0.43030205 -0.49460366 -1.0733753  -0.05742983\n",
            " -0.10367824  0.27074906 -0.15680252  0.02819774  0.04222158 -0.09393371\n",
            " -0.31015563]\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0555531  -0.53624475 -0.2807223   0.20768304 -0.25579974 -1.0662544\n",
            "  0.1659858   0.7509604  -0.17719981 -0.8505771  -1.2094177  -0.14701478\n",
            " -0.27525863 -0.02688154 -0.02224033 -0.05678881  0.07034625 -0.29429093\n",
            " -0.36100623]\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.02485717 -0.17270035 -0.45192063  0.14738443 -0.17707787 -0.72443426\n",
            "  0.13146694  0.5621122   0.56007624 -0.6573988  -1.1470152  -0.08653668\n",
            " -0.23414034  0.31803635 -0.28034157  0.08520473 -0.00459939 -0.18322775\n",
            " -0.27046946]\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.18143556 -0.75547314 -0.6411692   0.17013791 -0.22769293 -1.016207\n",
            " -0.07930955  1.07197     0.17974387 -0.8272163  -1.1393608  -0.00533409\n",
            " -0.25243235  0.19417751 -0.07354194 -0.08179809  0.06921245 -0.50370735\n",
            " -0.18911426]\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.03722023 -0.4858928  -0.41827193  0.13291383 -0.2719739  -0.6906597\n",
            "  0.05941419  0.5855125   0.07809857 -0.7887025  -1.2298911  -0.12616885\n",
            " -0.22445974  0.38946134  0.01412354 -0.10659801  0.06345803 -0.23545796\n",
            " -0.2747018 ]\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.04944777 -0.4907903  -0.2488475   0.1812652  -0.25038883 -0.87921685\n",
            "  0.0959722   0.668993   -0.312798   -0.83205926 -1.2764564  -0.16883361\n",
            " -0.24711767  0.30333716  0.06588706 -0.06330416  0.09506626 -0.24858035\n",
            " -0.18574059]\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.00253717 -0.5119099  -0.55908287  0.0165472  -0.27065092 -0.7222261\n",
            "  0.1365432   0.6846744   0.3296328  -0.9262273  -1.269323   -0.12849474\n",
            " -0.2929584   0.65658647 -0.18795365  0.09988654 -0.10075173 -0.36679307\n",
            " -0.15139623]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.21601559 -0.5868299  -0.59130454 -0.04094096 -0.15697643 -0.74175745\n",
            "  0.02498069  0.7306681   0.40497556 -0.8846325  -1.3095853   0.07416722\n",
            " -0.15555933  0.695203   -0.18650836  0.0281933   0.25885668 -0.40661076\n",
            " -0.11960357]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.09834823 -0.36073384 -0.386386    0.07060918 -0.24158333 -0.5668173\n",
            "  0.1669349   0.26293284  0.37219697 -0.6374872  -1.0647724  -0.02412559\n",
            " -0.26680726  0.5109373  -0.11721782 -0.03844868  0.2119137  -0.3098351\n",
            " -0.14955996]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2120722  -0.53215533 -0.6001295  -0.0386107  -0.20382431 -0.6911288\n",
            " -0.0124197   0.58347565  0.520128   -0.7658147  -1.0693356   0.14294611\n",
            " -0.1994453   0.59581083 -0.24915673 -0.09596655  0.21458727 -0.30838737\n",
            " -0.11779831]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.08744194 -0.2831828  -0.49405473 -0.09830178 -0.16260426 -0.6590794\n",
            "  0.2849891   0.17765309  0.5920193  -0.6186562  -1.2241123   0.19173694\n",
            " -0.13142547  0.7047839  -0.207055   -0.07792643  0.06464566 -0.1784021\n",
            " -0.19012931]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.00269881 -0.31835285 -0.41037002  0.01769    -0.23388578 -0.9092698\n",
            "  0.2767376   0.38671032  0.52471834 -0.7470794  -1.1677756   0.28666636\n",
            " -0.13890469  0.7903445  -0.30593553 -0.0646259   0.07682768 -0.10449699\n",
            " -0.18553726]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.16229893 -0.41512114 -0.6232205  -0.12196717 -0.22427975 -0.61304164\n",
            "  0.05381976  0.47055158  0.8233235  -0.8285474  -1.1347977   0.20941249\n",
            " -0.22774532  0.8291553  -0.32393372  0.00477539  0.09747486  0.09343999\n",
            " -0.06923487]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03711526 -0.35978752 -0.36990944  0.07912053 -0.20372196 -0.8369076\n",
            "  0.23276158  0.37218052  0.47830597 -0.6963644  -1.1704025   0.28123078\n",
            " -0.14461479  0.84072924 -0.2529278  -0.10045286  0.08143362  0.08090906\n",
            " -0.14189723]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.08316371 -0.4081421  -0.57054055 -0.01630398 -0.3042572  -0.5047558\n",
            "  0.05668815  0.29872844  0.4678992  -0.64974856 -0.8512571  -0.04419787\n",
            " -0.3521527   0.70985806 -0.3425079   0.19100304  0.14814255  0.26696417\n",
            " -0.10972964]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.18467201 -0.4492982  -0.5680633   0.0228582  -0.20867169 -0.70463926\n",
            "  0.06902707  0.6001088   0.4269162  -0.60845333 -0.94036174  0.15192056\n",
            " -0.1668276   0.7363985  -0.25250006  0.03035179  0.13335721  0.32484564\n",
            " -0.17089401]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.11492439 -0.2865356  -0.49968863 -0.13667262 -0.16395928 -0.6830726\n",
            "  0.2318713   0.17879324  0.6122932  -0.70277655 -1.2640207   0.19821297\n",
            " -0.12068579  0.89425385 -0.1795891  -0.16631038  0.11500384  0.50534487\n",
            " -0.08391857]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.05113601 -0.33063048 -0.39860874  0.04544595 -0.2242186  -0.70422596\n",
            "  0.21786655  0.42740744  0.36394596 -0.6578856  -1.1796643   0.20293751\n",
            " -0.2007433   0.855902   -0.2635406  -0.02544536  0.07446527  0.65025884\n",
            " -0.0674665 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.16469766 -0.38277674 -0.5186197  -0.06292643 -0.15720768 -0.59869444\n",
            "  0.08323941  0.4160909   0.6926134  -0.70042783 -1.0874441   0.1687699\n",
            " -0.21632895  0.9239125  -0.33391398 -0.05238485  0.11499343  0.7253067\n",
            " -0.05453225]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.06851171 -0.34701934 -0.38754353  0.02186705 -0.19972244 -0.78796685\n",
            "  0.1165776   0.42705348  0.36478332 -0.6783264  -1.2040031   0.17139265\n",
            " -0.0635097   1.0072553  -0.19692396 -0.19166033  0.20964295  0.9862503\n",
            "  0.01382743]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 25 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.17768796 -0.1974488  -0.5554885  -0.10979638 -0.21821126 -0.63562995\n",
            "  0.10900261  0.299898    0.7012122  -0.6772384  -1.1311668   0.2576772\n",
            " -0.07300805  0.68583775 -0.19459681  0.01669639 -0.01723828  0.7872032\n",
            " -0.02766067]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 26 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.00167289 -0.07748629 -0.60572237 -0.10382976 -0.26363114 -0.565529\n",
            "  0.14264293  0.38408494  0.5513436  -0.413911   -0.9326399   0.2816412\n",
            " -0.14596976  0.5766779  -0.16693124  0.17617169 -0.11606315  0.91180205\n",
            " -0.02442924]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1072943  -0.24409075 -0.36989117  0.24632792 -0.23461564 -0.5269385\n",
            "  0.078467    0.39420864  0.44521338 -0.71953547 -1.1407243   0.10668401\n",
            " -0.43406278  0.56614506 -0.11962287  0.33966106  0.04307263  0.7825929\n",
            " -0.06028574]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.09510846 -0.20298705 -0.48475295  0.18643501 -0.2759577  -0.36934444\n",
            " -0.03783537  0.36876136  0.6048488  -0.4484707  -0.94887835  0.4155658\n",
            " -0.35760996  0.48741055  0.00523245  0.27311894 -0.07899464  0.7777682\n",
            " -0.12152221]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.18737705 -0.14892785 -0.61811024  0.12321104 -0.1943404  -0.5725757\n",
            "  0.06523397  0.35683236  0.7058231  -0.7942636  -1.1714337   0.3631687\n",
            " -0.26161826  0.63233936 -0.2022968   0.06415509  0.00222097  1.0387311\n",
            " -0.05463225]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.21170257 -0.21012725 -0.54912925  0.11916739 -0.15854405 -0.47405916\n",
            "  0.02105395  0.43691215  0.68910146 -0.7221663  -1.2468625   0.50291264\n",
            " -0.26299086  0.72506833 -0.19261111  0.10411558  0.01450252  1.0844898\n",
            " -0.03348508]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.14031994 -0.08427761 -0.5454842   0.20367008 -0.25403807 -0.25324893\n",
            "  0.05620063  0.2756734   0.43764278 -0.45531836 -0.8667734   0.367245\n",
            " -0.42401108  0.36224264 -0.0204036   0.30332825 -0.137761    0.9182419\n",
            " -0.15242295]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.11977725 -0.32098967 -0.532112    0.17926252 -0.17680484 -0.23864083\n",
            " -0.01954885  0.45575386  0.48103264 -0.75461775 -1.2407837   0.4748769\n",
            " -0.619203    0.4997401  -0.20074676  0.302736    0.05351928  1.1655241\n",
            " -0.01100163]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.08333726 -0.17381755 -0.5216157   0.20279132 -0.21867554 -0.2535667\n",
            "  0.11882196  0.4710538   0.41352192 -0.53504467 -1.051376    0.49539378\n",
            " -0.4958112   0.4056385  -0.04686916  0.32352477  0.00590938  1.1312245\n",
            " -0.18713214]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.24659427 -0.34583384 -0.66329074  0.0229462  -0.13494074 -0.46791282\n",
            "  0.03404471  0.3731001   0.80055094 -0.9534375  -1.579556    0.6557225\n",
            " -0.43265837  0.85144573 -0.26310372  0.07465371  0.16201007  1.421607\n",
            " -0.16590299]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.23327722 -0.36297676 -0.63407755  0.03975615 -0.10866769 -0.35802394\n",
            " -0.00182531  0.70441127  0.474708   -0.846897   -1.5502511   0.7585873\n",
            " -0.23542143  0.8842019  -0.03051789  0.03047296  0.12014771  1.5390116\n",
            " -0.14038986]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.10284872 -0.07961661 -0.5525208   0.19258939 -0.2699162  -0.20411974\n",
            "  0.05688182  0.22739795  0.43422657 -0.50506175 -0.89066786  0.6576512\n",
            " -0.40697438  0.42113608 -0.01612644  0.26847658 -0.13939881  1.0095965\n",
            " -0.15212113]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1260039  -0.31000838 -0.3883815   0.22816022 -0.17049874 -0.27298537\n",
            "  0.06771066  0.46701443  0.3458462  -0.75039804 -1.3029733   0.70400447\n",
            " -0.496024    0.764856   -0.14695542  0.2884391   0.16633037  1.441086\n",
            " -0.13110183]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.10062072 -0.32514918 -0.6281535   0.05209589 -0.03969731 -0.20906694\n",
            "  0.00666897  0.51550126  0.47916704 -0.5939803  -1.515066    0.91579014\n",
            " -0.48206782  0.8623703  -0.13700938  0.14417894  0.173445    1.6981555\n",
            " -0.22069687]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.09323528 -0.4306852  -0.66672546  0.04440502 -0.25857404 -0.19211663\n",
            " -0.08789231  0.6684044   0.53512573 -0.9019742  -1.5499978   0.9323452\n",
            " -0.5790254   0.9797089  -0.2254962   0.24882081  0.08401158  1.6134274\n",
            "  0.01044269]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12 from 6\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2585478  -0.04882168 -0.5448207  -0.02601729 -0.2762506  -0.13599594\n",
            " -0.0648687   0.42204174  0.60147506 -0.7273408  -1.2853879   1.1298672\n",
            " -0.4169594   0.8597157  -0.18707456  0.35544997 -0.0994278   1.5464525\n",
            "  0.01397054]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2380344  -0.24565326 -0.6839782  -0.13103369 -0.23159231  0.00197811\n",
            " -0.05442246  0.5088731   0.4261612  -0.66593057 -1.5337547   0.9971598\n",
            " -0.6586681   0.82273334 -0.25802875  0.08631491  0.22106935  1.7048694\n",
            " -0.07359034]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.31922364 -0.50946623 -0.78598505 -0.09671935 -0.0177624  -0.17892177\n",
            "  0.13715656  0.51096714  0.296389   -1.1233103  -1.8405007   1.266633\n",
            " -0.77986234  1.0852871  -0.4800667   0.13728367  0.5499209   2.0767546\n",
            " -0.26619154]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.17966601 -0.35871238 -0.79471654 -0.12369008 -0.39808527  0.06185834\n",
            " -0.05450805  0.7885375   0.3718101  -0.82880807 -1.7780665   1.1601785\n",
            " -0.66658247  0.79866236 -0.10442828  0.14344966  0.08871745  1.741567\n",
            "  0.08968031]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.25950822 -0.08999406 -0.75001687 -0.20057777 -0.24788667 -0.2508683\n",
            " -0.02174341  0.53566     0.57777965 -0.8831328  -1.5239084   1.508279\n",
            " -0.30977616  0.9291106  -0.23254135  0.10113374  0.16705126  1.8746402\n",
            "  0.08822758]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.37017807 -0.2834853  -0.84254616 -0.23820832 -0.21737696 -0.19814701\n",
            "  0.05568916  0.34323943  0.5618356  -0.9645135  -1.7536099   1.6757112\n",
            " -0.45366293  1.1362314  -0.5896071  -0.02542745  0.43537226  2.123272\n",
            " -0.12961821]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.27311933 -0.3171952  -0.7978777  -0.2025482  -0.14205915 -0.07896196\n",
            " -0.02109293  0.42966118  0.43452793 -0.7815056  -1.7653544   1.312161\n",
            " -0.6812598   1.1128588  -0.27792028 -0.05299649  0.2460846   2.074387\n",
            " -0.03985045]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 47 reward=-1 new_state=[0 0 1 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.34656587 -0.22864579 -0.47942328 -0.16564626 -0.05600079  0.05629826\n",
            " -0.14699645  0.5209406   0.17043468 -0.9584927  -1.697738    1.5283078\n",
            " -0.5246901   0.9774128  -0.23919585  0.20269981  0.26097718  1.8852596\n",
            "  0.01316344]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 0 1 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.27734336 -0.05263636 -0.7287137  -0.17251475 -0.03518702  0.00266721\n",
            " -0.05452748  0.2900835   0.2779781  -0.590119   -1.3486494   1.3608739\n",
            " -0.37950695  0.7683184  -0.24924545  0.12558618  0.21549572  1.6373166\n",
            " -0.14094882]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 1 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.23869291 -0.34504035 -0.7334151  -0.03866468 -0.00436443 -0.06129535\n",
            "  0.0042498   0.7786507   0.32630506 -0.81988674 -1.4157221   1.6635438\n",
            " -0.31683913  1.0739785  -0.22503902  0.09005328  0.31281328  2.014439\n",
            " -0.2278166 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 1 1 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.21481487 -0.0901776  -0.57537246 -0.00382915 -0.05452069  0.12602496\n",
            " -0.18151009  0.4949657   0.28041178 -0.68649614 -1.1259289   1.243843\n",
            " -0.3928762   0.97535694  0.0183911   0.24344414 -0.09377795  1.6681064\n",
            "  0.0824419 ]\n",
            "Epsilon reduced to 0.09841500000000002\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.15930578 -0.6155641  -0.7748996  -0.1560415  -0.3117957  -0.3704403\n",
            " -0.30991787  1.2837446   0.1458882  -0.76463103 -1.4204795   1.8218915\n",
            " -0.27651665  1.0451915   0.1782025   0.18310982 -0.03315426  2.2971203\n",
            " -0.12658986]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 2 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.18772478 -0.5759211  -0.43731344 -0.134403   -0.08254449 -0.02220168\n",
            "  0.0769037   0.38937867  0.14322853 -0.49745268 -1.3102043   1.2777593\n",
            " -0.24324486  0.6885634  -0.04946718 -0.06445748  0.18938503  1.5165689\n",
            " -0.433127  ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 3 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.12345374 -0.20198412 -0.55548054 -0.23772141 -0.2102711  -0.34155053\n",
            "  0.05052581  0.5009512  -0.13975035 -0.5287789  -1.0788693   1.2966799\n",
            " -0.18145134  0.70709443 -0.09806308 -0.03424247  0.09558604  1.4730132\n",
            " -0.22802845]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 4 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02939249 -0.2708005  -0.4484727  -0.2376377  -0.21900077  0.11769595\n",
            "  0.23596272  0.3547331   0.38204497 -0.6869643  -1.3461405   1.3228657\n",
            " -0.28305444  0.7964149  -0.21738163 -0.27551416  0.09126766  1.3458349\n",
            " -0.21350077]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 5 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 7.9938531e-02 -2.8154472e-01 -2.0429438e-01  3.3173565e-02\n",
            "  3.8298115e-02 -2.5857799e-02  2.5156839e-04  4.9751246e-01\n",
            "  1.8911797e-01 -5.6486654e-01 -1.0019251e+00  1.2980345e+00\n",
            " -1.6802782e-01  7.6879215e-01 -6.8230912e-02 -2.2181240e-01\n",
            " -6.8614133e-02  1.4068391e+00 -1.2807932e-02]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 6 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.07519833 -0.52725905 -0.54829127 -0.22816932 -0.16694087  0.04644159\n",
            "  0.10805838  0.298075    0.1874769  -0.6296807  -1.1884487   1.3780156\n",
            " -0.10750954  0.9019573  -0.04642174 -0.01712586  0.10971276  1.3468204\n",
            " -0.40144482]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 7 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.0554921  -0.3709911  -0.5055485  -0.22084686 -0.26703158 -0.30291253\n",
            " -0.00637912  0.6442163   0.06366004 -0.5934564  -1.1550823   1.422772\n",
            " -0.3068434   1.106021   -0.01132727  0.10721343  0.0438844   1.5260259\n",
            "  0.05690046]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16 from 10\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 9.8038867e-02 -2.5113401e-01 -3.0341583e-01 -7.7603236e-02\n",
            " -2.6136485e-01  2.3544022e-01  2.1648774e-01  3.0011648e-01\n",
            "  4.2633829e-01 -6.3899338e-01 -1.3398371e+00  1.2522653e+00\n",
            " -3.6781469e-01  1.0855497e+00 -2.4156742e-01 -2.7464408e-01\n",
            "  6.2262462e-03  1.3861501e+00 -4.9026124e-04]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.13343161 -0.19810078 -0.37330392 -0.06889489 -0.04264339 -0.09586792\n",
            "  0.07080945  0.5409373   0.23643492 -0.53695047 -1.0745192   1.3790857\n",
            " -0.3551447   1.0929716  -0.3560217  -0.21972635  0.02773219  1.7190788\n",
            "  0.23744455]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.06453485 -0.5597686  -0.42651364 -0.05370113 -0.21843432 -0.09404052\n",
            " -0.04554891  0.34559047  0.4161298  -0.5548976  -1.1803738   1.4729004\n",
            " -0.13292599  1.169628   -0.22496581  0.00680981  0.14680536  1.5385841\n",
            "  0.00312726]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.34340253 -0.40862197 -0.69469357 -0.19612148 -0.11347836 -0.36177316\n",
            " -0.04779311  0.6584224   0.08571389 -0.4956904  -1.4631585   1.8907268\n",
            "  0.04534443  1.6104801  -0.01436441  0.14433776  0.15590212  2.0675898\n",
            "  0.07737505]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.17324962 -0.64475626 -0.47941107 -0.17527074 -0.15062945 -0.0960269\n",
            "  0.05898408  0.35774174  0.14499848 -0.90527064 -1.8312289   2.2127097\n",
            " -0.07634555  1.4379418  -0.18662634  0.08988594  0.42209473  2.2642152\n",
            "  0.03889078]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.3736159  -0.33550754 -0.8010392  -0.07566845 -0.12862313 -0.43890154\n",
            "  0.07009674  0.7498858   0.10396273 -0.6713401  -1.5959073   2.0981562\n",
            "  0.02535637  1.8689442  -0.2457504   0.04272627  0.34508508  2.3669538\n",
            "  0.17492309]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.2741816  -0.6707034  -0.36879385 -0.00577451 -0.15435748 -0.00630852\n",
            "  0.0293975   0.22621185  0.32290643 -0.85396475 -1.7551094   2.0960915\n",
            " -0.17549878  1.5005805  -0.24913588 -0.0610658   0.45992205  2.1706588\n",
            "  0.06223075]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.25485244 -0.19393685 -0.72611314 -0.15644462 -0.28054684 -0.33173007\n",
            " -0.05254908  0.74386615  0.07907557 -0.6062349  -1.4700216   1.9275193\n",
            " -0.30566493  1.9097943  -0.24699116 -0.14145352  0.43275765  2.3556697\n",
            "  0.42196292]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.19807017 -0.36219347 -0.7404655  -0.18876582 -0.29543224 -0.26436952\n",
            "  0.05459185  0.7283668   0.72420377 -1.1169617  -2.029967    2.6639125\n",
            " -0.23856047  2.3475533  -0.37531695 -0.23340178  0.45845205  2.4384432\n",
            "  0.58057463]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.272375   -0.41267127 -0.79257727 -0.30138928 -0.2427086  -0.23550959\n",
            "  0.02651981  0.39520568 -0.07264741 -0.5931285  -1.3166227   1.8281431\n",
            " -0.23131499  1.9389122  -0.07410545  0.09151904  0.49323297  2.1488564\n",
            "  0.2694895 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.00691093 -0.49906638 -0.631432   -0.30965    -0.22165842 -0.06952259\n",
            "  0.19281165  0.37628826  0.16018699 -0.761689   -1.3368864   1.6580205\n",
            " -0.12160799  1.867622   -0.13776605  0.05643447  0.60981303  1.9323167\n",
            "  0.06315326]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.15455538 -0.46463177 -0.6500602  -0.29654366 -0.27807412 -0.33627126\n",
            "  0.01858222  0.781231    0.14530416 -0.6205221  -1.3144221   1.7523314\n",
            " -0.21396859  1.9739197  -0.15051913  0.25080618  0.6845567   2.0027053\n",
            "  0.33801708]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 20 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.13252425 -0.50415784 -0.7202449  -0.17705038 -0.33261374  0.05660011\n",
            "  0.17122151  0.28596315  0.3467253  -0.44897726 -1.1600418   1.814402\n",
            " -0.16310449  1.2902043  -0.1225784   0.25751007  0.5007091   1.6404235\n",
            "  0.19025281]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 21 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.12777787 -0.32691643 -0.6629171  -0.25165403 -0.23967758 -0.04299951\n",
            "  0.21759792  0.21866788  0.2044454  -0.4327502  -1.1081284   1.7027706\n",
            "  0.05273279  1.2152357  -0.11824372  0.17372838  0.5186355   1.5279564\n",
            "  0.04434858]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 22 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.12833846 -0.44519016 -0.59693986 -0.17333367 -0.24449125  0.05617729\n",
            "  0.2590392   0.22592448  0.25498465 -0.41219893 -1.19554     1.7409543\n",
            " -0.07111282  1.4108794  -0.05677496  0.11101023  0.5339455   1.6917942\n",
            "  0.05892507]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 23 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1279805  -0.3374149  -0.7404821  -0.33436105 -0.3232437   0.02680451\n",
            "  0.23972028  0.16562948  0.22325027 -0.5105385  -1.1805742   1.8344134\n",
            " -0.00705955  1.5448138  -0.11683023  0.21479727  0.5295488   1.9730005\n",
            "  0.18542582]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 24 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.13437217 -0.44971678 -0.612174   -0.1724402  -0.2538957   0.0559396\n",
            "  0.261836    0.23710462  0.26004228 -0.4289639  -1.2327656   1.8517387\n",
            " -0.08379909  1.5951592  -0.06110626  0.11373722  0.5797727   1.9041727\n",
            "  0.10071079]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 25 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.21536537 -0.47979867 -0.69543356 -0.23051324 -0.2787648   0.01172682\n",
            "  0.21502268  0.20400956  0.24194258 -0.48367956 -1.222638    1.9342647\n",
            " -0.08014483  1.8218229  -0.08379783  0.11565018  0.54438084  2.023198\n",
            "  0.17401594]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 26 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.71112105e-01 -4.23922569e-01 -7.25638568e-01 -2.23905578e-01\n",
            " -2.93426096e-01 -1.88524416e-03  2.34933466e-01  1.86632276e-01\n",
            "  2.01318800e-01 -4.78474975e-01 -1.18973160e+00  1.98205936e+00\n",
            " -6.38755709e-02  1.86574984e+00 -1.00865036e-01  1.47879139e-01\n",
            "  6.02209568e-01  2.11525655e+00  1.86513796e-01]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 27 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1029855  -0.07435957 -0.6948425  -0.26588595 -0.38230863 -0.04398681\n",
            "  0.21924083  0.27556497  0.40783536 -0.7003544  -1.3239073   2.0366838\n",
            " -0.20406227  2.0158587  -0.2743072  -0.02679037  0.6134922   2.1049397\n",
            "  0.46622294]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 28 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.0883468e-01 -4.1649786e-01 -7.0549279e-01 -2.3644803e-01\n",
            " -3.0932185e-01 -1.7049471e-03  3.2358125e-01  2.7727520e-01\n",
            "  3.2054287e-01 -5.2683461e-01 -1.3371940e+00  2.1221478e+00\n",
            " -1.2343119e-01  2.3213856e+00 -1.4334375e-01  1.6131167e-01\n",
            "  6.7441934e-01  2.4180877e+00  1.8336669e-01]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 29 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.11404708 -0.46287033 -0.8381051  -0.16254918 -0.32277516  0.02490366\n",
            "  0.2221217   0.36074924  0.34238896 -0.5547481  -1.3176694   2.3866384\n",
            " -0.1214899   2.3855996  -0.16910186  0.3157956   0.8188615   2.438921\n",
            "  0.33823583]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 30 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.13462959 -0.21752818 -0.72425234 -0.28922242 -0.4748787   0.3018389\n",
            "  0.3380296   0.49453777  0.47261465 -0.6479118  -1.6459872   2.3404694\n",
            " -0.414384    2.8998904  -0.21511228  0.03242399  0.76131886  2.5882616\n",
            "  0.66962934]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 31 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.17704222 -0.34644702 -0.90683144 -0.21147175 -0.2824192  -0.00419819\n",
            "  0.33085227  0.2722782   0.27564156 -0.6110421  -1.3329703   2.5023408\n",
            " -0.0699139   2.757393   -0.26264304  0.23681393  0.7881163   2.6947408\n",
            "  0.24704403]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 32 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.24914719 -0.37705618 -0.6187362  -0.2614867  -0.3748519   0.11807928\n",
            "  0.27274808  0.47196358  0.37676528 -0.5516306  -1.6469083   2.567224\n",
            " -0.09510195  3.0715325  -0.07881849  0.20850241  0.7596439   2.7478518\n",
            "  0.46418896]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 33 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.9223219e-01 -3.4456024e-01 -7.7512878e-01 -2.9622376e-01\n",
            " -3.4568629e-01  1.3911363e-03  2.2106367e-01  3.0618832e-01\n",
            "  2.2546963e-01 -5.8305955e-01 -1.4551936e+00  2.7138543e+00\n",
            " -2.1868784e-02  3.0082705e+00 -1.2332529e-01  2.5209004e-01\n",
            "  7.9230636e-01  2.6952713e+00  3.7546423e-01]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 34 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.19960624 -0.4653973  -0.7154575  -0.22059494 -0.35046196  0.10433691\n",
            "  0.26043797  0.32612327  0.28216624 -0.5781576  -1.5598587   2.9605732\n",
            " -0.16263226  3.4615734  -0.05794232  0.18462925  0.7999948   2.9425645\n",
            "  0.40850887]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 35 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.19054826 -0.38863608 -0.81417066 -0.24904111 -0.3384264  -0.08470268\n",
            "  0.26652032  0.3378362   0.2476211  -0.60365486 -1.4650601   3.0960503\n",
            " -0.07812133  3.277352   -0.17110105  0.22584595  0.84403026  2.798236\n",
            "  0.34091744]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 36 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.09655422 -0.24152803 -0.73718405 -0.21631691 -0.5449729   0.14757925\n",
            "  0.37087727  0.47809383  0.5855407  -0.755389   -1.7577701   3.4166543\n",
            " -0.3869272   3.977717   -0.23230848 -0.03347602  0.80612385  2.9286416\n",
            "  0.7633055 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 37 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.23460177 -0.42211112 -0.871792   -0.20578358 -0.35788605 -0.05855796\n",
            "  0.27919173  0.30660897  0.28476653 -0.66607344 -1.5389758   3.5105402\n",
            " -0.12490888  3.5582373  -0.22914949  0.18978359  0.8383886   2.9098601\n",
            "  0.3802566 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 38 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.17562686 -0.444023   -0.82081246 -0.29663768 -0.41724595  0.03297836\n",
            "  0.3327342   0.385753    0.35043728 -0.6967471  -1.6986985   3.9989595\n",
            " -0.21567927  4.209577   -0.14540352  0.24839322  0.88953245  3.420073\n",
            "  0.4948711 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 39 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 1.4267544e-01 -2.0934516e-01 -7.9504836e-01 -2.4731238e-01\n",
            " -3.8899228e-01 -4.8857601e-04  2.8831008e-01  3.6197403e-01\n",
            "  3.8073441e-01 -8.7801117e-01 -1.6112101e+00  3.6449318e+00\n",
            " -4.1777214e-01  3.7850933e+00 -1.7964275e-01  9.4186924e-02\n",
            "  8.8800216e-01  3.1046858e+00  6.2334394e-01]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 40 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.18958397 -0.61212134 -0.86395323 -0.18945011 -0.41189876  0.00734348\n",
            "  0.32495964  0.45907876  0.35880455 -0.6971072  -1.7441022   4.4906216\n",
            " -0.36425444  4.362484   -0.15727815  0.22440264  0.99510515  3.5422292\n",
            "  0.51722246]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 41 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2014078  -0.4665837  -0.94018567 -0.16585779 -0.39085442 -0.04550255\n",
            "  0.27564743  0.4702743   0.35970986 -0.72009486 -1.6545451   4.8474345\n",
            " -0.16355367  4.421069   -0.17243205  0.35324094  1.0439377   3.6864269\n",
            "  0.5478215 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 42 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.18927439 -0.583097   -0.8119489  -0.14527234 -0.30719587 -0.01950835\n",
            "  0.28115067  0.4662346   0.4485551  -0.7237718  -1.7942255   5.059683\n",
            " -0.28711087  4.7502165  -0.11269565  0.23583485  0.9573579   3.7959979\n",
            "  0.58137345]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 43 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.23171517 -0.21099745 -0.8926668  -0.29877394 -0.23398693 -0.11340362\n",
            "  0.30175072  0.44630137  0.41795394 -0.9758904  -1.7756628   4.680454\n",
            " -0.42766932  4.40084    -0.34345308  0.02352797  0.9639787   3.705139\n",
            "  0.7129767 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 44 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.24061811 -0.5659254  -0.8527185  -0.18291505 -0.02039264 -0.03056335\n",
            "  0.33914235  0.4669337   0.3345395  -0.7374108  -1.824089    5.3964753\n",
            " -0.3048417   4.9129295  -0.12563789  0.20568979  1.0111843   4.211097\n",
            "  0.54238266]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 45 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.19695641 -0.17545255 -0.9266324  -0.28303933 -0.10875765 -0.125402\n",
            "  0.2929215   0.48851326  0.49986833 -0.9896135  -1.8870548   5.3162117\n",
            " -0.41142753  4.794564   -0.3744763   0.05149629  0.9771199   4.2403646\n",
            "  0.8751826 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 46 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2513687  -0.5814617  -0.87817115 -0.18462345  0.18819311 -0.04435243\n",
            "  0.34629473  0.49573952  0.34129807 -0.7750503  -1.8900905   5.8418818\n",
            " -0.32153985  5.1787405  -0.13187228  0.21961348  1.0510042   4.765122\n",
            "  0.5839045 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 47 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.3132672  -0.12003054 -0.9818374  -0.3014683   0.1983147  -0.05940783\n",
            "  0.40621245  0.58513576  0.42831087 -0.9576156  -2.0154893   5.864533\n",
            " -0.50979894  5.2300615  -0.4073941   0.0378693   1.0924597   5.106322\n",
            "  0.9474552 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 48 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1964343  -0.30038315 -0.9003495  -0.26163924  0.15593486  0.08697188\n",
            "  0.47739092  0.6243319   0.53502184 -0.9428292  -2.127524    6.092554\n",
            " -0.52267236  5.568449   -0.29642624 -0.01234028  1.049651    5.255132\n",
            "  0.96939844]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 49 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.300888   -0.3427598  -0.9991387  -0.449357    0.41412717 -0.05747345\n",
            "  0.28296974  0.6241956   0.40528286 -0.91473866 -1.8895036   5.6903496\n",
            " -0.25178385  4.8709607  -0.23383377  0.32890314  0.9942271   5.146993\n",
            "  0.7748965 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 50 reward=2 new_state=[0 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.20121041 -0.60573006 -1.0397266  -0.32420135  0.5004532  -0.03324318\n",
            "  0.38505736  0.5976425   0.44253904 -0.9366811  -2.1476526   6.920026\n",
            " -0.43222332  5.8723044  -0.22594735  0.37362286  1.1860048   6.431374\n",
            "  0.8099208 ]\n",
            "Epsilon reduced to 0.08857350000000001\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.31213006 -0.7447919  -0.76587266  0.05167373  0.52558416 -0.5758288\n",
            " -0.15841073  1.2804896   0.4602362  -0.9162517  -2.119274    7.3472233\n",
            " -0.5825119   6.756302   -0.03132191  0.396829    1.2859082   7.0243645\n",
            "  1.3033051 ]\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.04994601 -0.6372727  -0.959754   -0.09415009  0.81033295 -0.1130396\n",
            "  0.38269266  0.92479914  0.39788568 -1.0978799  -2.3627577   7.24985\n",
            " -0.48953173  6.72121    -0.15192156  0.49584544  1.237901    7.2620025\n",
            "  1.0060502 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.05983152 -0.40457022 -0.8008223   0.09514043  0.8096553  -0.3476481\n",
            "  0.12445189  1.0429624   0.3892974  -0.96302295 -2.2687712   6.6324134\n",
            " -0.41811216  6.1955395  -0.20383723  0.53646386  1.1386627   6.890466\n",
            "  1.2504597 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.24787757 -0.2607175  -1.0333173  -0.23667361  0.94086784 -0.085677\n",
            "  0.26351213  0.47353616  0.2891445  -0.6773084  -2.034102    6.457492\n",
            " -0.41899994  5.7223263  -0.44646662  0.5783632   1.3288183   6.3201256\n",
            "  0.91351366]\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.19910106 -0.15611094 -0.7850945  -0.11991979  0.64103156 -0.13710222\n",
            "  0.32952368  0.64566845  0.26295364 -0.52811676 -1.7074442   5.63135\n",
            " -0.40509006  4.540899   -0.19495481  0.7697507   0.9896829   5.3669677\n",
            "  0.80888426]\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.10630757 -0.36620852 -0.7984383   0.18474193  0.6071447  -0.1817072\n",
            "  0.13619728  0.92622125  0.585934   -0.6118391  -2.0157804   6.044757\n",
            " -0.50574     5.584092   -0.1963959   1.3031186   0.880608    6.3755503\n",
            "  1.2606922 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.13470358 -0.34805617 -1.0778304  -0.16383111  0.8545021  -0.1383986\n",
            "  0.20517927  0.85788614  0.501257   -0.4621854  -2.6680536   7.328694\n",
            " -0.5525085   6.425236   -0.16442986  1.6068501   1.0546067   7.556689\n",
            "  1.2553078 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.3799755  -0.11000905 -1.1710231  -0.43008554  0.9761987  -0.30314898\n",
            "  0.48603982  0.5273324   0.18816118 -0.02082932 -2.3789802   7.1977477\n",
            " -0.66757834  5.717425   -0.5448915   1.8038121   1.1419455   7.0702252\n",
            "  0.91835177]\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.0951435  -0.23190595 -0.8514927   0.10541504  0.82927775 -0.3881625\n",
            "  0.39893794  0.5918807   0.40535453  0.01405657 -2.4092984   6.6956825\n",
            " -0.55743295  5.4973016  -0.18588683  2.0674624   1.046096    6.778414\n",
            "  0.94306797]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 10 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.41000602 -0.4294761  -1.0994177   0.01825195  1.2985622  -0.37337935\n",
            " -0.09800731  1.0142206   0.47359034  0.6833128  -2.5710664   8.419072\n",
            " -0.49055418  7.298375   -0.39424512  2.7150233   1.3221152   8.648193\n",
            "  1.3959032 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 11 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.24164318 -0.50050974 -1.0519758  -0.04420878  1.0043577  -0.13668919\n",
            "  0.20484807  0.843317    0.5129279   0.4710077  -2.7199686   7.611008\n",
            " -0.8173194   6.2878547  -0.3618018   2.7528703   1.1770253   7.728907\n",
            "  1.2219169 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 12 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.21763976 -0.32805094 -1.0692306  -0.04686608  1.0214198  -0.22814791\n",
            "  0.13840024  0.90869486  0.55914176  0.7000764  -2.705854    7.6312656\n",
            " -0.74218637  6.6750913  -0.27714315  3.2724392   1.0644661   8.182714\n",
            "  1.4487897 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.30078933 -0.3264315  -0.9824705  -0.073144    1.3057587  -0.35745543\n",
            "  0.32660758  1.0437528   0.413543    0.9445475  -2.694648    8.093464\n",
            " -0.61507857  7.016734   -0.3494883   3.6819916   1.1822302   8.411348\n",
            "  1.1079421 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.21444881 -0.40385357 -0.8869665   0.00792147  1.0527017  -0.41048938\n",
            "  0.2165923   0.80279493  0.45652243  1.236344   -2.4844718   7.2143435\n",
            " -0.4677412   6.0422907  -0.14828683  3.4471593   1.169676    7.4321823\n",
            "  1.1588175 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.47942996 -0.51462424 -0.97204643 -0.03888854  1.5070084  -0.84385496\n",
            "  0.13876867  0.9941133   0.28975427  1.8546333  -2.9714613   8.7624235\n",
            " -0.5171028   7.7700734  -0.394092    4.955778    1.333894    9.509731\n",
            "  1.3337492 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.28879347 -0.44675878 -0.984972   -0.27235007  1.1853763  -0.19715673\n",
            "  0.11431147  0.75753     0.46285892  1.4288169  -2.7778256   8.017663\n",
            " -0.65303373  6.79901    -0.16773859  4.384803    1.1773741   8.210601\n",
            "  1.3795286 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.12455102 -0.4595067  -0.9938993  -0.06065653  1.1271173  -0.20591848\n",
            "  0.13287732  0.9676864   0.54042345  1.7029798  -2.650271    7.8370357\n",
            " -0.72025716  6.528375   -0.24344172  4.875931    1.1706654   8.224937\n",
            "  1.4617887 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.31632248 -0.3327135  -1.205143   -0.19561556  1.6112728  -0.5864557\n",
            "  0.13489018  1.0667716   0.34487984  2.37171    -2.794957    8.931425\n",
            " -0.47871363  7.8820014  -0.30560765  5.9475985   1.3518366   9.586722\n",
            "  1.4831431 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.13775568 -0.52693564 -0.95308954  0.02310545  1.2268025  -0.4079676\n",
            "  0.2292572   0.6860332   0.51847714  1.938347   -2.5390172   7.780294\n",
            " -0.5618183   6.357947   -0.26485637  5.298928    1.2708286   8.010822\n",
            "  1.2360152 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.4993446  -0.42030773 -1.0244393  -0.03469679  1.518539   -0.66422564\n",
            "  0.01951966  1.0932763   0.3704018   2.1908524  -2.8657167   8.810594\n",
            " -0.5947268   7.590088   -0.36703575  6.5173016   1.2594774   9.343443\n",
            "  1.373784  ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.14967571 -0.49264485 -0.9695029  -0.13261446  1.2253327  -0.17401882\n",
            "  0.18868245  0.81198955  0.42567396  1.9238843  -2.6744106   7.998711\n",
            " -0.6055121   6.6669025  -0.23344521  5.516958    1.223208    8.154128\n",
            "  1.2826474 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.33584604 -0.5860807  -0.8847015   0.01582574  1.4241433  -0.40428874\n",
            "  0.17738144  1.2424351   0.48206314  2.4495833  -2.7908096   8.27527\n",
            " -0.79943585  7.017896   -0.24301077  6.666264    1.3173026   8.964964\n",
            "  1.5190738 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.35372302 -0.39867762 -1.2079958  -0.0342179   1.9524966  -0.74403733\n",
            "  0.37741816  1.1992725   0.31170863  2.942304   -3.0109174   9.227742\n",
            " -0.47066787  8.270906   -0.34863326  7.4442573   1.3789169  10.20139\n",
            "  1.3391556 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.17407893 -0.31579536 -0.9333674   0.01697414  1.0429878  -0.2568474\n",
            "  0.24757929  0.67596924  0.53670937  2.0386631  -2.423186    7.6826053\n",
            " -0.47695738  6.145789   -0.17347214  5.659369    1.204423    7.618872\n",
            "  1.0676666 ]\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.5016271  -0.40116328 -0.99514806 -0.01230535  1.6953851  -0.61948067\n",
            "  0.10188804  1.2279497   0.3856497   2.6819496  -2.8996906   9.089948\n",
            " -0.7312773   7.7616363  -0.36540136  7.6353326   1.2674189   9.754345\n",
            "  1.4659225 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.27972054 -0.75700647 -0.9591006  -0.20546249  1.4799337  -0.32171103\n",
            "  0.21632434  0.9338958   0.38158587  2.7936053  -2.826059    8.409578\n",
            " -0.5867504   7.249664   -0.28394476  6.686767    1.3937455   8.812981\n",
            "  1.3499463 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.19080098 -0.6033971  -1.0268344  -0.21733525  1.4952781  -0.5288499\n",
            "  0.30616322  1.0607331   0.49833447  3.121592   -3.0152657   8.689161\n",
            " -0.82714975  7.4054027  -0.29232916  7.433556    1.3384533   9.462027\n",
            "  1.6619198 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.18085136 -0.2717823  -1.2910262  -0.17119393  1.741151   -0.4976432\n",
            "  0.47916278  1.0461504   0.44740975  3.124571   -2.9490273   9.307311\n",
            " -0.747209    7.9633765  -0.5005268   7.741577    1.3924005   9.906952\n",
            "  1.361843  ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 29 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.17833638 -0.2968081  -0.98476684 -0.09899907  0.9901124  -0.18047464\n",
            "  0.24775247  0.7653319   0.5727652   2.6487117  -2.392879    7.86178\n",
            " -0.62317747  5.923942   -0.18162085  5.978528    1.0896274   7.6460495\n",
            "  1.2036612 ]\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 30 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.60104096 -0.72364795 -1.0652586  -0.06867789  1.7166493  -0.7983327\n",
            " -0.16078626  1.251748    0.628362    3.933521   -3.0204463   9.589966\n",
            " -0.47397882  8.242712   -0.39699745  8.054697    1.3320793  10.211149\n",
            "  1.5526836 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 31 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.26615646 -0.67510295 -0.99927515 -0.18376628  1.5557075  -0.32153732\n",
            "  0.24169412  1.0500581   0.43956533  3.549629   -2.867131    8.526536\n",
            " -0.6828685   7.3823543  -0.27371398  6.88564     1.38256     9.174252\n",
            "  1.380023  ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 32 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.27318445 -0.41242123 -0.9475524  -0.05710066  1.3326808  -0.29461142\n",
            "  0.1103897   0.91998297  0.45999783  3.3630881  -2.7610874   8.352251\n",
            " -0.67347205  6.886542   -0.17536125  6.7487655   1.1221794   8.801841\n",
            "  1.5064936 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 33 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.3286992  -0.2210223  -1.2536627  -0.03883819  1.8499193  -0.61840194\n",
            "  0.33523104  1.1108265   0.41670728  3.8509185  -2.9134583   9.1575\n",
            " -0.51677805  7.983331   -0.34351844  7.1073613   1.267244    9.917965\n",
            "  1.2753545 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 34 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.28468156 -0.52913225 -1.0480646   0.01297854  1.2947137  -0.41130558\n",
            "  0.32584637  0.943721    0.6122961   3.62343    -2.7455387   8.080913\n",
            " -0.4741573   6.660815   -0.29383242  6.1022596   1.2990903   8.394127\n",
            "  1.1060414 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 35 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.4293579  -0.6942674  -0.9777609  -0.13888992  1.5936037  -0.53844905\n",
            " -0.04792668  1.1012374   0.5071207   4.264696   -2.9198887   8.973842\n",
            " -0.5896409   7.7732377  -0.19647542  7.0168624   1.4108125   9.566158\n",
            "  1.6245794 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.30132213 -0.6561918  -1.0389953  -0.22799902  1.3610668  -0.22484715\n",
            "  0.15194017  0.838596    0.4739962   3.6262515  -2.824232    8.51266\n",
            " -0.76050323  6.930981   -0.26588732  6.1206      1.320909    8.542578\n",
            "  1.3758637 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 37 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.28329396 -0.25908408 -0.7721065  -0.05146451  1.385961   -0.30314547\n",
            "  0.39623448  0.68726856  0.23628615  3.1192498  -2.4698422   7.144484\n",
            " -0.60410625  5.9943795  -0.23925301  5.4478264   1.0555391   7.666048\n",
            "  1.0366186 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 38 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.41087338 -0.62906253 -1.2201252   0.05664173  1.8778486  -0.6209429\n",
            "  0.18406326  1.2027923   0.4073239   4.930223   -2.9048092   9.25649\n",
            " -0.67195743  8.075128   -0.5022736   6.8226795   1.4482424  10.07732\n",
            "  1.62848   ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 39 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.21311629 -0.5888339  -0.9074839  -0.01542156  1.3900193  -0.3408683\n",
            "  0.21577656  0.67102206  0.37265676  4.1027684  -2.6252103   8.109316\n",
            " -0.47303158  6.58465    -0.16387434  5.5883837   1.3010077   8.290975\n",
            "  1.1188033 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 40 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.33566925 -0.509108   -1.0481982  -0.13395411  1.4171431  -0.2717084\n",
            " -0.16553147  1.016992    0.6106876   4.290396   -2.7179291   8.69459\n",
            " -0.5800905   7.4426236  -0.26627967  6.3250027   1.3155689   9.1450815\n",
            "  1.5585864 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 41 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.30533853 -0.42907542 -1.121028   -0.20455444  1.3868798  -0.27485946\n",
            "  0.17720859  0.9927149   0.4917953   4.042713   -2.7940817   8.687733\n",
            " -0.68701494  7.2013335  -0.27085784  5.8788743   1.2265953   8.939003\n",
            "  1.3854994 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 42 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.3221064  -0.48085836 -0.96508414  0.10193416  1.4937881  -0.3605733\n",
            "  0.2618253   0.9223931   0.70717496  4.34418    -2.6646516   8.213245\n",
            " -0.53323     6.835851   -0.22861727  5.8253016   1.2299231   8.747757\n",
            "  1.0836844 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 43 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.5173338  -0.47623783 -1.309849    0.10992189  1.9018123  -0.6432601\n",
            "  0.08303721  1.2231604   0.56955814  5.1491404  -2.8669598   9.150149\n",
            " -0.603744    8.111152   -0.47554442  6.33613     1.357531   10.097537\n",
            "  1.5321795 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 44 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.20992912 -0.281559   -0.999447    0.05780345  1.3497813  -0.44700265\n",
            "  0.5072404   0.7226838   0.41905177  3.741679   -2.654479    7.6697087\n",
            " -0.37824836  6.1585116  -0.20790692  5.046745    1.2107496   8.027456\n",
            "  0.8150301 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 45 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.2930176  -0.402161   -0.9719832  -0.16747773  1.5909418  -0.44722578\n",
            "  0.1860408   0.9877384   0.4622571   4.4447427  -2.8694038   8.730912\n",
            " -0.68656987  7.511907   -0.32764396  6.2551365   1.2626468   9.411977\n",
            "  1.474024  ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 46 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.3116377  -0.59055954 -1.2074257  -0.342939    1.5584111  -0.4419976\n",
            "  0.2945676   0.98547494  0.5885868   5.027207   -3.1470013   9.035366\n",
            " -0.63931215  7.6525664  -0.45285416  6.3073783   1.2925996   9.636335\n",
            "  1.5255989 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 47 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.09878114 -0.62862897 -1.0345043  -0.05918363  1.4524498  -0.4476984\n",
            "  0.28422984  1.0894306   0.48299688  4.808919   -2.7224429   8.1865635\n",
            " -0.6928294   6.9386444  -0.3574271   5.9517574   1.2994753   8.932967\n",
            "  1.443891  ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.30739057 -0.33924267 -1.286274   -0.07948354  1.6462623  -0.38426182\n",
            " -0.02013017  1.1201054   0.54148597  4.95934    -2.6947765   8.980615\n",
            " -0.66261965  7.7134175  -0.37581393  6.064152    1.3512346   9.517948\n",
            "  1.6563742 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.18277411 -0.3412732  -0.99053854 -0.06538917  1.1451771  -0.2262374\n",
            "  0.31280002  0.6320997   0.34715664  3.9263244  -2.4144714   7.612223\n",
            " -0.42456925  6.035052   -0.2026962   4.8462305   1.1982259   7.6310577\n",
            "  0.91606176]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [ 3.2809880e-01 -6.8676966e-01 -1.0102797e+00  3.3673134e-02\n",
            "  1.8331486e+00 -6.4961100e-01 -1.3062865e-03  1.3053156e+00\n",
            "  5.9375787e-01  4.9639525e+00 -2.9075665e+00  8.5763779e+00\n",
            " -6.2390459e-01  7.6124096e+00 -2.3183618e-01  6.3950295e+00\n",
            "  1.3467666e+00  9.4358902e+00  1.3929932e+00]\n",
            "Epsilon reduced to 0.07971615000000001\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.36368224 -0.9920229  -1.3978508   0.04390124  2.2045693  -0.9890833\n",
            "  0.29397944  1.4648793  -0.2186683   6.19925    -3.4103184   9.631022\n",
            " -0.7132195   8.501045    0.02518558  6.7613006   1.6792127  10.180936\n",
            "  1.3062024 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.41185609 -0.5185154  -1.4197844   0.09897152  1.8786833  -0.88371056\n",
            "  0.30151382  1.6161548  -0.21119533  6.0098176  -3.075617    9.339505\n",
            " -1.1348954   8.326532    0.16219045  6.7666507   1.5203604   9.554799\n",
            "  1.9181302 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.48093396 -0.57083184 -1.2576377  -0.1544194   1.3338188  -0.31979424\n",
            "  0.1586931   1.1134174   0.09651543  4.3872633  -2.6919615   7.89301\n",
            " -0.4459371   6.8877406  -0.10665289  5.5586324   1.3696398   7.803019\n",
            "  1.3587302 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.7632748  -0.897724   -1.2616816   0.17579164  2.4420087  -0.62575495\n",
            "  0.10714148  1.2196549  -0.2167401   6.190159   -2.8659372   9.061834\n",
            " -1.0187155   8.049199    0.13354917  6.8369317   1.1509722   9.40319\n",
            "  1.6311319 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.43189162 -0.5292515  -1.2181948  -0.17876501  1.2832763  -0.3887043\n",
            "  0.21827829  1.0056187   0.18534109  4.29604    -2.6406555   7.2741423\n",
            " -0.42703313  6.662219   -0.2626058   5.2704744   1.3116399   7.462177\n",
            "  1.1419965 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 1.9222519e-01 -5.4104578e-01 -1.1147447e+00  1.8999721e-01\n",
            "  2.0557976e+00 -2.8750485e-01  3.3424860e-01  1.4294785e+00\n",
            "  2.3719083e-01  5.6468587e+00 -3.2762003e+00  7.8485317e+00\n",
            " -1.2331375e+00  8.0883751e+00 -7.3434943e-03  6.0910525e+00\n",
            "  1.3711478e+00  8.4704075e+00  1.8078223e+00]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.35648394 -0.58836097 -1.3292592  -0.07303467  1.5072261  -0.6482218\n",
            "  0.07687248  1.078099    0.25223592  4.7586446  -2.7477794   7.3882346\n",
            " -0.57510096  7.506434   -0.19034286  5.748606    1.5571506   7.7681575\n",
            "  1.0880764 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.6410879  -0.9234339  -1.1938819   0.16807315  2.3733106  -0.64291745\n",
            "  0.11181045  1.2134762  -0.22955246  5.96372    -2.6352682   7.332464\n",
            " -0.93265676  7.648541    0.15276706  6.3148828   1.1655244   8.42274\n",
            "  1.5038898 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.58186084 -0.6530291  -1.2423099  -0.02818923  1.6035651  -0.6528636\n",
            " -0.02723409  1.1502281  -0.00850185  4.8741164  -2.7017782   6.6372814\n",
            " -0.5812055   7.3819056  -0.08521537  5.459036    1.4337802   7.6149\n",
            "  1.0232868 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.48511317 -0.5461541  -1.1033798   0.24776006  2.1252596  -0.42832267\n",
            "  0.31003067  1.5156536   0.01530833  5.919763   -2.8296032   6.4479427\n",
            " -1.0923321   7.5748963   0.00947144  6.0789037   1.2268595   8.171898\n",
            "  1.842541  ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.40111265 -0.8108589  -1.2833203  -0.03179069  1.5230137  -0.26980525\n",
            "  0.11411229  1.0867199  -0.03359639  4.676273   -2.5737514   6.001282\n",
            " -0.6129179   6.8844543  -0.19716306  5.4971943   1.5001708   7.2808967\n",
            "  1.1773145 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.16647074 -0.7701699  -1.3815632   0.07623483  1.7976164  -0.7161806\n",
            "  0.32575116  1.2603526  -0.3918117   5.362288   -3.033794    5.9121547\n",
            " -0.7621663   7.595475    0.28895447  6.4856606   1.5640862   7.4975643\n",
            "  1.1562432 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.47437692 -0.6745757  -1.2628218  -0.05282154  1.3361931  -0.35763022\n",
            "  0.08528822  1.0372099   0.2025976   4.4916625  -2.7749228   5.5760384\n",
            " -0.57700783  6.562891   -0.2524974   5.682503    1.2982162   7.033414\n",
            "  1.300846  ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.38898265 -0.266681   -1.3083005  -0.18840551  1.4262484  -0.5227443\n",
            "  0.5924288   0.89377975  0.19282904  5.209812   -2.748715    5.4540052\n",
            " -0.91338646  6.441488   -0.18822215  5.6976004   1.0344628   7.342943\n",
            "  1.4049524 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.25312304 -0.52587044 -1.352556   -0.29585755  1.3072073  -0.551952\n",
            "  0.33211282  1.0083038   0.13775502  4.4948387  -2.3087666   5.037122\n",
            " -0.58782756  5.4249606  -0.23195991  5.2743096   1.0435941   6.4853086\n",
            "  1.1064336 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.4599745  -0.6733454  -1.3417032  -0.24827166  1.6557899  -0.7037701\n",
            "  0.3950822   0.6789899  -0.03647375  5.173503   -2.375662    5.400847\n",
            " -0.7390755   6.138874   -0.03624962  5.684674    1.0235238   7.136398\n",
            "  0.9688054 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.35938668 -0.47624564 -1.2234306  -0.17954402  1.0723673  -0.4143058\n",
            "  0.2414454   0.8611036   0.250041    4.088409   -2.1988087   5.004499\n",
            " -0.46182844  5.2389355  -0.2660882   4.759414    1.1033309   6.068489\n",
            "  0.9021831 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.5063562  -0.72515565 -1.2786515  -0.180019    1.6508185  -0.6570022\n",
            "  0.33473894  0.6723206  -0.08673802  5.120404   -2.3569899   5.3181114\n",
            " -0.73135334  5.9135427   0.03688928  5.5383368   1.0195296   6.931444\n",
            "  0.97017354]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.35783395 -0.47437513 -1.2206637  -0.17962185  1.0689205  -0.4129168\n",
            "  0.24002801  0.8578597   0.24968277  4.0807004  -2.1928084   5.0569053\n",
            " -0.4600246   5.1387773  -0.26634142  4.742478    1.1003199   6.007467\n",
            "  0.8988057 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.5054691  -0.72408277 -1.2771525  -0.18021235  1.6491209  -0.6561479\n",
            "  0.3338669   0.6705028  -0.08656339  5.1171427  -2.3535552   5.4134545\n",
            " -0.7301215   5.8150344   0.03626732  5.5265193   1.0181687   6.8748684\n",
            "  0.9682832 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.37491283 -0.411082   -1.2881479  -0.15598044  1.1437175  -0.25933602\n",
            "  0.2224705   0.8294288   0.20224631  4.353973   -2.233798    5.444995\n",
            " -0.44811735  5.3057323  -0.26664492  5.018632    1.1989555   6.3402014\n",
            "  0.9527087 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.4568748  -0.20583059 -1.3128567  -0.24005792  1.5003469  -0.6491819\n",
            "  0.45529518  0.7487145   0.02207679  4.8265567  -2.483824    5.2720814\n",
            " -0.9061658   5.5920625  -0.18325569  5.1607146   1.0557761   6.776898\n",
            "  1.3038718 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 23 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.33547208 -0.2691175  -0.94868666 -0.0798588   1.0658933  -0.12954605\n",
            "  0.2876716   0.66563857  0.36687276  3.413999   -2.0660102   5.0193515\n",
            " -0.48158213  4.6076202  -0.21743788  4.3837333   0.9355801   5.6389303\n",
            "  0.9038772 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.45542136 -0.51189715 -1.0707625   0.14174348  1.0392505  -0.21238688\n",
            "  0.14722146  0.754137    0.44994625  3.6933043  -2.2371712   5.1619864\n",
            " -0.68400294  4.8425193  -0.39606628  4.773507    1.0049074   5.6746445\n",
            "  1.0055265 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.35951504 -0.2961071  -1.1872612   0.2639442   1.2415819  -0.42102963\n",
            "  0.21814062  0.90900254  0.2815646   4.3708034  -2.3404179   5.266469\n",
            " -0.79252505  4.9181314  -0.30117407  4.991443    0.9148101   6.219158\n",
            "  1.2460612 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 26 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.34336084 -0.5804966  -1.2897515   0.50391     1.4743135  -0.30341795\n",
            "  0.24098873  0.87334085  0.38862765  4.8381076  -2.3943865   5.8644376\n",
            " -0.7667479   5.4814973  -0.11189246  5.535859    0.9899914   6.699785\n",
            "  1.0681612 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 27 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.2821051  -0.30842865 -0.9839059   0.6292073   0.8570423  -0.1247922\n",
            "  0.13206531  0.672636    0.45087707  3.3443024  -2.0243847   4.84217\n",
            " -0.5352961   4.1256394  -0.25800487  4.4075365   0.9141942   5.018682\n",
            "  0.9969076 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 28 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.4481537  -0.51767266 -1.1109414   0.8246096   1.5921494  -0.48805892\n",
            "  0.20069993  0.68961614  0.08775486  4.335248   -2.4399538   5.042476\n",
            " -0.7839666   4.9278555  -0.06445722  5.474552    1.1076565   5.9880877\n",
            "  1.0656139 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 29 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.26135626 -1.098411   -1.2303438   0.99433863  1.5233521   0.16815647\n",
            "  0.34329706  0.8642528   0.40675637  4.625042   -2.442485    5.688541\n",
            " -0.5443772   6.3725543  -0.22548772  5.07294     1.2657187   6.7765303\n",
            "  1.102029  ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 30 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.5722436  -0.6404026  -1.2823042   1.1737748   1.8031687  -0.79252166\n",
            "  0.3937327   0.7769757  -0.05657089  5.135346   -2.5745902   5.761385\n",
            " -0.7764835   5.2052836   0.0481736   5.7390103   1.1430956   6.686906\n",
            "  0.94942284]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 31 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.3322541  -0.2953926  -1.1326059   1.2239641   1.1535013  -0.61157244\n",
            "  0.26758724  0.74486184  0.2951112   3.8428998  -2.2015104   5.2220244\n",
            " -0.8534412   4.3344793  -0.00750334  4.8836784   0.9376071   5.6512356\n",
            "  0.5012517 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 32 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.3314002  -0.7957592  -1.1971673   1.2912273   1.4475267  -0.15393758\n",
            "  0.2021668   0.5738706   0.4096393   4.4672933  -2.6228416   6.060551\n",
            " -0.7219931   5.8363156  -0.290648    5.28759     1.3742602   6.782842\n",
            "  0.87527794]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 33 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.1239942  -0.4761001  -1.097289    1.6945322   1.6132113  -0.4694357\n",
            "  0.15858334  1.0963345   0.27537966  4.7710752  -2.7827246   5.105678\n",
            " -0.8702824   5.593168    0.09095741  6.299391    1.1437469   6.5605817\n",
            "  1.2418548 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 34 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.4264352  -0.6878004  -1.0570965   1.3894827   1.4734231  -0.30182332\n",
            "  0.25685394  0.62801266  0.19311075  4.83798    -2.274       6.0777836\n",
            " -0.6116877   5.9093814  -0.14284281  4.931967    1.0728996   7.0431337\n",
            "  0.875873  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 35 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.35797217 -0.53262866 -1.2482185   1.437966    1.6781693  -0.4698909\n",
            "  0.33135694  0.96653676  0.22104098  4.829825   -2.3191907   5.944632\n",
            " -0.65829766  5.2032876  -0.11381555  5.6619873   1.1386124   6.6485286\n",
            "  0.9525395 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 36 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.23222797 -0.31617337 -1.3465964   1.62749     1.5168878  -0.4908733\n",
            "  0.23080778  0.6374579   0.32542855  4.3227262  -2.560588    5.2604785\n",
            " -0.90696126  5.2419667  -0.00968942  5.604921    1.248211    6.4462056\n",
            "  1.0678923 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 37 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.1786685  -0.4998577  -1.1691298   2.1236186   1.5301917  -0.0843339\n",
            "  0.64833635  1.1208724   0.589022    4.8780456  -2.884076    5.5301757\n",
            " -0.630243    6.6336246  -0.31993285  5.3005214   1.4332833   7.3217974\n",
            "  1.2593313 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 38 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.29639116 -0.5203737  -1.095311    2.092708    1.5812851  -0.6295619\n",
            "  0.28402275  0.8669415  -0.0621337   4.71102    -2.4172394   4.8793974\n",
            " -0.7479798   4.5835605   0.05226307  5.3957486   0.9942284   5.971065\n",
            "  0.93237704]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 39 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.3541582  -0.16454862 -1.1746128   1.7013756   1.227525   -0.20387387\n",
            "  0.38980156  0.6621592   0.39334786  3.984627   -2.3044813   5.624728\n",
            " -0.6462091   5.433615   -0.37316692  4.2829566   1.1291294   6.549191\n",
            "  1.0935724 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 40 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.10461542 -0.7067404  -1.4763744   2.243017    1.5577521  -0.36089295\n",
            "  0.19621049  0.68312925  0.35011125  4.654738   -2.7445905   5.577393\n",
            " -0.65287745  5.677218   -0.03883687  5.6093855   1.3895652   6.504393\n",
            "  0.96109986]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 41 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.17908633 -0.5812684  -1.131938    2.6810317   1.5424533  -0.3961298\n",
            "  0.36017558  1.0919397   0.5261497   4.3125715  -2.616979    4.374212\n",
            " -0.57670015  5.3376417   0.07930347  5.676718    1.1682223   6.110008\n",
            "  1.1788079 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 42 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.39315856 -0.18689823 -1.0256059   2.6438549   1.2694961  -0.16323079\n",
            "  0.57381076  0.8706901   0.7572559   4.9030747  -2.5015097   5.403494\n",
            " -0.6457877   5.7707324  -0.28809     4.9846945   1.1236203   6.758985\n",
            "  1.2287079 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 43 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.52731615 -0.3253733  -1.1410247   2.6519406   1.5692749  -0.653542\n",
            "  0.37773407  0.8195835   0.43053722  4.707085   -2.3108423   5.106079\n",
            " -0.69028854  4.5469584   0.01612714  5.8625727   0.8555825   6.0721955\n",
            "  0.94731617]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 44 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.45753554 -0.2279401  -0.9960073   2.767461    1.4294653  -0.05880596\n",
            "  0.43482202  0.7520332   1.1739789   4.3478775  -2.5723815   5.495209\n",
            " -0.6724129   5.8890123  -0.448226    5.056473    1.1664784   6.9370255\n",
            "  1.2341025 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 45 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.17349172 -0.681542   -1.393889    3.3970313   1.5652789  -0.24473052\n",
            "  0.5953309   1.2287446   1.3690449   5.4800663  -3.4060476   5.3636737\n",
            " -1.231227    6.9000435  -0.11659022  6.3432436   1.5021995   7.5291433\n",
            "  1.4674839 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 46 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.14527878 -0.3805222  -1.0889943   2.8135      1.4641962  -0.46127215\n",
            "  0.27547434  0.94469845  1.0014313   4.6643343  -2.4082372   4.958197\n",
            " -0.62470585  4.7629104   0.1034832   5.478969    0.9879727   6.1079993\n",
            "  1.0341693 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 47 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.18838318 -0.7261997  -1.1792994   2.7484164   1.2378756  -0.22393274\n",
            "  0.33332565  0.7563569   1.4162259   4.4945264  -2.3360233   5.9876604\n",
            " -0.6037465   5.576165   -0.250423    5.076442    1.1836988   6.749057\n",
            "  0.846803  ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 48 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.37160042 -0.31483206 -1.0997657   3.1014411   1.5981627  -0.7467225\n",
            "  0.33725458  0.6949047   0.9742968   4.3383746  -2.3326588   4.821818\n",
            " -0.7744023   4.753911   -0.00615712  5.786811    1.0131222   6.027952\n",
            "  1.0294756 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 49 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.19502604 -0.5185991  -1.1440663   2.920448    1.4067392  -0.17000706\n",
            "  0.24960366  0.9157399   1.3982488   4.160153   -2.5497816   5.0706224\n",
            " -0.65857947  5.8051257  -0.4006633   4.800467    1.3462265   6.6845865\n",
            "  1.2272071 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 10\n",
            "\n",
            "Step 50 reward=0 new_state=[0 1 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.33425415 -0.8243543  -1.1982472   3.2457843   1.7370024  -0.42204803\n",
            "  0.25960922  0.9517538   1.2490836   5.2788906  -2.5183618   5.3354397\n",
            " -0.8213373   5.457572   -0.06727278  5.460631    1.2028923   6.958622\n",
            "  0.9089381 ]\n",
            "Epsilon reduced to 0.07174453500000001\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.39429918 -0.40258718 -1.1269734   3.3710754   1.6946295  -0.72592497\n",
            " -0.18138738  1.2869161   1.8662475   4.294665   -2.8731186   5.3988886\n",
            " -0.64319885  5.76766    -0.2161922   5.7555227   1.3979154   6.4649506\n",
            "  1.0399265 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.6399826  -0.48309734 -0.843995    3.9472454   2.3130853  -0.80777955\n",
            " -0.34788403  1.4146183   1.8414171   5.0629425  -2.8159194   5.0278716\n",
            " -0.82346344  5.5329785  -0.12657867  6.1392145   1.0282848   6.440655\n",
            "  1.5354614 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.481588   -0.3116087  -0.94145244  3.2672558   1.4269347  -0.4973914\n",
            " -0.5420733   1.1797676   1.9990165   4.1786046  -2.7646346   5.1517315\n",
            " -0.6214022   5.548089   -0.4120298   5.4379454   1.1321579   6.243694\n",
            "  1.1465379 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.28666887 -0.51431537 -1.084279    3.8590267   1.8128192  -0.9877301\n",
            " -0.44650924  1.3781333   1.8640435   4.736228   -3.363789    5.113874\n",
            " -0.81080526  5.9129586   0.06093024  6.2614326   1.5951244   6.0587416\n",
            "  1.3889242 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.454028   -0.3804624  -1.0123521   2.9268203   1.4485048  -0.47886658\n",
            " -0.44379288  1.2201805   1.7146945   3.9737146  -2.6334887   4.874557\n",
            " -0.7554596   5.096226   -0.3940543   4.8649964   1.2836899   5.467947\n",
            "  1.1241668 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2918063  -0.58778965 -1.0309423   3.2565975   1.5592004  -0.5222652\n",
            " -0.53698045  1.3187093   1.5382016   3.9685273  -2.335679    4.292834\n",
            " -0.68718016  4.9645343  -0.26172435  5.0431104   1.1907545   5.1350546\n",
            "  1.2625388 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.34369254 -0.3558796  -1.0165712   2.7532423   1.1739721  -0.31236914\n",
            " -0.48253393  0.8657707   1.7167877   3.3495865  -2.1386182   4.4454455\n",
            " -0.7066975   4.5506244  -0.35647386  4.5140944   1.2500578   4.741516\n",
            "  0.99396133]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.19490045 -0.17593992 -0.566052    1.9625623   0.89672744 -0.1183269\n",
            " -0.23884456  0.66352004  1.1986582   2.566106   -1.5166112   3.677789\n",
            " -0.39719334  3.6353133  -0.25625792  3.3297853   0.91718423  3.863048\n",
            "  0.852232  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.416458   -0.34152228 -0.8544661   2.7040393   1.162377   -0.22161737\n",
            " -0.42824903  0.8483127   1.4670821   3.0646722  -2.0823174   4.2210617\n",
            " -0.79690415  4.4637046  -0.27595192  4.4185834   1.0538298   4.3674116\n",
            "  1.1285039 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.3572818  -0.59707403 -1.0823493   2.8401477   1.3019493  -0.2802969\n",
            " -0.52437717  1.0179998   1.6333797   3.464957   -2.374494    4.623263\n",
            " -0.72783446  4.614719   -0.52668935  4.9618926   1.1866693   4.510423\n",
            "  1.1227403 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2749494  -0.33728373 -0.6954655   2.0822272   0.9796046  -0.14256297\n",
            " -0.19812682  0.7107588   1.2262788   2.9977872  -1.7872459   4.092374\n",
            " -0.40745378  3.8943927  -0.17256479  3.5369327   0.9943674   3.8378632\n",
            "  0.9193283 ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.38124585 -0.40852475 -1.1905656   2.5865123   1.070833   -0.32891566\n",
            " -0.13570149  0.93701476  1.6911502   3.4744887  -2.111659    4.6225867\n",
            " -0.54037553  4.441764   -0.34886733  4.4078736   1.0978742   4.2370863\n",
            "  0.98361945]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.16303411 -0.32029667 -1.1685832   2.3758326   1.0037259  -0.29477307\n",
            " -0.25324938  0.90917045  1.5362898   3.2719376  -1.9438729   4.141928\n",
            " -0.72030437  4.086257   -0.15009484  4.6787944   1.0348787   3.6505148\n",
            "  1.1892592 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.25546947 -0.29005963 -1.0259941   2.356178    0.8638583  -0.12010803\n",
            " -0.23732078  0.9060708   1.7965341   3.1121686  -1.887923    4.736297\n",
            " -0.59099525  4.3573227  -0.25545773  4.448743    1.1423129   3.9774976\n",
            "  1.130921  ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.25184998 -0.4330926  -0.6776002   2.3443742   0.93585247 -0.10497433\n",
            " -0.24081154  0.9175881   1.5626132   3.1774154  -1.8356782   4.1875496\n",
            " -0.32497814  4.1750383  -0.13307528  4.081101    1.0896553   3.8447845\n",
            "  1.1593823 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2676133  -0.49165788 -0.9875928   3.0415444   1.12535    -0.47853675\n",
            " -0.3479631   0.83971244  1.6348573   3.5529914  -2.286318    4.698387\n",
            " -0.6405538   4.3904314  -0.3118803   4.689047    1.0036907   3.9828153\n",
            "  0.8574419 ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.24253923 -0.1728881  -1.2132524   2.4832823   0.90020305 -0.53609353\n",
            " -0.16308993  0.83629924  1.7204583   3.0365927  -1.9461428   4.5843844\n",
            " -0.77282715  4.1958456  -0.310235    4.5708127   0.99638927  3.7815762\n",
            "  1.0021809 ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.17155579 -0.25631696 -1.0470281   2.3661754   0.7612051  -0.2928596\n",
            " -0.23069914  0.83842665  1.6798838   2.87406    -1.4939572   4.2636533\n",
            " -0.5413593   3.9746401  -0.19985937  4.009188    0.97261214  3.544561\n",
            "  1.0718807 ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03506238 -0.741156   -1.0202231   2.8726406   1.0530174  -0.30054718\n",
            " -0.20708461  0.9270969   1.5435228   3.5251899  -1.674117    4.2203665\n",
            " -0.58284295  4.2127814  -0.24868591  4.527151    1.0844656   3.5274377\n",
            "  1.0214729 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2832042  -0.38671938 -0.763819    2.4376192   0.982183   -0.28070015\n",
            " -0.26843873  0.914734    1.6820352   3.239647   -1.2799234   4.337221\n",
            " -0.40423876  4.258807   -0.11959295  4.0273814   1.0532695   3.6623452\n",
            "  1.0407759 ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.04565144 -0.4245254  -0.9900776   2.4412255   0.77167344 -0.22885343\n",
            " -0.29511523  0.78253543  1.6638334   3.0488653  -1.3609356   4.512895\n",
            " -0.5566439   4.019442   -0.29537526  4.296611    1.0235845   3.40476\n",
            "  0.9403224 ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.08076195 -0.53667474 -0.9492373   2.7706625   0.9580083  -0.20411627\n",
            " -0.44122967  0.94073045  1.6096883   2.9538472  -1.4336097   4.353817\n",
            " -0.5669649   4.4981503  -0.23997217  4.453935    1.0532593   3.3456028\n",
            "  1.1744757 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.20091459 -0.5041179  -0.92442375  3.0693197   1.1115949  -0.16191405\n",
            " -0.42959234  0.95772135  1.6996741   3.4538462  -1.3158469   4.467827\n",
            " -0.443785    4.5028534  -0.14085895  4.4889646   1.1456616   3.6186256\n",
            "  0.96534455]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.31770098 -0.5514497  -1.1768625   3.2083266   1.2749798  -0.49122113\n",
            " -0.51780427  1.2860514   1.9851409   4.159661   -1.3721342   4.819868\n",
            " -0.8518705   4.5483284  -0.02147625  5.629119    1.0206194   3.5262797\n",
            "  1.2507646 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.39195597 -0.25302684 -0.9106934   2.5290182   1.0866073  -0.19660664\n",
            " -0.40503928  0.8619063   1.7439449   3.1203816  -1.2052034   4.8192887\n",
            " -0.70752656  4.3628416  -0.1043012   4.5290546   1.0191649   3.530366\n",
            "  1.1093923 ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.09193646 -0.58780223 -0.88628584  2.8206537   0.9907554  -0.03976359\n",
            " -0.5590593   0.92762446  1.563363    3.1376925  -1.2090572   4.335548\n",
            " -0.5565074   4.527944   -0.07910448  4.599956    0.98919004  3.1550093\n",
            "  1.2789335 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.16929711 -0.55028826 -0.912806    2.911792    0.98174024 -0.2625323\n",
            " -0.43879446  0.9867618   1.6443139   3.404118   -1.0393553   4.4087353\n",
            " -0.49329773  4.5069695  -0.16630587  4.279247    1.121249    3.2556279\n",
            "  0.99779284]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.24142523 -0.38516167 -0.9717946   2.760639    0.9601202  -0.365809\n",
            " -0.24649924  0.7778434   1.8251851   3.416695   -1.1543213   4.945044\n",
            " -0.40049356  4.4594564  -0.140214    4.3787827   0.9455886   3.6941257\n",
            "  0.7871438 ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1263026  -0.44656438 -0.88190573  2.6757278   1.1837327  -0.35265407\n",
            " -0.34356296  0.6896884   1.3270143   3.3379815  -1.122382    4.3370385\n",
            " -0.6084862   4.2696123  -0.02898846  4.45958     1.0389574   3.0499108\n",
            "  0.90803427]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.2208382  -0.40928587 -0.8256412   2.5394664   1.0813562  -0.11538037\n",
            " -0.49376097  0.9136272   1.6525631   2.955691   -0.7963095   4.237972\n",
            " -0.4755411   4.7892175  -0.05960959  4.0602837   1.0150206   3.2151244\n",
            "  1.1919726 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.00516649 -0.70430773 -1.0512515   2.7317846   1.1101857  -0.32278427\n",
            " -0.18873557  0.95527226  1.5494996   4.015128   -0.8630776   4.2869725\n",
            " -0.63952774  4.628943   -0.30455452  4.5511527   1.1949905   3.2708614\n",
            "  0.95417976]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.24008837 -0.4095856  -1.1215062   2.8483377   0.9598462  -0.47234526\n",
            " -0.20960037  0.8577908   1.8723755   3.7745216  -1.0745615   5.0669055\n",
            " -0.535641    4.825542   -0.23073989  4.686921    1.108197    3.5516908\n",
            "  0.7900154 ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.33143616 -0.32019183 -0.8830434   2.6225493   1.0801574  -0.31876394\n",
            " -0.3746154   0.5962941   1.4718848   3.302352   -0.99768656  4.658923\n",
            " -0.64040494  4.636163   -0.06731853  4.2879276   1.1355984   3.1502504\n",
            "  0.8125836 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1906182  -0.66604155 -1.0015681   2.8822823   1.2450485  -0.1712619\n",
            " -0.5703105   1.1690121   1.6473953   3.724495   -0.7768327   4.2540884\n",
            " -0.8125343   4.972485   -0.10868533  4.8423347   1.1248324   2.952746\n",
            "  1.369465  ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.21816047 -0.36543295 -0.69789684  2.6805763   1.0169991  -0.19522557\n",
            " -0.33968014  0.8463567   1.6237928   3.8693614  -0.7442463   4.664618\n",
            " -0.3370792   4.8269377  -0.06250326  4.3621726   0.983699    3.5485778\n",
            "  0.9382198 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.22560036 -0.51701635 -1.1206574   2.94136     1.0064547  -0.37078068\n",
            " -0.36913034  0.91223365  1.7097858   3.9944723  -1.0104562   4.8558455\n",
            " -0.5700054   4.834514   -0.18702227  4.792852    0.99107474  3.2934685\n",
            "  0.8638533 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.17687394 -0.36346322 -0.8350896   2.694929    1.0143951  -0.31688464\n",
            " -0.41593978  0.65394974  1.5514631   3.4574695  -0.8565536   4.4826984\n",
            " -0.5116797   4.780691   -0.04721203  4.3190117   1.08229     3.0549767\n",
            "  0.9159463 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 38 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.02227098 -0.5258538  -0.8184957   2.6004531   1.1100445  -0.17954342\n",
            " -0.48425677  1.0586826   1.6463568   3.3459811  -0.5654439   4.0387616\n",
            " -0.3867088   5.215065   -0.14055617  4.2305455   1.0841573   3.1052358\n",
            "  1.3078201 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 39 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.072198   -0.42065603 -0.8891427   2.6339316   1.0018314  -0.24502312\n",
            " -0.15863909  0.6963482   1.6283195   3.80576    -0.7157663   4.3534756\n",
            " -0.45814043  4.859346   -0.16007896  4.2840276   0.92021525  3.5799906\n",
            "  0.8860311 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 40 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.25493598 -0.39552122 -0.9587518   2.453916    1.0131516  -0.22237188\n",
            " -0.17515936  0.89704806  1.7452127   3.6559346  -0.7357224   4.58424\n",
            " -0.39780402  4.8400197  -0.165242    4.387389    1.0336057   3.322762\n",
            "  0.91905487]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 41 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.6738547e-02 -3.2435429e-01 -8.2272118e-01  2.4769278e+00\n",
            "  8.5845709e-01 -5.3928651e-02 -3.5774307e-03  5.9131467e-01\n",
            "  1.6184074e+00  3.0626693e+00 -6.5672660e-01  4.3382859e+00\n",
            " -2.6640946e-01  4.8197212e+00 -2.1247976e-01  4.0087972e+00\n",
            "  9.4798166e-01  3.3439803e+00  8.6147386e-01]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 42 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-2.9857801e-02 -5.3616798e-01 -7.9477245e-01  2.7574880e+00\n",
            "  1.0266769e+00 -4.7464166e-03  8.1779100e-02  9.3297803e-01\n",
            "  1.5681834e+00  3.5318863e+00 -7.9042393e-01  4.2578721e+00\n",
            " -2.8431398e-01  5.4625921e+00 -4.9155176e-02  4.2030787e+00\n",
            "  1.0444589e+00  3.3315039e+00  1.0725085e+00]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 43 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.02997061 -0.46572146 -0.81588924  2.6760821   1.1405346  -0.19909397\n",
            "  0.50166774  0.94841313  1.6125474   4.0611463  -0.5444201   4.252412\n",
            " -0.4002547   5.5123973  -0.19091411  4.390655    1.2303034   3.479725\n",
            "  0.9031316 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 44 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.11034611 -0.38858867 -0.963562    2.6597917   0.9321428  -0.1590956\n",
            "  0.27508277  0.85245633  1.8532854   3.6475039  -0.7834455   4.9034395\n",
            " -0.34922916  5.3382773  -0.2550751   4.6961384   0.8665122   3.5723705\n",
            "  0.7766937 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 45 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.11699896 -0.4491185  -0.82200825  2.9023743   1.1746776  -0.28941545\n",
            "  0.38838908  1.0387982   1.8207748   4.1125116  -0.8273912   4.5229893\n",
            " -0.7497343   5.898219   -0.08861431  5.0593762   1.2589155   3.1732175\n",
            "  1.0739774 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 46 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.02659285e-01 -4.19552386e-01 -9.57612336e-01  2.95853758e+00\n",
            "  1.03989851e+00 -8.49258080e-02  5.61365306e-01  1.19679797e+00\n",
            "  1.93672156e+00  3.90257001e+00 -7.18171954e-01  4.48562956e+00\n",
            " -6.43825889e-01  6.21180105e+00  5.60052041e-03  4.89788342e+00\n",
            "  1.12566328e+00  3.40672565e+00  1.38266540e+00]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 47 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.10468189 -0.60000616 -0.8626942   2.4891222   1.0111865  -0.09865271\n",
            "  0.6703833   0.91263425  1.694986    3.9947631  -0.37620333  4.070793\n",
            " -0.39070928  5.968671   -0.26601827  4.2532144   1.1822631   3.261327\n",
            "  1.065844  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 48 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.1989404  -0.36523098 -0.91350025  2.5372229   0.9500382  -0.19246659\n",
            "  0.71432316  0.76868325  1.6222408   3.7276523  -0.78227526  4.6648154\n",
            " -0.26615947  5.998662   -0.16613321  4.418183    0.93983847  3.4580219\n",
            "  0.74255294]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 49 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.00901959 -0.4151941  -0.74572647  2.9141867   1.0489222  -0.26116654\n",
            "  0.69321764  0.8019475   1.7705113   3.9092906  -0.86545974  4.653271\n",
            " -0.5027118   6.569177   -0.01164777  4.8556485   1.0637333   3.265964\n",
            "  0.9672392 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 50 reward=2 new_state=[0 0 0 0 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.13256986 -0.5526037  -0.9092893   2.850798    1.0875839  -0.12897822\n",
            "  0.7822275   1.1071488   1.9209237   3.7880921  -0.6940635   4.4427743\n",
            " -0.60159004  6.9660716  -0.05784462  4.7231245   1.187821    3.1656191\n",
            "  1.3207486 ]\n",
            "Epsilon reduced to 0.06457008150000002\n",
            "Total reward: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4vs91yBCeYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_conv_26(env):\n",
        "    input_shape = env.observation_shape()\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape + (1, ), input_shape=input_shape))\n",
        "    model.add(Conv1D(3, kernel_size=3, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(env.action_space.n, activation=\"linear\"))\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(lr=0.01))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DyGU3bdUjuI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec3405fa-d67d-4137-a47a-7f1544a6058c"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=50, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=8, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_26(env), max_epsilon=0.2, epsilon_decay=0.8)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_14 (Reshape)         (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_14 (Conv1D)           (None, 187, 3)            12        \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 561)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 19)                10678     \n",
            "=================================================================\n",
            "Total params: 10,690\n",
            "Trainable params: 10,690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |████████████----------------------------------------------------------------------------------------| 12.5% \r\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.02773669 -0.497844    0.3137277  -0.05656578 -0.16181946 -0.10193048\n",
            "  0.4414969   0.15559205 -0.27363232  0.11945047 -0.1051182   0.12645145\n",
            "  0.15797016  0.14302644  0.13940848  0.4087819   0.02485208  0.13727969\n",
            "  0.2629143 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.11620858 -0.05808623  0.0986305  -0.02675154  0.29625452  0.00390583\n",
            "  0.27296993  0.12573762 -0.07668282  0.04252211 -0.36471096  0.2540346\n",
            "  0.11821681 -0.08756311  0.08621334  0.32004568 -0.00404035 -0.1414945\n",
            "  0.0514483 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.29019493 -0.29641578  0.2566091   0.00868536  0.12917957 -0.02264285\n",
            "  0.09631816  0.20692912 -0.06719008  0.00969391 -0.17069681  0.02669269\n",
            "  0.14791733  0.03897058  0.008174    0.27581933  0.20570245  0.05985141\n",
            "  0.1298949 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.22292061 -0.24614681  0.01553205 -0.0833928  -0.10774403  0.01205133\n",
            "  0.03610442 -0.1379224  -0.23852469 -0.01287437 -0.19941458  0.23903815\n",
            " -0.03425058 -0.03511427 -0.24253632  0.10730793  0.08967048  0.05968801\n",
            " -0.02652221]\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.07417396 -0.28729996  0.21954519 -0.05483922 -0.08386794  0.02267912\n",
            " -0.22998188  0.13205539 -0.10956037  0.25061134 -0.2291731  -0.09937951\n",
            " -0.06950177 -0.03938364  0.09123909  0.13118125  0.22433019  0.11324111\n",
            "  0.09199161]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.11121459 -0.0866157   0.0766077  -0.07689434 -0.19317436  0.0067289\n",
            " -0.2525584   0.10542358 -0.21897686  0.13617681 -0.28519684  0.00580208\n",
            " -0.04883638 -0.1546941   0.1304023  -0.31486726 -0.06223229  0.22938786\n",
            " -0.02884681]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 7 reward=1 new_state=[0 0 0 0 1 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.17603034 -0.18631983  0.08966538 -0.03960983 -0.25742427 -0.15876605\n",
            " -0.11463194 -0.09626514 -0.16769329  0.09709159 -0.20695826 -0.04002915\n",
            "  0.03070778 -0.09813771  0.14500907 -0.19036123 -0.09736021  0.1183024\n",
            "  0.01997231]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 14\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.14248982 -0.10415374 -0.01168806 -0.09580776 -0.09527541 -0.04790917\n",
            " -0.08792435 -0.00090097 -0.03623534  0.04142449 -0.19024354 -0.09014737\n",
            "  0.03678246 -0.10891578 -0.03231532 -0.15509115 -0.0370583   0.13101442\n",
            "  0.04357121]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1566776  -0.1456163   0.09257402 -0.09331935 -0.04550987 -0.04443809\n",
            " -0.1307314  -0.03666127 -0.09811155 -0.08627082 -0.30601552 -0.09456312\n",
            "  0.0854419  -0.14295985  0.02545652 -0.25486127 -0.05070137  0.18579237\n",
            "  0.09848975]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.07365727 -0.13767494  0.17866148 -0.08754294 -0.13532971 -0.08440035\n",
            " -0.10595125  0.09261204  0.0149476  -0.08290147 -0.25673658 -0.16603263\n",
            "  0.04055827 -0.07967611 -0.08878058 -0.21073368 -0.0809458   0.00712105\n",
            "  0.10710221]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.07771984 -0.13304646 -0.01578896 -0.09915957 -0.18040487 -0.09388018\n",
            " -0.11471175  0.017387   -0.08673272  0.00484397 -0.16202304 -0.1132631\n",
            "  0.11051504 -0.06204954 -0.13266827 -0.2258908  -0.02719225  0.08547068\n",
            "  0.05066032]\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.12765628 -0.05001072  0.04035594 -0.19404253 -0.06198058 -0.05412124\n",
            " -0.16386539  0.04224086 -0.14046204 -0.11300766 -0.10764623 -0.15253907\n",
            "  0.06347455 -0.18323466 -0.14479741 -0.33596173  0.04076654  0.32353166\n",
            "  0.05195681]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 13 reward=-1 new_state=[1 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.01472427 -0.01155696 -0.07018825 -0.18237899 -0.13951437 -0.07151226\n",
            " -0.1744504   0.04107523 -0.12327409 -0.20669825 -0.17325458 -0.25395304\n",
            "  0.00844072 -0.07507008 -0.09998012 -0.14490297  0.03353014  0.32328078\n",
            "  0.06698517]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 14 reward=-1 new_state=[1 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.24528728 -0.14201254 -0.06616725 -0.03147317 -0.12205462 -0.03187725\n",
            " -0.19539642 -0.07163955 -0.11618602 -0.21504545 -0.21677901 -0.24024214\n",
            "  0.20279421 -0.05919522 -0.09945222 -0.21455969 -0.06724031  0.28217998\n",
            "  0.07284849]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 15 reward=-1 new_state=[1 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.32960352 -0.24034953 -0.1482984  -0.10009082 -0.31401914 -0.03819542\n",
            " -0.27899718 -0.0238932  -0.17992026 -0.22139154 -0.21514444 -0.2578975\n",
            "  0.17859365 -0.12118415 -0.1539365  -0.26361728 -0.05953088  0.35672185\n",
            "  0.01079865]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 7\n",
            "\n",
            "Step 16 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.29425907 -0.2261117  -0.12459399 -0.19864383 -0.11140723 -0.05530258\n",
            " -0.10994279 -0.02981172 -0.22884084 -0.25384763 -0.06747466 -0.143375\n",
            "  0.23600903 -0.18415102 -0.11764009 -0.29005063  0.07611269  0.16137818\n",
            " -0.00274892]\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 17 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.07962792 -0.17753932 -0.13987665 -0.10436515 -0.12539521 -0.06915088\n",
            " -0.12614128 -0.10927731 -0.10878795 -0.08296235 -0.10686439 -0.14671311\n",
            "  0.12018456  0.04275556 -0.15698588 -0.27920875 -0.07705314  0.12660003\n",
            "  0.00904353]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 7\n",
            "\n",
            "Step 18 reward=-2 new_state=[1 0 0 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.28107733 -0.2180794  -0.09321048  0.05011916 -0.30080986  0.11395947\n",
            " -0.17938238 -0.23179023  0.01978117 -0.2769184  -0.03074982 -0.38127372\n",
            "  0.00558268 -0.0356117  -0.20817772 -0.2196768   0.11282469 -0.0346229\n",
            " -0.08921757]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 19 reward=-3 new_state=[1 0 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.45436418 -0.21516725 -0.05141475 -0.16521446 -0.14901826 -0.10407051\n",
            " -0.01630021 -0.24490274 -0.33646178 -0.19340296 -0.19550073 -0.37496042\n",
            "  0.08760956 -0.11912848 -0.27081794 -0.23402652  0.04536381  0.18054377\n",
            "  0.13602312]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 20 reward=-3 new_state=[1 0 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.14572653 -0.3032025  -0.08314155  0.04130338 -0.04938501 -0.13188533\n",
            " -0.06407513 -0.15401891 -0.134802   -0.26549962 -0.13311954 -0.20669684\n",
            "  0.07804061 -0.07206458 -0.18068051 -0.21108618  0.09658994  0.0470954\n",
            " -0.02774948]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 7\n",
            "\n",
            "Step 21 reward=-3 new_state=[1 0 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.36837757 -0.13993932 -0.00168175 -0.00340154 -0.13846853 -0.0290991\n",
            "  0.10778212 -0.14669874 -0.19702666 -0.09225087 -0.19255956 -0.26985514\n",
            "  0.03067544 -0.26140898 -0.0532366  -0.1792613   0.0840662  -0.03845637\n",
            "  0.02697677]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 22 reward=-2 new_state=[1 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.3169458  -0.30523688 -0.14683935  0.10719144 -0.1472699  -0.28236452\n",
            " -0.16466215 -0.30603996 -0.18008247 -0.03958622 -0.01272218 -0.24643113\n",
            " -0.00495717  0.06269415 -0.16891226 -0.2202104   0.02383339  0.11887225\n",
            " -0.01172485]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 23 reward=-3 new_state=[1 1 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.36475894 -0.37804517 -0.15805984  0.01475567 -0.20924209 -0.13787507\n",
            " -0.2689559  -0.34167394 -0.06727587 -0.14442506 -0.12202129 -0.22880565\n",
            "  0.08758432 -0.00515649 -0.29307708 -0.2627846  -0.01758275  0.0913045\n",
            " -0.00980112]\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 24 reward=-3 new_state=[1 1 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.22130503 -0.22631179 -0.18276832 -0.07136402 -0.11846521 -0.09231987\n",
            " -0.15789132 -0.3252061  -0.03957485 -0.1602448  -0.05507193 -0.3218747\n",
            "  0.01788891 -0.1533024  -0.30648878 -0.21463427  0.07039015  0.09267819\n",
            " -0.02404051]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 7\n",
            "\n",
            "Step 25 reward=-4 new_state=[1 1 1 1 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.24630542 -0.07447606 -0.12092926 -0.02877885 -0.02310275 -0.3030158\n",
            "  0.08441091 -0.34827796 -0.08257252 -0.15102236 -0.09831228 -0.21863642\n",
            " -0.02533124 -0.03245444 -0.2545832  -0.0990076   0.15857661 -0.19034158\n",
            "  0.0642148 ]\n",
            "\n",
            "Taking action 12 from 16\n",
            "\n",
            "Step 26 reward=-5 new_state=[1 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3393234  -0.28298753 -0.03709059 -0.12925375 -0.15301321 -0.24089709\n",
            " -0.00331488 -0.4388283   0.00732047 -0.1673809  -0.02504982 -0.24232063\n",
            " -0.13804004 -0.01019796 -0.308494   -0.00970569  0.23361179 -0.22391072\n",
            "  0.07299545]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 27 reward=-5 new_state=[1 1 1 1 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18077427 -0.31475967 -0.04780699  0.02597279 -0.00778896 -0.25658968\n",
            " -0.16303265 -0.3366552   0.02872321 -0.12211558 -0.1651985  -0.26065943\n",
            " -0.07718424 -0.0444309  -0.15213548 -0.09644777  0.04434232 -0.17758417\n",
            " -0.0469519 ]\n",
            "\n",
            "Taking action 16 from 8\n",
            "\n",
            "Step 28 reward=-6 new_state=[1 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3389869  -0.2985931  -0.04605208 -0.02555002 -0.1554293  -0.2962244\n",
            " -0.07610394 -0.4186418   0.09039171 -0.09078366 -0.1163717  -0.14916891\n",
            " -0.1621831  -0.04172804 -0.24883214 -0.04757473  0.07991819 -0.26371396\n",
            "  0.06938745]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 29 reward=-6 new_state=[1 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.20727432 -0.3129967  -0.14370462 -0.00077554 -0.05924011 -0.28376582\n",
            " -0.16036923 -0.35390973 -0.00929587 -0.14009179 -0.11526478 -0.24734595\n",
            " -0.04906966 -0.00749462 -0.14738831 -0.13138205 -0.13029404 -0.1746259\n",
            "  0.06812265]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 30 reward=-6 new_state=[1 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.18474981 -0.2783323  -0.00108004 -0.12285494 -0.05970579 -0.13756327\n",
            " -0.18054467 -0.32413408 -0.03182761 -0.19516958 -0.16204761 -0.1690751\n",
            " -0.03483221  0.05328362 -0.1450551  -0.20746216 -0.08311671 -0.13997889\n",
            "  0.02779368]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 31 reward=-5 new_state=[1 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.22578299 -0.23275761 -0.08269243 -0.09220287 -0.07185425 -0.13100028\n",
            " -0.14250186 -0.39036268 -0.08050932 -0.22013158 -0.18710348 -0.20926394\n",
            "  0.0130818  -0.00076023 -0.11700934 -0.22563434 -0.10052263 -0.12424886\n",
            " -0.03925396]\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 32 reward=-6 new_state=[1 1 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.32346886 -0.2799307  -0.04086732 -0.15107751 -0.17375715 -0.23655716\n",
            " -0.1526346  -0.40690154 -0.205218   -0.2006616  -0.1740548  -0.18407279\n",
            " -0.06600438  0.05654484 -0.21661928 -0.28824627 -0.12261996 -0.15854806\n",
            " -0.06074986]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 33 reward=-5 new_state=[1 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.16950886 -0.24352925 -0.02028584 -0.12956136 -0.10445622 -0.18760063\n",
            " -0.14650133 -0.37506777 -0.17906827 -0.157685   -0.13857128 -0.2392586\n",
            " -0.09547912  0.03727671 -0.09781414 -0.24477841 -0.14345151 -0.1615887\n",
            " -0.1155971 ]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 34 reward=-5 new_state=[1 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.23027866 -0.3511194  -0.04526142 -0.205388   -0.18413717 -0.22286594\n",
            " -0.22635607 -0.50583774 -0.11051944 -0.26373678 -0.30551746 -0.27438733\n",
            " -0.13383193 -0.0588118  -0.2030319  -0.23635115 -0.1663914  -0.26432392\n",
            " -0.06148538]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 35 reward=-4 new_state=[1 0 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.36641514 -0.35770783 -0.07482276 -0.32440528 -0.21037154 -0.17715164\n",
            " -0.3285702  -0.5275928  -0.2027738  -0.11313457 -0.2663636  -0.19728972\n",
            " -0.15917766 -0.17639425 -0.19499014 -0.2566408  -0.2114353  -0.12393799\n",
            " -0.14074531]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 36 reward=-4 new_state=[1 0 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.24755272 -0.4465322  -0.1609761  -0.19321543 -0.10858552 -0.32976645\n",
            " -0.36544544 -0.7424975  -0.21030198 -0.09487648 -0.10581905 -0.21997805\n",
            " -0.21018773 -0.12545504 -0.24355766 -0.25836483 -0.16724852 -0.23165394\n",
            " -0.22296217]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 37 reward=-3 new_state=[1 0 1 1 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4596348  -0.45784155 -0.32237104 -0.25848562 -0.3457266  -0.4115275\n",
            " -0.25950977 -0.6532222  -0.2778278  -0.39930618 -0.12726481 -0.37739095\n",
            " -0.20767906 -0.18773487 -0.39894128 -0.34567326 -0.15926017 -0.27361074\n",
            " -0.09204955]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 38 reward=-2 new_state=[1 0 1 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.21703802 -0.17580444 -0.2786196  -0.09070538 -0.12825745 -0.32929093\n",
            " -0.18230596 -0.55572575 -0.1999321  -0.30312616 -0.14059722 -0.12020748\n",
            " -0.27714232 -0.22873452 -0.15433176 -0.1457549  -0.08453304 -0.22429587\n",
            " -0.09431836]\n",
            "\n",
            "Taking action 12 from 16\n",
            "\n",
            "Step 39 reward=-2 new_state=[1 0 1 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.21950772 -0.27105147 -0.42645127 -0.18416363 -0.19651228 -0.40120378\n",
            " -0.25912553 -0.71324134 -0.27466577 -0.39206916 -0.2248386  -0.23191898\n",
            " -0.35694033 -0.32499796 -0.30206352 -0.20448025 -0.226142   -0.50853956\n",
            " -0.11859615]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 40 reward=-2 new_state=[1 0 1 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.16100419 -0.4588068  -0.3491875  -0.3095665  -0.3204831  -0.5004074\n",
            " -0.5076819  -0.7707029  -0.42646834 -0.368577   -0.18853791 -0.43510285\n",
            " -0.6052672  -0.2693308  -0.21701902 -0.44847852 -0.2580815  -0.25971556\n",
            " -0.20898883]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12 from 16\n",
            "\n",
            "Step 41 reward=-2 new_state=[1 0 1 1 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.45906147 -0.35269994 -0.38695535 -0.2659178  -0.24161628 -0.5365662\n",
            " -0.5934732  -0.8023944  -0.22473949 -0.64694846 -0.2625213  -0.3390464\n",
            " -0.6136221  -0.35207915 -0.3944757  -0.30449522 -0.30664614 -0.33572376\n",
            " -0.34599113]\n",
            "\n",
            "Taking action 16 from 8\n",
            "\n",
            "Step 42 reward=-3 new_state=[1 0 1 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.32201582 -0.53400725 -0.5459039  -0.24972987 -0.2970106  -0.47613466\n",
            " -0.4338081  -0.91849774 -0.31492656 -0.5456412  -0.46520007 -0.3408497\n",
            " -0.45845723 -0.37541118 -0.2782396  -0.2655591  -0.41244537 -0.45808455\n",
            " -0.43561807]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 43 reward=-2 new_state=[1 0 1 1 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.38584495 -0.62514895 -0.8638646  -0.3575338  -0.24016565 -0.46369362\n",
            " -0.47594833 -0.92132294 -0.4291666  -0.48832405 -0.4478703  -0.33957157\n",
            " -0.47668692 -0.6420464  -0.41358736 -0.32599527 -0.57824576 -0.6220343\n",
            " -0.47926044]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 44 reward=-1 new_state=[1 0 0 1 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.35748646 -0.54638225 -0.6647223  -0.3396029  -0.20854346 -0.452631\n",
            " -0.39776868 -0.6561823  -0.36135268 -0.6283811  -0.41762096 -0.34605443\n",
            " -0.5216915  -0.5142598  -0.47066513 -0.2943376  -0.4733689  -0.32570726\n",
            " -0.4276424 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 45 reward=-1 new_state=[1 0 0 1 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.4107733  -0.74321306 -0.69944465 -0.32639208 -0.30708116 -0.51615775\n",
            " -0.61233574 -0.92565334 -0.65452    -0.72158414 -0.375792   -0.43123075\n",
            " -0.83485234 -0.64387125 -0.33906648 -0.2987218  -0.50391966 -0.48519158\n",
            " -0.740546  ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 46 reward=0 new_state=[1 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.32845578 -0.70539117 -0.97089005 -0.3247746  -0.4491331  -0.6456897\n",
            " -0.489691   -1.0313054  -0.7308596  -0.7435516  -0.6130104  -0.3679275\n",
            " -0.6795351  -0.7619855  -0.4550436  -0.38129476 -0.79711705 -0.5690278\n",
            " -0.81533664]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.554235   -0.59943306 -1.0040195  -0.41361263 -0.5888975  -0.62085843\n",
            " -0.46084806 -1.1854712  -0.6575831  -0.9733373  -0.6509875  -0.59040385\n",
            " -0.8552731  -0.8835946  -0.5196489  -0.28296563 -0.68197435 -0.5441665\n",
            " -0.6548966 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.7367831  -0.70859945 -0.9740466  -0.43628025 -0.911328   -0.8002157\n",
            " -0.5765105  -1.3056848  -0.839455   -1.0545313  -0.81568253 -0.62811166\n",
            " -1.0696172  -0.7760596  -0.653339   -0.2556738  -0.72648966 -0.7126853\n",
            " -0.80871016]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.6139468  -0.62387246 -0.9161893  -0.4996324  -1.1409934  -0.92199355\n",
            " -0.76427937 -1.3679143  -1.0232093  -1.278969   -0.7260871  -0.870435\n",
            " -1.1466339  -0.7406343  -0.6438805  -0.28882128 -0.8465221  -0.8757305\n",
            " -0.72205985]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.2270441  -0.64943    -0.908478   -0.37071845 -0.72378343 -0.61705405\n",
            " -0.5133825  -0.8994846  -0.6647798  -1.0498363  -0.48933348 -0.49420583\n",
            " -0.7491162  -0.6433451  -0.49214837  0.0511284  -0.6082952  -0.8554407\n",
            " -0.767569  ]\n",
            "Epsilon reduced to 0.16000000000000003\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6908848  -0.7439536  -0.5675155  -0.29947263 -1.3381749  -0.5515339\n",
            " -0.90409863 -1.1217313  -0.810015   -1.3645447  -0.5125908  -0.6456849\n",
            " -0.78820175 -0.6054936  -0.4375223  -0.25324816 -0.8371899  -0.8556003\n",
            " -0.6501446 ]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.30453724 -0.5395929  -1.0475283  -0.26131904 -1.0662966  -0.6112792\n",
            " -0.7295296  -1.1324924  -0.82357436 -0.9263436  -0.72330993 -0.81995463\n",
            " -0.73590904 -0.78166085 -0.50177914  0.03016268 -0.91808605 -0.6035056\n",
            " -0.58908135]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.1760256  -0.5885609  -1.0979464  -0.37377086 -1.3084221  -0.73479766\n",
            " -0.8392052  -1.3366212  -0.98702407 -0.8425917  -0.25815922 -0.69099176\n",
            " -0.85732883 -0.81963205 -0.48910698  0.23454566 -1.1435131  -0.75245625\n",
            " -0.7493305 ]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09875782 -0.6229103  -0.8951841  -0.4320858  -1.22176    -0.6060294\n",
            " -0.6296078  -1.3076217  -0.79929376 -0.784549   -0.5248668  -0.46213505\n",
            " -0.63792646 -0.6882627  -0.5820796   0.08166004 -1.1669527  -0.69820905\n",
            " -0.58688986]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.06953692 -0.61646694 -0.88074666 -0.4816796  -1.1846849  -0.5253457\n",
            " -0.6741021  -1.1450999  -0.9339592  -0.8444997  -0.5089235  -0.5120441\n",
            " -0.6553981  -0.6845484  -0.3697946   0.03997538 -0.9493066  -0.57748276\n",
            " -0.6383556 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.12003987 -0.7289314  -1.1355445  -0.29254922 -1.1530716  -0.69337463\n",
            " -0.77663964 -1.3603495  -1.2739823  -0.938811   -0.26151928 -0.6783166\n",
            " -0.9672198  -0.66183186 -0.3625534   0.03134209 -1.0804341  -0.5254844\n",
            " -0.8221331 ]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.21447507 -0.59455323 -1.16954    -0.36718062 -1.4414647  -0.6995203\n",
            " -0.68352365 -1.42377    -1.0948442  -0.7051286  -0.41522855 -0.67036486\n",
            " -0.7804462  -0.7661708  -0.67695916 -0.22075963 -1.1418601  -0.6163025\n",
            " -0.88005453]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2500269  -0.44546038 -0.8459166  -0.42492092 -0.999421   -0.54134804\n",
            " -0.6592736  -1.1020578  -0.87565905 -0.63069224 -0.24473545 -0.46989352\n",
            " -0.6664374  -0.6676948  -0.3372512  -0.2694627  -0.90522504 -0.6293291\n",
            " -0.39756966]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2631902  -0.42412066 -0.8946635  -0.37553936 -1.1157142  -0.56394434\n",
            " -0.85543877 -1.0047922  -0.89448726 -0.70956147 -0.318887   -0.64316505\n",
            " -0.70382285 -0.65970033 -0.3100037  -0.61689883 -1.0786935  -0.82407105\n",
            " -0.4373469 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 10 reward=-2 new_state=[0 0 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3984421  -0.52099705 -1.1739221  -0.48967552 -1.4319732  -0.7375434\n",
            " -0.75629365 -1.2805676  -1.0531784  -0.5931755  -0.17296499 -0.70722276\n",
            " -0.76900274 -0.78415674 -0.46153232 -0.7854611  -1.1969824  -0.9327834\n",
            " -0.7939419 ]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3041289  -0.70105296 -1.0960094  -0.4858203  -1.5127114  -0.65208226\n",
            " -0.5377332  -1.1987786  -1.0020182  -0.9971468  -0.16542985 -0.50205404\n",
            " -0.94061935 -0.73275745 -0.6372187  -0.44782618 -1.1587669  -1.0112522\n",
            " -1.1228924 ]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4263081  -0.6324728  -0.8327941  -0.3459286  -1.1980878  -0.76100445\n",
            " -0.4978982  -1.1812743  -0.8674682  -0.9483025   0.17745617 -0.3316008\n",
            " -1.0054076  -0.52779377 -0.4488669   0.01455922 -0.6779953  -0.8159958\n",
            " -1.0994227 ]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 13 reward=-2 new_state=[0 0 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.47554812 -0.4452808  -0.95343244 -0.44111165 -1.063149   -0.5110286\n",
            " -0.48521054 -1.0667816  -0.8515701  -0.70956504 -0.38726267 -0.40009013\n",
            " -0.7535157  -0.49342775 -0.34506685 -0.39342284 -0.6740045  -0.505256\n",
            " -0.62120444]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.94750977 -0.5684926  -1.0031102  -0.40973797 -1.5622863  -0.6763777\n",
            " -0.58853847 -1.4727421  -1.0501808  -0.54199475 -0.7837436  -0.52044153\n",
            " -0.7364415  -0.764496   -0.57723975 -1.1223532  -1.1140064  -0.81883365\n",
            " -0.80094016]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.48093033 -0.4464828  -1.2209979  -0.30695117 -1.3820518  -0.38978535\n",
            " -0.43771544 -1.3415552  -0.8121681  -0.449164   -0.78023803 -0.55584615\n",
            " -0.66853994 -0.9684477  -0.47660816 -0.82279557 -1.1254933  -0.57855976\n",
            " -0.7269951 ]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 16 reward=-2 new_state=[0 1 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.24229655 -0.5586563  -0.9611839  -0.45229116 -1.0640746  -0.5049205\n",
            " -0.38668352 -0.9578273  -0.6663307  -0.71131945 -0.52414155 -0.48066694\n",
            " -0.74883467 -0.5622951  -0.5067226  -0.25904942 -0.88375324 -0.6097292\n",
            " -0.6817844 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 17 reward=-2 new_state=[0 1 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.81070215 -0.660164   -0.8233355  -0.42391163 -1.2269721  -0.69747454\n",
            " -0.65301716 -1.3324175  -1.1287065  -0.9352687  -0.73606807 -0.60158855\n",
            " -1.0921267  -0.8041957  -0.6427808  -0.50589395 -1.0648379  -0.6586919\n",
            " -0.9654331 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 18 reward=-2 new_state=[0 1 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.1491703  -0.6233369  -1.0440516  -0.5669402  -1.3051101  -0.6578064\n",
            " -0.49206972 -1.6342688  -1.0388066  -0.6368164  -1.447223   -0.6602435\n",
            " -0.8298505  -0.889491   -0.8376927  -1.2071536  -1.2691308  -0.67385143\n",
            " -0.8017739 ]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.5111476  -0.61006004 -0.93371654 -0.86439425 -1.3383251  -0.702561\n",
            " -0.81537175 -2.1473186  -1.372856   -0.9559334  -1.4253544  -0.7419844\n",
            " -1.1407498  -0.80401003 -0.644152   -1.01423    -1.1859701  -1.0326157\n",
            " -0.96036434]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 20 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5203523  -0.40901658 -0.725661   -0.6193417  -0.86864567 -0.29315358\n",
            " -0.43205532 -1.2492102  -0.6938643  -0.45086432 -1.050989   -0.42271727\n",
            " -0.8795963  -0.86073107 -0.2510259  -0.88721204 -0.9121894  -0.5341432\n",
            " -0.6242173 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 21 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.35166323 -0.38133132 -0.5622912  -0.36211735 -0.7271255  -0.29748008\n",
            " -0.3642456  -0.7403764  -0.3635109  -0.32214487 -0.6023459  -0.27468395\n",
            " -0.53728354 -0.45863625 -0.3959449  -0.3520946  -0.6746893  -0.43414807\n",
            " -0.3510319 ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 22 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2971208  -0.5024905  -0.8446014  -0.47335118 -1.0528395  -0.37547395\n",
            " -0.5334903  -1.0191519  -0.5494363  -0.36196643 -0.8484755  -0.41885442\n",
            " -0.6490352  -0.49337137 -0.5940271  -0.4598251  -0.96740615 -0.8927728\n",
            " -0.62220955]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 23 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.2593094  -0.8974031  -0.89576674 -0.6933082  -1.3477585  -0.6869305\n",
            " -0.9472655  -1.4477329  -1.0566561  -0.4311366  -1.4119736  -0.5982634\n",
            " -0.90508765 -0.5314012  -1.0618194  -1.1465183  -1.4162673  -0.90247345\n",
            " -0.5810212 ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 24 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3562763  -1.9661071  -1.3666962  -1.3502188  -2.0756783  -0.88412535\n",
            " -1.7398554  -2.6396904  -1.8973272  -0.9091866  -2.3033025  -1.1358199\n",
            " -1.7019564  -1.2122756  -1.716421   -2.1313207  -1.8846745  -1.952902\n",
            " -1.2042183 ]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 25 reward=-3 new_state=[1 1 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.5331475  -1.9078827  -1.2121531  -1.3656135  -2.003818   -0.61172336\n",
            " -1.30473    -2.6312675  -1.5417783  -1.276813   -1.6590247  -0.9928346\n",
            " -1.7245531  -1.1620514  -1.4647391  -1.1468258  -1.5594954  -1.5356392\n",
            " -1.5488515 ]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 26 reward=-3 new_state=[1 1 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4690736  -1.1180446  -0.8854081  -0.7273145  -1.4871483  -0.49280193\n",
            " -0.7488289  -1.3866479  -0.77419657 -0.8213315  -0.87034136 -0.50469697\n",
            " -0.9537818  -0.74193853 -1.2502693  -0.31242037 -1.2077805  -1.0346669\n",
            " -0.7991452 ]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 27 reward=-4 new_state=[1 1 1 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8521341  -1.71876    -1.1568073  -0.92838264 -1.8986813  -1.1005567\n",
            " -0.8934276  -1.934092   -1.2408594  -1.2472557  -0.97464293 -0.7353825\n",
            " -1.4470943  -0.7967711  -1.6578282   0.08508788 -1.4624531  -1.386645\n",
            " -1.1762992 ]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 28 reward=-4 new_state=[1 1 1 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.1997348  -2.143762   -1.2042323  -0.9782667  -2.1890604  -1.5893067\n",
            " -1.2456642  -2.193074   -1.749892   -1.5430219  -1.1601797  -0.9417525\n",
            " -1.7503083  -0.9771521  -2.0826223  -0.05392781 -1.7759552  -1.590883\n",
            " -1.5152285 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 29 reward=-5 new_state=[1 1 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.56052    -2.189884   -1.512779   -1.3207769  -2.1923535  -1.9438602\n",
            " -1.0620104  -2.0234978  -1.4897958  -1.6691604  -1.4225353  -0.8932798\n",
            " -1.6964751  -0.95059705 -2.4220266  -0.69769585 -1.8342017  -1.2406676\n",
            " -1.3403904 ]\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 30 reward=-4 new_state=[1 1 1 1 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.0200014  -2.8386161  -1.5145525  -1.5735004  -3.206163   -2.8863482\n",
            " -1.3938551  -2.6532125  -1.9230317  -1.7088212  -1.3139927  -0.93279874\n",
            " -2.2853835  -1.1784173  -2.9030576  -0.34496582 -1.9769601  -1.7788482\n",
            " -1.6334432 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 31 reward=-4 new_state=[1 1 1 1 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1501052  -3.072803   -1.8438295  -1.7694814  -3.1815975  -3.0565493\n",
            " -1.3814756  -3.078272   -2.1705554  -2.2075434  -1.3626633  -1.5783862\n",
            " -2.25955    -0.9876042  -3.2532003  -0.91271216 -2.2316074  -2.0050159\n",
            " -1.6411481 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 32 reward=-3 new_state=[1 1 1 1 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-2.7405581 -4.0901127 -2.4060717 -2.5101454 -4.5548754 -4.454036\n",
            " -2.0703046 -5.2178082 -2.9373407 -3.0036128 -2.0006135 -2.5288825\n",
            " -3.0069962 -1.9152794 -4.586074  -2.2081532 -3.1542382 -2.2116046\n",
            " -2.245598 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 33 reward=-3 new_state=[1 1 1 1 1 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-2.6717942 -4.27124   -2.816762  -2.5815969 -4.778139  -4.6085196\n",
            " -2.1930854 -5.70184   -2.8674169 -3.3746517 -2.457436  -3.0720522\n",
            " -2.8853724 -2.6911142 -4.647393  -2.8842087 -3.386528  -2.6118064\n",
            " -2.442011 ]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 34 reward=-2 new_state=[1 1 1 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.0227296 -5.580549  -2.6205518 -3.0748212 -5.7101555 -5.7564187\n",
            " -2.5723515 -7.2139688 -2.9580626 -4.230758  -2.4847744 -4.123786\n",
            " -3.3858516 -3.430086  -5.5017214 -3.4436011 -3.6565056 -2.9320276\n",
            " -2.550048 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 35 reward=-2 new_state=[1 1 1 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-4.0944824 -6.1050553 -3.3112376 -3.6346424 -6.213208  -6.4701343\n",
            " -3.888993  -9.107776  -3.5854692 -4.8502374 -3.8150983 -4.7241654\n",
            " -3.3245363 -4.285623  -6.5550003 -5.208199  -4.731694  -3.4422643\n",
            " -3.3692496]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 36 reward=-1 new_state=[1 0 1 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-4.8704977 -6.7074656 -3.5103486 -4.2953186 -6.118312  -6.9819713\n",
            " -5.0631585 -9.997156  -4.168946  -5.3337426 -4.367356  -5.7156672\n",
            " -3.2759666 -4.685889  -6.928963  -6.844613  -4.994902  -3.0988867\n",
            " -3.1302419]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 37 reward=-2 new_state=[1 0 1 1 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.3085558 -4.868891  -3.0407562 -2.876495  -5.2730865 -4.8226357\n",
            " -3.945693  -7.2341933 -2.7964742 -4.417376  -2.7888167 -4.286523\n",
            " -2.4541461 -4.1441503 -5.262817  -4.7192855 -3.7031167 -2.3543434\n",
            " -2.2514012]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 38 reward=-2 new_state=[1 0 1 1 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-4.062851  -5.8794956 -3.6638145 -3.0192704 -6.1532054 -6.087235\n",
            " -5.231753  -8.793953  -3.4456697 -5.418681  -3.1853092 -5.6216745\n",
            " -2.9461975 -5.1314178 -6.2769647 -6.5516915 -4.9197345 -2.6435165\n",
            " -2.638163 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 39 reward=-1 new_state=[1 0 1 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ -4.986314   -7.318748   -4.676853   -4.0220423  -7.3833895  -7.277225\n",
            "  -6.376486  -11.127228   -4.2144322  -6.1961093  -2.922189   -6.429333\n",
            "  -3.5437     -6.0398335  -7.4728165  -7.6388483  -5.346597   -3.6398938\n",
            "  -4.3742156]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 40 reward=-1 new_state=[1 0 1 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ -4.986186   -8.150369   -5.848455   -3.9257572  -8.536767   -8.109917\n",
            "  -7.503055  -12.723305   -4.787406   -7.171367   -2.242296   -7.667002\n",
            "  -3.8076375  -7.4750905  -8.396996   -8.74027    -5.680247   -4.744714\n",
            "  -5.877208 ]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 41 reward=-1 new_state=[1 0 1 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ -4.8268685  -8.514887   -7.217836   -4.664187   -8.7441635  -8.279225\n",
            "  -8.800656  -13.849927   -5.1084466  -7.0816107  -2.6731498  -8.415511\n",
            "  -3.6756637  -8.116757   -8.645327  -10.308491   -6.512246   -5.188273\n",
            "  -6.6341   ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 42 reward=-2 new_state=[1 0 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ -3.4935892  -6.46517    -5.6433773  -3.3537302  -7.086669   -6.264025\n",
            "  -7.3388066 -10.818841   -3.5654755  -6.101735   -2.0421057  -7.220262\n",
            "  -3.0591915  -6.7372303  -6.8652763  -8.29973    -4.9882274  -4.5989494\n",
            "  -5.376506 ]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 43 reward=-2 new_state=[1 0 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ -3.8107915  -6.8073535  -6.163282   -3.288229   -7.3798566  -6.6816144\n",
            "  -7.547435  -11.295307   -2.7954142  -6.3461666  -2.2652836  -7.4452395\n",
            "  -3.3302443  -7.3327804  -7.2249656  -8.81573    -4.9822073  -5.354535\n",
            "  -6.1383233]\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 44 reward=-2 new_state=[1 0 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ -5.2912755  -8.812613   -7.9217405  -4.56005    -8.815687   -8.711926\n",
            "  -9.864759  -15.110322   -2.60407    -7.5999947  -4.5545297  -9.2252\n",
            "  -3.9797523  -9.321497   -9.203759  -11.593085   -6.1610317  -7.090983\n",
            "  -8.296636 ]\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 45 reward=-2 new_state=[1 0 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ -5.9009743  -9.349239   -8.450233   -4.4799867  -9.469987   -9.210863\n",
            " -10.488414  -15.929225   -3.059425   -8.440952   -5.978632   -9.973471\n",
            "  -4.506002   -9.698306   -9.714289  -12.342777   -6.619937   -7.8648076\n",
            "  -9.254252 ]\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 46 reward=-2 new_state=[1 0 1 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ -5.7290382  -8.761614   -7.988923   -4.1940756  -8.349876   -8.118507\n",
            " -10.422184  -14.56294    -3.3867514  -7.9252486  -6.681998   -9.78708\n",
            "  -3.9199615  -9.224832   -8.912915  -12.099954   -6.831939   -7.6304293\n",
            "  -8.53975  ]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 47 reward=-3 new_state=[1 0 1 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ -6.517038   -9.364109   -8.796753   -4.5128646  -7.8482103  -8.392216\n",
            " -11.610454  -15.938268   -5.565598   -8.37114    -7.515921  -10.454737\n",
            "  -4.1317635 -10.211512   -9.570913  -13.526175   -6.8701844  -8.055105\n",
            "  -9.666332 ]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 48 reward=-4 new_state=[1 1 1 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ -5.961458   -9.408821   -8.884814   -4.840185   -8.3167925  -8.651955\n",
            " -11.717307  -16.411575   -5.857282   -8.209226   -7.3710136 -11.210288\n",
            "  -5.635145  -10.37928    -9.785277  -13.239035   -6.220676   -8.373261\n",
            "  -9.5606785]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 49 reward=-3 new_state=[0 1 1 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ -4.59216    -8.103169   -8.271854   -5.0240803  -7.7979     -6.9795375\n",
            " -10.628221  -14.515243   -5.492193   -7.168786   -6.716253   -9.573426\n",
            "  -5.3145757  -9.888955   -8.503101  -11.783528   -5.38992    -8.070852\n",
            "  -9.232445 ]\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 50 reward=-4 new_state=[0 1 1 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ -5.927045   -8.252042   -9.074934   -6.26379    -7.538572   -7.0387287\n",
            " -10.959654  -15.02555    -6.1095366  -6.9391494  -7.281325  -10.307339\n",
            "  -6.153108  -10.057231   -8.528259  -12.822452   -5.225055   -7.989535\n",
            "  -9.506623 ]\n",
            "Epsilon reduced to 0.12800000000000003\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ -8.887154  -11.166389  -11.711187   -9.137499   -9.53398   -10.641089\n",
            " -14.98636   -20.059784   -9.779443  -10.210696   -9.484365  -14.222154\n",
            "  -9.196988  -13.444565  -11.325924  -17.785074   -5.7904716 -11.143159\n",
            " -13.071457 ]\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-10.406264 -12.609457 -13.20557  -11.033518 -12.110854 -11.489714\n",
            " -16.196955 -22.570309 -10.440272 -11.258459 -10.556115 -15.811414\n",
            " -11.354939 -14.844268 -13.151611 -17.229237  -5.285674 -12.353559\n",
            " -14.929536]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 3 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-11.269156  -11.750341  -12.900998  -11.547694   -9.798178  -11.0012865\n",
            " -15.772246  -21.113886  -11.06291    -9.626323  -11.163874  -14.493937\n",
            " -10.829798  -14.201007  -11.323043  -18.719816   -4.5449924 -11.96316\n",
            " -13.952041 ]\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 4 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-11.409079  -11.917953  -13.362673  -12.350201  -11.221778  -10.931962\n",
            " -15.344906  -21.115292  -11.075591   -9.60703   -11.217469  -14.541537\n",
            " -11.755496  -12.646292  -12.07658   -17.076645   -3.8703253 -11.814812\n",
            " -14.395008 ]\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 5 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-12.104624  -11.145693  -11.995327  -12.349147   -9.1766405  -9.976871\n",
            " -14.6061945 -19.491596  -11.428862   -8.974651  -11.334793  -13.578665\n",
            " -11.753153  -10.361006  -11.305221  -16.503565   -4.9863186 -11.215171\n",
            " -13.187467 ]\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 6 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-12.895603  -10.850648  -11.607554  -12.936735   -9.744358   -9.834496\n",
            " -14.41222   -19.678747  -11.71612    -9.802789  -11.580377  -13.818048\n",
            " -12.45919    -9.6233635 -11.022953  -15.9823065  -6.3674154 -11.427428\n",
            " -13.437114 ]\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 7 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-11.930496   -9.827792  -11.291501  -12.161707   -9.166929   -9.087648\n",
            " -13.576702  -18.366062  -11.001531   -8.574771  -11.128929  -12.6164055\n",
            " -11.066685   -7.6343856  -9.955768  -15.410017   -6.797124  -10.034497\n",
            " -12.415413 ]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 8 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-13.601245 -12.105045 -13.783179 -14.992025 -10.686762 -10.803984\n",
            " -16.093147 -21.35126  -12.940163  -9.271576 -12.33594  -15.267439\n",
            " -13.893759  -8.019438 -11.988212 -17.599995  -8.754999 -12.165506\n",
            " -14.974774]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 9 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-13.517188  -10.464683  -11.469047  -13.378151   -8.810617   -9.616154\n",
            " -13.80312   -18.471397  -12.285071   -8.794757  -11.073223  -13.0407915\n",
            " -12.731924   -6.396877  -10.717419  -14.757123   -9.375791  -10.997233\n",
            " -12.662965 ]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 10 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-13.80656  -10.605775 -11.76643  -14.474148 -10.177536  -9.888044\n",
            " -14.399564 -19.685202 -12.668433  -9.544253 -12.220212 -13.549694\n",
            " -13.234161  -6.685571 -10.900378 -16.356365 -10.171928 -11.310418\n",
            " -13.511631]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-13.942231  -11.173826  -12.598794  -15.018292  -10.327592  -10.730636\n",
            " -14.675366  -19.90165   -13.028376   -9.010674  -12.292184  -13.997429\n",
            " -13.543013   -5.8313484 -11.152465  -16.423382  -11.358583  -11.755139\n",
            " -14.259688 ]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-14.810317  -11.62346   -13.647629  -15.99377    -9.74481   -10.382795\n",
            " -15.798782  -20.529142  -13.851884   -9.824237  -12.429021  -14.619706\n",
            " -14.75389    -5.5115714 -11.135023  -17.874477  -12.482155  -12.382856\n",
            " -14.613307 ]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 13 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-14.580188  -10.706512  -11.634112  -14.722249   -9.574938   -9.780359\n",
            " -14.461049  -19.376942  -13.098982   -9.82949   -12.005245  -13.507448\n",
            " -13.722959   -5.8108172 -10.908807  -16.539352  -12.432205  -11.87735\n",
            " -13.891305 ]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 14 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-13.361305   -9.506667  -11.536666  -14.858054   -8.352095   -8.971321\n",
            " -13.680516  -17.576614  -11.923744   -8.648091  -11.326029  -12.918762\n",
            " -13.14558    -4.932613   -9.837319  -15.394377  -12.527339  -10.61454\n",
            " -12.7874155]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 15 reward=-2 new_state=[0 0 0 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-16.023203  -11.638091  -13.334832  -16.451855  -10.543097  -11.181562\n",
            " -15.826093  -21.292664  -14.7817135  -9.70608   -13.335938  -14.648006\n",
            " -15.101293   -6.319757  -12.011088  -18.337915  -14.441199  -13.341586\n",
            " -14.718008 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 16 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-15.290034  -10.4250555 -12.651186  -16.890852   -9.934956   -9.524611\n",
            " -14.958342  -20.012516  -13.55129    -8.807578  -12.621875  -14.092947\n",
            " -14.738192   -7.0786767 -11.07915   -15.743562  -14.2019205 -12.197753\n",
            " -14.370506 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 17 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-15.125344  -10.659544  -12.5456    -16.019955   -8.317765   -9.500938\n",
            " -14.744102  -19.126919  -13.167047   -8.747721  -12.744158  -13.196776\n",
            " -14.306681   -6.43414   -10.2858715 -14.829843  -14.580019  -11.378326\n",
            " -13.477987 ]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 18 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-13.093331   -9.576847  -10.951056  -13.509247   -9.078219   -8.766964\n",
            " -12.537392  -16.76579   -12.132567   -8.273496  -11.690513  -11.621766\n",
            " -12.929025   -5.8837767  -9.526071  -11.219502  -13.022191  -10.264848\n",
            " -12.356074 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 19 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-10.305771   -7.1694393  -8.629898  -11.069617   -6.9116006  -6.315085\n",
            "  -9.948996  -12.7884     -9.748149   -6.978051   -9.674278   -9.334797\n",
            " -10.317929   -5.36348    -7.3863835  -9.131565   -9.729923   -8.476788\n",
            "  -9.884946 ]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 20 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-11.527612   -7.5266247  -9.337977  -12.631445   -6.849767   -7.1892223\n",
            " -10.051758  -14.103661  -10.066811   -7.0177827 -10.376447  -10.1742115\n",
            " -11.0581     -5.7501445  -8.185972   -9.489673  -11.031692   -8.923409\n",
            " -10.164936 ]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 21 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-13.981285   -9.747169  -11.110041  -14.7985525  -8.005826   -9.303283\n",
            " -10.995183  -17.196384  -12.174277   -8.134608  -11.998652  -12.008495\n",
            " -13.293462   -7.1844964  -9.576733  -10.529962  -15.026016  -10.447104\n",
            " -12.363145 ]\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 22 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-12.065991   -8.866868  -10.120841  -13.06454    -8.4596     -7.8850727\n",
            "  -8.65749   -15.437174  -11.363871   -7.7671475 -10.561407  -10.642397\n",
            " -12.192161   -7.2199545  -8.725715   -6.952521  -12.750197   -9.470301\n",
            " -11.474637 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 23 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-10.436511   -7.1699553  -8.198674  -11.487148   -6.6876006  -6.6267605\n",
            "  -6.8433847 -12.619204   -9.456802   -6.5566487  -9.566652   -8.881688\n",
            "  -9.874401   -6.9773726  -7.3380084  -6.935691  -10.35394    -7.900849\n",
            "  -9.374536 ]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 24 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-10.80155    -7.533059   -9.416031  -11.892359   -6.7871323  -7.086261\n",
            "  -6.3931646 -13.737254   -9.81502    -6.4350324  -9.232001   -9.467949\n",
            " -10.684948   -7.384808   -7.592299   -6.7537265 -11.595478   -8.207254\n",
            " -10.183703 ]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 25 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-12.829672   -8.599871   -9.666644  -13.04269    -7.729946   -8.120636\n",
            "  -7.209159  -15.140638  -11.775591   -7.7972965 -11.134349  -10.478433\n",
            " -12.023234   -8.69295    -8.774737   -6.415327  -13.255646   -9.823402\n",
            " -10.956327 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 26 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-12.48901    -8.305717   -9.804101  -13.59453    -7.919875   -7.8965735\n",
            "  -6.8442173 -14.707376  -11.29162    -7.705098  -10.880317  -10.389212\n",
            " -11.790489   -9.128793   -8.731479   -6.7850585 -13.507745   -9.188341\n",
            " -11.283881 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 27 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-12.226664   -8.703212  -10.138913  -13.905716   -8.345319   -8.197231\n",
            "  -7.5534477 -15.659634  -11.150974   -7.6100073 -11.108286  -10.560274\n",
            " -12.116271   -9.852441   -8.80886    -7.5983357 -13.748307   -9.488215\n",
            " -11.59143  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 28 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-12.925337   -8.958092  -10.043078  -13.137217   -7.940819   -8.277193\n",
            "  -7.3122373 -15.28607   -11.934969   -8.035034  -11.532568  -10.518712\n",
            " -12.21899   -10.007089   -8.969848   -6.6954517 -13.996213   -9.815073\n",
            " -11.259698 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 29 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-11.983842   -8.144274   -9.557496  -13.295935   -7.6654625  -7.214827\n",
            "  -6.986124  -14.238909  -10.770319   -7.4611588 -10.536697  -10.105414\n",
            " -11.46644   -10.012007   -8.558407   -6.582554  -13.415023   -8.820017\n",
            " -10.918692 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 30 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-11.431434   -7.9840794  -9.663706  -12.934726   -7.5537167  -7.6473336\n",
            "  -7.4475775 -14.486093  -10.629292   -7.0831227 -10.202848  -10.182541\n",
            " -11.529306  -10.381197   -8.284855   -7.7842474 -12.712516   -8.800771\n",
            " -10.635885 ]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 31 reward=-2 new_state=[0 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-12.907671   -8.674716  -10.062817  -13.278841   -8.519264   -8.372306\n",
            "  -7.186865  -15.212307  -11.709708   -7.934854  -11.2465105 -10.552094\n",
            " -12.486923  -10.8125925  -8.753186   -7.6162505 -14.44634    -9.619838\n",
            " -11.246437 ]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 32 reward=-2 new_state=[0 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-12.286812  -8.29136  -10.161933 -14.014361  -9.86003   -8.165128\n",
            "  -7.798121 -15.008263 -11.120371  -8.419065 -11.221897 -10.934741\n",
            " -12.345534 -11.247671  -8.857496  -8.363312 -14.272051  -9.36097\n",
            " -11.108711]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 33 reward=-2 new_state=[0 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-11.5538435  -7.501168   -9.227369  -12.838991   -9.252339   -7.2036695\n",
            "  -7.515814  -13.527424  -10.298314   -8.212148  -10.066907   -9.776031\n",
            " -11.3337345 -10.39473    -7.9529963  -8.359059  -12.406692   -8.610056\n",
            "  -9.9795   ]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 34 reward=-3 new_state=[0 0 1 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-12.163806  -7.927477  -9.246447 -12.845037 -10.162317  -7.147191\n",
            "  -7.304371 -13.905115 -11.181797  -9.300121 -10.729124 -10.041121\n",
            " -11.750495 -11.21569   -8.106448  -8.270246 -13.224293  -9.272494\n",
            " -10.525284]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 35 reward=-3 new_state=[0 0 1 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-11.701356   -7.9802566 -10.151158  -13.8976965 -11.541792   -8.175429\n",
            "  -8.519956  -14.716135  -10.947399   -9.89284   -10.266192  -10.541418\n",
            " -11.876826  -12.224147   -8.214133   -8.800768  -13.0778885  -9.348366\n",
            " -10.761268 ]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 36 reward=-4 new_state=[1 0 1 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-11.961164   -8.020285   -9.35378   -13.2196245 -12.52063    -9.054373\n",
            "  -8.345011  -15.140648  -11.080109  -10.2935095 -10.238799   -9.948964\n",
            " -12.213454  -10.712183   -8.556201   -7.4788737 -12.424672   -9.658744\n",
            " -11.087128 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 37 reward=-4 new_state=[1 0 1 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-10.508986  -8.059253  -8.843617 -11.881544 -11.608492  -8.609807\n",
            "  -7.953109 -13.465576 -10.332519 -10.115638  -9.412156  -9.526638\n",
            " -11.261096 -11.279618  -7.432974  -7.001905 -11.294638  -9.267799\n",
            " -10.144251]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 38 reward=-4 new_state=[1 0 1 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-12.241058 -10.239729  -9.851399 -13.08422  -13.778843 -10.711253\n",
            "  -8.90418  -14.893485 -11.53244  -11.747574 -11.319096 -10.290074\n",
            " -12.407862 -11.995585  -8.560603  -8.871934 -13.413761  -9.905319\n",
            " -11.328566]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 39 reward=-3 new_state=[1 0 1 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-13.468737  -11.84263   -10.377592  -13.6809435 -15.048013  -11.995282\n",
            "  -9.495797  -16.10025   -12.326553  -12.32155   -11.596686  -11.194131\n",
            " -13.507946  -12.027244   -9.372843   -8.929002  -14.872643  -10.735937\n",
            " -12.107844 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 40 reward=-4 new_state=[1 0 1 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-14.503123 -13.529452 -10.762265 -14.839537 -16.538223 -13.135559\n",
            " -10.252109 -17.016459 -12.967144 -13.699439 -12.589121 -11.738959\n",
            " -13.710578 -13.877645 -10.73394  -10.920942 -15.747357 -10.923832\n",
            " -12.766742]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 41 reward=-3 new_state=[1 0 1 1 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-13.227944  -13.8528805 -10.295199  -15.321114  -16.30398   -12.619566\n",
            " -11.191647  -16.363794  -12.343208  -13.730127  -12.818305  -11.40943\n",
            " -13.250933  -14.454382  -11.363593  -12.38701   -15.1765375 -10.710344\n",
            " -12.478128 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 42 reward=-3 new_state=[1 0 1 1 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-13.373634  -13.90275   -10.408431  -15.4870615 -15.93392   -13.242859\n",
            " -12.185004  -16.609352  -12.420996  -14.475982  -12.423812  -11.097568\n",
            " -13.647638  -13.518704  -12.1359    -13.717476  -14.320914  -10.886843\n",
            " -12.296237 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 43 reward=-3 new_state=[1 0 1 1 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-15.881153 -17.205595 -13.725434 -17.29664  -19.857773 -15.643577\n",
            " -13.421312 -19.590216 -14.946312 -15.803414 -13.454792 -11.630023\n",
            " -15.966248 -16.411634 -15.659878 -14.94718  -17.054943 -12.896573\n",
            " -14.68074 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 44 reward=-3 new_state=[1 0 1 1 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-17.134018 -18.695963 -15.491694 -18.572992 -19.824564 -17.091516\n",
            " -15.176353 -20.089031 -15.675499 -16.922613 -14.682746 -13.000296\n",
            " -16.906954 -16.599176 -17.229307 -16.671059 -18.458912 -13.041805\n",
            " -14.806252]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 45 reward=-2 new_state=[1 0 1 1 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-16.258701 -19.778236 -17.113441 -19.215157 -22.318895 -17.414608\n",
            " -15.198505 -20.851114 -15.477351 -18.446095 -14.416699 -14.581005\n",
            " -17.028929 -17.403955 -17.95295  -16.995237 -18.403212 -13.582362\n",
            " -15.619223]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 46 reward=-2 new_state=[1 0 1 1 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-15.647146 -19.922115 -17.133877 -17.768618 -22.183144 -16.991222\n",
            " -14.550233 -20.201012 -15.029424 -17.313963 -14.786855 -14.517776\n",
            " -16.310934 -17.08323  -18.842098 -16.381727 -18.251684 -12.464179\n",
            " -15.1463  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 47 reward=-2 new_state=[1 0 1 1 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-15.780091 -19.926891 -17.86384  -18.148659 -20.946692 -17.823154\n",
            " -14.965991 -20.01843  -14.604736 -18.6843   -14.388385 -16.37672\n",
            " -16.50464  -15.7785   -18.432255 -17.94125  -17.620586 -10.465662\n",
            " -14.733691]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 48 reward=-2 new_state=[1 0 1 1 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-16.248787 -20.376923 -18.513197 -17.67656  -21.684986 -18.176533\n",
            " -14.710109 -20.050787 -15.138993 -17.987616 -14.442833 -16.33878\n",
            " -16.739706 -15.625795 -19.928703 -18.123377 -17.22096   -8.389172\n",
            " -14.877151]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 49 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-17.595491  -23.58913   -22.80076   -20.31542   -24.20148   -20.521343\n",
            " -17.577364  -22.087523  -16.53754   -20.954926  -15.349768  -19.63201\n",
            " -17.930319  -19.116817  -22.754543  -21.3712    -20.110296   -7.3347254\n",
            " -16.62353  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 50 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-18.082838 -23.633638 -23.026308 -19.270002 -24.036207 -20.345737\n",
            " -16.621502 -21.823984 -16.153873 -20.579512 -15.253371 -19.087543\n",
            " -15.935803 -17.893305 -22.73798  -20.783035 -20.47992   -5.533178\n",
            " -16.062624]\n",
            "Epsilon reduced to 0.10240000000000003\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-15.010319  -19.896467  -19.088987  -16.71265   -19.764975  -16.949835\n",
            " -15.081632  -17.926472  -13.102274  -18.85754   -12.712396  -18.149067\n",
            " -12.1087055 -14.651283  -18.727882  -18.92447   -15.280626   -2.721166\n",
            " -13.128086 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 2 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-14.210802  -19.499895  -19.370066  -15.87025   -20.396906  -16.085352\n",
            " -13.464775  -17.759106  -13.230913  -16.910505  -11.779537  -17.682028\n",
            " -10.855631  -14.156744  -19.322395  -16.730175  -15.979972   -0.9135724\n",
            " -12.806978 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 3 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-13.714982  -20.16298   -19.90049   -16.006556  -20.676065  -17.434563\n",
            " -13.764159  -17.653534  -12.731507  -17.196695  -11.409859  -18.045668\n",
            "  -9.973132  -11.831623  -19.648634  -16.998075  -16.85915     0.5073171\n",
            " -13.444755 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-13.031488  -18.099297  -18.30075   -14.464887  -18.462978  -14.939835\n",
            " -12.287359  -16.036715  -11.994886  -15.452118  -10.343596  -16.447754\n",
            "  -8.1407795 -11.356876  -17.875349  -15.098737  -14.173653    2.1607397\n",
            " -11.413175 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-11.67224   -17.150892  -17.197449  -13.677308  -17.333305  -14.514754\n",
            " -11.386842  -14.4341135 -10.6283655 -12.952975   -9.535086  -15.065134\n",
            "  -6.708969  -10.069184  -16.764788  -14.060703  -13.505908    3.2248938\n",
            " -10.740509 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-10.974042  -15.542209  -15.834357  -12.477091  -15.419576  -13.094969\n",
            " -11.127647  -13.200336   -9.654926  -11.017106   -8.711682  -14.4984865\n",
            "  -5.596809   -8.259387  -15.139597  -13.836425  -11.900355    3.752452\n",
            "  -9.673689 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-10.192292  -13.983571  -13.918118  -11.161125  -13.603035  -11.907759\n",
            "  -9.752918  -11.830832   -9.008175   -8.827682   -8.16448   -12.621197\n",
            "  -4.5633388  -7.2567334 -13.948784  -12.500247  -10.8646345   3.6719246\n",
            "  -8.600718 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-10.481276  -14.651503  -15.066579  -11.117931  -14.517557  -12.786936\n",
            "  -9.544647  -12.315701   -9.559874   -8.584961   -8.370437  -12.99927\n",
            "  -4.442428   -7.1506333 -14.8364315 -12.354839  -11.733806    3.964489\n",
            "  -8.961866 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-10.043876  -15.232537  -14.82603   -11.588626  -14.911859  -13.037835\n",
            " -10.174677  -12.383049   -9.22661    -8.826874   -8.751007  -13.610663\n",
            "  -4.0455728  -7.468526  -14.780657  -13.0983305 -11.716391    4.4833465\n",
            "  -9.229483 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -8.926611  -13.266447  -13.47857   -10.232661  -12.928566  -11.69713\n",
            "  -9.219308  -10.96187    -7.976136   -6.8705497  -7.171176  -12.485069\n",
            "  -3.4386988  -5.8854628 -13.067448  -12.010797  -10.304103    4.117418\n",
            "  -7.8848386]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -7.692821  -11.559817  -11.370941   -8.585733  -11.99511   -10.084519\n",
            "  -7.5309553  -9.547527   -7.124539   -5.607946   -6.712124  -10.294903\n",
            "  -2.8130329  -4.371383  -11.83561   -10.003451   -8.766872    3.1260366\n",
            "  -6.8996234]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -8.658778  -13.835884  -14.049079  -10.642871  -13.837951  -12.134867\n",
            "  -9.051406  -11.5742655  -7.9086967  -6.1041346  -7.0077043 -12.812545\n",
            "  -2.687068   -5.122228  -13.669768  -11.64972   -10.804669    4.0563245\n",
            "  -8.217545 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -8.9102955 -12.894914  -13.146087   -9.8163595 -12.527288  -11.54947\n",
            "  -9.016404  -10.778926   -8.026018   -5.325315   -7.457506  -11.867557\n",
            "  -2.5216851  -5.046582  -13.1815195 -11.766776  -10.501604    3.7604213\n",
            "  -7.584853 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -7.9796686 -12.480322  -12.199846   -9.399182  -12.431988  -11.039725\n",
            "  -8.323525  -10.3099575  -7.5252028  -5.146764   -7.118593  -11.365041\n",
            "  -2.3897138  -4.8712254 -12.411384  -11.238387   -9.89336     3.3717031\n",
            "  -7.4504423]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -7.7157354 -13.221797  -13.160874   -9.830602  -13.2428    -11.700602\n",
            "  -8.585587  -10.679454   -7.4386764  -5.1814837  -6.628646  -12.446581\n",
            "  -2.2363262  -4.596632  -12.891849  -11.4586    -10.270038    3.549877\n",
            "  -7.6221447]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-6.3408637 -9.792602  -9.572028  -7.0541735 -9.868258  -8.60064\n",
            " -6.396139  -7.830695  -5.813006  -3.6039064 -5.300562  -8.68281\n",
            " -1.8055019 -3.206914  -9.836803  -8.324193  -7.2539396  2.0214474\n",
            " -5.535365 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -8.011144  -12.157016  -12.293363   -9.102646  -11.701057  -10.862074\n",
            "  -7.9979873  -9.812407   -7.093147   -3.781479   -6.078968  -11.140776\n",
            "  -1.7344645  -3.8653805 -12.2001915 -10.28493    -9.538712    2.543647\n",
            "  -6.8563557]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -7.2290998 -11.099344  -10.777215   -8.417573  -11.018519  -10.011618\n",
            "  -7.464564   -9.059934   -6.5569644  -3.7553902  -6.2518315 -10.134561\n",
            "  -1.7719256  -3.6866312 -11.154173   -9.934443   -8.673171    1.8712692\n",
            "  -6.3015513]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -6.5480876 -10.588921  -10.462304   -7.785092  -10.587203   -9.472943\n",
            "  -7.095382   -8.683684   -6.2107263  -3.4364772  -5.6376953  -9.912411\n",
            "  -1.4899725  -3.5842178 -10.633017   -9.42314    -8.42164     1.9687619\n",
            "  -6.0139937]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -6.7353225 -10.887802  -10.990504   -8.1491995 -10.824614   -9.684591\n",
            "  -7.039812   -8.8881445  -6.255822   -2.9443533  -5.237662  -10.190175\n",
            "  -1.2286649  -3.1389315 -10.674139   -9.118018   -8.513873    1.4626645\n",
            "  -6.185446 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-6.3840256  -9.121831   -9.279558   -6.691156   -8.784765   -8.223221\n",
            " -6.2010984  -7.424844   -5.5985208  -2.5142138  -5.1186223  -8.397894\n",
            " -1.1899714  -2.5755465  -9.531263   -8.157352   -7.082846    0.83588815\n",
            " -5.14818   ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -6.7771196 -11.22662   -11.120087   -8.342413  -11.157559   -9.978559\n",
            "  -7.405325   -8.960739   -6.152541   -3.0996356  -6.054923  -10.607705\n",
            "  -1.1370502  -3.1484866 -11.050894   -9.830783   -8.691364    0.7970197\n",
            "  -6.4891376]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -6.283416  -10.220626  -10.312206   -7.633767  -10.205293   -9.33863\n",
            "  -7.028934   -8.367141   -5.8613873  -2.885183   -5.427123   -9.6702585\n",
            "  -1.1598543  -3.0312123 -10.272521   -9.159438   -7.851972    0.7010552\n",
            "  -5.725188 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-5.7374015  -9.218164   -9.217564   -6.7856097  -9.282341   -8.288945\n",
            " -6.1053777  -7.529203   -5.596699   -2.317819   -4.881771   -8.46293\n",
            " -0.96997696 -2.6066837  -9.37923    -8.099762   -7.091211    0.67870605\n",
            " -5.3169937 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -7.3580213 -11.302812  -11.5070715  -8.386813  -10.920071  -10.099912\n",
            "  -7.3123546  -8.987038   -6.6941686  -2.5794616  -5.392283  -10.576985\n",
            "  -1.0863746  -2.6171877 -11.377085   -9.326094   -8.457012    0.4924519\n",
            "  -6.2645526]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-6.030317   -9.524289   -9.176084   -7.0618405  -9.3197365  -8.4574375\n",
            " -6.242198   -7.6496186  -5.6044927  -2.4337718  -5.363686   -8.466756\n",
            " -1.0250745  -2.7315454  -9.67787    -8.655895   -7.2598815  -0.04266087\n",
            " -5.179933  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 27 reward=-1 new_state=[1 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [ -5.8549733  -10.182616   -10.215282    -7.517824   -10.177186\n",
            "  -9.121504    -6.407129    -8.156307    -5.6410522   -2.4993494\n",
            "  -4.781837    -9.641625    -0.80663955  -2.652505    -9.891921\n",
            "  -8.408771    -7.6578045   -0.11732531  -5.7637734 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 28 reward=-1 new_state=[1 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-6.091615   -8.968022   -9.3559885  -7.1712675  -9.273069   -8.420268\n",
            " -6.338152   -7.6423545  -5.554613   -2.3050487  -5.407573   -8.634643\n",
            " -0.98289216 -2.7316754  -9.460976   -8.395854   -7.383293   -0.25247398\n",
            " -5.3695693 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 29 reward=-1 new_state=[1 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-5.6969323  -7.715694   -8.831095   -6.3448057  -8.398203   -7.762093\n",
            " -5.981539   -6.921459   -5.136918   -2.2011716  -5.254517   -8.060568\n",
            " -0.92961454 -2.3442323  -8.830472   -8.019113   -6.6866107  -0.5678211\n",
            " -4.9718275 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 30 reward=-1 new_state=[1 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-5.582052   -7.492939   -8.832584   -6.6582594  -8.620655   -7.8856206\n",
            " -6.1133265  -7.2448564  -5.030559   -2.1303153  -4.8454742  -8.629601\n",
            " -0.8431459  -1.9728796  -8.644081   -8.087327   -6.6978765  -0.65338707\n",
            " -4.8771453 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 31 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.953748   -7.8308764  -9.501822   -6.852295   -9.182974   -8.42527\n",
            " -6.1868787  -7.5085745  -5.4920406  -2.1766784  -5.2306476  -8.645063\n",
            " -0.90105414 -2.6622446  -9.7306     -8.083652   -7.652184   -0.70300835\n",
            " -5.376319  ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 32 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.49987    -6.5063777  -8.301471   -6.1441503  -8.277689   -7.427705\n",
            " -5.8052273  -6.6983147  -4.974143   -1.9558578  -5.329166   -7.761196\n",
            " -0.81845725 -2.3096404  -8.765547   -7.8187575  -6.774618   -1.0130472\n",
            " -5.0134015 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 33 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.1214576 -5.567298  -7.5008025 -5.8465595 -7.3433495 -6.6510954\n",
            " -5.3865523 -6.361897  -4.188516  -1.5345207 -3.9969583 -7.377678\n",
            " -0.7342322 -1.5975611 -7.409332  -6.851942  -5.6589003 -1.1001283\n",
            " -4.080589 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 34 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.25399   -6.144765  -8.110139  -6.0847554 -8.31949   -7.4179716\n",
            " -5.4880166 -6.6354437 -5.1635637 -1.5782844 -4.635093  -7.716136\n",
            " -0.7107386 -2.2312498 -8.770733  -7.2146826 -6.5989523 -1.0761398\n",
            " -4.8865576]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 35 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.3130336 -5.4358516 -7.025061  -5.6081285 -7.6891346 -6.996035\n",
            " -5.4021487 -6.1825395 -4.836634  -1.8430074 -4.7183223 -7.061611\n",
            " -1.0703094 -2.33968   -8.1318445 -7.040853  -6.5186186 -1.3209687\n",
            " -4.553503 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 36 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.8385215 -5.725135  -7.2898173 -6.5221696 -8.222934  -7.599355\n",
            " -6.041132  -7.1519537 -4.9305024 -1.8959504 -5.1575265 -8.35588\n",
            " -1.008191  -2.047127  -8.677511  -8.256252  -6.7470236 -1.949673\n",
            " -4.8431497]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 37 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.687686  -4.953259  -6.0740614 -5.50811   -7.541808  -6.835379\n",
            " -4.5841317 -5.9627867 -4.5299554 -1.505114  -4.095989  -6.821463\n",
            " -1.1997781 -2.2872918 -7.758438  -6.2136574 -5.9872456 -1.3885307\n",
            " -4.351315 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 38 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.7762012 -4.5165143 -5.4666047 -5.179989  -7.0913043 -6.3641157\n",
            " -4.998327  -5.782034  -4.3868012 -1.510879  -4.457659  -6.6290026\n",
            " -1.2976708 -2.0936892 -7.3693924 -6.4265637 -6.1013474 -1.5390228\n",
            " -4.1645555]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 39 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.4539084 -4.588297  -5.5871773 -5.813249  -7.1569695 -6.7644696\n",
            " -5.574625  -6.291945  -4.413223  -1.6061337 -4.566966  -7.6144114\n",
            " -1.4212191 -1.8117676 -7.623213  -7.249709  -5.949511  -2.1007798\n",
            " -4.3204904]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 10\n",
            "\n",
            "Step 40 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.9156065 -3.7639618 -4.2316947 -4.774804  -6.2517066 -5.6274433\n",
            " -3.874938  -4.962763  -3.7783728 -1.0629584 -3.3541641 -5.6700807\n",
            " -1.4203162 -1.7193184 -6.436929  -5.292621  -4.900948  -1.6801512\n",
            " -3.6094015]\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 41 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.127618  -3.6123917 -4.3646016 -4.611752  -6.276806  -5.5884027\n",
            " -4.394904  -5.157827  -3.9214208 -1.3938671 -4.0309973 -5.8014617\n",
            " -1.8063705 -1.7812678 -6.575734  -5.972933  -5.3356075 -1.8020698\n",
            " -3.5469685]\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 42 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.287475  -4.1455455 -4.866316  -5.6114125 -6.89546   -6.5838237\n",
            " -5.4114223 -6.196369  -4.3169603 -1.6458302 -4.2713075 -7.4791875\n",
            " -2.0061903 -1.8592631 -7.462459  -7.1543922 -5.8179154 -2.2991784\n",
            " -4.180822 ]\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 43 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-4.466923  -3.9612477 -4.490161  -5.2291036 -7.1043453 -6.4152446\n",
            " -4.699845  -5.8184166 -4.148397  -1.3119574 -3.4664106 -6.7108064\n",
            " -2.1097732 -1.7602942 -7.4226837 -6.1037807 -6.028816  -2.0727289\n",
            " -4.1912556]\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 44 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.8605258 -3.264852  -3.5787554 -4.4577746 -6.0071564 -5.3916593\n",
            " -4.0276875 -4.7796392 -3.6005645 -1.5403178 -3.515325  -5.4745293\n",
            " -2.0467124 -1.9047426 -6.0781374 -5.4909754 -4.942486  -1.7687124\n",
            " -3.4622908]\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 45 reward=-2 new_state=[1 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.067732  -3.8268216 -4.3351502 -5.394662  -6.6885614 -6.5339923\n",
            " -5.4238396 -6.016298  -4.2526684 -1.9449946 -4.0885944 -7.187724\n",
            " -2.346609  -1.7606282 -7.2934513 -7.324152  -5.6223936 -2.3614454\n",
            " -4.060466 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 46 reward=-1 new_state=[1 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-4.179826  -3.5410047 -4.01178   -4.8020244 -6.6467714 -5.845337\n",
            " -4.1526523 -5.294102  -3.9323535 -1.428985  -2.8825412 -6.0948715\n",
            " -2.4455075 -1.8210862 -6.8219805 -5.4314404 -5.5051184 -2.0915182\n",
            " -3.9108374]\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 47 reward=-1 new_state=[1 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-3.2334661 -2.597906  -2.8643234 -3.9553602 -5.2406564 -4.5019145\n",
            " -3.6563742 -4.417545  -3.2253013 -1.1860921 -2.7107651 -4.87948\n",
            " -1.7912045 -1.4869354 -5.262255  -5.005891  -4.418781  -1.8738321\n",
            " -3.0939422]\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 48 reward=-1 new_state=[1 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-4.412394  -3.1987271 -3.4591484 -4.8361387 -6.0049543 -5.579493\n",
            " -4.553871  -5.123796  -3.9270709 -1.4913346 -2.6917214 -6.2874146\n",
            " -2.4785903 -1.3682848 -6.3420773 -5.570759  -5.2434955 -2.0240188\n",
            " -3.5596173]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 49 reward=-1 new_state=[1 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-3.9680028 -3.1019378 -3.5396223 -4.6421967 -6.169926  -5.660576\n",
            " -3.9549174 -5.0893583 -3.9609666 -1.562622  -2.75669   -5.7660103\n",
            " -2.622768  -1.6653528 -6.6319537 -5.339553  -5.1424685 -2.0557659\n",
            " -3.593303 ]\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 50 reward=-1 new_state=[1 0 0 0 1 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-4.0900908 -2.80904   -3.167896  -4.3395605 -5.431967  -5.191476\n",
            " -4.1314116 -4.7681    -3.777734  -1.5706154 -2.73755   -5.341186\n",
            " -2.3016393 -1.7496245 -5.973357  -5.4484773 -4.642968  -1.8667965\n",
            " -3.3296943]\n",
            "Epsilon reduced to 0.08192000000000003\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-4.511356  -3.8354056 -3.895555  -5.2546034 -7.2150955 -6.363532\n",
            " -4.6784134 -5.624645  -3.9862123 -2.0409214 -2.6133966 -7.0317464\n",
            " -2.4856791 -1.7875553 -7.4399033 -6.029496  -5.1886683 -2.585332\n",
            " -4.249789 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 2 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-4.407808  -3.709454  -3.7387345 -5.498562  -7.004857  -6.4253144\n",
            " -4.7735486 -5.903621  -3.9180057 -2.3750825 -2.9377487 -7.331672\n",
            " -2.6544755 -1.4392023 -7.2747865 -6.743819  -5.5600247 -2.9123375\n",
            " -4.241405 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 3 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-3.8181062 -3.0302062 -3.53578   -4.6803536 -6.340074  -5.5212445\n",
            " -3.9965367 -5.259932  -3.5722833 -1.4338293 -1.6335136 -6.5179644\n",
            " -2.3867335 -1.5003244 -6.8239822 -5.126287  -4.7261086 -1.9001536\n",
            " -3.504124 ]\n",
            "\n",
            "Taking action 7 from 9\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-3.1017988 -2.6873038 -2.7107344 -3.8992126 -5.251491  -4.713334\n",
            " -3.5307055 -4.0939612 -3.1042018 -1.3859875 -2.1361666 -4.8733845\n",
            " -1.9160593 -1.407201  -5.2653036 -4.751537  -4.0534043 -1.6336218\n",
            " -2.900186 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-3.213616  -2.8215604 -2.9534302 -4.32095   -5.6716447 -5.072023\n",
            " -3.720255  -4.484564  -3.1874518 -1.4723835 -1.8760486 -5.4596786\n",
            " -2.1274343 -1.2692541 -5.5547905 -4.9833837 -4.3630123 -1.8978767\n",
            " -3.0846646]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.9114676 -2.3082685 -2.2162278 -3.6423035 -4.728006  -4.272176\n",
            " -3.301118  -3.742184  -2.8480637 -1.3540986 -1.8976204 -4.4432297\n",
            " -1.8841969 -1.2875669 -4.890341  -4.3844156 -3.899341  -1.7360876\n",
            " -2.648051 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-3.217762  -2.407362  -2.312617  -3.8878217 -4.9121957 -4.5063243\n",
            " -3.5587022 -3.9907293 -2.9220116 -1.3230696 -1.799823  -4.7469244\n",
            " -1.9750046 -1.194534  -5.0295143 -4.4032598 -3.9690065 -1.9052374\n",
            " -2.6605458]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.9541404 -2.4764771 -2.408784  -3.7596684 -5.124479  -4.575421\n",
            " -3.2841113 -4.0824285 -2.9066074 -1.3523821 -1.8981137 -4.7685127\n",
            " -2.0536282 -1.1885715 -5.1973953 -4.4780316 -4.05867   -1.676189\n",
            " -2.7903285]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-3.4371417 -2.8352177 -2.876787  -4.5584245 -5.938868  -5.1275625\n",
            " -3.7885845 -4.5592394 -3.2490175 -1.6433947 -1.4868214 -5.8177385\n",
            " -2.411702  -1.135516  -5.7824078 -4.6298203 -4.4305367 -2.0896235\n",
            " -3.3336916]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-3.0093162 -2.4799724 -2.3359706 -3.7042487 -4.8313327 -4.4286766\n",
            " -3.4001615 -3.849208  -2.8714924 -1.2644656 -1.6183059 -4.658604\n",
            " -1.8806295 -1.1731796 -4.9861336 -4.11883   -3.8413634 -1.8736337\n",
            " -2.623101 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-3.3552632 -2.8686645 -2.730999  -4.1980915 -5.814385  -5.10662\n",
            " -3.6559713 -4.441139  -3.0451002 -1.4397271 -1.6113069 -5.4643817\n",
            " -2.2284102 -1.0510634 -5.7283454 -4.7017055 -4.5273767 -2.0485728\n",
            " -3.2804842]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 7\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.329671  -2.5308146 -2.5972447 -4.3967433 -5.237283  -4.8864107\n",
            " -3.972926  -4.558347  -3.0210764 -1.6833819 -1.9248462 -5.4773526\n",
            " -2.2515895 -1.0668089 -5.451435  -4.937624  -4.476239  -2.1297092\n",
            " -3.0297365]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.6937926 -2.4446635 -2.874758  -4.0831337 -4.8755074 -4.922027\n",
            " -3.9802506 -4.4118814 -3.1405187 -1.5443559 -2.183383  -5.348938\n",
            " -2.3139153 -1.1258456 -5.666023  -5.2311144 -4.536664  -1.7322823\n",
            " -2.8646276]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-4.017595  -2.7825193 -3.1320033 -4.6219645 -5.80688   -5.296558\n",
            " -4.394795  -4.5345597 -3.4786906 -2.0621908 -2.3344474 -6.1595416\n",
            " -2.6626387 -1.1137245 -6.247044  -5.64569   -4.8576446 -2.01677\n",
            " -3.4706285]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.3999486 -2.3804955 -2.3573732 -3.9233449 -4.883694  -4.5876136\n",
            " -3.6374435 -3.8358173 -2.986314  -1.7733814 -2.3164356 -4.75794\n",
            " -2.02207   -1.0839207 -5.310156  -4.8458195 -4.0743685 -1.9974627\n",
            " -2.890052 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-2.9135907  -2.2051194  -2.5583422  -3.6335022  -4.59732    -4.0323057\n",
            " -3.323251   -3.1344652  -2.7022014  -1.4018154  -1.6983122  -4.882822\n",
            " -1.8360524  -0.85679233 -4.80483    -4.4012756  -3.4956908  -1.4666134\n",
            " -2.6848202 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.4050386  -2.044426   -2.463391   -3.8518662  -4.500958   -4.413359\n",
            " -3.4733553  -3.2451456  -2.7678154  -1.4733448  -1.7799325  -4.860123\n",
            " -2.0341337  -0.73892736 -5.0798163  -4.407492   -3.9563613  -1.7082095\n",
            " -2.4873452 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 7\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.3783636 -2.095862  -2.3580074 -3.6517017 -4.3049126 -4.3166766\n",
            " -3.5983658 -3.1782477 -2.7924893 -1.6471275 -2.0430393 -4.6384406\n",
            " -2.0853581 -1.1190662 -5.0062428 -4.666855  -4.1894836 -1.6745172\n",
            " -2.5090187]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.3057492 -2.3760545 -2.7430925 -4.15215   -5.061252  -4.546897\n",
            " -3.7860954 -3.036505  -2.9675443 -1.9344656 -2.0044284 -5.2965446\n",
            " -2.1761131 -0.9782359 -5.1871443 -4.902596  -3.9994025 -1.9914904\n",
            " -3.1107786]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-2.6210961  -1.7636487  -1.866644   -2.9816613  -3.6875272  -3.4227912\n",
            " -2.7737322  -2.2211986  -2.26696    -1.0946833  -1.586417   -3.6285696\n",
            " -1.4204026  -0.61736554 -4.0277467  -3.5148218  -3.0811791  -1.4433324\n",
            " -2.0601485 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 1 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.0125437 -1.909919  -2.23951   -3.3747263 -4.0561013 -3.9216857\n",
            " -3.1586456 -2.286269  -2.5530133 -1.2439296 -1.8304628 -4.257403\n",
            " -1.9300485 -0.5847972 -4.510973  -4.2659616 -3.7642093 -1.3254706\n",
            " -2.452178 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12 from 16\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.1945906 -2.0193002 -2.2763162 -3.685249  -4.2868104 -4.120985\n",
            " -3.4940314 -2.324446  -2.6822577 -1.7177303 -1.938743  -4.737459\n",
            " -2.209811  -0.7254377 -4.5943623 -4.633149  -4.073839  -1.5644315\n",
            " -2.6402533]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.8149438  -1.6598133  -1.8449839  -3.2047188  -3.753226   -3.680108\n",
            " -2.9997544  -1.9811147  -2.2630692  -1.5077937  -1.9968494  -3.877095\n",
            " -1.8766361  -0.65461063 -4.247009   -4.004083   -3.4738247  -1.5120759\n",
            " -2.1768832 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.062808   -1.9235843  -2.0266337  -3.2730343  -3.7521975  -3.5300372\n",
            " -3.3062985  -1.5954041  -2.4757118  -1.3344678  -1.5282283  -4.323902\n",
            " -1.9847977  -0.46460772 -4.3641496  -3.9279187  -3.3026924  -1.4719807\n",
            " -2.3326225 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4654417 -1.5558114 -1.580835  -2.7209673 -3.4649909 -3.0508294\n",
            " -2.3480852 -1.5343411 -2.2252688 -1.1190678 -1.6911883 -3.1866593\n",
            " -1.8065076 -0.6980521 -3.6662624 -3.2509937 -3.0570118 -1.1566795\n",
            " -1.9221181]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.9554698 -1.9771123 -2.257576  -3.541063  -4.2747912 -3.943942\n",
            " -3.237361  -1.7405447 -2.6541464 -1.4999424 -1.7263491 -4.2951384\n",
            " -2.2255101 -0.785553  -4.4927406 -4.2545633 -3.6972404 -1.5660213\n",
            " -2.4751587]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.0205958 -1.8334737 -1.9836857 -3.428182  -3.6093998 -3.643069\n",
            " -3.2691705 -1.4871093 -2.4718678 -1.5539657 -1.7657812 -4.1755414\n",
            " -2.0236444 -0.5100098 -4.1738467 -4.120308  -3.2099967 -1.6707826\n",
            " -2.2921143]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3984745  -1.2914166  -1.4928219  -2.394917   -2.6709538  -2.7295177\n",
            " -2.4214313  -1.1817431  -1.8480734  -0.9730509  -1.4456359  -2.927248\n",
            " -1.4759862  -0.39175737 -3.2199876  -3.0664034  -2.531755   -1.1169243\n",
            " -1.5611676 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7344253  -1.676407   -1.8349879  -3.0246978  -3.6900632  -3.2851772\n",
            " -2.6744833  -1.1844457  -2.301097   -1.2200271  -1.3012578  -3.6020331\n",
            " -1.887428   -0.30107927 -3.8254557  -3.31345    -2.9192731  -1.2891374\n",
            " -2.0868776 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7013261  -1.914425   -1.8937371  -3.2384062  -3.8214436  -3.6794212\n",
            " -2.8989701  -1.4861592  -2.432927   -1.3463964  -1.8924348  -3.6981409\n",
            " -2.0833278  -0.63745433 -4.098734   -3.8053708  -3.3175461  -1.5695529\n",
            " -2.3287923 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.566638  -1.663504  -1.8370137 -3.0226493 -3.295863  -3.3726184\n",
            " -3.0891829 -1.1257379 -2.2529736 -1.4501828 -1.6109585 -3.8274875\n",
            " -1.8259245 -0.3944448 -3.8862214 -3.9287531 -2.7110763 -1.3512492\n",
            " -2.1189566]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3808026  -1.3259176  -1.6403795  -2.5563831  -2.9468455  -2.90042\n",
            " -2.482075   -0.8290314  -1.9062362  -1.085851   -1.3511575  -3.185733\n",
            " -1.3689194  -0.18297999 -3.2941241  -3.1746535  -2.2374017  -1.0125953\n",
            " -1.7560462 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4273903  -1.4126732  -1.5551894  -2.6147761  -3.202649   -2.9596317\n",
            " -2.5250664  -0.96109766 -2.231582   -1.0340588  -1.5826255  -2.9119575\n",
            " -1.6710069  -0.47992617 -3.522284   -3.2608705  -2.5304785  -1.0657407\n",
            " -1.9099922 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6218162  -1.6024098  -2.0082421  -3.205073   -3.7090487  -3.3927848\n",
            " -2.8658288  -0.9613191  -2.352639   -1.3918805  -1.488768   -3.7809916\n",
            " -1.9696821  -0.46507683 -3.782289   -3.5690944  -2.9323666  -1.5221353\n",
            " -2.282691  ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6442163 -1.5942873 -1.4900128 -2.6479902 -3.2057612 -3.0307028\n",
            " -2.781091  -1.0852815 -2.2507083 -1.3341237 -1.9057503 -3.415012\n",
            " -1.7036614 -0.413889  -3.755292  -3.8851745 -2.5341902 -1.1270113\n",
            " -2.0318875]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.2804883  -1.2839304  -1.534852   -2.50381    -2.6608367  -2.626283\n",
            " -2.4191709  -0.73040557 -1.8679429  -1.0171578  -1.2029002  -3.149168\n",
            " -1.5278368  -0.18052523 -3.0436192  -3.0665855  -2.1510756  -1.0591305\n",
            " -1.6447    ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4344091  -1.4817835  -1.604482   -2.717522   -3.0147092  -2.8145964\n",
            " -2.5200574  -0.7458573  -2.0206013  -1.1224481  -1.2818251  -3.266026\n",
            " -1.8411654  -0.24466407 -3.1857884  -3.0024328  -2.526468   -1.1670026\n",
            " -1.8435518 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6646948 -1.5599549 -1.7132496 -2.863951  -3.3054178 -3.2910757\n",
            " -2.699222  -0.8523138 -2.340813  -1.2868401 -1.5528044 -3.3391433\n",
            " -1.774296  -0.672513  -3.865436  -3.391048  -2.6561253 -1.3132519\n",
            " -1.8944756]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4902897  -1.4634327  -1.7614621  -2.8016436  -3.1041749  -2.909228\n",
            " -2.5469213  -0.7361815  -2.0094764  -1.3076932  -1.5455604  -3.4062247\n",
            " -1.7231418  -0.18162034 -3.4580755  -3.2348957  -2.2551062  -1.3449489\n",
            " -1.9884552 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.3384373  -1.268317   -1.3016095  -2.1583765  -2.5694497  -2.6009617\n",
            " -2.4172668  -0.9215951  -1.8530302  -1.0193353  -1.4942608  -2.8242834\n",
            " -1.37716    -0.20667651 -3.282798   -3.0980315  -2.184215   -0.9698875\n",
            " -1.4972683 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4200783  -1.5502906  -1.6557666  -2.6719756  -3.12053    -2.8871942\n",
            " -2.4574206  -0.66428024 -1.9916614  -1.0610517  -1.1857547  -3.3630245\n",
            " -1.791149   -0.2063341  -3.4201477  -2.9644442  -2.4640357  -1.2241837\n",
            " -1.8738261 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.7706792  -1.8531187  -2.056492   -3.1245391  -3.791689   -3.48204\n",
            " -3.141149   -0.8450203  -2.5164275  -1.3729948  -1.7115965  -4.065706\n",
            " -1.981828   -0.56061196 -4.251679   -3.8474596  -2.7935238  -1.3771261\n",
            " -2.2726617 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.6413972 -1.49851   -1.6395663 -2.7022052 -2.9563932 -2.8943856\n",
            " -2.7211626 -0.7493894 -2.1391096 -1.1247379 -1.8995109 -3.2668948\n",
            " -1.5957663 -0.6084228 -3.5893443 -3.6283727 -2.1860814 -1.3444862\n",
            " -1.8824648]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.2005048  -1.1787257  -1.5934746  -2.4889197  -2.6732063  -2.5669627\n",
            " -2.3336422  -0.61994016 -1.8593706  -0.9433569  -1.2127565  -2.9400988\n",
            " -1.3569785  -0.10288699 -2.897444   -2.859098   -1.8936514  -1.1151702\n",
            " -1.7193694 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.5145328  -1.5223178  -1.5625343  -2.5002716  -3.1482704  -2.867966\n",
            " -2.5526438  -0.72610193 -2.1926894  -1.0574948  -1.4808182  -3.130993\n",
            " -1.6842978  -0.34624392 -3.546909   -3.2515664  -2.437757   -0.9684292\n",
            " -1.7712106 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.2625248  -1.5126144  -1.6805876  -2.6551874  -3.1147676  -3.0056663\n",
            " -2.3341444  -0.62672645 -2.0228019  -1.1233357  -1.3949032  -3.1604795\n",
            " -1.7280146  -0.37335846 -3.344789   -3.1246507  -2.4267004  -1.1368902\n",
            " -1.788938  ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.4013922  -1.4293776  -1.6365353  -2.7466645  -2.931658   -2.8026302\n",
            " -2.6367807  -0.72441775 -2.0190167  -1.3084741  -1.5579382  -3.5098286\n",
            " -1.7877148  -0.21827568 -3.2671225  -3.4762385  -2.239118   -1.2299039\n",
            " -1.9393195 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1852057  -1.0638855  -1.2327904  -1.9969286  -2.0886853  -2.228928\n",
            " -2.114215   -0.6448176  -1.6597342  -0.74777424 -1.4523046  -2.4686186\n",
            " -1.3380967  -0.35513985 -2.789008   -2.7420702  -1.8389287  -0.9249584\n",
            " -1.2858944 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.23847    -1.2920921  -1.612143   -2.499002   -2.94948    -2.5896492\n",
            " -2.341688   -0.5480255  -1.9357547  -0.9388062  -1.0746748  -3.101334\n",
            " -1.6134741  -0.26969063 -2.9981248  -2.8318505  -2.2015867  -1.1599529\n",
            " -1.8583349 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 1 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.456639   -1.5460017  -1.498394   -2.622694   -2.9877455  -2.8696232\n",
            " -2.6236959  -0.7189554  -2.0897155  -1.3373139  -1.5608977  -3.0063632\n",
            " -1.7364953  -0.53185016 -3.344624   -3.2449775  -2.4956827  -1.1457537\n",
            " -1.8058321 ]\n",
            "Epsilon reduced to 0.06553600000000002\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.9766102 -1.9604042 -1.9342525 -3.0610394 -3.996146  -3.9234295\n",
            " -2.6943552 -0.6499062 -2.6694305 -1.7983717 -1.7697397 -3.853466\n",
            " -1.9342126 -0.7772444 -4.3759265 -4.159024  -2.6869993 -1.4782294\n",
            " -2.487302 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 2 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.6677365  -1.5981926  -1.6956218  -2.6208692  -3.583344   -3.2584598\n",
            " -2.692773   -0.73981285 -2.5508285  -1.1333771  -1.2576557  -3.2662332\n",
            " -1.6474136  -0.35740823 -3.85341    -3.9944704  -2.6781476  -0.92115116\n",
            " -2.1068292 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 3 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.9057584  -0.88266224 -1.2178288  -1.9269161  -2.2776508  -2.2753587\n",
            " -1.8740184  -0.50414616 -1.7708422  -0.88062483 -1.2062651  -2.3561566\n",
            " -1.2926677  -0.13722049 -2.7369401  -2.6480265  -1.6845337  -0.7775594\n",
            " -1.3323137 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 4 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.1807647  -1.2021344  -1.5605122  -2.2403107  -2.9104855  -2.7118459\n",
            " -2.2199743  -0.53814197 -2.2319288  -0.8432684  -1.3337634  -2.8968503\n",
            " -1.3470123  -0.3972549  -3.285667   -3.5225863  -2.1436832  -1.2358294\n",
            " -1.7283523 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 5 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.037639   -1.1458322  -1.3849764  -2.1476967  -2.7120311  -2.476475\n",
            " -2.1350095  -0.4741973  -1.9813045  -0.8982131  -1.4450686  -2.7423687\n",
            " -1.4272249  -0.20906186 -3.1744883  -3.1372745  -1.9076132  -0.7243624\n",
            " -1.6561826 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 6 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.2919693  -1.3093584  -1.497471   -2.213667   -3.0273173  -2.7712672\n",
            " -2.2910976  -0.53141904 -2.1875725  -1.0729561  -1.2428906  -2.8655868\n",
            " -1.376979   -0.07855005 -3.3196247  -3.737913   -2.1249566  -0.77635944\n",
            " -1.7027547 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 7 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7383612  -0.83520496 -1.1421901  -1.8435292  -2.0840862  -2.148065\n",
            " -1.7413294  -0.31042978 -1.670301   -0.88312745 -1.1653427  -2.1326435\n",
            " -1.2241277  -0.08596852 -2.481914   -2.4321592  -1.5935416  -0.83220935\n",
            " -1.2547306 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 9\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.2843332  -1.2957196  -1.3611705  -2.1878798  -2.6882071  -2.7182846\n",
            " -2.3346212  -0.43225318 -2.0695832  -1.075501   -1.2750819  -2.8322794\n",
            " -1.2846929  -0.06311692 -3.2024615  -3.6695533  -2.0898633  -0.9265923\n",
            " -1.6285914 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 12\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.3513328  -0.73132384 -0.7628986  -1.4477811  -1.4895766  -1.6204696\n",
            " -1.3571984  -0.26663867 -1.1858598  -0.68351936 -1.2126993  -1.6828506\n",
            " -0.83453155 -0.17154989 -1.8926399  -1.8882886  -1.3023504  -0.6977453\n",
            " -1.0421969 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8350222  -1.0527091  -1.0348802  -1.8215297  -1.8284898  -2.2116463\n",
            " -2.094109   -0.27213478 -1.7216254  -0.94196945 -1.2423648  -2.4152424\n",
            " -1.1182456  -0.1229292  -2.7580817  -2.8372693  -1.6763824  -0.7347388\n",
            " -1.4017038 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.5840563  -0.8175919  -0.9735543  -1.7546645  -1.4236871  -1.9439254\n",
            " -1.7703633  -0.25610742 -1.5413054  -0.665329   -1.054213   -1.9315249\n",
            " -1.0244492  -0.18189734 -2.2197251  -2.4316864  -1.4593408  -0.8038238\n",
            " -1.1200202 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7618549  -0.94209594 -1.0154328  -1.8520534  -1.4848428  -1.9935567\n",
            " -1.9654444  -0.19154777 -1.7113276  -0.8381784  -1.134392   -2.188426\n",
            " -1.1269817  -0.31189147 -2.5323095  -2.464622   -1.4438226  -0.86388993\n",
            " -1.1163533 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8302468  -0.9448384  -1.0270451  -1.853802   -1.569848   -2.1485848\n",
            " -1.8585131  -0.11161251 -1.6886668  -0.76923466 -1.1212229  -2.1439269\n",
            " -1.1663013  -0.13617925 -2.5227537  -2.5486965  -1.552479   -0.9086497\n",
            " -1.212195  ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.412087   -0.79954624 -0.922913   -1.544621   -1.3052733  -1.76838\n",
            " -1.5471334  -0.17446524 -1.3813411  -0.68666434 -1.2076565  -1.8213342\n",
            " -1.0535337  -0.10339138 -2.0789244  -2.2193437  -1.3487353  -0.6913974\n",
            " -1.0788546 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7284305  -0.9155366  -0.9813138  -1.7700162  -1.4514613  -2.0931537\n",
            " -1.8462911  -0.07328871 -1.7268947  -0.87378246 -1.1909051  -2.1590555\n",
            " -1.108884   -0.20753193 -2.5033553  -2.6040146  -1.4144485  -0.7870468\n",
            " -1.3636261 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 8\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.9245688  -1.0801772  -1.3414822  -1.9888929  -1.4309796  -2.4555151\n",
            " -2.1075416  -0.2631016  -1.898144   -0.8291448  -1.0708097  -2.5688992\n",
            " -1.1338494   0.05731102 -2.8896158  -3.3869236  -1.8368047  -0.6897066\n",
            " -1.4734057 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.9059539  -1.0073502  -1.1100552  -1.7513553  -1.1892791  -2.1465058\n",
            " -1.8546032  -0.13552162 -1.6226687  -0.9515256  -1.0537113  -2.109949\n",
            " -1.1029198   0.06208378 -2.5849404  -2.7719707  -1.6025614  -0.63837284\n",
            " -1.4053007 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.9278119  -1.0620846  -1.2044286  -1.8052928  -1.4693731  -2.3322592\n",
            " -1.9365153  -0.1519128  -1.741349   -0.7630265  -1.164849   -2.414482\n",
            " -1.1526924   0.12714843 -2.843298   -3.0282674  -1.808828   -0.68568397\n",
            " -1.4710696 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8624096  -1.0637922  -1.1092595  -1.8064275  -1.1933265  -2.2328267\n",
            " -1.9918542  -0.20271148 -1.5395379  -0.93588936 -1.0573686  -2.2984748\n",
            " -1.0902642   0.12970011 -2.7291822  -2.9481044  -1.7005624  -0.64224136\n",
            " -1.5161277 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.0022602  -1.0022477  -1.203762   -2.0377254  -1.5772209  -2.2782297\n",
            " -2.0603116  -0.05504391 -1.7154597  -0.6463778  -1.1552535  -2.6437933\n",
            " -1.2904811  -0.18302232 -2.9270868  -2.6315117  -1.7037072  -0.91254175\n",
            " -1.3835995 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.9195957  -1.0210773  -1.1835476  -1.8384632  -1.0525273  -2.2349863\n",
            " -1.9247656  -0.11541796 -1.5197225  -0.85196555 -0.9847137  -2.3076642\n",
            " -1.0809597   0.070256   -2.7513154  -2.8855352  -1.6831433  -0.6537063\n",
            " -1.3541989 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7705066  -0.8394062  -1.27332    -1.8927201  -1.3896762  -2.1635268\n",
            " -1.7382717  -0.0666278  -1.5167445  -0.60126483 -1.1403892  -2.2762847\n",
            " -1.251013   -0.07870094 -2.679233   -2.359056   -1.605083   -0.9651555\n",
            " -1.2557184 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8333969  -0.95177567 -1.1230156  -1.6994596  -0.97751683 -2.1276126\n",
            " -1.7886955  -0.13082868 -1.3586887  -0.9220909  -1.0395852  -2.0302863\n",
            " -1.0832772   0.1939691  -2.539322   -2.6763945  -1.5544336  -0.6095112\n",
            " -1.3657659 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.781323   -0.8850831  -1.2080266  -1.7057968  -1.145029   -2.2218015\n",
            " -1.7643495  -0.1436647  -1.469598   -0.7511964  -1.0597239  -2.2363887\n",
            " -1.1002045   0.25661838 -2.6319702  -2.7644033  -1.6544677  -0.70474756\n",
            " -1.2819866 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7221143  -0.9521783  -1.09353    -1.6474315  -0.8411069  -2.0239651\n",
            " -1.74179    -0.02792626 -1.2478139  -0.9110079  -1.0349063  -2.0568366\n",
            " -1.0630063   0.21021639 -2.4523323  -2.7078037  -1.5048245  -0.54097736\n",
            " -1.362402  ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.713754   -0.9314636  -1.2350656  -1.7386471  -1.0837543  -2.0993626\n",
            " -1.6779225   0.11099834 -1.3989561  -0.6243838  -1.1367879  -2.2800539\n",
            " -1.1198653   0.10052002 -2.5545828  -2.678576   -1.6252245  -0.8296427\n",
            " -1.3087283 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7006097  -0.96227574 -1.2224504  -1.7111046  -1.0687356  -2.0490124\n",
            " -1.7822952   0.17124027 -1.3071231  -0.70985895 -1.1108447  -2.234998\n",
            " -1.025655    0.00993651 -2.5950763  -2.6659446  -1.6576838  -0.98846376\n",
            " -1.3301259 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7704042  -0.9488082  -1.1136332  -1.7659372  -0.9866656  -2.10148\n",
            " -1.7316817   0.3321183  -1.3137841  -0.6390192  -1.0183656  -2.182094\n",
            " -1.0717093   0.17087579 -2.5236273  -2.5631156  -1.5806247  -0.92799026\n",
            " -1.2818258 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8003271  -1.0873302  -1.0895929  -1.6880381  -1.0009836  -2.1303422\n",
            " -1.9139364   0.19782305 -1.2352428  -0.8806616  -1.0511034  -2.2320187\n",
            " -1.019525    0.38229597 -2.6686869  -2.8394623  -1.6856855  -0.69493914\n",
            " -1.3991146 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7511768  -1.000762   -1.1022552  -1.6782818  -1.0332118  -2.191854\n",
            " -1.8538924   0.33066967 -1.3028151  -0.7569644  -1.1058955  -2.3323107\n",
            " -1.0769758   0.40936738 -2.6684     -2.8699994  -1.7177609  -0.6976786\n",
            " -1.3977635 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8318175  -0.9540877  -1.0864328  -1.7215662  -0.7201569  -2.0982697\n",
            " -1.8281957   0.34248185 -1.199421   -0.8230187  -0.91131854 -2.1122196\n",
            " -1.0178099   0.37120733 -2.5334218  -2.7313561  -1.5697328  -0.6106515\n",
            " -1.2650272 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.798073   -0.9843268  -1.1316136  -1.7362806  -1.0922309  -2.1095748\n",
            " -1.7254312   0.9388578  -1.3122061  -0.59332234 -1.1365587  -2.2482042\n",
            " -1.0955234   0.22360694 -2.6098251  -2.6319137  -1.632429   -0.9000758\n",
            " -1.3558453 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7676768  -0.9375383  -1.2181213  -1.724005   -1.0589781  -2.1067972\n",
            " -1.7920072   0.8905142  -1.2510803  -0.70578617 -1.0903516  -2.1641338\n",
            " -1.0202715   0.14416827 -2.6221437  -2.576754   -1.6668482  -1.0401344\n",
            " -1.3009273 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.6005197  -0.86334807 -1.1369377  -1.6363838  -0.9430993  -2.0420914\n",
            " -1.5755903   0.82891244 -1.1733413  -0.6446606  -0.99941033 -1.9922004\n",
            " -1.0006136   0.35180405 -2.3372505  -2.42162    -1.5498524  -0.8494412\n",
            " -1.2402847 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8683115  -1.0453022  -1.1441638  -1.7179302  -0.7839973  -2.1661997\n",
            " -1.8671765   1.018653   -1.2283211  -0.8465067  -0.9799472  -2.205345\n",
            " -1.0468253   0.48209068 -2.626458   -2.7724695  -1.6454039  -0.6942599\n",
            " -1.2996172 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.5485864  -0.86462903 -1.1355214  -1.6028173  -0.8268986  -2.0198667\n",
            " -1.6214972   1.0046258  -1.1434861  -0.745503   -1.0022998  -2.085374\n",
            " -0.9951913   0.4548023  -2.336738   -2.6071491  -1.5775585  -0.6829796\n",
            " -1.2269607 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7923602  -1.0480433  -1.1714922  -1.7015865  -0.7834629  -2.12574\n",
            " -1.8595079   1.2708642  -1.186208   -0.88521767 -1.0310616  -2.2155335\n",
            " -1.049146    0.48960787 -2.6362789  -2.8568425  -1.6478819  -0.62076175\n",
            " -1.3214179 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.6008676  -0.7660047  -1.0799664  -1.6538597  -0.98159456 -1.9931602\n",
            " -1.6743344   1.3083452  -1.1218027  -0.68047166 -0.9871471  -2.1193268\n",
            " -1.1051244   0.28873724 -2.436074   -2.2878737  -1.4600074  -0.7073573\n",
            " -1.1319656 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.950416   -1.0631497  -1.1601732  -1.8308318  -0.7477784  -2.2941048\n",
            " -1.9235487   1.8195157  -1.2777066  -0.8610271  -0.99356383 -2.299257\n",
            " -1.1033492   0.5902484  -2.7253294  -2.9958663  -1.7232927  -0.7327809\n",
            " -1.3623406 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.4819231  -0.7775684  -0.97810924 -1.5472852  -0.79860973 -1.9280547\n",
            " -1.5093604   1.4818821  -0.9771298  -0.77893233 -0.9496022  -1.8976973\n",
            " -0.89214694  0.4916076  -2.1886828  -2.3766747  -1.4750457  -0.76332325\n",
            " -1.167778  ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7917063  -1.0903965  -1.1100883  -1.743104   -0.97686607 -2.1897595\n",
            " -1.9616194   2.0616074  -1.1631409  -0.86737406 -1.0816517  -2.3094893\n",
            " -1.0521152   0.51551336 -2.706025   -2.8118424  -1.7418113  -0.82168627\n",
            " -1.3902205 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.664105   -0.9233472  -1.1032877  -1.660644   -0.9388344  -2.1031857\n",
            " -1.7061279   2.1305087  -1.1704386  -0.7235271  -1.0409564  -2.1120863\n",
            " -0.999641    0.55267686 -2.4470973  -2.659276   -1.6446495  -0.737409\n",
            " -1.3293657 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8532596  -1.0253252  -1.3979182  -1.919025   -1.0631386  -2.342014\n",
            " -1.9939506   2.4987445  -1.3369807  -0.7474165  -1.1441591  -2.4878666\n",
            " -1.1183059   0.23603639 -2.9223437  -2.958013   -1.8501128  -1.0242885\n",
            " -1.426212  ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7901601  -0.94610304 -1.2141398  -1.9360272  -1.3036366  -2.2446754\n",
            " -1.8589989   2.5121152  -1.2285274  -0.6456541  -1.1514386  -2.328887\n",
            " -1.2406762   0.19917375 -2.7663162  -2.4483633  -1.6252307  -0.95539325\n",
            " -1.4044207 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 45 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.1052501  -1.1326742  -1.3103542  -1.9853551  -0.86152107 -2.538886\n",
            " -2.0687385   3.0176747  -1.399246   -0.93105304 -1.0884206  -2.4783812\n",
            " -1.2089115   0.6924857  -2.9902554  -3.2314777  -1.88299    -0.77103686\n",
            " -1.4826393 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8110371  -1.0089841  -1.0857589  -1.8161299  -0.79280144 -2.1069374\n",
            " -1.8106709   3.0998755  -1.1759167  -0.73474306 -1.0639932  -2.0981727\n",
            " -1.0910145   0.47281277 -2.528349   -2.7015328  -1.6708242  -0.8670713\n",
            " -1.2677184 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.120785   -1.157172   -1.2623577  -1.9894806  -0.76667887 -2.4976287\n",
            " -2.0854034   3.563756   -1.3615358  -0.9240457  -1.0686634  -2.499835\n",
            " -1.1939678   0.6950555  -2.9613085  -3.2524834  -1.8781449  -0.7864596\n",
            " -1.4802438 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.6647475 -1.0314852 -1.1657872 -1.7701044 -0.94976   -2.113576\n",
            " -1.7575879  3.0439632 -1.1947498 -0.7163993 -1.1632563 -2.1631901\n",
            " -1.0950899  0.5741982 -2.5136595 -2.867167  -1.7343012 -0.7223563\n",
            " -1.2635455]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.075617   -1.1628733  -1.3189977  -2.0060477  -0.81427747 -2.4792533\n",
            " -2.1145961   3.668721   -1.3234078  -0.9682467  -1.0840027  -2.5735512\n",
            " -1.2303904   0.6403278  -3.0381806  -3.2438912  -1.8787637  -0.7193252\n",
            " -1.2890813 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.631502   -0.85852575 -1.1125857  -1.8229634  -1.0570202  -2.0856056\n",
            " -1.7404155   2.7023284  -1.0542575  -0.72865623 -1.1323436  -2.2597392\n",
            " -1.1768861   0.3273432  -2.5293112  -2.3837123  -1.605576   -0.9029782\n",
            " -0.9511307 ]\n",
            "Epsilon reduced to 0.052428800000000025\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.9553506 -1.7169398 -1.6372559 -2.7178583 -2.7799642 -3.2382083\n",
            " -2.7277622  1.4857646 -2.0644226 -1.1027851 -1.8252105 -3.505074\n",
            " -1.7625244 -0.3108348 -4.112204  -3.6460767 -2.6404533 -1.1497148\n",
            " -1.9489237]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.3992162  -1.4257698  -1.221633   -2.1700597  -1.604442   -2.6334636\n",
            " -2.3197863   1.6933403  -1.4923395  -1.3372356  -1.527332   -2.7983093\n",
            " -1.395863    0.13349628 -3.2316456  -3.228869   -2.0128717  -0.9781682\n",
            " -1.4408607 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.6478884  -1.4562881  -1.3062342  -2.3212605  -2.1771753  -2.8022199\n",
            " -2.3760853   1.3401879  -1.5915644  -1.0675875  -1.7077457  -3.1595588\n",
            " -1.4573805  -0.01247455 -3.6081898  -3.361301   -2.2110636  -1.1520642\n",
            " -1.4117103 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.477355   -1.3688179  -1.171103   -2.0869071  -1.4969186  -2.6019845\n",
            " -2.1745722   1.5912737  -1.4241753  -1.2438514  -1.4660249  -2.6031537\n",
            " -1.1963984   0.24708346 -3.1981103  -3.2747936  -1.9995195  -1.183244\n",
            " -1.1461964 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.50388    -1.4511138  -1.4263052  -2.3592913  -2.0763917  -2.7331715\n",
            " -2.5462139   0.9461625  -1.6245558  -1.077942   -1.6515332  -3.3153534\n",
            " -1.6049525   0.01048908 -3.5802875  -3.3903794  -2.1668162  -0.7967883\n",
            " -1.4248697 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.283568   -1.2736661  -1.0511091  -1.9159809  -1.3486015  -2.4131656\n",
            " -1.9891716   1.6576117  -1.3052207  -1.2404338  -1.4446212  -2.5024123\n",
            " -1.2378974   0.20749462 -2.9464374  -3.0993376  -1.8593562  -1.0858012\n",
            " -1.0333675 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.4688642  -1.3678921  -1.2189683  -2.1750455  -2.1551468  -2.6391912\n",
            " -2.102449    1.1026723  -1.6184326  -1.1071264  -1.6070075  -2.6050735\n",
            " -1.3016151   0.20493478 -3.3477278  -3.0076332  -1.875058   -1.2455307\n",
            " -1.201605  ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.2785802  -1.411092   -1.0794164  -1.9917052  -1.6846715  -2.481622\n",
            " -2.2011979   1.1505395  -1.4170892  -1.2365028  -1.4505796  -2.5494645\n",
            " -1.2783701   0.29388326 -3.1422224  -2.9643435  -1.8843787  -1.0103385\n",
            " -1.2357109 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.55986   -1.575804  -1.2608263 -2.3691947 -2.3431191 -2.9505537\n",
            " -2.3589528  0.9409385 -1.6765677 -1.1930679 -1.6601068 -2.9335718\n",
            " -1.2959971  0.3039735 -3.7128186 -3.4296312 -2.0630083 -1.2686405\n",
            " -1.3381644]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.3691823  -1.3460473  -1.4319837  -2.1928842  -2.1532574  -2.53267\n",
            " -2.2059166   1.7041619  -1.4798211  -0.99484843 -1.7508936  -2.909493\n",
            " -1.2844553  -0.16243917 -3.3242056  -3.2615848  -2.0705159  -1.3539047\n",
            " -1.155201  ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.5382874  -1.4107056  -1.3911489  -2.2411547  -2.080743   -2.7534366\n",
            " -2.3582175   0.99638087 -1.5530462  -1.0502146  -1.7120513  -3.122606\n",
            " -1.3952626   0.00747751 -3.497378   -3.3533454  -2.0780048  -1.0181975\n",
            " -1.0946437 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.2298121  -1.2719779  -1.0802444  -1.8410845  -1.3909634  -2.3233123\n",
            " -1.8708769   1.0548363  -1.3255813  -1.1315187  -1.4043645  -2.353784\n",
            " -1.1795564   0.19361635 -2.9006944  -2.8430066  -1.801366   -1.051777\n",
            " -0.8660158 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.5205688  -1.4214994  -1.4228024  -2.197749   -2.0758102  -2.7371876\n",
            " -2.2843761   0.90126145 -1.5918864  -1.0724291  -1.6769483  -3.1094325\n",
            " -1.3953996   0.02057718 -3.5130334  -3.44064    -2.034397   -0.92609954\n",
            " -1.0629362 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.96169    -1.152966   -1.0093998  -1.6850921  -1.1785191  -2.174882\n",
            " -1.7694148   0.9866541  -1.1915444  -1.1261946  -1.2651     -2.1679564\n",
            " -1.1147704   0.26415017 -2.6658542  -2.70857    -1.7134808  -0.92412\n",
            " -0.7553799 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.3932676e+00 -1.2786589e+00 -1.1663444e+00 -2.0710549e+00\n",
            " -1.9085793e+00 -2.4833422e+00 -2.0711780e+00  7.3978114e-01\n",
            " -1.4342061e+00 -9.1538906e-01 -1.5017440e+00 -2.7273324e+00\n",
            " -1.2898148e+00  1.8836930e-03 -3.1890862e+00 -2.9580650e+00\n",
            " -1.9137388e+00 -1.0029646e+00 -9.0153980e-01]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.1018994  -1.1012257  -1.094523   -1.8396049  -1.556991   -2.1491163\n",
            " -1.738169    1.3182144  -1.2256775  -0.88168687 -1.4381824  -2.3103004\n",
            " -1.153409   -0.11099679 -2.745551   -2.529203   -1.7657338  -1.2620753\n",
            " -0.6745343 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.3047602  -1.4147413  -1.1270418  -2.2153728  -2.0198514  -2.6120665\n",
            " -2.2114584   0.68834823 -1.5237368  -1.118448   -1.4325154  -2.6123142\n",
            " -1.322579    0.19797552 -3.2644382  -2.8641286  -1.7685156  -1.0828041\n",
            " -1.0760064 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8812188  -1.0380548  -0.92556196 -1.5993897  -1.053797   -1.9625716\n",
            " -1.7467742   0.7668111  -1.0715514  -1.0443298  -1.2232484  -1.9985296\n",
            " -1.0932521   0.2478379  -2.423064   -2.4539576  -1.6501709  -0.8431481\n",
            " -0.6027974 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.3939776  -1.3144858  -1.256342   -2.1080074  -1.9695976  -2.5637841\n",
            " -2.1900694   0.44178993 -1.4827328  -0.86567473 -1.52189    -2.901582\n",
            " -1.2919285   0.06304263 -3.3160582  -3.156189   -1.9911721  -0.9307145\n",
            " -0.9732568 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 16\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.876673   -1.1492249  -0.97181183 -1.735134   -1.2272826  -2.104987\n",
            " -1.9663879   0.58990777 -1.1325759  -1.050195   -1.1690369  -2.2188501\n",
            " -1.1719518   0.26953876 -2.5900884  -2.6298046  -1.7441633  -0.78703696\n",
            " -0.80007035]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.3919382  -1.4839722  -1.3059915  -2.2272213  -2.1825838  -2.8083773\n",
            " -2.2758305   0.3835681  -1.5948191  -1.1348413  -1.6186712  -2.826703\n",
            " -1.2218771   0.29066372 -3.5007343  -3.3184314  -1.8185531  -1.1111755\n",
            " -1.0967051 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8769873  -1.1467617  -1.0971569  -1.8166682  -1.556548   -2.050363\n",
            " -1.8463953   0.9853112  -1.1701989  -0.92141175 -1.356757   -2.3431995\n",
            " -1.270461   -0.14367782 -2.6279988  -2.3773026  -1.5529325  -1.0714613\n",
            " -0.75790405]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.241203   -1.3554331  -1.2229289  -2.0456934  -1.9518924  -2.560396\n",
            " -2.011745    0.26053858 -1.4552591  -1.0908097  -1.545365   -2.573166\n",
            " -1.1877908   0.2974924  -3.2039394  -3.0564728  -1.5700907  -1.0428017\n",
            " -0.9355011 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5067133  -0.812102   -0.97409546 -1.5021868  -1.1560524  -1.6412512\n",
            " -1.5034273   1.0169551  -0.85171384 -0.5166863  -0.9611114  -2.025987\n",
            " -0.94680065  0.02833509 -2.2191958  -1.9207793  -1.3940357  -0.8286965\n",
            " -0.4733577 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6093098  -0.8923954  -0.9103896  -1.5636607  -1.2778575  -1.8113717\n",
            " -1.522893    0.5297119  -1.0849406  -0.52301073 -0.9597227  -1.791147\n",
            " -0.9515282   0.3540059  -2.3739462  -1.9114949  -1.1938708  -0.68786234\n",
            " -0.5373206 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5410079  -0.8168037  -0.724246   -1.3005564  -0.7152791  -1.5801406\n",
            " -1.3861252   0.71183705 -0.8245643  -0.63364935 -0.8416954  -1.5526358\n",
            " -0.82227254  0.3620926  -2.0203512  -1.8488027  -1.1368332  -0.5996802\n",
            " -0.30000007]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6408333  -0.8179002  -0.8738896  -1.5105151  -1.1193548  -1.6963137\n",
            " -1.5044265   0.68031394 -0.9280215  -0.3709864  -0.949169   -1.872918\n",
            " -0.97345936  0.10108674 -2.2237456  -1.7174882  -1.1146331  -0.6638844\n",
            " -0.44354892]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.4315159  -0.7465624  -0.8243229  -1.3046671  -0.9331252  -1.4591765\n",
            " -1.3162262   1.1450344  -0.7422346  -0.5178357  -0.9651379  -1.6036025\n",
            " -0.87703633  0.08007662 -1.9222984  -1.6680378  -1.1629369  -0.8014379\n",
            " -0.2574266 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6019782  -0.85535353 -0.8211314  -1.3839202  -1.1931754  -1.640852\n",
            " -1.5075148   0.63150376 -0.99062306 -0.34197405 -1.0080618  -1.8790294\n",
            " -0.97381186  0.21559735 -2.2562778  -1.8515911  -1.1045679  -0.48928183\n",
            " -0.48008686]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5746939  -0.9207212  -0.7831507  -1.3592172  -0.90242213 -1.6794512\n",
            " -1.4926986   0.6404239  -0.9347351  -0.6082522  -0.8265184  -1.6493374\n",
            " -0.8209942   0.40020302 -2.2073634  -1.9244101  -1.0764827  -0.61697286\n",
            " -0.456927  ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5266541  -0.80969465 -0.82909054 -1.4627295  -1.2201982  -1.6539719\n",
            " -1.5204911   0.6313505  -0.9684423  -0.35195476 -0.90076727 -1.9279495\n",
            " -0.9376874   0.16640985 -2.2494032  -1.8057944  -1.0746529  -0.59493816\n",
            " -0.5091983 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5578728  -0.84206855 -0.79770315 -1.3718463  -0.83066344 -1.6575378\n",
            " -1.4886978   0.8158784  -0.89675707 -0.6409024  -0.88624626 -1.6893489\n",
            " -0.8378536   0.34375224 -2.134082   -1.9182085  -1.0346066  -0.5759084\n",
            " -0.3163424 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6599408  -1.00634    -0.90083873 -1.538948   -1.3244238  -1.9008383\n",
            " -1.6090093   0.8926155  -1.1124442  -0.62392825 -1.0859419  -1.8372915\n",
            " -0.93800414  0.42044327 -2.450006   -1.9970286  -1.10335    -0.69042516\n",
            " -0.5200783 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5566614  -0.8492332  -0.8007967  -1.3295546  -0.74819714 -1.6391197\n",
            " -1.4526048   1.0090293  -0.8565653  -0.6750224  -0.88594025 -1.612241\n",
            " -0.85434765  0.30176657 -2.0415792  -1.9242857  -1.0104043  -0.5690755\n",
            " -0.29330456]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6124114  -0.9765435  -0.8932727  -1.5003884  -1.2719717  -1.832492\n",
            " -1.5175766   0.9788577  -1.0925214  -0.5761974  -1.0642105  -1.7911297\n",
            " -0.946923    0.45320964 -2.403673   -1.9878175  -1.0854477  -0.6340127\n",
            " -0.5197108 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6006587  -0.942437   -0.85500246 -1.3908149  -0.95017505 -1.7339916\n",
            " -1.5360625   0.94318557 -0.8985302  -0.7185949  -0.8354904  -1.9067849\n",
            " -0.9612698   0.38459602 -2.2783813  -1.9925641  -1.1263026  -0.6355781\n",
            " -0.5167583 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6360766  -0.8556648  -0.99709123 -1.5336185  -1.1228166  -1.7769282\n",
            " -1.6223066   0.9792057  -1.0295668  -0.38548052 -0.9979236  -2.096715\n",
            " -1.0286294   0.20452654 -2.3948512  -2.0216136  -1.0513879  -0.4944502\n",
            " -0.44832724]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5487539  -0.8200941  -0.72707546 -1.307288   -0.718063   -1.5880148\n",
            " -1.3931454   1.2700802  -0.8281449  -0.63601625 -0.84581023 -1.5599229\n",
            " -0.8256036   0.39208096 -2.029939   -1.8573791  -1.0079997  -0.6017925\n",
            " -0.2745325 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7340412  -0.92265934 -0.89454406 -1.51366    -1.2705483  -1.8151351\n",
            " -1.5891174   1.3357671  -1.0625476  -0.5182632  -1.0571983  -1.829665\n",
            " -0.989406    0.40234077 -2.407766   -1.8663908  -1.018466   -0.6821455\n",
            " -0.44657195]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5955193  -0.83257145 -0.75597256 -1.373455   -0.782183   -1.6455612\n",
            " -1.4405798   1.4933199  -0.8708659  -0.66654754 -0.8775354  -1.6256245\n",
            " -0.85724795  0.4195309  -2.1092687  -1.9232924  -1.0347803  -0.6293195\n",
            " -0.28014895]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7557683  -1.0469968  -0.93448395 -1.6228502  -1.446242   -1.9639173\n",
            " -1.6508558   1.7177593  -1.1394739  -0.5838883  -1.1305406  -1.9003218\n",
            " -0.97055113  0.5020315  -2.5621994  -2.0717297  -1.0783019  -0.7219344\n",
            " -0.61158615]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.5366275  -0.93173397 -0.8227356  -1.376821   -0.821319   -1.738462\n",
            " -1.6104093   1.5799934  -0.8836962  -0.74831927 -0.90616405 -1.8249568\n",
            " -0.9316443   0.44727737 -2.2214382  -2.1636438  -1.1225147  -0.59564763\n",
            " -0.43489915]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7239432  -0.9369132  -0.97051585 -1.5935613  -1.2287906  -1.8699154\n",
            " -1.7052447   1.7629176  -1.0452563  -0.49494004 -1.07455    -2.157328\n",
            " -1.0331994   0.28379798 -2.4887574  -2.0508857  -1.0908136  -0.6304873\n",
            " -0.46816713]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7463261  -0.984727   -0.9106942  -1.4847581  -0.925651   -1.8466015\n",
            " -1.5972674   1.9946207  -1.0232373  -0.70857    -1.0198127  -1.7770487\n",
            " -0.9445667   0.39969718 -2.3490112  -2.0856733  -1.0267247  -0.6032616\n",
            " -0.33653298]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.4689076  -0.79795986 -0.8903557  -1.3386315  -1.2652639  -1.5281461\n",
            " -1.3439779   1.5140519  -0.9204606  -0.27418602 -0.945093   -1.7811192\n",
            " -0.9905834   0.12287688 -2.1064193  -1.5698878  -1.0032605  -0.60412467\n",
            " -0.3831724 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6686234  -0.9366127  -1.0796311  -1.5902399  -1.3279238  -1.7803532\n",
            " -1.5310345   2.1385157  -1.0486548  -0.55772054 -1.1432408  -1.9133296\n",
            " -0.99955493  0.18121332 -2.4457903  -2.0021358  -1.2648475  -0.95491403\n",
            " -0.54411095]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.6871076  -0.9027938  -0.898007   -1.5071895  -1.4060086  -1.7011793\n",
            " -1.536026    2.0637789  -1.0990696  -0.28801736 -1.122749   -2.023411\n",
            " -1.077164    0.19252917 -2.4025834  -1.9196033  -1.0309418  -0.58331084\n",
            " -0.48823625]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7929919  -0.93362236 -0.9490226  -1.5595175  -1.004732   -1.7860851\n",
            " -1.4901243   2.3509707  -1.0400413  -0.58685255 -0.9552367  -1.9805771\n",
            " -0.99385685  0.3580426  -2.422831   -2.081039   -1.0823785  -0.5607056\n",
            " -0.3731671 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.7246122  -0.87820077 -0.9272527  -1.5330619  -1.3103167  -1.7180823\n",
            " -1.4925379   2.229777   -1.1016176  -0.31180856 -1.0994745  -2.0334861\n",
            " -1.0949174   0.17226422 -2.3972833  -1.8853952  -1.0421842  -0.61475545\n",
            " -0.37685493]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 50 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.2404691  -0.7248794  -0.65886927 -1.1805711  -0.587828   -1.4317455\n",
            " -1.2106593   2.4504952  -0.6968322  -0.66013515 -0.86495924 -1.4601732\n",
            " -0.83923936  0.34774515 -1.7259386  -1.7020344  -0.7940748  -0.7027568\n",
            " -0.04360116]\n",
            "Epsilon reduced to 0.04194304000000002\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 1 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.7172891  -1.0105072  -0.8793421  -1.8435402  -1.5529554  -2.0215845\n",
            " -1.7676402   1.3220916  -1.10783    -0.7828097  -1.2396808  -2.118953\n",
            " -1.1752285   0.01148407 -2.5179634  -2.406653   -1.6961309  -0.9941898\n",
            " -0.73867893]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 2 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.2091398  -1.2711453  -1.277788   -2.2908583  -1.9390433  -2.4607024\n",
            " -2.1143973   1.9037486  -1.3642135  -0.9108106  -1.2909428  -2.9234788\n",
            " -1.3788681   0.1221745  -3.0453167  -2.958024   -1.4584017  -1.1726141\n",
            " -0.86934996]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 3 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.2359424  -1.2879252  -1.3066083  -2.3498166  -1.7297347  -2.3996766\n",
            " -2.1006513   1.8572675  -1.365416   -1.056272   -1.2589492  -2.9226713\n",
            " -1.3277901   0.09566879 -2.9038434  -2.9396     -1.5198429  -1.2019801\n",
            " -1.0623657 ]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 4 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.2096832  -1.1754428  -1.1613984  -2.267856   -2.0993552  -2.3987148\n",
            " -1.9305353   0.9222082  -1.456644   -0.94319767 -1.3180983  -2.9403455\n",
            " -1.2728407   0.07848512 -2.858841   -2.9429479  -1.2229811  -1.0001554\n",
            " -1.0133648 ]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 5 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.0210397  -1.1415728  -1.0699843  -1.8842537  -1.2071614  -2.2285552\n",
            " -1.7858572   2.0422044  -1.1773716  -0.95030224 -1.0805687  -2.3764565\n",
            " -1.1859374   0.30549413 -2.6128411  -2.7107916  -1.2011672  -0.7425254\n",
            " -0.58346003]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 6 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.0773568  -1.1173319  -1.0339895  -2.0620337  -1.5783741  -2.24266\n",
            " -1.8322288   1.4667085  -1.2497104  -0.8541679  -1.1740007  -2.4388185\n",
            " -1.2360771   0.24081387 -2.7327437  -2.5931735  -1.2514033  -0.9348764\n",
            " -0.7132837 ]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 7 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.4578452  -1.3196706  -1.3489252  -2.3364146  -2.2497182  -2.5863235\n",
            " -2.2232554   1.1639054  -1.6226463  -0.95302093 -1.261806   -3.2278938\n",
            " -1.4137082   0.04720086 -3.485725   -2.9128876  -1.3801095  -1.0766065\n",
            " -0.9806084 ]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 8 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.9595393  -1.0680517  -0.9290269  -1.6362782  -1.3035378  -1.972195\n",
            " -1.4817543   1.0359466  -1.186114   -0.8476007  -1.2016089  -2.1581717\n",
            " -1.1655414   0.1407805  -2.5061855  -2.452345   -1.172258   -0.85127187\n",
            " -0.68740463]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 9 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.8238982  -1.1136526  -0.9520495  -1.9365807  -1.9095676  -1.8622086\n",
            " -1.5735158  -0.4173091  -1.2360183  -0.8591056  -1.0014777  -2.2090669\n",
            " -1.3032129   0.08623883 -2.3763924  -2.0173643  -1.1716934  -0.9033248\n",
            " -1.1181135 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 10 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.3644297 -1.4479352 -1.1881864 -2.2701015 -2.1725168 -2.620559\n",
            " -2.1001039  0.6541996 -1.5158865 -1.0794259 -1.4219168 -2.9190578\n",
            " -1.3525925  0.4839236 -3.1893723 -3.1826591 -1.2734011 -1.018323\n",
            " -1.175331 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 11 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.3340096  -0.8360167  -0.8050192  -1.53501    -1.3773383  -1.6765405\n",
            " -1.3286625   0.46593028 -0.92400897 -0.6027498  -0.83876735 -1.748861\n",
            " -0.8863364   0.19036058 -2.013695   -1.7417139  -1.0952171  -0.77303964\n",
            " -0.5914816 ]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 12 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.881562   -1.107224   -1.011799   -1.8763202  -1.6424162  -2.1159558\n",
            " -1.5207735   1.0159239  -1.2406389  -0.6965032  -0.91406626 -2.2619421\n",
            " -1.1247866   0.4701889  -2.6683402  -2.2312274  -1.2432423  -0.7876532\n",
            " -0.85465056]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.3714427  -0.8231577  -0.8017445  -1.3354645  -1.0382489  -1.6532762\n",
            " -1.2549543   1.0212594  -0.90614545 -0.61576545 -0.9379461  -1.7430888\n",
            " -0.9679807   0.2620954  -2.1090322  -2.0959787  -1.0455408  -0.5355466\n",
            " -0.4724773 ]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.5281106  -0.8623647  -0.8532243  -1.6457678  -1.5657331  -1.6294081\n",
            " -1.3801186   0.10552646 -1.0380044  -0.51958054 -0.8355906  -1.9120564\n",
            " -1.0099792   0.19925533 -2.017606   -1.6792346  -1.0360782  -0.6846456\n",
            " -0.8103867 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8590766  -1.075278   -1.0129503  -1.798087   -1.7088734  -1.9782689\n",
            " -1.5892513  -0.00936542 -1.2561505  -0.71778816 -1.0899371  -2.2988846\n",
            " -1.2166688   0.23125058 -2.4883077  -2.3458343  -1.2989436  -0.77522635\n",
            " -0.8896336 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.3138884e+00 -8.0234134e-01 -8.2276326e-01 -1.3467413e+00\n",
            " -1.2404246e+00 -1.4463727e+00 -1.2014570e+00  8.0901384e-04\n",
            " -8.1935740e-01 -5.4550916e-01 -8.0810261e-01 -1.7319610e+00\n",
            " -8.9644116e-01  7.1102589e-02 -1.8848221e+00 -1.6528820e+00\n",
            " -1.0368881e+00 -6.3148248e-01 -5.6624776e-01]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.6711178  -0.9487     -0.9598146  -1.6885496  -1.6030687  -1.8142473\n",
            " -1.4290429   0.3443212  -1.1201385  -0.50992626 -0.9221926  -1.9676089\n",
            " -0.9578738   0.17712542 -2.3926754  -1.9397196  -1.1162438  -0.89261866\n",
            " -0.6472668 ]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.33857    -0.69552857 -0.7233802  -1.1879147  -0.8739631  -1.4280841\n",
            " -1.0423309   0.6476362  -0.76605284 -0.52296287 -0.8205881  -1.4343063\n",
            " -0.8029964   0.15685618 -1.7905118  -1.730321   -0.8132464  -0.40636942\n",
            " -0.4182452 ]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.3425261  -0.7451297  -0.8044859  -1.4406737  -1.3687351  -1.4123743\n",
            " -1.1871254  -0.12731655 -0.8869457  -0.3650866  -0.7658665  -1.7871921\n",
            " -0.9221295   0.13704751 -1.7396033  -1.6261152  -0.906985   -0.56847465\n",
            " -0.7957199 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.4493563  -0.7390457  -0.767735   -1.3101165  -0.9877869  -1.4608235\n",
            " -1.1354036   0.48263925 -0.9077955  -0.512879   -0.7149582  -1.6176211\n",
            " -0.81144285  0.16541725 -1.8725731  -1.7299645  -0.70268565 -0.41433388\n",
            " -0.43649986]\n",
            "\n",
            "Taking action 5 from 7\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.977361   -0.6763757  -0.6000679  -1.0218743  -1.0220597  -1.0802035\n",
            " -0.8994371  -0.06413007 -0.63141644 -0.35881656 -0.703537   -1.2287666\n",
            " -0.6814527   0.10289259 -1.4371433  -1.2986188  -0.6198436  -0.4735189\n",
            " -0.420678  ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.3784153  -0.9751377  -0.9783723  -1.6965929  -1.5956894  -1.7762487\n",
            " -1.4073205  -0.3736733  -1.0197827  -0.7000614  -0.8876319  -1.9628924\n",
            " -0.99023134  0.11339751 -2.190986   -1.8120244  -1.1920031  -0.92132854\n",
            " -0.85598063]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.289084   -0.6686407  -0.720729   -1.2102629  -0.8865433  -1.4494877\n",
            " -1.0234865  -0.08213837 -0.7582569  -0.61793715 -0.8139548  -1.4217691\n",
            " -0.8764145   0.07453083 -1.7512333  -1.6643617  -0.943634   -0.49627167\n",
            " -0.43821895]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.1420267  -0.7558815  -0.7214771  -1.3463467  -1.2035875  -1.3249891\n",
            " -1.1089883  -0.4688792  -0.83591074 -0.44740957 -0.7006222  -1.5327075\n",
            " -0.79604775  0.20886108 -1.6380496  -1.4645008  -0.8510166  -0.6666003\n",
            " -0.61778283]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.379396   -0.6757819  -0.8026025  -1.3702284  -1.1975539  -1.4021572\n",
            " -1.1901137  -0.19887047 -0.89946735 -0.3196417  -0.82503825 -1.8279839\n",
            " -0.8817503  -0.08704717 -1.8379284  -1.6129297  -0.87335086 -0.63451356\n",
            " -0.68958217]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0231491  -0.6488131  -0.6633755  -1.041864   -0.92889565 -1.2070397\n",
            " -0.98148096 -0.26952052 -0.69479024 -0.4370538  -0.77028567 -1.276889\n",
            " -0.75147504  0.03774806 -1.509254   -1.4450203  -0.8012644  -0.5445502\n",
            " -0.2753913 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.4098864  -0.90846413 -0.8981863  -1.6361533  -1.4771552  -1.6823919\n",
            " -1.3214926  -0.5956358  -0.978823   -0.5524386  -0.81744486 -1.8303399\n",
            " -0.9449983   0.08619729 -2.002819   -1.6993066  -1.0799255  -0.83105\n",
            " -0.757709  ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0416801  -0.6074687  -0.58628607 -0.99790454 -0.7392665  -1.216604\n",
            " -0.87585884  0.08856186 -0.68163246 -0.46665418 -0.6653531  -1.2491211\n",
            " -0.7280179   0.16198489 -1.534089   -1.5319747  -0.77286434 -0.40614387\n",
            " -0.33271357]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 29 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0551174  -0.7580439  -0.6712527  -1.1949482  -1.2388101  -1.2289934\n",
            " -1.0446748  -0.39905217 -0.83225876 -0.37977585 -0.66585684 -1.4262584\n",
            " -0.6755112   0.1203018  -1.6268361  -1.3988478  -0.76466453 -0.6109494\n",
            " -0.6485949 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 30 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.3171628  -0.71689844 -0.8015316  -1.3316927  -1.1523831  -1.4370301\n",
            " -1.1300395  -0.31551543 -0.92495567 -0.40439945 -0.8003974  -1.7391701\n",
            " -0.90425205 -0.04293162 -1.8289434  -1.8068795  -0.77423954 -0.3280502\n",
            " -0.54478663]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 31 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0479741  -0.6386181  -0.6557512  -1.1562109  -0.95129114 -1.2226447\n",
            " -0.98399246 -0.53815967 -0.7242878  -0.5794042  -0.7229147  -1.3047712\n",
            " -0.7982407  -0.02663994 -1.4487445  -1.2933252  -0.8291561  -0.60540473\n",
            " -0.4300535 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 32 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.1958431  -0.6503807  -0.6889742  -1.2764742  -1.0800234  -1.2633379\n",
            " -1.1138964  -0.37163556 -0.7498139  -0.28326046 -0.72133106 -1.5489824\n",
            " -0.8027445  -0.13129275 -1.5575341  -1.3819914  -0.7563129  -0.59763604\n",
            " -0.4690407 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 33 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8978848  -0.56259096 -0.61897534 -0.88063824 -0.70984745 -1.0359907\n",
            " -0.79388803 -0.05322031 -0.56332225 -0.45606494 -0.6369416  -1.1907443\n",
            " -0.6698067   0.01624318 -1.367212   -1.344988   -0.6416884  -0.39357567\n",
            " -0.3703226 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 34 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0060986  -0.754609   -0.7604176  -1.2664865  -1.2358861  -1.2876846\n",
            " -1.0985796  -0.84924746 -0.84767723 -0.47933757 -0.7622384  -1.4742953\n",
            " -0.7406649  -0.05135265 -1.6617361  -1.5074131  -0.79823893 -0.5887475\n",
            " -0.6585188 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 35 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.3645686  -0.7422615  -0.76543707 -1.3558751  -1.1427575  -1.5251801\n",
            " -1.1132158  -0.58260435 -0.92618245 -0.53893507 -0.76131463 -1.5738163\n",
            " -0.8817676   0.06999978 -1.8213167  -1.6921785  -0.88512737 -0.5485386\n",
            " -0.5992017 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.95836323 -0.5838142  -0.6305282  -0.9394765  -0.8301335  -1.055357\n",
            " -0.8401866  -0.17911685 -0.6458899  -0.35570386 -0.5734594  -1.1785529\n",
            " -0.70556927 -0.01398745 -1.3722326  -1.1343743  -0.71939176 -0.5092566\n",
            " -0.28364348]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 37 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.2214143  -0.6271678  -0.73457557 -1.1933986  -1.1527344  -1.1981685\n",
            " -1.0819236  -0.4318748  -0.77887475 -0.18396787 -0.68815607 -1.6179155\n",
            " -0.81383497 -0.34725365 -1.669347   -1.3730114  -0.79110545 -0.58202344\n",
            " -0.49270743]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.1291196  -0.55110073 -0.66974086 -1.0475063  -0.71826595 -1.2046797\n",
            " -0.870595   -0.41519827 -0.60656226 -0.5546928  -0.8358264  -1.2573427\n",
            " -0.71347356 -0.05138858 -1.5237726  -1.5485419  -0.7195944  -0.42968136\n",
            " -0.34291917]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.9250836  -0.64559793 -0.67961496 -1.0899298  -1.0126333  -1.1052428\n",
            " -0.9845628  -0.5613029  -0.7136276  -0.3611467  -0.7210374  -1.4399322\n",
            " -0.68575376 -0.23099948 -1.5617517  -1.4224743  -0.735933   -0.55880225\n",
            " -0.5628602 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.896043   -0.5588194  -0.5549391  -0.9059017  -0.66584486 -1.0122565\n",
            " -0.80169696 -0.03898535 -0.5866506  -0.41499722 -0.7569017  -1.1218867\n",
            " -0.6033952  -0.00575265 -1.3059103  -1.3194885  -0.6123283  -0.40654224\n",
            " -0.35407582]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.75327986 -0.49039984 -0.51566994 -0.8019068  -0.7251492  -0.82214415\n",
            " -0.6513352  -0.30214128 -0.45458496 -0.30257368 -0.51196575 -1.0111808\n",
            " -0.56847966 -0.0641859  -1.1341124  -1.0204971  -0.5320651  -0.4184031\n",
            " -0.3100417 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.064148   -0.6113985  -0.6310825  -1.1037729  -0.83825827 -1.1673331\n",
            " -0.9297189  -0.48511058 -0.687996   -0.59619874 -0.7831174  -1.3320729\n",
            " -0.62530637 -0.10744174 -1.5054055  -1.546164   -0.65435106 -0.48516452\n",
            " -0.53279275]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.7916701  -0.51512086 -0.5477294  -0.9043889  -0.5541355  -1.0260285\n",
            " -0.7927387  -0.18647939 -0.516747   -0.5415036  -0.7214044  -1.1273144\n",
            " -0.6125642  -0.1250655  -1.3240316  -1.346737   -0.6336901  -0.47924113\n",
            " -0.24595359]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.94947374 -0.5198299  -0.6047076  -1.0907904  -0.8449915  -1.0320593\n",
            " -0.8874633  -0.6552975  -0.6197952  -0.39078507 -0.62975544 -1.2769312\n",
            " -0.66769207 -0.13141838 -1.1853666  -1.2885127  -0.7410794  -0.47524416\n",
            " -0.4745758 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.9687136  -0.5607032  -0.5389082  -0.97247696 -0.5955496  -1.0755833\n",
            " -0.84390336 -0.04285358 -0.60063905 -0.5102885  -0.67197824 -1.2319285\n",
            " -0.60079443 -0.02608391 -1.3749577  -1.4755856  -0.6576455  -0.44001547\n",
            " -0.29553553]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 46 reward=-1 new_state=[0 1 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.9432235  -0.5870022  -0.5940478  -1.1062381  -0.8920454  -1.0220602\n",
            " -0.84875876 -0.9354796  -0.60125047 -0.48352656 -0.73649687 -1.2781571\n",
            " -0.68566066 -0.0999928  -1.2388515  -1.2543229  -0.6404444  -0.5503752\n",
            " -0.44887847]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 47 reward=-1 new_state=[0 1 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.89822197 -0.5735196  -0.54319245 -1.0421164  -0.7253285  -1.0262322\n",
            " -0.9078803  -0.6231437  -0.59396946 -0.4470403  -0.6372725  -1.2242705\n",
            " -0.6506864  -0.13707325 -1.1869392  -1.2290175  -0.7182682  -0.48591447\n",
            " -0.41954866]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 1 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.69053113 -0.4016255  -0.40198022 -0.7463516  -0.53246856 -0.7966228\n",
            " -0.57416147 -0.3191463  -0.47459966 -0.32371405 -0.58547056 -0.75072056\n",
            " -0.5271195  -0.20501988 -1.0373921  -0.8916987  -0.5147326  -0.4230805\n",
            " -0.23737824]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 1 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.1403792  -0.6736199  -0.7377982  -1.1016716  -1.0049102  -1.2615292\n",
            " -0.93383586 -0.99398994 -0.745007   -0.5541567  -0.7809778  -1.4514219\n",
            " -0.7970631  -0.21614678 -1.6330105  -1.5082101  -0.80168694 -0.5105779\n",
            " -0.5535266 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 1 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.9974991  -0.67848206 -0.675462   -1.0051092  -0.96178097 -1.0731292\n",
            " -0.92190117 -0.8057041  -0.66695476 -0.43354326 -0.7511414  -1.2600037\n",
            " -0.69057536 -0.23457873 -1.4219719  -1.3882586  -0.61850417 -0.43173224\n",
            " -0.49650222]\n",
            "Epsilon reduced to 0.033554432000000016\n",
            "Total reward: -35\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foCiQDV_Uxb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7bda2315-7aff-4105-f432-bdabf395cc76"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=25, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=16, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_26(env), max_epsilon=0.1, epsilon_decay=0.9)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_15 (Reshape)         (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_15 (Conv1D)           (None, 187, 3)            12        \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 561)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 19)                10678     \n",
            "=================================================================\n",
            "Total params: 10,690\n",
            "Trainable params: 10,690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |██████----------------------------------------------------------------------------------------------| 6.2% \r\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 1 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5516665   0.5373595   0.20347917  0.42419386 -0.19720158 -0.13524523\n",
            " -0.00278012 -0.36333653 -0.60453093  0.14835647  0.11699706 -0.2864721\n",
            "  0.5524902  -0.2513274   0.34475592  0.09362406  0.11464518  0.49420103\n",
            " -0.11642995]\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 2 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.02173577  0.11967862 -0.37374356 -0.20124851  0.3000963   0.02914625\n",
            " -0.03740277 -0.02097949 -0.37030506 -0.12255792  0.2365051  -0.0999438\n",
            "  0.31476608  0.00391993  0.20020498  0.05002028 -0.15172613 -0.0674182\n",
            "  0.17896669]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 3 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4445761   0.04165869 -0.12317467 -0.09019965  0.19140457 -0.24536107\n",
            "  0.13983406 -0.17530946 -0.6383528  -0.32947722  0.11428023 -0.517737\n",
            "  0.12283733 -0.4545964   0.41872862 -0.00681784 -0.16419435  0.39629254\n",
            "  0.23800865]\n",
            "\n",
            "Taking action 10 from 14\n",
            "\n",
            "Step 4 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10205355 -0.16503909  0.19135755  0.30663317  0.07085017 -0.13851258\n",
            " -0.03843318 -0.48413873 -0.24568413  0.37435836  0.38831562 -0.55964494\n",
            " -0.15256344 -0.07640825  0.16774608  0.35604146 -0.2719082   0.36931133\n",
            "  0.2608277 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 5 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.30409104 -0.25977805 -0.18104516 -0.008105   -0.00945612 -0.1435983\n",
            "  0.01657284 -0.14066799 -0.04598871 -0.1298665   0.15921807  0.03248759\n",
            " -0.2374477  -0.00907908 -0.3833234   0.04714864 -0.27903244 -0.09201009\n",
            "  0.12319459]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 6 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.04201393 -0.5112287   0.00808532 -0.03488134 -0.36133885 -0.27232057\n",
            "  0.07245575 -0.3851403  -0.08378436  0.06335878  0.17383227 -0.72506595\n",
            " -0.6051236   0.08086279  0.06621981 -0.03629207 -0.03948209  0.25179428\n",
            "  0.19408551]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 7 reward=-1 new_state=[1 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.13100679 -0.43636274  0.16704029  0.01594098 -0.27267814 -0.38504082\n",
            "  0.43582094 -0.3856962  -0.1912949  -0.2105363  -0.05794632 -1.0300105\n",
            " -0.9553468  -0.20840026 -0.83442     0.11291523 -0.08433485  0.23367622\n",
            "  0.17400374]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 8 reward=-1 new_state=[1 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.30057243 -0.4823539  -0.4694977  -0.27369758 -0.6046993  -0.11037336\n",
            " -0.02430808 -0.03244694 -0.42786646 -0.04178006 -0.48585546 -0.6411001\n",
            " -0.611608    0.09966848 -0.08094525 -0.25710896  0.06328936  0.19954437\n",
            " -0.39967772]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.1930015  -1.0672414   0.06060893  0.11335627 -0.55034035 -0.10886593\n",
            "  0.10900372 -0.49969557 -0.0613706  -0.04356488 -0.06192577 -1.3760512\n",
            " -0.90727264 -0.07946014 -0.92813194 -0.1419228   0.35974345  0.11377214\n",
            "  0.3202579 ]\n",
            "\n",
            "Taking action 12 from 16\n",
            "\n",
            "Step 10 reward=-1 new_state=[1 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.06379753 -0.66393054 -0.06472276  0.0513128  -0.5826615   0.02370489\n",
            " -0.30042893 -0.14988501  0.05274472  0.17954363 -0.6961814  -1.0228561\n",
            " -0.888212    0.02983277 -0.67047757 -0.5007699  -0.03018751  0.04717828\n",
            " -0.03049203]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 11 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.19676495 -0.83033127 -0.1538281   0.02906652 -0.7851346  -0.25094292\n",
            " -0.587915   -0.30032918  0.18549246 -0.10387313 -0.6740931  -1.7317616\n",
            " -1.0587215   0.06049105 -1.1046733  -0.7244322  -0.7513985   0.74161613\n",
            " -0.11760213]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 12 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.08649781 -0.825376   -0.08002599 -0.06862679 -0.6155166   0.00817153\n",
            " -0.3142531  -0.31965613 -0.08887675  0.3631621  -0.8016059  -1.1998243\n",
            " -1.0887003   0.04452116 -0.6690027  -0.6526627  -0.49744442  0.3990684\n",
            " -0.06057862]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 13 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.56083316 -1.2528691  -0.0791632  -0.17169069 -0.86470807  0.0735762\n",
            " -0.5791726  -0.38033578 -0.25265867  0.30621856 -1.0177408  -1.3303288\n",
            " -1.192462   -0.3426786  -1.0577629  -0.4524739  -0.3182662   0.67719996\n",
            " -0.10923277]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 14 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.00977649 -0.9767061   0.03695985  0.23435842 -0.66839206 -0.09687053\n",
            " -0.72208416 -0.42616153  0.00906197  0.5019425  -0.6448924  -1.5514734\n",
            " -1.1813791  -0.09706907 -0.8288663  -0.7934519  -0.8494114   0.8315753\n",
            "  0.05036172]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 15 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.22929974 -0.7401372  -0.21414898 -0.00997173 -0.86557615  0.01335584\n",
            " -0.64785445 -0.19222444 -0.1972765   0.43895483 -0.9655841  -0.8984394\n",
            " -0.9900883   0.06473341 -0.95732224 -0.7161694  -0.6843877   0.93960226\n",
            "  0.27948546]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 16 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.03752635 -1.2972659  -0.06963421 -0.03726289 -1.0776799   0.01527215\n",
            " -0.8026529  -0.24990058 -0.04893751  0.6896238  -1.2649912  -1.3309743\n",
            " -1.3328674   0.12888503 -1.2337182  -0.98505235 -1.054137    1.5476977\n",
            "  0.11006697]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 17 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11697178 -1.7266085   0.18258527  0.4342079  -1.0536835   0.0666741\n",
            " -1.2444997  -0.6476538  -0.10662778  0.9395553  -0.93627715 -2.0710394\n",
            " -1.527675   -0.06852485 -1.610445   -1.1294595  -1.3162898   1.9499288\n",
            "  0.09581247]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 18 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18359685 -0.8836017  -0.11499435  0.0599787  -1.2377918   0.14922382\n",
            " -0.9238488  -0.33029962 -0.26454252  0.9443572  -0.9212834  -1.7323079\n",
            " -1.3081224  -0.040261   -1.1245499  -1.1590173  -1.1801623   2.2410512\n",
            " -0.11398895]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 19 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.06276608 -1.0719203  -0.1378043   0.13305312 -1.2492908  -0.01820909\n",
            " -1.2910998  -0.45978624 -0.24194102  1.2959461  -1.3955213  -2.1698968\n",
            " -1.6631771   0.17195338 -1.2889676  -1.6809629  -1.5374647   3.1982722\n",
            " -0.01597149]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 20 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.12805758 -0.8285747  -0.21704215 -0.23782218 -1.4407029   0.01149717\n",
            " -1.5089663  -0.07062397 -0.4605947   1.1051129  -1.5513164  -1.8086604\n",
            " -1.6126955  -0.08153521 -1.4528736  -1.4186935  -1.301101    3.4080513\n",
            "  0.03814199]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 21 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02343504 -1.7079431   0.07553084  0.1005441  -2.0209913  -0.03879116\n",
            " -1.7746662  -0.53883797 -0.37603354  1.7776531  -1.7619364  -2.8255558\n",
            " -2.5940516   0.05354852 -1.9856555  -2.1784468  -2.2623928   5.398624\n",
            " -0.25602615]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 22 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.13801809 -1.4600754  -0.06151773  0.06781377 -1.7120631   0.11227529\n",
            " -1.7042885  -0.44923544 -0.29113913  1.6448749  -1.4915881  -2.778418\n",
            " -2.0008981   0.16469866 -1.5962477  -2.2437153  -1.9456226   5.199615\n",
            " -0.17414431]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 23 reward=1 new_state=[1 0 0 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.19376281 -0.9611999  -0.3101713   0.00655396 -1.6178923   0.15115707\n",
            " -1.2482725  -0.17327034 -0.4828717   1.2908058  -1.7874366  -1.7598721\n",
            " -1.60626     0.16269502 -1.29686    -1.44832    -1.4339684   4.1658115\n",
            " -0.17172435]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16 from 8\n",
            "\n",
            "Step 24 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [ 1.8637808e-01 -1.2278932e+00 -1.2089737e-01  4.7342502e-02\n",
            " -1.5701624e+00 -2.4047381e-01 -1.9053118e+00 -4.5490167e-01\n",
            " -2.4358729e-01  1.7355763e+00 -1.6723001e+00 -2.6008189e+00\n",
            " -1.9942389e+00  4.1142665e-03 -1.5969335e+00 -2.2333236e+00\n",
            " -2.2188730e+00  5.2322907e+00  2.6432914e-01]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[1 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.28025243e-01 -1.49804091e+00  1.24796525e-01  2.58097239e-03\n",
            " -1.65327203e+00  1.60676867e-01 -1.59211814e+00 -5.17812610e-01\n",
            " -3.02139461e-01  1.45021319e+00 -1.63800097e+00 -2.28381014e+00\n",
            " -2.11542082e+00 -2.06646323e-01 -1.83463943e+00 -1.75427878e+00\n",
            " -1.85823393e+00  4.77086210e+00  6.13307729e-02]\n",
            "Epsilon reduced to 0.09000000000000001\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.52230597 -1.3076696  -0.14792664 -0.1570453  -1.9542019  -0.17933317\n",
            " -2.6408243  -0.18191671  0.20576842  1.9515069  -2.1387222  -3.2638266\n",
            " -2.5273185  -0.34576005 -2.6786253  -2.4753003  -3.0515013   6.388924\n",
            "  0.15230475]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6337808  -1.4305471  -0.09023714 -0.18354584 -2.0909085   0.10455404\n",
            " -2.2806382  -0.06994651  0.3456353   1.8884689  -2.2061057  -2.572731\n",
            " -2.3584266  -0.35473254 -2.4041307  -2.230311   -1.9825934   5.421099\n",
            "  0.09189684]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.28059998 -1.101825    0.03361785  0.16489452 -1.4544522  -0.08176924\n",
            " -1.8457177  -0.19881974  0.50081855  1.7709247  -1.3596418  -2.9994242\n",
            " -1.9275994  -0.08615823 -2.159953   -2.2966912  -2.361367    4.4056525\n",
            "  0.0361913 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.46719438 -1.3008419  -0.06461669 -0.07864571 -1.6541464   0.15907533\n",
            " -1.8429828  -0.1441657   0.6902423   1.7170547  -1.9822673  -2.1895754\n",
            " -2.0075014  -0.24583559 -2.024883   -1.9966037  -1.8130795   3.807882\n",
            "  0.02280988]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.14565359 -1.0278121  -0.09847149 -0.02180347 -1.5798409  -0.10510047\n",
            " -1.9552969   0.00816286  0.8168851   1.8148457  -1.6316003  -2.9429603\n",
            " -1.7391456  -0.01363013 -2.007276   -2.0254269  -2.2662206   3.6177962\n",
            " -0.04869053]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.52127063 -1.2879795  -0.04002722 -0.06535792 -1.5023384   0.17229773\n",
            " -1.6770499  -0.19204429  0.80900383  1.6330861  -1.8445768  -2.0513518\n",
            " -1.8445843  -0.20561454 -1.9351687  -1.90018    -1.724392    2.8363194\n",
            " -0.03443143]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.11536204 -0.74852824 -0.16185705 -0.10855816 -1.3651378  -0.18646796\n",
            " -1.672014    0.14433615  0.77377933  1.5446569  -1.3141686  -2.3460655\n",
            " -1.6259881  -0.05747554 -1.6941432  -1.7368177  -1.5716214   2.4933934\n",
            "  0.09163587]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.06872481 -0.8183438   0.22599128 -0.19390646 -1.254575    0.19613418\n",
            " -1.3519619   0.0830975   0.90472126  1.7606963  -1.2837058  -1.9024302\n",
            " -1.6085017  -0.04922141 -1.435144   -1.6102328  -1.4339125   1.8945031\n",
            " -0.22727376]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.1591576  -1.0754279  -0.01582667 -0.15126416 -1.6300863   0.5468416\n",
            " -1.3964999  -0.09219685  1.0382962   1.5192438  -1.250943   -1.8184509\n",
            " -1.5363506  -0.5633653  -1.3893956  -1.7347279  -1.7091414   1.3458517\n",
            " -0.03316413]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.01188081 -0.727992   -0.22732002 -0.2423549  -1.3633997   0.46976393\n",
            " -1.3381151  -0.21673702  0.95225257  1.3914037  -1.3340144  -1.5903484\n",
            " -1.4569938  -0.15046227 -1.2224599  -1.2897696  -1.6314108   1.4516186\n",
            "  0.05970061]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.05242918 -0.6905625   0.13153577 -0.18484607 -1.1280935   0.5545685\n",
            " -1.345641   -0.28785518  1.010249    0.9249741  -1.0058134  -1.5140446\n",
            " -1.3623729  -0.16638067 -1.1990819  -1.3429918  -1.54678     1.0344791\n",
            " -0.03607564]\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 12 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.12137156 -0.85685056  0.01135883  0.17127785 -1.1759663   0.769229\n",
            " -1.2222704  -0.16747224  0.9509649   0.83855385 -1.1655873  -1.7176108\n",
            " -1.1831628  -0.28519267 -1.1574173  -1.4281042  -1.2711275   0.83461654\n",
            " -0.0288349 ]\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.06665661 -1.0334289  -0.1736829  -0.29627243 -1.1692884   0.836516\n",
            " -1.2682191  -0.084685    0.728712    0.5812301  -1.237845   -1.4675795\n",
            " -1.1528455  -0.10691375 -1.1063443  -1.2725439  -1.3275614   0.46916118\n",
            " -0.02434297]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.03143704 -0.6749818  -0.12229098 -0.1432157  -1.159352    0.7278182\n",
            " -1.3452096  -0.01109069  0.5873431   0.49264336 -1.2658908  -1.5114287\n",
            " -1.2042971  -0.22280838 -1.366698   -1.1981674  -1.1352948   0.92238754\n",
            " -0.19495137]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.37293583 -0.93042386 -0.16302195 -0.12480119 -1.127009    1.169759\n",
            " -1.3392016   0.01313033  0.7759175   0.09060504 -1.618109   -1.6786643\n",
            " -1.480227   -0.27648565 -1.4370549  -1.5226164  -1.2983972   0.22841501\n",
            " -0.16425896]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.01244701 -0.63607883 -0.1507294  -0.12723322 -1.1700389   0.6309612\n",
            " -1.1844053  -0.11078091  0.5637634   0.15030044 -0.97507465 -1.6148546\n",
            " -1.2189139   0.01251004 -1.0796345  -1.2497592  -1.2273356   0.19342558\n",
            " -0.16232693]\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.71117589e-01 -9.60154772e-01  4.56685796e-02 -1.14738859e-01\n",
            " -7.82617211e-01  2.90017575e-01 -1.30417788e+00  2.45486408e-01\n",
            "  3.60786654e-02  1.19675994e-01 -1.12954831e+00 -1.62115836e+00\n",
            " -1.07268965e+00 -1.94772720e-01 -1.35452950e+00 -1.48922896e+00\n",
            " -1.32381678e+00  2.22507864e-04  1.28006473e-01]\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 18 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.02764744 -0.874328   -0.19556938 -0.20696557 -1.060243    0.02881331\n",
            " -1.5653483  -0.08386591  0.4434356   0.07824185 -1.371083   -1.376049\n",
            " -1.4240397  -0.01083422 -1.2067902  -1.3293177  -1.4389129   0.07398258\n",
            " -0.08460428]\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 19 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.17395735 -0.3992704  -0.06461582  0.07198097 -0.7168174   0.08470098\n",
            " -1.405118   -0.10717209 -0.12400256 -0.23737389 -0.8978836  -1.2178444\n",
            " -1.203513   -0.24070811 -1.2044615  -1.0713658  -1.2154639  -0.46744072\n",
            " -0.20303059]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 20 reward=-3 new_state=[0 1 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.07193596 -0.6272765   0.08558734 -0.14167464 -0.8321652   0.00709817\n",
            " -1.0565637  -0.3292613  -0.06746004  0.10851157 -0.81610495 -1.4153376\n",
            " -1.1335288  -0.12222032 -0.9580214  -1.1431744  -0.9375301   0.1547464\n",
            " -0.26283985]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 21 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.00394446 -0.4086552   0.13174705 -0.280725   -0.7094703  -0.34601787\n",
            " -0.95567167 -0.22863914  0.05093419  0.01567748 -0.7715692  -0.7863485\n",
            " -0.7497236  -0.13564956 -0.78273165 -0.70565265 -0.7392276   0.22787999\n",
            " -0.10473756]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 22 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.06849492 -0.48069876 -0.03985851 -0.49334857 -0.7660986  -0.22654557\n",
            " -1.1365838  -0.486752   -0.2502893  -0.0315792  -0.79132086 -1.3975265\n",
            " -0.97763735 -0.09454063 -1.0903224  -1.0656358  -0.99656516 -0.11215596\n",
            " -0.24217567]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 23 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.05130789 -0.44806558 -0.49920216 -0.6977132  -0.5380904  -0.3802392\n",
            " -1.0048528  -0.6804543  -0.60770416 -0.46216476 -0.5962326  -1.426885\n",
            " -1.0727514  -0.16975315 -0.9562752  -1.0013885  -1.0146422  -1.0029128\n",
            " -0.17073026]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 24 reward=-2 new_state=[0 0 1 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18454975 -0.5396747  -0.46071318 -0.72110367 -0.5933571  -0.15226716\n",
            " -0.8411954  -0.5041847  -0.36742768 -0.46824002 -0.67278624 -0.77403647\n",
            " -0.7800798  -0.06128133 -0.5526084  -0.85622954 -0.6705401  -0.48682103\n",
            " -0.11753993]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.140548   -0.5148966  -0.5184667  -0.5031606  -0.74450564 -0.2943744\n",
            " -0.6772738  -0.5554608  -0.31238398 -0.01222589 -0.59597707 -0.7552671\n",
            " -0.9138566  -0.07653169 -0.6806148  -0.6303245  -0.7498101   0.04284053\n",
            "  0.00384943]\n",
            "Epsilon reduced to 0.08100000000000002\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.42147687 -0.43875945 -0.41524407 -0.40565726 -0.93096256 -0.19046494\n",
            " -0.64433175 -0.6386925  -0.14847939  0.0154839  -0.47472996 -0.86636734\n",
            " -0.8894662   0.00402626 -0.5823422  -0.9311845  -0.9221595   0.10946815\n",
            "  0.0317209 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.76505953 -0.856566   -0.7939467  -0.7482949  -0.99549514 -0.43369487\n",
            " -0.8489785  -0.9255224  -0.36200944 -0.29965454 -0.5509313  -1.150235\n",
            " -1.0264618  -0.22816879 -0.65397614 -1.1174088  -1.2305826  -0.0895241\n",
            " -0.00602016]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.87555236 -0.46204984 -0.9482325  -0.8935416  -0.8036991  -0.2536602\n",
            " -0.89971596 -0.869945   -0.44950318 -0.36563247 -0.9717869  -1.1588593\n",
            " -0.63405484 -0.49661323 -0.6582011  -1.0044757  -0.8071609  -0.31497836\n",
            " -0.28168932]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7880176  -0.47731876 -0.7416115  -0.86634773 -0.55219084 -0.31776237\n",
            " -0.82421374 -0.8187067  -0.28276032 -0.35387954 -0.55075073 -0.8448187\n",
            " -0.6366723  -0.46294144 -0.5737404  -0.58304125 -0.541206   -0.7530884\n",
            " -0.19884104]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8076931  -0.4235621  -0.88957655 -0.9654816  -0.43949592 -0.35585827\n",
            " -0.4817658  -0.706444   -0.5006988  -0.53641355 -0.374386   -0.62798065\n",
            " -0.5625269  -0.63145506 -0.4557359  -0.43206757 -0.43837017 -0.77794325\n",
            " -0.39486423]\n",
            "\n",
            "Taking action 14 from 10\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0625461  -0.33875564 -0.7911515  -0.7469286  -0.57902175 -0.56038886\n",
            " -0.6915928  -0.73780257 -0.4338342  -0.6860498  -0.60794693 -0.8196965\n",
            " -0.62363577 -0.6789037  -0.56701046 -0.8450047  -0.65436685 -0.80293083\n",
            " -0.49252024]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 7 reward=-2 new_state=[1 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.92592525 -0.64427745 -0.69899416 -0.7732993  -0.40479532 -0.48692578\n",
            " -0.6275319  -0.69146055 -0.2711119  -0.12182991 -0.47753355 -0.6654592\n",
            " -0.48758143 -0.4950939  -0.7027756  -0.75608563 -0.6647877  -0.2852619\n",
            " -0.52485234]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 8 reward=-1 new_state=[1 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.66814834 -0.40958628 -0.5286047  -0.5198434  -0.42574182 -0.39633748\n",
            " -0.41466478 -0.47051814 -0.1639527  -0.22116464 -0.43068218 -0.60623276\n",
            " -0.3977816  -0.3906365  -0.5029397  -0.57397133 -0.5869889  -0.14047025\n",
            " -0.44034156]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 9 reward=-1 new_state=[1 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8667329  -0.63129556 -0.59256226 -0.82489204 -0.34371227 -0.3860315\n",
            " -0.555131   -0.75994533 -0.26040038 -0.38390458 -0.7089296  -0.48198065\n",
            " -0.44755805 -0.6584284  -0.68021566 -0.5993123  -0.6666492  -0.15294607\n",
            " -0.654771  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 10 reward=-1 new_state=[1 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.8492263  -0.64685845 -0.92894405 -0.8803738  -0.37684035 -0.55861807\n",
            " -0.3716998  -0.7908513  -0.4116182  -0.40990865 -0.7658036  -0.6450682\n",
            " -0.54981464 -0.73972046 -0.5889721  -0.49274924 -0.6118817  -0.33147028\n",
            " -0.77679604]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 11 reward=-1 new_state=[1 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.1192263  -0.7565446  -0.7665168  -0.67407477 -0.5698856  -0.84463775\n",
            " -0.56247246 -0.9327758  -0.4485542  -0.61104554 -0.5785982  -0.8604446\n",
            " -0.6911643  -0.71295595 -0.55803376 -0.6746373  -0.8427178  -0.60788\n",
            " -0.79852045]\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 12 reward=-2 new_state=[1 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1333128  -0.73585737 -0.8270072  -0.8655487  -0.5828317  -0.65875566\n",
            " -0.5976803  -0.7663652  -0.3594259  -0.38954067 -0.7919423  -0.5687258\n",
            " -0.5500316  -0.7417125  -0.71712345 -0.66607904 -0.6808915  -0.20451158\n",
            " -0.9109975 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 13 reward=-2 new_state=[1 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1579951  -0.77682173 -0.73581827 -0.97047013 -0.41695124 -0.7792837\n",
            " -0.59550625 -0.8888152  -0.5062065  -0.59045374 -0.7796959  -0.6587112\n",
            " -0.56856287 -0.8396865  -0.6881846  -0.7577126  -0.6041361  -0.2916544\n",
            " -0.95599455]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 14 reward=-2 new_state=[1 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.347381   -1.0152109  -1.0758034  -1.2672888  -0.72138095 -0.9322937\n",
            " -0.8645611  -0.8685707  -0.5376394  -0.57223827 -1.1874882  -0.8802294\n",
            " -0.62229264 -0.99334216 -0.9618265  -0.802406   -0.82680494 -0.32107064\n",
            " -1.3282493 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 15 reward=-2 new_state=[1 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.4173522  -0.82972544 -1.0630906  -0.9114812  -0.6013471  -0.91351724\n",
            " -0.6877239  -0.99378735 -0.824699   -0.87146133 -0.91862166 -0.8173101\n",
            " -0.42756322 -0.9174315  -0.69762623 -0.6175888  -0.5433938  -0.72359294\n",
            " -0.9994926 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 16 reward=-2 new_state=[1 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.4191034  -1.0816678  -1.0090299  -1.2135586  -0.51504683 -1.0410597\n",
            " -0.73994863 -1.0516303  -0.7148806  -0.817238   -1.2113565  -0.79325175\n",
            " -0.67755884 -1.1857067  -0.7319699  -0.81908023 -0.68918324 -0.7424163\n",
            " -1.2575213 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 17 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7312365  -1.3461701  -1.5024987  -1.453481   -0.9873662  -0.9622359\n",
            " -0.95330274 -1.0590646  -0.953896   -0.95165133 -1.4486558  -1.0437597\n",
            " -1.2381097  -1.2268527  -1.1793183  -0.80845857 -1.013618   -1.1725309\n",
            " -1.8728471 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 18 reward=-2 new_state=[1 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.565917   -1.0072619  -1.2549877  -1.1324048  -0.7493719  -0.7873706\n",
            " -0.71734595 -0.8213608  -0.751841   -0.83926654 -1.2162052  -0.8430826\n",
            " -0.9641807  -0.9547112  -0.63862056 -0.70825815 -0.739626   -1.0238913\n",
            " -1.3487706 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 10\n",
            "\n",
            "Step 19 reward=-2 new_state=[1 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8787029  -1.2443125  -1.5125246  -1.3879129  -0.9735291  -0.98405963\n",
            " -0.8320992  -1.0002532  -0.94966716 -0.7373155  -1.7262396  -0.9660437\n",
            " -1.0955998  -1.2804557  -0.8900012  -0.8371065  -0.7211113  -0.9301915\n",
            " -1.8828912 ]\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 20 reward=-3 new_state=[1 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.5905924  -1.0532621  -1.4028909  -1.1649468  -0.94969887 -0.8103661\n",
            " -0.94779444 -1.0250541  -0.95182717 -0.6963217  -1.4193952  -0.88590354\n",
            " -1.1095619  -1.0866874  -1.0186112  -0.8974763  -0.6796017  -0.8754801\n",
            " -1.7526615 ]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 21 reward=-2 new_state=[1 0 0 0 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.501211   -1.0576048  -1.1673542  -1.0554528  -1.10956    -0.9072126\n",
            " -0.8437143  -0.9377261  -0.6594085  -0.42986292 -1.3045106  -0.9341059\n",
            " -1.2288296  -0.98104924 -1.0821235  -1.0765407  -0.9400151  -0.484466\n",
            " -1.4725785 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 22 reward=-1 new_state=[1 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.3271474  -1.2064545  -1.178847   -0.81498754 -1.120165   -0.78909236\n",
            " -0.7491877  -1.0138314  -0.21902215 -0.7636181  -1.4196173  -0.8097166\n",
            " -1.2715719  -0.9766055  -0.89554775 -1.0798806  -1.0906692  -0.67220193\n",
            " -1.3253071 ]\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 23 reward=-2 new_state=[1 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.0913832 -1.4510422 -1.7890692 -1.5668058 -1.4333647 -1.1069984\n",
            " -0.8840837 -1.4987001 -0.859694  -1.5445117 -2.26632   -1.0366721\n",
            " -1.6652575 -1.5632704 -1.0175675 -1.4340212 -1.3477266 -1.7408386\n",
            " -2.172819 ]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 24 reward=-2 new_state=[1 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.2722259  -1.5164163  -1.6054561  -1.3362582  -1.5321808  -0.84411216\n",
            " -1.0474268  -1.2248363  -0.7094506  -1.3954291  -2.0954037  -1.0854048\n",
            " -1.7039939  -1.34662    -1.3492862  -1.6556225  -1.5004401  -1.0368232\n",
            " -2.124136  ]\n",
            "\n",
            "Taking action 12 from 8\n",
            "\n",
            "Step 25 reward=-2 new_state=[1 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.7845721 -1.8234367 -1.8809391 -1.8273877 -2.1124341 -1.6036239\n",
            " -1.6009753 -1.3828142 -1.3847052 -1.5340471 -2.9494095 -1.2437385\n",
            " -2.1295223 -1.906129  -1.4926159 -2.2419403 -2.2257583 -1.5221568\n",
            " -2.7979558]\n",
            "Epsilon reduced to 0.07290000000000002\n",
            "\n",
            "Taking action 8 from 8\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.769208  -2.0006657 -1.8622559 -1.6885791 -2.2580922 -1.454415\n",
            " -1.5917584 -1.7529786 -1.5998334 -2.0867763 -2.7703469 -1.5845613\n",
            " -2.552564  -1.9224538 -2.018448  -2.7829309 -2.4472942 -1.8260201\n",
            " -2.7294228]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 2 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-3.1129944 -2.2942994 -2.423004  -2.6761897 -2.6548996 -2.3576941\n",
            " -1.9437066 -2.0336807 -2.4478428 -2.572772  -3.384354  -1.6725372\n",
            " -2.5771546 -2.461315  -2.1883287 -3.0704446 -2.6456778 -2.6597943\n",
            " -2.9325895]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.3824844 -1.9595492 -2.3738832 -2.3417377 -2.6321273 -2.0764346\n",
            " -2.036038  -2.2788215 -2.4538984 -2.56314   -3.346093  -1.7737914\n",
            " -2.6608047 -2.183405  -2.06316   -2.649731  -2.6294236 -1.9366667\n",
            " -3.3041565]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-3.0653172 -2.2200718 -2.0613348 -1.8427213 -2.9644284 -2.7227964\n",
            " -2.274732  -2.3121953 -2.112185  -2.340846  -3.4692419 -2.0636542\n",
            " -2.790422  -2.0816345 -2.189909  -2.9398878 -2.8471324 -2.498142\n",
            " -3.3701265]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 5 reward=-2 new_state=[0 1 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.9415493 -2.528538  -2.2447047 -2.0794396 -2.9384046 -2.8731592\n",
            " -1.9874421 -1.6143025 -2.586918  -2.1572998 -3.3847415 -2.3808067\n",
            " -2.9162464 -2.086592  -1.8348354 -3.4084926 -3.1989522 -2.4336298\n",
            " -2.8532329]\n",
            "\n",
            "Taking action 13 from 7\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 1 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.2358236 -2.2002714 -2.5807822 -2.2512631 -3.0531259 -2.6533065\n",
            " -2.4402552 -1.9806603 -2.2698326 -2.749813  -3.4047291 -2.6840556\n",
            " -2.8642194 -2.1672475 -1.9766425 -3.2431707 -3.0363119 -1.9311371\n",
            " -3.2529855]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 1 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.122874  -2.1297116 -1.5867558 -2.1874595 -2.8515456 -1.9922615\n",
            " -2.1808581 -1.9759738 -1.7164049 -2.4261837 -3.106688  -2.4246883\n",
            " -2.6497993 -2.0506752 -1.856129  -2.7978938 -2.6343243 -1.7132921\n",
            " -2.828121 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 8 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-2.8445234 -2.415939  -2.0816422 -2.363577  -2.917196  -2.6962554\n",
            " -2.0937424 -2.030314  -1.9039015 -2.7170475 -3.697687  -2.7256083\n",
            " -2.8957775 -2.1312687 -1.6075692 -2.8755832 -3.0450742 -1.9991641\n",
            " -2.8756413]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.2828898 -2.7934933 -2.0865302 -2.9399626 -3.3087544 -2.8644276\n",
            " -2.746344  -2.7838862 -2.1505563 -3.1185985 -3.9567661 -3.1208289\n",
            " -3.3322875 -2.2676702 -2.3638475 -3.501696  -3.4338486 -2.309168\n",
            " -3.1742463]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 10 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.629929  -2.255895  -1.4483832 -3.1717196 -3.0850224 -2.7920043\n",
            " -2.519765  -2.5843084 -2.2565894 -3.012291  -3.4329987 -3.1346684\n",
            " -2.9591286 -2.3624449 -2.2082465 -3.032468  -2.9439871 -2.2974966\n",
            " -3.1508574]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.80006   -2.4434328 -1.366602  -3.2372823 -3.272898  -3.2815669\n",
            " -2.565209  -3.029834  -2.5994463 -2.7827358 -3.672896  -3.5716465\n",
            " -3.2822387 -2.2385678 -2.0603733 -3.3816755 -3.4601076 -1.8690963\n",
            " -3.255253 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.9896777 -2.525447  -1.4325306 -3.6623342 -3.5552492 -3.3836691\n",
            " -3.0239153 -3.4579964 -3.2920344 -3.6663613 -4.462612  -3.9677873\n",
            " -3.4628642 -2.6689537 -1.8348647 -3.7048686 -3.5106192 -2.557929\n",
            " -3.9444668]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.6905162 -2.8388622 -0.8565767 -3.915927  -3.6075268 -3.5933175\n",
            " -3.1787333 -3.3033803 -3.0390553 -3.8107173 -4.3819184 -3.7250361\n",
            " -3.4909465 -2.9345949 -1.6703185 -3.9376886 -3.7445052 -2.8330846\n",
            " -3.7330303]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.5531685 -2.396769   0.3176649 -3.376041  -3.3895774 -3.0362046\n",
            " -2.9022272 -3.166625  -2.5641322 -3.3737266 -3.5759106 -3.8597193\n",
            " -3.3340356 -2.555093  -1.3497326 -3.4468052 -3.1207633 -2.1812477\n",
            " -3.4289112]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.878616  -2.8880553  0.3028037 -3.8633113 -3.824694  -3.599029\n",
            " -3.4009278 -3.8458939 -2.9087424 -3.671329  -4.6305456 -4.3928366\n",
            " -3.8502343 -2.5197904 -1.3159012 -4.015502  -3.9182572 -2.265041\n",
            " -3.8377507]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-4.0603766  -2.9810162   0.402519   -4.65463    -4.0703654  -4.4495406\n",
            " -3.6535091  -4.1496563  -3.6275342  -3.9823115  -5.239188   -5.238207\n",
            " -4.0115714  -3.1145506  -0.83785415 -4.148001   -4.6164265  -2.7020175\n",
            " -4.1988525 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-4.4008627 -2.7869723  0.9560448 -4.550839  -4.061731  -4.422568\n",
            " -3.6010468 -4.18461   -4.0262527 -4.2008653 -5.183639  -5.145965\n",
            " -3.8560376 -2.8883483 -1.5710557 -4.5233636 -4.1415186 -2.5826411\n",
            " -4.5215306]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.5629072 -2.7665524  2.8412156 -3.7312515 -3.375393  -4.061422\n",
            " -3.2219756 -3.8297157 -2.9090092 -3.300834  -4.0705266 -4.3655224\n",
            " -3.5934098 -2.4828036 -1.0331395 -3.944529  -3.6081886 -2.3806753\n",
            " -3.3734019]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.6830857  -2.5755312   3.0935743  -3.8598526  -3.6142182  -3.52543\n",
            " -3.1278589  -3.606908   -2.7210753  -3.4926388  -4.1963367  -4.409819\n",
            " -3.5788243  -2.371433   -0.85296464 -3.5907705  -3.6072805  -1.9339106\n",
            " -3.4536984 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.3066592  -3.0737567   3.447009   -4.6901407  -3.981392   -4.913575\n",
            " -4.000287   -4.204699   -3.673708   -3.8901403  -4.939721   -5.1326485\n",
            " -3.8438044  -2.7826133  -0.44442123 -3.9908009  -4.5447516  -2.7951827\n",
            " -4.029563  ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-3.2717211 -2.6290982  3.896477  -4.507132  -3.7468967 -4.5367465\n",
            " -3.6325886 -4.025822  -3.6906517 -3.9199839 -4.553761  -5.0576363\n",
            " -3.4696286 -2.9519017 -1.1175996 -4.1155562 -3.9510646 -2.5480893\n",
            " -3.9519813]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-2.4004102 -2.3901396  4.0920343 -3.4486861 -3.2419586 -3.2332032\n",
            " -2.7086875 -3.6338875 -2.5812726 -2.8292959 -3.9533598 -4.0284133\n",
            " -3.1622493 -1.914277  -0.8133481 -3.434074  -3.3375077 -1.6775838\n",
            " -3.2217138]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.9428574  -2.7103634   4.886747   -3.6670754  -3.2231762  -3.8208861\n",
            " -3.2314382  -3.961893   -2.7306614  -3.1188293  -4.0641046  -4.3520827\n",
            " -3.1560898  -1.9486419  -0.27105436 -3.4957995  -3.5436864  -2.1430995\n",
            " -3.0978992 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-2.6071227  -2.6291354   5.239009   -4.678469   -3.7436323  -3.8325076\n",
            " -3.5166612  -3.9466166  -3.8445303  -4.3037143  -4.9424486  -5.144892\n",
            " -3.6541061  -2.9022074  -0.25450668 -3.8536448  -3.9831269  -2.8533287\n",
            " -3.9611561 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 1 0 0 1 1 1 0]\n",
            "Predicted scores for each action in next step: [-2.063194  -2.715016   5.5974064 -4.3344254 -3.50006   -4.033709\n",
            " -3.3349864 -4.029966  -3.3975513 -3.8287456 -4.3845024 -5.019356\n",
            " -3.3064446 -2.908722  -0.9623202 -4.103155  -3.8234854 -2.3561144\n",
            " -3.8192427]\n",
            "Epsilon reduced to 0.06561000000000002\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.5505207 -2.5061324  4.444445  -3.8729708 -3.332542  -4.801897\n",
            " -3.5716164 -3.7625263 -3.9101667 -3.6544771 -4.388595  -5.0347896\n",
            " -3.186958  -2.3386326 -0.28591   -3.8532963 -3.8639522 -2.5721347\n",
            " -3.3799713]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.4585314 -2.567421   4.604251  -4.1393223 -3.6183233 -4.946466\n",
            " -2.9781759 -3.863516  -3.7298791 -3.406487  -4.482063  -5.017259\n",
            " -3.1533892 -2.4455626 -0.6647362 -3.9416327 -3.8416116 -2.3617313\n",
            " -3.332924 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.5704329  -2.1380074   3.8310406  -3.4114878  -2.8511846  -3.4360468\n",
            " -2.3682709  -3.2144854  -3.0587468  -3.350821   -3.7086782  -4.201517\n",
            " -2.6986654  -1.9539955  -0.78919774 -3.3815205  -3.122169   -2.2575562\n",
            " -2.8302243 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.8477417 -2.157628   3.9027371 -3.523903  -2.8991868 -3.1530366\n",
            " -1.9091727 -3.3265314 -3.0821245 -3.0007572 -3.655974  -4.096898\n",
            " -2.983222  -2.1597338 -0.8382723 -3.6852531 -3.466325  -1.9694093\n",
            " -2.9055793]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.252171   -1.6907787   2.454046   -2.7995195  -2.368151   -2.7947216\n",
            " -1.548489   -2.524407   -2.6063836  -2.7629113  -3.0766232  -3.4548151\n",
            " -2.097838   -1.5030255  -0.56767607 -2.8067915  -2.5672991  -2.0802438\n",
            " -2.291981  ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1624293 -1.4974965  2.222814  -2.4689577 -2.3705425 -2.5339081\n",
            " -1.2495606 -2.3232255 -2.407013  -2.0291183 -2.8555398 -3.0601254\n",
            " -2.0536232 -1.491804  -0.2875768 -2.4682717 -2.5694084 -1.2516596\n",
            " -2.2404678]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6739081 -1.1451554  1.1981077 -1.8447957 -1.5006785 -1.905905\n",
            " -0.9844459 -1.6850158 -1.7508218 -1.9574233 -2.023161  -2.3231304\n",
            " -1.4314715 -1.0572358 -0.166888  -1.8497206 -1.6214018 -1.41616\n",
            " -1.6044097]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.79143137 -0.91606957  1.2403463  -1.7120999  -1.6851563  -1.7384437\n",
            " -0.78642344 -1.530304   -1.7585806  -1.4509084  -1.904642   -2.197953\n",
            " -1.4215939  -1.0693684  -0.08184886 -1.6627158  -1.8223515  -0.8119884\n",
            " -1.6122266 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5736086  -0.80247855  0.6660176  -1.3922619  -1.0658721  -1.3487\n",
            " -0.63628584 -1.1797087  -1.1908408  -1.442952   -1.4630153  -1.764528\n",
            " -0.9957704  -0.6874138  -0.23512568 -1.3384799  -1.2474594  -0.99931145\n",
            " -1.1438868 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 10 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.406668   -0.7305103   0.67071295 -1.1290032  -1.082409   -1.0301436\n",
            " -0.43651813 -1.0856105  -0.95920354 -1.1359634  -1.2124141  -1.4441926\n",
            " -1.0728558  -0.7661562  -0.07625713 -1.3059852  -1.2277375  -0.55858034\n",
            " -1.0394232 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 11 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.2967187  -0.75353813  0.22723976 -1.0443583  -0.7682703  -1.2356788\n",
            " -0.49267307 -1.040246   -0.98786575 -1.2209021  -1.1197644  -1.4836191\n",
            " -0.90783286 -0.5608195   0.00168758 -1.2411495  -1.1673027  -0.84732443\n",
            " -0.9380753 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 12 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.36049083 -0.6707048   0.41555193 -1.0667781  -1.0781732  -0.9585688\n",
            " -0.27758634 -0.93422216 -0.802841   -1.0960845  -1.1946282  -1.4104873\n",
            " -0.9748776  -0.6988635  -0.11083701 -1.2244505  -1.1351867  -0.50350964\n",
            " -0.9793176 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.12815292 -0.71207947  0.02754546 -0.83081234 -0.66548663 -1.1108075\n",
            " -0.37243578 -0.8040749  -0.7713714  -0.9573848  -0.92407036 -1.064846\n",
            " -0.68303525 -0.47563657  0.02327932 -0.89604574 -0.8989225  -0.6010062\n",
            " -0.7767701 ]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.24235831 -0.61512256  0.2865571  -0.97459525 -0.9128261  -1.0925095\n",
            " -0.43601203 -0.91444165 -0.82183284 -0.9330603  -1.123132   -1.0651197\n",
            " -0.8851917  -0.66550094  0.04682503 -0.96053505 -0.9400595  -0.43058103\n",
            " -0.8987751 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3535001  -0.5469811  -0.15286213 -0.8972021  -0.71971136 -0.8572307\n",
            " -0.24431342 -0.76449513 -0.7546062  -0.9663107  -1.0033234  -1.1276555\n",
            " -0.6315131  -0.46783483 -0.17922638 -0.873793   -0.8211001  -0.64404\n",
            " -0.78152966]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3385867  -0.5204582  -0.01500342 -0.9530971  -0.93389547 -0.9792049\n",
            " -0.23942299 -0.8100951  -0.9183922  -0.7830277  -1.0637412  -1.1552274\n",
            " -0.77210844 -0.600591   -0.0727206  -0.9436179  -1.0832816  -0.3634749\n",
            " -0.8674048 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.19303527 -0.5449555  -0.29352823 -0.74567604 -0.59796983 -0.92175066\n",
            " -0.3060084  -0.735722   -0.6672049  -0.8510025  -0.84717715 -0.9912994\n",
            " -0.64919704 -0.44041634 -0.15469205 -0.8804096  -0.8584255  -0.5202818\n",
            " -0.7177977 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.25886735 -0.52146816  0.01098753 -0.84666926 -0.78296554 -0.8159587\n",
            " -0.2817221  -0.73802215 -0.72430503 -0.8265035  -0.98208255 -0.9047289\n",
            " -0.72822344 -0.5458993  -0.08600008 -0.84670055 -0.811095   -0.37298614\n",
            " -0.74818623]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.21755539 -0.4432733  -0.39137837 -0.7420393  -0.59438604 -0.6849585\n",
            " -0.14988616 -0.63897526 -0.6051787  -0.817154   -0.813292   -0.95902365\n",
            " -0.5380775  -0.3543166  -0.2617514  -0.7393824  -0.6578356  -0.5581328\n",
            " -0.629114  ]\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.23597904 -0.42982358 -0.28146043 -0.7822951  -0.7887233  -0.77146006\n",
            " -0.17400691 -0.6293669  -0.68200445 -0.7653851  -0.9726343  -0.9536484\n",
            " -0.6267702  -0.45395952 -0.15196198 -0.7594803  -0.77234334 -0.34149903\n",
            " -0.7081565 ]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.14469823 -0.44951734 -0.3926756  -0.58950835 -0.5038347  -0.68823\n",
            " -0.15071931 -0.6524181  -0.57899487 -0.70296013 -0.6946018  -0.8198031\n",
            " -0.50478303 -0.29281604 -0.24367286 -0.6800695  -0.59554094 -0.5330193\n",
            " -0.5371126 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.18437208 -0.38103664 -0.3043108  -0.7181572  -0.70147765 -0.7584548\n",
            " -0.18498635 -0.60830104 -0.66124094 -0.6769956  -0.83978826 -0.85239315\n",
            " -0.5835279  -0.44445    -0.16828951 -0.69804096 -0.72301054 -0.3042533\n",
            " -0.6388392 ]\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.11938044 -0.4054805  -0.43132824 -0.56331646 -0.45720017 -0.69119596\n",
            " -0.21405183 -0.5459526  -0.46964517 -0.665327   -0.6780226  -0.751875\n",
            " -0.5177063  -0.32088685 -0.29937655 -0.7070372  -0.64520836 -0.4082504\n",
            " -0.52937174]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.12251727 -0.39509284 -0.12546633 -0.5197375  -0.49133956 -0.53231204\n",
            " -0.15500948 -0.5969758  -0.55747336 -0.6312872  -0.5971204  -0.61171985\n",
            " -0.6129489  -0.35224712 -0.14426512 -0.6722442  -0.6094973  -0.3055013\n",
            " -0.54893464]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 25 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.14514403 -0.41366464 -0.3593628  -0.4983371  -0.37092537 -0.6474153\n",
            " -0.1535011  -0.496072   -0.489801   -0.54284024 -0.54932886 -0.56130004\n",
            " -0.37074268 -0.29977304 -0.26225024 -0.5762187  -0.5532877  -0.36122048\n",
            " -0.49207357]\n",
            "Epsilon reduced to 0.05904900000000002\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.19020635 -0.4219718  -0.13851903 -0.6018004  -0.5003948  -0.5866809\n",
            " -0.28547177 -0.66053367 -0.47041327 -0.6246442  -0.6319305  -0.71345496\n",
            " -0.5569447  -0.36141288 -0.2072033  -0.6380903  -0.6869124  -0.34869644\n",
            " -0.6179147 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 12\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.19870442 -0.4671339  -0.3289857  -0.519773   -0.3462779  -0.6160674\n",
            " -0.24714705 -0.4862816  -0.45888466 -0.5551177  -0.5624599  -0.6103849\n",
            " -0.45304424 -0.3122302  -0.2989199  -0.500518   -0.5604628  -0.35353047\n",
            " -0.5013977 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1358324  -0.4313572  -0.27841997 -0.5337142  -0.5514913  -0.5994785\n",
            " -0.15923019 -0.6102177  -0.4994651  -0.5158992  -0.6040806  -0.6448227\n",
            " -0.5423181  -0.3335791  -0.29335323 -0.6580316  -0.75671107 -0.30172214\n",
            " -0.53281045]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.20724177 -0.4815241  -0.29994354 -0.4269703  -0.31974316 -0.5552457\n",
            " -0.31294364 -0.432858   -0.3512472  -0.49593747 -0.5350561  -0.56190073\n",
            " -0.41861227 -0.25378972 -0.33398417 -0.47190142 -0.5552744  -0.31673265\n",
            " -0.45653173]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.18069807 -0.42452592 -0.29404533 -0.5220534  -0.5200161  -0.54389733\n",
            " -0.11363606 -0.59333074 -0.49667513 -0.48345852 -0.55270743 -0.63012964\n",
            " -0.4884098  -0.3116121  -0.30611724 -0.6250092  -0.7090548  -0.31649977\n",
            " -0.4834412 ]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.22571874 -0.4788627  -0.3089015  -0.42194366 -0.34116182 -0.5806899\n",
            " -0.2617594  -0.44703287 -0.3639638  -0.5705236  -0.5288308  -0.55477023\n",
            " -0.43498304 -0.29576045 -0.33584872 -0.47154865 -0.51626533 -0.30470333\n",
            " -0.45596322]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.22516078 -0.39933378 -0.38521016 -0.5532173  -0.5278803  -0.5822315\n",
            " -0.17580137 -0.56776446 -0.4350869  -0.54665285 -0.6194916  -0.6322781\n",
            " -0.52614224 -0.3321171  -0.37116674 -0.61682165 -0.6762989  -0.3269093\n",
            " -0.50549364]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.29183063 -0.4923828  -0.33674353 -0.42542836 -0.32301763 -0.5905971\n",
            " -0.28042236 -0.4081359  -0.36458477 -0.53204775 -0.501852   -0.5562884\n",
            " -0.43698812 -0.2824907  -0.34904736 -0.46573788 -0.5141362  -0.30863968\n",
            " -0.44255632]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.24108043 -0.38360208 -0.3693828  -0.5528159  -0.50314116 -0.5739664\n",
            " -0.19596995 -0.5404983  -0.37561792 -0.5271676  -0.6085941  -0.61484015\n",
            " -0.5397617  -0.33701003 -0.30786175 -0.5983309  -0.60393715 -0.3161657\n",
            " -0.46372017]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.29206526 -0.42570168 -0.32121015 -0.5304914  -0.4630487  -0.5426891\n",
            " -0.18075082 -0.51164377 -0.45573026 -0.47016054 -0.6134302  -0.5486372\n",
            " -0.4105935  -0.31335634 -0.27971667 -0.55257475 -0.48426998 -0.29526293\n",
            " -0.46435934]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.2926833  -0.39466256 -0.36887413 -0.5650052  -0.5007433  -0.581426\n",
            " -0.20935902 -0.5286887  -0.37394303 -0.51724935 -0.6051067  -0.6100923\n",
            " -0.53399336 -0.31043458 -0.35991234 -0.5894111  -0.60786366 -0.30171397\n",
            " -0.46437797]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.25116947 -0.45429415 -0.23024052 -0.48744634 -0.34356436 -0.47764596\n",
            " -0.18558231 -0.4250345  -0.39833078 -0.42473677 -0.4905241  -0.4241615\n",
            " -0.39703178 -0.2344172  -0.26338857 -0.48129702 -0.40047723 -0.268552\n",
            " -0.38162318]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.26980376 -0.43971625 -0.29167455 -0.5591204  -0.46643317 -0.55147475\n",
            " -0.18425319 -0.55387956 -0.39213976 -0.48139375 -0.56437963 -0.5648562\n",
            " -0.5527083  -0.24990115 -0.3171631  -0.61020327 -0.626546   -0.29635987\n",
            " -0.44952625]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.33287284 -0.4909517  -0.23212112 -0.48114166 -0.3521716  -0.52352387\n",
            " -0.20430462 -0.48883575 -0.44512624 -0.48645565 -0.50190985 -0.4753104\n",
            " -0.4585622  -0.21791032 -0.2388738  -0.53934145 -0.4386528  -0.27914098\n",
            " -0.43420047]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.33546606 -0.42034537 -0.39434186 -0.5732971  -0.49811783 -0.5683111\n",
            " -0.22159252 -0.5215056  -0.34789392 -0.5058051  -0.61589503 -0.60240555\n",
            " -0.562925   -0.29697692 -0.37774307 -0.61422896 -0.60980487 -0.2945872\n",
            " -0.4544014 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.19762942 -0.34661025 -0.22123116 -0.30251214 -0.23855627 -0.29945612\n",
            " -0.19984822 -0.27561343 -0.23773396 -0.28075996 -0.3936005  -0.30405796\n",
            " -0.30100793 -0.11873376 -0.21222678 -0.30672354 -0.31966338 -0.16436988\n",
            " -0.26146084]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.44034916 -0.4727232  -0.25763905 -0.46918184 -0.3220604  -0.4584067\n",
            " -0.17140356 -0.3753252  -0.38683796 -0.38460734 -0.4609662  -0.486435\n",
            " -0.39232028 -0.12884526 -0.24138352 -0.37812734 -0.44352597 -0.2665952\n",
            " -0.3694573 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.40481025 -0.46334934 -0.33695573 -0.5144694  -0.4175038  -0.4550655\n",
            " -0.25674295 -0.4725351  -0.3652181  -0.48768866 -0.60260826 -0.57195824\n",
            " -0.57623374 -0.2681542  -0.2900187  -0.4930479  -0.58218944 -0.27305546\n",
            " -0.42297384]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.33307704 -0.40121168 -0.3446859  -0.5570853  -0.39682874 -0.42438748\n",
            " -0.21759796 -0.4388591  -0.34795082 -0.34365833 -0.51170003 -0.52714455\n",
            " -0.41276073 -0.27936262 -0.20406197 -0.45667523 -0.50633186 -0.26024878\n",
            " -0.41939652]\n",
            "\n",
            "Taking action 12 from 14\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.39125255 -0.4501673  -0.2856036  -0.49400714 -0.32779726 -0.44578195\n",
            " -0.2018638  -0.4294614  -0.4414426  -0.3711242  -0.50855577 -0.44285738\n",
            " -0.42752963 -0.305826   -0.20622417 -0.4083309  -0.42178527 -0.28772467\n",
            " -0.3777322 ]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4906691  -0.5246455  -0.28146744 -0.57594585 -0.36033222 -0.52051157\n",
            " -0.18797234 -0.44691604 -0.46443066 -0.41672063 -0.5200291  -0.5398956\n",
            " -0.41980797 -0.33910137 -0.30126572 -0.46059877 -0.4714578  -0.28498697\n",
            " -0.4090841 ]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.42483392 -0.41327482 -0.3184392  -0.6465558  -0.49723363 -0.5791647\n",
            " -0.3575935  -0.5203422  -0.42368844 -0.5212247  -0.5663452  -0.64000124\n",
            " -0.59834003 -0.37143993 -0.3958869  -0.54164195 -0.66173416 -0.27636617\n",
            " -0.43229666]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 1 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2945548  -0.4352071  -0.18092461 -0.52373815 -0.35266423 -0.46093962\n",
            " -0.2648542  -0.43241805 -0.41294047 -0.37839305 -0.50182194 -0.421423\n",
            " -0.3448796  -0.3856746  -0.2900731  -0.3957523  -0.44305483 -0.25009412\n",
            " -0.36327204]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 16\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3098872  -0.4714677  -0.14709193 -0.63798225 -0.37478647 -0.42885378\n",
            " -0.3563607  -0.42947376 -0.32350665 -0.46280393 -0.6449419  -0.5045293\n",
            " -0.49084672 -0.4290801  -0.15789604 -0.39846966 -0.53235066 -0.2758668\n",
            " -0.43570235]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.45081338 -0.58922124 -0.22374153 -0.73371345 -0.51869446 -0.61702764\n",
            " -0.31502452 -0.65434974 -0.54916674 -0.5728409  -0.5640756  -0.69064283\n",
            " -0.7927125  -0.45886528 -0.38330525 -0.71182597 -0.62060946 -0.41293848\n",
            " -0.539374  ]\n",
            "Epsilon reduced to 0.05314410000000002\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5618849  -0.46849388 -0.4055283  -0.7036809  -0.4644188  -0.46369743\n",
            " -0.40196285 -0.50639284 -0.3784007  -0.59188384 -0.6759946  -0.6847213\n",
            " -0.5738664  -0.45570534 -0.41199982 -0.44185752 -0.57052726 -0.38768053\n",
            " -0.54744935]\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.49758422 -0.553749   -0.511204   -0.6864181  -0.49001396 -0.6207384\n",
            " -0.39079204 -0.671673   -0.50812376 -0.54028714 -0.5860829  -0.70365775\n",
            " -0.61863434 -0.44616142 -0.5078737  -0.7102367  -0.63332015 -0.4359342\n",
            " -0.49376607]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6332692  -0.463095   -0.3280369  -0.70548683 -0.52518636 -0.46314526\n",
            " -0.46210915 -0.57140553 -0.49516538 -0.596455   -0.6471539  -0.73631394\n",
            " -0.66423655 -0.5251665  -0.34720594 -0.44917187 -0.5927528  -0.4862209\n",
            " -0.60870457]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6689828  -0.4563598  -0.49916804 -0.66995484 -0.5260298  -0.55864805\n",
            " -0.31216794 -0.5446377  -0.5164878  -0.5569178  -0.6009015  -0.69481087\n",
            " -0.55836165 -0.4768504  -0.5375106  -0.5901303  -0.57531345 -0.46426216\n",
            " -0.49520645]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.45961064 -0.6607507  -0.3977118  -0.6481855  -0.42214742 -0.6459428\n",
            " -0.53339255 -0.5687785  -0.43285623 -0.57815695 -0.6476619  -0.69296443\n",
            " -0.56058717 -0.49872097 -0.42006525 -0.5128982  -0.6756257  -0.3831309\n",
            " -0.5176616 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.635996   -0.68732506 -0.37753975 -0.76594996 -0.60021925 -0.6778829\n",
            " -0.29603565 -0.64396024 -0.5112174  -0.55266917 -0.58193207 -0.78212595\n",
            " -0.6401718  -0.4612264  -0.5313618  -0.76322544 -0.7474717  -0.38924056\n",
            " -0.5544249 ]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.50185627 -0.5303281  -0.29907945 -0.7445476  -0.42745483 -0.529938\n",
            " -0.4756334  -0.48662955 -0.48667908 -0.5830313  -0.6813897  -0.61261827\n",
            " -0.51269317 -0.50930023 -0.44071013 -0.42008966 -0.62083703 -0.4184697\n",
            " -0.54700404]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4342478  -0.6077809  -0.08373897 -0.66586083 -0.5860007  -0.58358824\n",
            " -0.4241752  -0.5770064  -0.4871384  -0.52039355 -0.6090946  -0.5529023\n",
            " -0.757274   -0.53095514 -0.4273305  -0.57923985 -0.61057675 -0.3585695\n",
            " -0.5901633 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.30803502 -0.5393157  -0.29824206 -0.5864917  -0.39667845 -0.55574834\n",
            " -0.5375056  -0.45742756 -0.3487633  -0.48861742 -0.5760244  -0.5572129\n",
            " -0.50599045 -0.41810337 -0.46368286 -0.52432346 -0.5955404  -0.3390768\n",
            " -0.50236726]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.48907357 -0.5992456  -0.3615873  -0.71437716 -0.62599355 -0.587528\n",
            " -0.28800517 -0.52595705 -0.44466344 -0.5576831  -0.5936961  -0.6723277\n",
            " -0.5904723  -0.5134367  -0.444476   -0.6365265  -0.68055445 -0.3397108\n",
            " -0.52143806]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.41616914 -0.5270122  -0.1460118  -0.5397473  -0.35419643 -0.5073465\n",
            " -0.51604795 -0.5042714  -0.37072575 -0.4284168  -0.5543953  -0.47259626\n",
            " -0.5011221  -0.37900835 -0.3997864  -0.38961345 -0.4873784  -0.37551075\n",
            " -0.413503  ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.45570302 -0.60349524 -0.08222893 -0.7389563  -0.5789729  -0.5791528\n",
            " -0.5211412  -0.63521886 -0.5371407  -0.53185236 -0.5663909  -0.60152435\n",
            " -0.7681315  -0.59864926 -0.42656067 -0.59776473 -0.6455017  -0.44516024\n",
            " -0.641512  ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.41043642 -0.5479215  -0.28502017 -0.6623272  -0.38979638 -0.6256906\n",
            " -0.6899527  -0.5651093  -0.48842472 -0.5923996  -0.6184517  -0.58311486\n",
            " -0.6268081  -0.52409947 -0.44891885 -0.4888625  -0.65695477 -0.44469884\n",
            " -0.54521817]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.48581743 -0.57658255 -0.37440917 -0.7990852  -0.5876498  -0.63949585\n",
            " -0.48871085 -0.5775534  -0.56593424 -0.5704646  -0.6753267  -0.7134775\n",
            " -0.6286678  -0.560013   -0.508111   -0.679993   -0.7509     -0.36193806\n",
            " -0.5271654 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 11\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6114899  -0.4951923  -0.42448395 -0.6751507  -0.3890553  -0.4884583\n",
            " -0.5918509  -0.47664443 -0.53916025 -0.5335327  -0.5841738  -0.5582023\n",
            " -0.5386485  -0.54168856 -0.41253418 -0.49622    -0.63643235 -0.42657596\n",
            " -0.5611361 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 16 reward=-2 new_state=[0 0 1 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6082324  -0.54438406 -0.16232538 -0.8632488  -0.4944911  -0.6294534\n",
            " -0.35147402 -0.59153295 -0.76039565 -0.5571142  -0.55790246 -0.65650064\n",
            " -0.49984783 -0.66363    -0.41937488 -0.5024363  -0.7147534  -0.54288685\n",
            " -0.5062299 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 17 reward=-2 new_state=[0 0 1 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.29821467 -0.51250833 -0.13754745 -0.56327224 -0.44174978 -0.62249213\n",
            " -0.21898028 -0.6224069  -0.6112352  -0.45495027 -0.43058217 -0.6512961\n",
            " -0.48266435 -0.47788292 -0.41902423 -0.5046025  -0.49427146 -0.4958052\n",
            " -0.41361886]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 18 reward=-2 new_state=[0 0 1 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1708908  -0.41033068 -0.09819343 -0.37742075 -0.36069736 -0.42009908\n",
            " -0.21700606 -0.44813508 -0.37954953 -0.3153281  -0.311192   -0.41131407\n",
            " -0.45660836 -0.4426391  -0.33772463 -0.46599475 -0.33993763 -0.36440837\n",
            " -0.36295328]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 19 reward=-2 new_state=[0 0 1 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5367148  -0.53948164 -0.39604554 -0.7169378  -0.5416337  -0.66253436\n",
            " -0.3464601  -0.53306615 -0.67354536 -0.59521276 -0.64762056 -0.5537853\n",
            " -0.57485926 -0.75637186 -0.30523255 -0.48989564 -0.62556595 -0.5057626\n",
            " -0.5083086 ]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 20 reward=-2 new_state=[0 0 1 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.45855352 -0.5978959  -0.39661932 -0.8362286  -0.5064233  -0.7108426\n",
            " -0.41811597 -0.56705034 -0.86477244 -0.55459416 -0.6421024  -0.6674716\n",
            " -0.42082316 -0.61495656 -0.46503097 -0.48916292 -0.7289045  -0.5371486\n",
            " -0.46402818]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 21 reward=-2 new_state=[0 0 1 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.35624182 -0.55098367 -0.45730996 -0.6394047  -0.5342706  -0.9134197\n",
            " -0.51709396 -0.7752576  -0.8352565  -0.57384735 -0.54713213 -0.86955523\n",
            " -0.5111319  -0.71170866 -0.54580075 -0.49095055 -0.5739903  -0.5181755\n",
            " -0.486351  ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 22 reward=-2 new_state=[0 0 1 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5547152  -0.55800027 -0.3079027  -0.7901726  -0.42326564 -0.82706386\n",
            " -0.32147866 -0.657474   -0.7651574  -0.5924512  -0.53392416 -0.7987511\n",
            " -0.47630554 -0.71708894 -0.377149   -0.5002178  -0.6816461  -0.49784806\n",
            " -0.4899893 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 23 reward=-2 new_state=[0 0 1 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.39043024 -0.5300488  -0.60712975 -0.57581246 -0.48750523 -0.7838115\n",
            " -0.52035105 -0.6170322  -0.84447014 -0.6489394  -0.5286508  -0.6387903\n",
            " -0.47091067 -0.6649941  -0.5760513  -0.5556137  -0.632806   -0.46993923\n",
            " -0.6270188 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 24 reward=-2 new_state=[0 0 1 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.40161216 -0.5702839  -0.46485466 -0.5690696  -0.46082342 -0.7538837\n",
            " -0.4457096  -0.6315506  -0.6285669  -0.5151655  -0.6105215  -0.59349376\n",
            " -0.63408196 -0.7201373  -0.5623207  -0.6056131  -0.54481816 -0.4886595\n",
            " -0.55827284]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 25 reward=-2 new_state=[0 0 1 1 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1626993  -0.6943002  -0.76749647 -1.1672305  -0.6465551  -1.0534906\n",
            " -0.9464102  -0.767121   -0.9620728  -0.8181678  -0.85115004 -0.9742445\n",
            " -0.6294766  -1.0697572  -0.7912431  -0.7442248  -0.99156827 -0.67958885\n",
            " -0.6509017 ]\n",
            "Epsilon reduced to 0.04782969000000002\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8866665  -0.62008166 -0.7592498  -0.8428917  -0.68033934 -0.8766164\n",
            " -0.554157   -0.77866626 -0.76526964 -0.7139702  -0.69553393 -0.79543215\n",
            " -0.72541153 -0.6802875  -0.7436139  -0.79266    -0.8572675  -0.5614759\n",
            " -0.61820656]\n",
            "\n",
            "Taking action 4 from 6\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6276603  -0.65204793 -0.7308924  -0.77760637 -0.5176096  -1.0429825\n",
            " -0.8893761  -0.6766174  -0.71093    -0.7260068  -0.8007887  -0.79411674\n",
            " -0.77513915 -0.6435558  -0.69237626 -0.5765928  -0.7146329  -0.3868717\n",
            " -0.69106597]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9252805  -0.64580524 -0.73571974 -0.8146416  -0.5637947  -0.9029925\n",
            " -0.55298114 -0.68176866 -0.8578914  -0.7319978  -0.8443269  -0.7470117\n",
            " -0.68000114 -0.737813   -0.7478328  -0.7081786  -0.8580918  -0.6363975\n",
            " -0.6904892 ]\n",
            "\n",
            "Taking action 4 from 6\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.78895646 -0.64232457 -0.5821867  -0.6574259  -0.5212009  -0.7345034\n",
            " -0.6495784  -0.54153615 -0.5298881  -0.6460922  -0.6930561  -0.7476002\n",
            " -0.6275462  -0.56408614 -0.5982496  -0.54987544 -0.5816543  -0.65703183\n",
            " -0.67041314]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0011529  -0.6569952  -0.7306357  -0.9267083  -0.5924838  -0.94794935\n",
            " -0.7938676  -0.8121943  -0.9401225  -0.71597284 -0.7509368  -0.96100074\n",
            " -0.7246078  -0.89267373 -0.7118144  -0.80303836 -0.7902885  -0.72451174\n",
            " -0.6946895 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7694908  -0.715072   -0.72965527 -0.69191283 -0.521409   -1.0089707\n",
            " -0.75210524 -0.6499947  -0.59590185 -0.7249414  -0.5916471  -0.83013344\n",
            " -0.8326194  -0.5771938  -0.6514701  -0.6487359  -0.73942053 -0.5598643\n",
            " -0.7092811 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7690207  -0.63871014 -0.6684656  -0.69839805 -0.52210057 -0.8162514\n",
            " -0.5655517  -0.64026845 -0.7797812  -0.7622608  -0.7742267  -0.72380894\n",
            " -0.6778722  -0.69948506 -0.7047406  -0.7175024  -0.75415933 -0.6607526\n",
            " -0.71389437]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.81369525 -0.6388409  -0.79384947 -0.8581438  -0.4323222  -1.1214494\n",
            " -0.8735333  -0.68088216 -0.6984414  -0.76669765 -0.78921133 -0.8471681\n",
            " -0.80005276 -0.6328552  -0.7966738  -0.594156   -0.7671295  -0.479619\n",
            " -0.7484814 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.99746895 -0.6348267  -0.82632864 -0.9723145  -0.46401    -0.93935245\n",
            " -0.60400283 -0.8144583  -0.93406904 -0.74117935 -0.87902755 -0.9646519\n",
            " -0.6798012  -0.75538576 -0.8168661  -0.76900995 -1.0175576  -0.77649134\n",
            " -0.74264413]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9730286  -0.6412958  -0.73914    -0.7884012  -0.302313   -0.92309743\n",
            " -0.5117873  -0.62188274 -0.5788895  -0.6859613  -0.76732177 -0.83013904\n",
            " -0.67300266 -0.5700177  -0.7963222  -0.59710795 -0.6930507  -0.71943104\n",
            " -0.71148777]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0118003  -0.6195818  -0.8865817  -0.91041267 -0.39549315 -0.99906397\n",
            " -0.5982919  -0.8337874  -1.0321518  -0.73975676 -0.76473117 -0.86544573\n",
            " -0.6131303  -0.7431794  -0.89649117 -0.7209045  -0.8807071  -0.76760334\n",
            " -0.7003323 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0635045  -0.6101186  -0.83618367 -0.7792151  -0.2808478  -1.1681664\n",
            " -0.8777366  -0.66389984 -0.7397638  -0.8125808  -0.806213   -0.87811726\n",
            " -0.80141056 -0.64967394 -0.8063762  -0.61303604 -0.67831767 -0.55907524\n",
            " -0.7494119 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.98925316 -0.6632479  -0.8558197  -0.85386765 -0.24318066 -0.9039157\n",
            " -0.62246406 -0.75964093 -0.891015   -0.73614013 -0.85022175 -0.8549507\n",
            " -0.65722084 -0.76637787 -0.8151034  -0.7924634  -0.9093622  -0.72023016\n",
            " -0.7527687 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8135593  -0.7263596  -0.62628675 -0.7462809  -0.19768533 -0.96010566\n",
            " -0.77649087 -0.58376634 -0.54397595 -0.70247364 -0.75755787 -0.8096949\n",
            " -0.75999486 -0.6281584  -0.67601955 -0.5726806  -0.7327659  -0.6589985\n",
            " -0.7547948 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0088826  -0.6459502  -0.7570046  -0.92196333 -0.10866101 -0.9334247\n",
            " -0.663982   -0.7975317  -0.8708334  -0.69926745 -0.7046138  -0.92265594\n",
            " -0.6632515  -0.7913217  -0.7686579  -0.77703947 -0.8486517  -0.6542316\n",
            " -0.6670245 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8279493  -0.72949624 -0.63598216 -0.75017023 -0.10942439 -0.9694847\n",
            " -0.78074026 -0.58662343 -0.54690415 -0.7059601  -0.76109993 -0.8147787\n",
            " -0.76324147 -0.63143235 -0.684865   -0.5751762  -0.73635054 -0.66382587\n",
            " -0.75894463]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0387874  -0.6263516  -0.8082662  -0.88574934  0.01304104 -1.0055693\n",
            " -0.8201555  -0.8258363  -0.77949953 -0.6704472  -0.73136383 -0.9017603\n",
            " -0.751948   -0.87140864 -0.7659794  -0.7402635  -0.7495742  -0.7173545\n",
            " -0.64312965]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0733008  -0.5861747  -0.7486899  -0.83599454 -0.16581129 -0.7941784\n",
            " -0.70289356 -0.6023663  -0.5815654  -0.708357   -0.7612493  -0.8813817\n",
            " -0.7025543  -0.7373731  -0.65460724 -0.5296234  -0.7064971  -0.70774204\n",
            " -0.7732409 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0833964  -0.71734416 -0.82389593 -0.95604384  0.08093105 -1.0295514\n",
            " -0.70437396 -0.82540345 -0.74129045 -0.68036914 -0.8076735  -1.0056826\n",
            " -0.6117721  -0.8366748  -0.80042964 -0.7600801  -0.8034841  -0.7193157\n",
            " -0.6197112 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9368192  -0.68135613 -0.8242097  -0.82803     0.07955256 -1.1292849\n",
            " -0.87845737 -0.6585277  -0.595971   -0.80314565 -0.753476   -0.8963709\n",
            " -0.7636373  -0.62577844 -0.8193962  -0.6877647  -0.8414351  -0.61695945\n",
            " -0.80226856]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2428404  -0.69731164 -0.8917665  -1.0682268   0.05525628 -1.0878906\n",
            " -0.6470921  -0.8922783  -0.96941346 -0.71436894 -0.8024274  -1.0149956\n",
            " -0.66775465 -0.8462471  -0.9191901  -0.80640924 -0.9157394  -0.6955844\n",
            " -0.69390357]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.81402576 -0.7205658  -0.5604864  -0.75093424 -0.00727756 -0.9457333\n",
            " -0.46226335 -0.5704917  -0.41857728 -0.7074053  -0.6623336  -0.75648165\n",
            " -0.78591055 -0.7319278  -0.73818135 -0.621462   -0.7590294  -0.6004616\n",
            " -0.78592074]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0648822  -0.7193324  -0.7756105  -0.9336552   0.13391776 -1.0081623\n",
            " -0.74930394 -0.74803483 -0.72721493 -0.6825689  -0.8174414  -0.85995036\n",
            " -0.6897924  -0.87020046 -0.8171175  -0.73531866 -0.7668441  -0.66619563\n",
            " -0.64711237]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9121766  -0.7266637  -0.7336603  -0.7552693   0.2292314  -1.0448103\n",
            " -0.8640867  -0.63987976 -0.6132877  -0.7387523  -0.76908064 -0.86070037\n",
            " -0.83208233 -0.6733961  -0.721662   -0.6053677  -0.75624204 -0.7099712\n",
            " -0.7800872 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0853361  -0.60913223 -0.85901344 -0.9005031   0.01466562 -1.0747924\n",
            " -0.8087771  -0.87895143 -0.8000822  -0.71477354 -0.715777   -0.95411485\n",
            " -0.7522107  -0.85376847 -0.7695135  -0.73833513 -0.7830285  -0.7310453\n",
            " -0.63377345]\n",
            "Epsilon reduced to 0.043046721000000024\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.82815015 -0.64151984 -0.8748278  -0.8624859  -0.4175729  -0.9006506\n",
            " -0.9441423  -0.7880632  -0.8316914  -0.68521583 -0.9527998  -0.93551856\n",
            " -0.8340302  -0.74623054 -0.8839979  -0.63721186 -0.84033203 -0.44660503\n",
            " -0.77534366]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.064705   -0.7580236  -0.7491821  -0.88560045 -0.23482546 -0.8162329\n",
            " -0.99035966 -0.7950376  -0.92364734 -0.6984912  -0.90500355 -0.81967247\n",
            " -0.707353   -0.93484473 -0.8360882  -0.6902431  -0.9212667  -0.5229701\n",
            " -0.8478181 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.915645   -0.7901907  -0.71702296 -0.8799422  -0.16851343 -0.9066852\n",
            " -0.42905515 -0.7892202  -0.6998491  -0.71631414 -0.75938874 -0.8904996\n",
            " -0.7869183  -0.6972078  -0.7620238  -0.78596073 -0.9368586  -0.52630246\n",
            " -0.7501015 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1606485  -0.67112535 -0.6746161  -0.96368146 -0.3880751  -0.8414003\n",
            " -0.8815695  -0.7412492  -0.74451625 -0.6857004  -0.8309366  -0.7240484\n",
            " -0.6561986  -0.84135413 -0.8552232  -0.67729217 -1.0251393  -0.5179757\n",
            " -0.80692714]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0697387  -0.7608594  -0.7607603  -0.969354   -0.13553295 -0.91941667\n",
            " -0.4818467  -0.72712845 -0.71405196 -0.6903624  -0.8622008  -0.8045632\n",
            " -0.751083   -0.7783398  -0.8230859  -0.69283134 -0.9069488  -0.6308126\n",
            " -0.8118894 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0807976  -0.7374065  -0.62195015 -0.84247977 -0.3520666  -0.81352973\n",
            " -0.9989543  -0.67257655 -0.76706386 -0.5996205  -0.7957557  -0.65096897\n",
            " -0.56032366 -0.8734582  -0.79022723 -0.52883166 -0.81643695 -0.473886\n",
            " -0.7898186 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.94218457 -0.85760856 -0.70139134 -0.8794036  -0.3230896  -0.9762877\n",
            " -0.5401053  -0.7823732  -0.6421615  -0.69272196 -0.7566843  -0.79678696\n",
            " -0.8056778  -0.72678626 -0.79887545 -0.7233252  -0.91740793 -0.43497747\n",
            " -0.71772075]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1170297  -0.65720695 -0.80791724 -0.77959406 -0.3982594  -0.82122266\n",
            " -1.0197377  -0.7213365  -0.8928693  -0.693838   -0.8500953  -0.7046701\n",
            " -0.63563716 -0.8434378  -0.82525563 -0.72018796 -0.92005616 -0.595231\n",
            " -0.870149  ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.88010657 -0.76213586 -0.72631496 -0.833663   -0.32825208 -0.7240601\n",
            " -0.60920215 -0.69303507 -0.5775385  -0.6218895  -0.76086986 -0.73943996\n",
            " -0.7336315  -0.7101109  -0.71475804 -0.6407533  -0.78820753 -0.5595962\n",
            " -0.74167895]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.08222    -0.6692529  -0.675217   -0.91749376 -0.5114953  -0.7727109\n",
            " -1.0716214  -0.74159503 -0.7178908  -0.6788546  -0.81527174 -0.64110935\n",
            " -0.6442226  -0.7959974  -0.8040663  -0.66732484 -1.0151006  -0.4985923\n",
            " -0.78779   ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8807146  -0.68866116 -0.7583892  -0.7831321  -0.5064401  -0.9471394\n",
            " -0.71479857 -0.73498327 -0.66278726 -0.65714335 -0.8398319  -0.7293619\n",
            " -0.6997213  -0.667119   -0.76751816 -0.68393326 -0.78394854 -0.41803515\n",
            " -0.7106108 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0983508  -0.6723096  -0.75685096 -0.84198606 -0.40797693 -0.8002588\n",
            " -0.9978312  -0.71466446 -0.9447427  -0.74984086 -0.89503354 -0.6333443\n",
            " -0.4738564  -0.87576187 -0.8655301  -0.6292418  -0.8858402  -0.64451003\n",
            " -0.9065365 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.92312884 -0.71433645 -0.6863561  -0.8482567  -0.32956648 -0.7431423\n",
            " -0.5240566  -0.6871356  -0.68575966 -0.6645677  -0.78901124 -0.8157322\n",
            " -0.7398817  -0.702656   -0.69623077 -0.68520963 -0.80216306 -0.6118469\n",
            " -0.7877543 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1148897  -0.66218185 -0.70616007 -0.8961551  -0.16003713 -0.8118688\n",
            " -0.9254267  -0.7372324  -0.91749394 -0.7337978  -0.8815681  -0.7912365\n",
            " -0.6486719  -0.85918194 -0.8339881  -0.75965345 -0.99825644 -0.57498336\n",
            " -0.89983124]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 5\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.378569   -0.8509111  -0.83698785 -0.89039457 -0.14673746 -1.0208708\n",
            " -0.49959323 -0.6410818  -0.77198046 -0.7287179  -0.7403884  -0.8952493\n",
            " -0.8060993  -0.8974362  -0.97123957 -0.560533   -0.86208427 -0.65740657\n",
            " -0.83631694]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.6473355  -0.94441545 -0.950843   -1.0229826  -0.18111232 -1.3717067\n",
            " -0.8894099  -0.9003833  -1.1177316  -0.8219621  -0.9382052  -1.0810597\n",
            " -0.8304546  -1.1462054  -1.0530353  -0.88531977 -0.98255575 -0.57340425\n",
            " -0.9028566 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.108151   -0.8866284  -0.72931457 -0.7575875  -0.21980774 -0.96649987\n",
            " -0.6553431  -0.6783177  -0.8202126  -0.8521904  -0.846002   -0.8700495\n",
            " -0.7943395  -0.87099284 -0.9130019  -0.65668046 -0.748574   -0.4542267\n",
            " -0.75195926]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.87134147 -0.6549456  -0.59348196 -0.73821354 -0.4617784  -0.90863085\n",
            " -1.0225844  -0.65002066 -0.74536645 -0.691185   -0.9386526  -0.8599776\n",
            " -0.749919   -0.7770694  -0.672236   -0.7065249  -0.85751534 -0.42543277\n",
            " -0.7796549 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1027107  -0.77906156 -0.7644786  -0.94522405 -0.23706707 -0.987061\n",
            " -0.6464399  -0.8159684  -0.7481109  -0.77417004 -0.6687018  -0.93913305\n",
            " -0.9279034  -0.77327394 -0.81267476 -0.7180826  -0.89746416 -0.44690984\n",
            " -0.7985419 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8224161  -0.71235675 -0.60216707 -0.85151136 -0.19391352 -0.8605435\n",
            " -0.5945716  -0.67582303 -0.71859115 -0.6781633  -0.88471967 -0.75049233\n",
            " -0.6048441  -0.8085203  -0.7681592  -0.75297344 -0.73088527 -0.5381222\n",
            " -0.7036794 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.88605076 -0.7514867  -0.51990616 -0.812469   -0.41523904 -0.76125365\n",
            " -0.7365652  -0.67404354 -0.52155656 -0.6724111  -0.7855059  -0.8456327\n",
            " -0.90572333 -0.71240515 -0.6491002  -0.5580105  -0.76521397 -0.4036195\n",
            " -0.808997  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.92875606 -0.6742226  -0.5657855  -0.7427962  -0.24128115 -0.85726637\n",
            " -0.68112665 -0.64950585 -0.720734   -0.60988015 -0.6571452  -0.75754297\n",
            " -0.6688994  -0.74147165 -0.6658716  -0.72335005 -0.76273406 -0.47527933\n",
            " -0.6874912 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.94887334 -0.68631434 -0.640728   -0.808423   -0.16522208 -0.82137895\n",
            " -0.52830064 -0.66274613 -0.4900555  -0.6729417  -0.5661245  -0.6962451\n",
            " -0.77259576 -0.6510291  -0.6266518  -0.5872339  -0.75999355 -0.3105222\n",
            " -0.6227158 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2475199  -0.7348858  -0.74728554 -1.0252699  -0.01810932 -1.1960479\n",
            " -1.0806692  -0.76021653 -0.96991545 -0.8417655  -1.0237769  -0.9778091\n",
            " -0.71481913 -1.0198998  -0.85839885 -0.83334714 -0.89109087 -0.56914216\n",
            " -0.86355966]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0254505  -0.67719924 -0.56823575 -1.0119057  -0.27548936 -1.0394664\n",
            " -1.0269709  -0.7592041  -0.7107729  -0.78756404 -0.987313   -0.9529212\n",
            " -0.9960743  -0.8531353  -0.7319979  -0.5793588  -0.8351375  -0.2899801\n",
            " -0.8920582 ]\n",
            "Epsilon reduced to 0.03874204890000002\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.89203596 -0.7909397  -0.6267508  -1.037009   -0.37944883 -1.0070525\n",
            " -0.62324417 -0.89849275 -0.8690027  -0.59251326 -0.6652008  -0.8979123\n",
            " -0.70544106 -0.8668951  -0.8278972  -0.6225395  -0.9171512  -0.5311662\n",
            " -0.49150383]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.735291   -0.8730404  -0.70025647 -0.84231055 -0.41693008 -1.0803821\n",
            " -1.0161393  -0.8694841  -0.7865979  -0.7842479  -0.689679   -0.95559335\n",
            " -1.1096327  -0.64481467 -0.7969569  -0.8534457  -0.82823735 -0.43994504\n",
            " -0.75309646]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.78593934 -0.72635466 -0.8659515  -0.78530324 -0.4255044  -1.0610088\n",
            " -1.0035763  -0.70507324 -0.64386797 -0.6437422  -0.8229542  -0.9522507\n",
            " -0.72053987 -0.74724746 -0.7772131  -0.72053325 -0.7853234  -0.37497675\n",
            " -0.6500305 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7358653  -0.79451716 -0.90495706 -0.9123291  -0.3380595  -1.0395283\n",
            " -0.81827056 -0.7092565  -0.6866476  -0.7258841  -0.7646978  -1.0584095\n",
            " -0.86521804 -0.5942083  -0.9444584  -0.86005336 -0.8257434  -0.41571385\n",
            " -0.67050236]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7647519  -0.8547525  -0.7458676  -0.8057149  -0.06661869 -1.0791984\n",
            " -0.9510992  -0.77562857 -0.6789814  -0.829924   -0.745912   -0.9460269\n",
            " -0.9592998  -0.6755223  -0.7953037  -0.7873429  -0.81898284 -0.43118656\n",
            " -0.7092651 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8643967  -0.71283376 -1.2291378  -0.9685425  -0.34304485 -1.0966352\n",
            " -1.0330919  -0.72546506 -0.7706312  -0.67338216 -0.96178263 -1.0669161\n",
            " -0.8439566  -0.65991616 -0.9948703  -0.738941   -0.79481006 -0.27314478\n",
            " -0.70108795]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.78146136 -0.759179   -0.6192621  -0.7250484  -0.093856   -1.0312741\n",
            " -0.8086281  -0.73318774 -0.65555876 -0.63849866 -0.6154288  -0.8558929\n",
            " -0.77650225 -0.71918255 -0.66977185 -0.68473333 -0.7172886  -0.4066992\n",
            " -0.6442397 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0360495  -0.92249835 -1.193288   -1.1122535  -0.03069402 -1.191717\n",
            " -0.9535669  -0.9837038  -0.81250805 -0.76902664 -0.8692289  -1.0398957\n",
            " -1.2781712  -0.7892164  -0.95524585 -0.9166381  -0.8263514  -0.44241238\n",
            " -0.7926312 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9594391  -0.7244561  -0.7245612  -0.91679835 -0.00301703 -1.3358322\n",
            " -1.096881   -0.807958   -0.77065074 -0.77726483 -0.8553834  -0.95944655\n",
            " -0.7423942  -0.81606036 -0.9691683  -0.77010345 -0.8499511  -0.47699565\n",
            " -0.7512222 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.1195157  -0.9387704  -1.1173726  -1.0898532  -0.12572476 -1.0605868\n",
            " -0.6396378  -0.92405015 -0.96371293 -0.7576351  -0.8368874  -1.0514569\n",
            " -1.1738418  -0.76186466 -1.055171   -0.9696549  -0.9034534  -0.3637001\n",
            " -0.807234  ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9161709  -0.70360905 -0.7618934  -0.8701359   0.14385273 -1.1713934\n",
            " -0.7623203  -0.7526817  -0.8092662  -0.6975583  -0.8013407  -0.8973346\n",
            " -0.769882   -0.6296321  -0.8172005  -0.6037354  -0.8041917  -0.44705737\n",
            " -0.69619346]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8839378  -0.7903992  -1.1783259  -0.9907999  -0.11277984 -1.0622125\n",
            " -0.8364014  -0.82778215 -0.7866255  -0.6819873  -0.8395826  -1.0979514\n",
            " -0.9530665  -0.7683413  -1.0125947  -0.8795644  -0.79884255 -0.34091336\n",
            " -0.71786284]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.2185153  -0.79276043 -1.0106266  -0.97415566  0.22252105 -1.2619314\n",
            " -1.0356176  -0.8198412  -0.85216826 -0.8764889  -0.93099266 -1.1344008\n",
            " -0.7964296  -0.81608653 -1.022899   -0.75535595 -0.87798977 -0.49306476\n",
            " -0.7707174 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.75798106 -0.7625828  -1.0380623  -0.9219626  -0.05161406 -0.93746704\n",
            " -0.73287326 -0.73246413 -0.69696677 -0.62150407 -0.7616905  -1.0365274\n",
            " -0.8760907  -0.6705636  -0.8864334  -0.79342496 -0.74272907 -0.28067213\n",
            " -0.66780525]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0125694  -0.678546   -0.6535769  -0.73178667  0.03095378 -1.1451119\n",
            " -1.050414   -0.751725   -0.7579821  -0.7574862  -0.81140316 -0.91514605\n",
            " -0.6757608  -0.641836   -0.86019623 -0.6146993  -0.7236034  -0.46312338\n",
            " -0.66904515]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0259144  -0.8827677  -1.034397   -1.0279917   0.13144234 -0.9971173\n",
            " -0.7735422  -0.9096641  -0.94355816 -0.790704   -0.77376163 -1.0764738\n",
            " -1.22132    -0.73481923 -0.8772687  -0.9176248  -0.84387046 -0.38193357\n",
            " -0.8073529 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8599196  -0.7473141  -0.67954236 -0.8743168   0.11948363 -1.2256703\n",
            " -0.9473177  -0.7485136  -0.7459595  -0.7356961  -0.76234204 -0.9549285\n",
            " -0.8075011  -0.6994584  -0.7622197  -0.6676045  -0.80326647 -0.44772398\n",
            " -0.72575426]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.77930784 -0.76457745 -1.0198133  -0.9230286   0.06997864 -0.90715426\n",
            " -0.6942386  -0.7252043  -0.6632755  -0.62313783 -0.7833913  -1.0070757\n",
            " -0.8475968  -0.65454054 -0.9013779  -0.7928017  -0.7431927  -0.2544555\n",
            " -0.6440375 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.83436435 -0.7169556  -0.7221869  -0.81273925 -0.02998606 -1.0642356\n",
            " -0.8528903  -0.6330802  -0.61304784 -0.71808493 -0.7889635  -0.9337468\n",
            " -0.71452665 -0.6399274  -0.79634094 -0.60682297 -0.7771692  -0.39539886\n",
            " -0.59500295]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7777187  -0.7629527  -1.0173942  -0.92101914  0.08737443 -0.90568197\n",
            " -0.6927028  -0.72371024 -0.66179526 -0.62224376 -0.7819168  -1.0048935\n",
            " -0.8456998  -0.6536304  -0.89957553 -0.7912876  -0.7418742  -0.24905974\n",
            " -0.64283097]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9305574  -0.61765647 -0.71685827 -0.85347515  0.05069891 -0.9793382\n",
            " -0.9881022  -0.60867375 -0.80467606 -0.82358944 -0.7960762  -0.9608982\n",
            " -0.62099    -0.7249562  -0.78837514 -0.57015204 -0.75618696 -0.46870887\n",
            " -0.6893811 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0935721  -0.6917517  -1.2998625  -0.92629904  0.05383736 -1.2548666\n",
            " -1.0087355  -0.84464985 -1.0502552  -0.667367   -0.8199749  -1.0602615\n",
            " -1.0716584  -0.71986204 -0.9644221  -0.7669359  -0.75445795 -0.20911661\n",
            " -0.7261306 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9196667  -0.6688882  -0.83133763 -0.8276761  -0.09671997 -1.1932533\n",
            " -1.1311705  -0.69979054 -0.79487383 -0.7335353  -0.8567615  -0.89475983\n",
            " -0.6692089  -0.7058775  -0.91989595 -0.64503855 -0.77343607 -0.41252828\n",
            " -0.73078305]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.93210846 -0.7041799  -1.2602154  -1.013041    0.018734   -1.1210382\n",
            " -1.0205278  -0.7543334  -0.78956616 -0.65287316 -1.0013645  -1.0472523\n",
            " -0.82852733 -0.7248246  -1.0791206  -0.7783448  -0.781942   -0.18858105\n",
            " -0.7114168 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.0199647  -0.6312053  -0.71924    -0.7649042   0.05355506 -0.9972528\n",
            " -1.0260334  -0.62976503 -0.75270796 -0.7595724  -0.79119515 -0.8798695\n",
            " -0.58085734 -0.67240304 -0.82430375 -0.56110626 -0.69140065 -0.46666345\n",
            " -0.72383565]\n",
            "Epsilon reduced to 0.03486784401000002\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0157661  -0.576445   -0.96754456 -0.8744234  -0.19592507 -0.9333601\n",
            " -1.0883559  -0.68267035 -0.80598855 -0.8283986  -0.89563596 -0.9686279\n",
            " -0.70127374 -0.8018284  -0.8260956  -0.67622375 -0.8369664  -0.38122115\n",
            " -0.7580141 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.92473423 -0.88065475 -0.9081481  -0.9765122  -0.2046341  -1.0149368\n",
            " -0.809147   -1.059707   -0.9297157  -0.72327816 -0.6954301  -1.1420085\n",
            " -0.9941875  -0.7681478  -0.8077195  -0.87087244 -0.89047307 -0.39795744\n",
            " -0.6791718 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.968163   -0.61041355 -1.0544211  -0.98795843 -0.24659285 -0.96672535\n",
            " -0.94718206 -0.7169288  -0.866066   -0.66748714 -0.8601117  -1.0376354\n",
            " -0.62054175 -0.76708055 -0.8487511  -0.70652884 -0.9886956  -0.40935236\n",
            " -0.8338546 ]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.1725732  -0.7419486  -1.453158   -1.0526707  -0.42332184 -1.2896391\n",
            " -1.1308228  -0.9682922  -1.0258311  -0.7155473  -0.9659405  -1.2633122\n",
            " -0.85988164 -0.94112444 -1.101279   -0.8501797  -0.81563365 -0.2887293\n",
            " -0.73878837]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.1871994  -0.6358689  -0.865649   -0.93759805 -0.4899816  -1.2733796\n",
            " -1.1512018  -0.8138843  -0.94097126 -0.92940867 -0.83662015 -1.0862807\n",
            " -0.77426785 -0.81007785 -0.9023588  -0.8429774  -0.8773583  -0.5130764\n",
            " -0.78058124]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.9162448  -0.6009811  -0.9525458  -0.9266149  -0.20219347 -0.85857856\n",
            " -0.49947125 -0.73552984 -0.86287713 -0.68067133 -0.5413538  -0.99896467\n",
            " -0.76138073 -0.6357562  -0.697049   -0.7714024  -0.763953   -0.32289106\n",
            " -0.62019604]\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.89729106 -0.69008493 -0.65143526 -0.8035505  -0.491223   -1.1628175\n",
            " -0.819397   -0.6478862  -0.696316   -0.70234776 -0.7096108  -0.8855418\n",
            " -0.70045793 -0.60597175 -0.83465487 -0.63590914 -0.83963823 -0.36218804\n",
            " -0.67285264]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8556606  -0.7126274  -0.9648354  -0.9770802  -0.48231965 -0.88376397\n",
            " -0.624825   -0.7385073  -0.86749375 -0.70919394 -0.7353587  -1.0681888\n",
            " -0.81359965 -0.6973954  -0.8584709  -0.8523829  -0.8329545  -0.30674988\n",
            " -0.6841049 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.9110286  -0.71825397 -0.7254411  -0.8984266  -0.59015965 -1.1372445\n",
            " -0.92246956 -0.6797784  -0.66268384 -0.78019476 -0.84357667 -0.95103705\n",
            " -0.727698   -0.7193147  -0.84086865 -0.6885595  -0.8808731  -0.4010407\n",
            " -0.6510112 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.0026021  -0.64762056 -1.2017527  -1.0710272  -0.7102138  -1.0994662\n",
            " -0.94897515 -0.76716393 -0.9957942  -0.73987603 -0.95875233 -1.1121747\n",
            " -0.7924316  -0.7619121  -1.0325787  -0.83854824 -0.87210935 -0.24960636\n",
            " -0.7532325 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.029149   -0.7134664  -0.71264195 -0.9187236  -0.66470426 -1.2168204\n",
            " -1.0980052  -0.7751531  -0.8469024  -0.8688506  -0.84851235 -0.9686239\n",
            " -0.7850326  -0.83178973 -0.75938725 -0.69872284 -0.88681245 -0.46894383\n",
            " -0.85941285]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8875129  -0.71368873 -1.0036584  -0.98261046 -0.8253236  -0.95158875\n",
            " -0.6517237  -0.8017381  -0.86739206 -0.7559562  -0.7566497  -1.1124517\n",
            " -0.8531016  -0.683827   -0.8535473  -0.8556648  -0.8889804  -0.29039422\n",
            " -0.6845747 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8031492  -0.7013875  -0.6766745  -0.7798506  -0.7550512  -1.0884455\n",
            " -0.89393586 -0.61390007 -0.6083859  -0.7202405  -0.76254255 -0.8620381\n",
            " -0.6661791  -0.6125896  -0.77056336 -0.622319   -0.83205664 -0.32430792\n",
            " -0.6164044 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.9280509  -0.7301719  -1.0448754  -1.0396926  -0.8315133  -1.024923\n",
            " -0.755557   -0.84738374 -0.9183724  -0.7718761  -0.80844486 -1.1431304\n",
            " -0.9449992  -0.77340084 -0.9033119  -0.92531514 -0.94167763 -0.31801185\n",
            " -0.72396314]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8927106  -0.64855266 -0.8363346  -0.86527497 -0.97118616 -1.2359262\n",
            " -1.1399443  -0.69370794 -0.76803684 -0.84147704 -0.88470274 -0.9272027\n",
            " -0.6944564  -0.73848426 -0.7917726  -0.6609105  -0.9442665  -0.37536126\n",
            " -0.70832574]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.086619   -0.7169839  -1.0956327  -1.0852112  -0.86216456 -1.1344857\n",
            " -0.66835755 -0.8907236  -0.997838   -0.8125247  -0.78262466 -1.1988869\n",
            " -0.9834305  -0.8150195  -0.97784436 -0.9388645  -0.95963955 -0.3298766\n",
            " -0.7568842 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.1662198  -0.579442   -0.7869532  -0.9178536  -1.1362193  -1.1768657\n",
            " -1.029803   -0.65284073 -0.891502   -0.7416984  -0.8081611  -0.96433014\n",
            " -0.6034379  -0.77865237 -0.8230369  -0.58972824 -0.79539436 -0.38514385\n",
            " -0.7646013 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.9579218  -0.7191458  -1.021987   -1.0974952  -0.9517404  -1.0871716\n",
            " -0.5662942  -0.84274924 -1.0163188  -0.73378146 -0.70029837 -1.195545\n",
            " -0.8552684  -0.7358918  -1.01267    -0.9067219  -0.9197017  -0.32581607\n",
            " -0.7652801 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7034039  -0.58856755 -0.73895484 -0.7937597  -1.008548   -1.0649085\n",
            " -1.0341614  -0.6628571  -0.7606926  -0.6528826  -0.69549954 -0.91043776\n",
            " -0.61919975 -0.63484836 -0.68281484 -0.6489399  -0.8142716  -0.32928914\n",
            " -0.7200092 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.9342841  -0.75663495 -1.0073794  -1.0295136  -0.89892226 -0.9585491\n",
            " -0.6884939  -0.8000484  -0.8682163  -0.7510953  -0.8409206  -1.0885298\n",
            " -0.86645097 -0.7646362  -0.95637774 -0.90887165 -0.8981482  -0.25654492\n",
            " -0.7261787 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.0347872  -0.55616724 -0.70623976 -0.88396835 -1.1986295  -0.90905446\n",
            " -0.9626659  -0.6104472  -0.7192896  -0.74312127 -0.85513073 -0.85947204\n",
            " -0.5727142  -0.7836641  -0.8149334  -0.55960816 -0.7998259  -0.25414777\n",
            " -0.7003666 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8545742  -0.5532805  -1.006844   -0.9863933  -1.0842967  -1.0058122\n",
            " -0.8410891  -0.7099745  -0.97201383 -0.671819   -0.86408496 -1.0767955\n",
            " -0.73470443 -0.66346633 -0.91030204 -0.73188275 -0.8666107  -0.18276149\n",
            " -0.6953008 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7414338  -0.6557169  -0.6237675  -0.7523226  -0.96838033 -1.0788829\n",
            " -0.90891135 -0.6118555  -0.6637824  -0.59512067 -0.66906065 -0.79301345\n",
            " -0.60997474 -0.6222819  -0.6915727  -0.54330313 -0.77289236 -0.26132673\n",
            " -0.61606735]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8037011  -0.79945207 -0.6094406  -0.9621205  -1.2575104  -0.91646767\n",
            " -0.7257166  -0.9154505  -0.9454972  -0.74430716 -0.6189738  -0.98301715\n",
            " -0.9869934  -0.72694796 -0.7404227  -0.8357881  -0.84787595 -0.28722912\n",
            " -0.74750245]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6607992  -0.53307223 -0.68537307 -0.75245893 -1.1513278  -0.8565415\n",
            " -0.92998636 -0.49541625 -0.71421117 -0.66491735 -0.6837312  -0.84882796\n",
            " -0.47038448 -0.5378865  -0.71314454 -0.59161526 -0.7202425  -0.20903844\n",
            " -0.60551107]\n",
            "Epsilon reduced to 0.03138105960900001\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0234712  -0.6238122  -0.70506704 -1.0282536  -1.1476244  -1.0718046\n",
            " -1.086259   -0.8317601  -0.79177487 -0.76128066 -0.88849854 -1.1336279\n",
            " -0.8943849  -0.8295881  -0.5867287  -0.83375496 -1.0980107  -0.15278178\n",
            " -0.75220466]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.96788013 -0.64641404 -0.7519232  -0.75250983 -0.6092546  -1.0532733\n",
            " -1.0866686  -0.60280204 -0.56796914 -0.64984035 -0.7184555  -0.95927536\n",
            " -0.7781936  -0.6526017  -0.8549219  -0.6900677  -0.8657721  -0.2656862\n",
            " -0.6875645 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.94708455 -0.6324308  -0.6965447  -1.0266182  -0.84647346 -0.9889214\n",
            " -0.7799999  -0.78820246 -0.7825808  -0.53748816 -0.7926986  -0.9377121\n",
            " -0.54561985 -0.7768982  -0.75862724 -0.684258   -1.0447559  -0.19074574\n",
            " -0.76849085]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9928139  -0.677469   -0.6207348  -0.7584635  -1.0577979  -0.96517354\n",
            " -1.1031163  -0.7417946  -0.72028375 -0.77459514 -0.7129454  -0.83613294\n",
            " -0.8515428  -0.66571486 -0.71344995 -0.67645305 -0.7316189  -0.37890455\n",
            " -0.7709985 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.81634283 -0.63682204 -0.5929459  -0.9294705  -0.847378   -0.90100116\n",
            " -0.79483324 -0.6617487  -0.7218552  -0.50997317 -0.7251002  -0.84065497\n",
            " -0.5429729  -0.74817365 -0.6336614  -0.6668425  -0.9514004  -0.15043658\n",
            " -0.68851393]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.88761127 -0.6224443  -0.86256313 -0.74801874 -0.593168   -1.0289658\n",
            " -0.9585427  -0.5930955  -0.64969164 -0.65834    -0.7096384  -0.9040803\n",
            " -0.70595235 -0.5580499  -0.8248863  -0.7213715  -0.7571769  -0.3621424\n",
            " -0.64647406]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7620359  -0.5938281  -0.48085248 -0.774202   -0.95505327 -0.84447116\n",
            " -0.8741746  -0.6210096  -0.6586102  -0.55772096 -0.6744006  -0.84365493\n",
            " -0.6856528  -0.6366444  -0.5110662  -0.6344051  -0.88509023 -0.09575865\n",
            " -0.6547901 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.928645   -0.6468313  -0.73502755 -0.71181065 -0.5443369  -0.965845\n",
            " -1.0198883  -0.5549009  -0.5264935  -0.6114595  -0.6744952  -0.91534215\n",
            " -0.68829906 -0.6146216  -0.84244233 -0.65886873 -0.85247195 -0.27197972\n",
            " -0.6407992 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7588455  -0.5772363  -0.6256813  -0.9044952  -0.8299496  -0.908602\n",
            " -0.78141814 -0.6768376  -0.7628178  -0.5143348  -0.71109897 -0.7855536\n",
            " -0.5432507  -0.7113726  -0.60175025 -0.62574035 -0.92813945 -0.18606892\n",
            " -0.6515366 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1758425  -0.5246819  -0.8775928  -0.83473223 -1.0737383  -0.9065106\n",
            " -0.8450074  -0.60696197 -0.7145932  -0.6701629  -0.84147656 -0.91908133\n",
            " -0.5666408  -0.7301063  -0.8203347  -0.56520796 -0.83231795 -0.2994786\n",
            " -0.7314016 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8957556  -0.57139075 -0.60213685 -0.90700185 -0.82341486 -1.0084358\n",
            " -0.78425837 -0.7239225  -0.8034241  -0.5919079  -0.7094779  -0.8117074\n",
            " -0.632979   -0.7456146  -0.67899555 -0.68038476 -0.942695   -0.2326014\n",
            " -0.70272744]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9237807  -0.43630192 -0.8125981  -0.8227606  -1.0211816  -0.8231783\n",
            " -0.6626785  -0.5647027  -0.53993547 -0.6881496  -0.7286719  -0.85569715\n",
            " -0.54192257 -0.68531877 -0.71653175 -0.572932   -0.73776126 -0.26092625\n",
            " -0.6327931 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8844613  -0.6838231  -0.5779122  -0.9970845  -1.0347775  -0.9782845\n",
            " -0.8153918  -0.7188111  -0.75057876 -0.67348313 -0.9503114  -0.9843823\n",
            " -0.65974927 -0.83771956 -0.6023401  -0.7458349  -1.0435163  -0.25604978\n",
            " -0.82020444]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.68656445 -0.6924571  -0.5511255  -0.6651451  -0.8358241  -0.9369292\n",
            " -1.0620193  -0.6731297  -0.5880768  -0.6078365  -0.67370886 -0.69766223\n",
            " -0.8312989  -0.47125733 -0.6071651  -0.65722275 -0.77015984 -0.20314248\n",
            " -0.696561  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7738219  -0.594559   -0.5542784  -0.90869135 -0.84013486 -0.8930958\n",
            " -0.75785935 -0.66487634 -0.73221517 -0.5256463  -0.6613035  -0.812081\n",
            " -0.53685254 -0.7149951  -0.64497685 -0.6474153  -0.89787006 -0.24940611\n",
            " -0.6232319 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.84922063 -0.59206516 -0.7334444  -0.69148505 -0.73841405 -0.9847283\n",
            " -0.8173123  -0.52850217 -0.5646573  -0.56915176 -0.7095703  -0.794594\n",
            " -0.5492582  -0.52952397 -0.8015888  -0.5948237  -0.7914243  -0.21048298\n",
            " -0.6461664 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7205731  -0.5824285  -0.5448073  -0.8224667  -1.1513461  -0.8890688\n",
            " -0.76195765 -0.69121045 -0.5863105  -0.68231916 -0.8234534  -0.878843\n",
            " -0.607103   -0.64504445 -0.5331601  -0.6039267  -0.8810216  -0.25036496\n",
            " -0.6443121 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.77873117 -0.74576855 -0.517182   -0.65298295 -0.9370347  -0.955811\n",
            " -1.0806103  -0.673021   -0.58844244 -0.6285808  -0.6806996  -0.72059816\n",
            " -0.85295326 -0.5638083  -0.68770075 -0.59478194 -0.8243226  -0.12944081\n",
            " -0.7198926 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6791356  -0.5515486  -0.5582598  -0.75413954 -0.7663716  -0.82410187\n",
            " -0.68234825 -0.5577139  -0.69184613 -0.47581643 -0.6494504  -0.6684364\n",
            " -0.48592824 -0.6267903  -0.55483377 -0.6665118  -0.7970338  -0.27393907\n",
            " -0.6167645 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1159427  -0.55287457 -0.8395587  -0.7777434  -1.1078317  -0.8298007\n",
            " -0.94295675 -0.6072514  -0.65354216 -0.6839826  -0.83988655 -0.9137806\n",
            " -0.5578332  -0.7565     -0.780424   -0.5227106  -0.82828534 -0.24499485\n",
            " -0.716035  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8364576  -0.5835144  -0.54585534 -0.8679588  -1.1885455  -0.92798626\n",
            " -0.7608515  -0.7364756  -0.63799006 -0.74253523 -0.8293202  -1.0018594\n",
            " -0.6265639  -0.6942419  -0.6460413  -0.6217018  -0.88572085 -0.29002988\n",
            " -0.67560214]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.94895166 -0.7410455  -0.597994   -0.7346666  -1.0063866  -1.118437\n",
            " -1.07618    -0.78819054 -0.7078218  -0.7089331  -0.7365248  -0.8055481\n",
            " -0.9286571  -0.64064336 -0.7612454  -0.63773155 -0.9187403  -0.16382292\n",
            " -0.78418547]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7643707  -0.601787   -0.55004084 -0.90185815 -1.0829337  -0.9603076\n",
            " -0.78030163 -0.73838204 -0.5718697  -0.6002799  -0.7541967  -0.92735255\n",
            " -0.62009823 -0.64870757 -0.67380756 -0.6494538  -0.94006574 -0.20316668\n",
            " -0.5828381 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.74696213 -0.73224103 -0.54564625 -0.6990441  -0.883915   -0.9995155\n",
            " -1.0690563  -0.6722032  -0.556694   -0.6306272  -0.74474937 -0.72330165\n",
            " -0.8493198  -0.6219613  -0.7413314  -0.6848274  -0.8257334  -0.14246842\n",
            " -0.7271343 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.81693137 -0.6161697  -0.6582459  -0.89034843 -1.1519902  -0.941772\n",
            " -0.74529994 -0.7472896  -0.62308174 -0.69152486 -0.87162834 -0.91124064\n",
            " -0.61180854 -0.69029903 -0.5991819  -0.6373881  -0.9395585  -0.2180404\n",
            " -0.6764755 ]\n",
            "Epsilon reduced to 0.028242953648100012\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8650677  -0.7305738  -0.6147383  -0.810865   -1.3271955  -0.94452214\n",
            " -0.64893657 -0.8155312  -0.565121   -0.66693914 -0.5414111  -0.8429779\n",
            " -0.7831487  -0.7125794  -0.668383   -0.7495577  -0.74357975 -0.18240334\n",
            " -0.63946915]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8706802  -0.64470696 -0.7127133  -0.9163032  -1.187241   -1.0305738\n",
            " -0.62258047 -0.6733182  -0.54985154 -0.7133746  -0.7943887  -1.0204095\n",
            " -0.70462614 -0.70129895 -0.8072665  -0.6542472  -0.8920758  -0.15193714\n",
            " -0.6284037 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8316105  -0.71351004 -0.52569515 -0.87631375 -1.1810417  -0.98346704\n",
            " -0.8855518  -0.80053705 -0.6708742  -0.6007079  -0.6037329  -0.83498347\n",
            " -0.862033   -0.75255203 -0.722999   -0.7304678  -0.8066697  -0.2329865\n",
            " -0.6697918 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9281045  -0.57804227 -0.70056796 -0.8999312  -1.2823931  -0.8657758\n",
            " -0.79262084 -0.5849395  -0.59416735 -0.61460316 -0.8203242  -0.977269\n",
            " -0.6726403  -0.8393664  -0.70364296 -0.5535395  -0.73365986 -0.22455907\n",
            " -0.68446255]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.84645206 -0.5718268  -0.62783164 -0.9001687  -1.1298094  -0.94039404\n",
            " -0.5424297  -0.6555502  -0.55357844 -0.5701213  -0.654866   -0.886888\n",
            " -0.56678927 -0.72751945 -0.84045625 -0.59828067 -0.73150456 -0.19035125\n",
            " -0.5674837 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.76668376 -0.57189465 -0.6006946  -0.8360595  -1.2386386  -0.758532\n",
            " -0.53291345 -0.50400996 -0.48419785 -0.63281    -0.83831435 -0.8712903\n",
            " -0.5828637  -0.67364305 -0.8013554  -0.5120822  -0.6752715  -0.15846087\n",
            " -0.63196826]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.74392724 -0.6830305  -0.42780596 -0.9343444  -1.3424644  -0.941333\n",
            " -0.7413204  -0.82919014 -0.60538054 -0.56926006 -0.61947703 -0.88937134\n",
            " -0.8280072  -0.6292153  -0.68278444 -0.5962246  -0.8106092  -0.19425485\n",
            " -0.60934085]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.82569337 -0.6432868  -0.65458214 -0.8617059  -1.1384016  -0.93121254\n",
            " -0.55434346 -0.6037547  -0.4956286  -0.6720922  -0.7656661  -0.96203166\n",
            " -0.6342842  -0.6481508  -0.7775508  -0.60234755 -0.82754135 -0.09322388\n",
            " -0.6045822 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7335663  -0.69587386 -0.43365565 -0.870228   -1.2856088  -0.909798\n",
            " -0.78828144 -0.7853646  -0.53208107 -0.5707666  -0.6781285  -0.8318564\n",
            " -0.87721795 -0.6157795  -0.6333081  -0.5847845  -0.81087637 -0.11591054\n",
            " -0.61609435]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8528992  -0.56564057 -0.6124455  -0.8670473  -1.2309926  -0.81443685\n",
            " -0.75754464 -0.5486184  -0.55478746 -0.5786201  -0.77710915 -0.914878\n",
            " -0.62852883 -0.7725409  -0.7027642  -0.53677404 -0.68822396 -0.19566818\n",
            " -0.67838097]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7866439  -0.6001147  -0.6355245  -0.86406684 -1.2336123  -0.9547133\n",
            " -0.76551193 -0.7605908  -0.56693125 -0.5934982  -0.6253533  -0.9335938\n",
            " -0.6391705  -0.6627501  -0.78003514 -0.6410925  -0.73153603 -0.20838577\n",
            " -0.5862879 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.77513045 -0.6920947  -0.55846    -0.77146125 -0.9597816  -1.0184124\n",
            " -0.76687413 -0.64165545 -0.50261277 -0.57718885 -0.73936284 -0.8314449\n",
            " -0.786746   -0.7911705  -0.6193368  -0.6399031  -0.7474102  -0.1957927\n",
            " -0.6061117 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.748684   -0.7099638  -0.418922   -0.88670236 -1.3859912  -0.9689375\n",
            " -0.79667276 -0.84473544 -0.5321215  -0.62285566 -0.7242576  -0.8433527\n",
            " -0.88795465 -0.6703882  -0.6347084  -0.6296101  -0.8461809  -0.09605649\n",
            " -0.65016973]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.68711865 -0.6794038  -0.56121755 -0.78778774 -1.0312041  -0.91669154\n",
            " -0.696044   -0.604339   -0.4571222  -0.6102896  -0.84172255 -0.83535796\n",
            " -0.67913604 -0.6969601  -0.6890196  -0.59488165 -0.798602   -0.13146394\n",
            " -0.651612  ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7130046  -0.69801164 -0.38734773 -0.8315695  -1.1747112  -0.9143935\n",
            " -0.7463023  -0.77294934 -0.51456606 -0.5631011  -0.6868562  -0.8478786\n",
            " -0.90636086 -0.63582474 -0.5986335  -0.5837267  -0.77926326 -0.14144175\n",
            " -0.61968017]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.891946   -0.5801754  -0.68631136 -0.87565553 -1.2874403  -0.83289254\n",
            " -0.61514115 -0.55281067 -0.53290945 -0.6237416  -0.8641952  -0.9122108\n",
            " -0.61236143 -0.711654   -0.80578345 -0.540653   -0.75191474 -0.10060163\n",
            " -0.64687276]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7910851  -0.58029956 -0.64465976 -0.8792583  -0.8935131  -0.92812955\n",
            " -0.61067134 -0.6438664  -0.5191943  -0.58123636 -0.69860244 -0.9769097\n",
            " -0.65816575 -0.698792   -0.70402616 -0.5437335  -0.73747957 -0.23524603\n",
            " -0.6102888 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6486288  -0.68710744 -0.49937278 -0.7593827  -0.9961881  -0.8804387\n",
            " -0.66303116 -0.5309788  -0.37206382 -0.56115556 -0.77237517 -0.8241191\n",
            " -0.6570169  -0.67618537 -0.70599973 -0.6143307  -0.7577455  -0.08846563\n",
            " -0.6071345 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.75170505 -0.57814217 -0.6252928  -0.8563778  -1.175385   -0.8784858\n",
            " -0.6130373  -0.64499277 -0.4804569  -0.5548538  -0.73226106 -0.90081054\n",
            " -0.58571947 -0.6365783  -0.7388627  -0.5362559  -0.78255415 -0.11390495\n",
            " -0.5443984 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8081293  -0.5782106  -0.6613363  -0.8485526  -1.2729847  -0.7799059\n",
            " -0.70024604 -0.5409982  -0.49601048 -0.6218297  -0.8647448  -0.8952655\n",
            " -0.6100263  -0.7075304  -0.72921145 -0.5356298  -0.72918844 -0.10603852\n",
            " -0.6338052 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7197488  -0.6911186  -0.40334368 -0.85834205 -1.0817779  -0.9470089\n",
            " -0.7691954  -0.7734865  -0.5665147  -0.5600012  -0.6559586  -0.8894776\n",
            " -0.93755394 -0.64622283 -0.547248   -0.5889732  -0.75242805 -0.10941852\n",
            " -0.63905567]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8489276  -0.5662252  -0.67174983 -0.85709757 -1.2319041  -0.8123332\n",
            " -0.7189548  -0.56232214 -0.5219611  -0.65023077 -0.8444162  -0.92169636\n",
            " -0.63723785 -0.73442554 -0.7020626  -0.54095006 -0.72086525 -0.11497182\n",
            " -0.6453341 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.91836244 -0.70160735 -0.53092885 -0.8870377  -0.9505292  -1.0256883\n",
            " -0.8591443  -0.8345688  -0.5971753  -0.70903635 -0.71625644 -0.93312293\n",
            " -0.8958198  -0.80985284 -0.5771298  -0.60683453 -0.7820549  -0.29136685\n",
            " -0.71173275]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7850312  -0.6590268  -0.6718867  -0.8153776  -1.0602721  -0.96216124\n",
            " -0.7406386  -0.6502402  -0.47374147 -0.60267943 -0.7896976  -0.9058323\n",
            " -0.7526155  -0.7090195  -0.65350145 -0.6480703  -0.8312298  -0.1119325\n",
            " -0.6621217 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6765703  -0.7670301  -0.404698   -0.8219943  -0.8744376  -0.942287\n",
            " -0.7961061  -0.7119296  -0.47705305 -0.5998725  -0.6422409  -0.81859154\n",
            " -0.91298246 -0.64153206 -0.4816621  -0.64230126 -0.70638263 -0.13380809\n",
            " -0.6925849 ]\n",
            "Epsilon reduced to 0.025418658283290013\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.878      -0.6453146  -0.7533114  -0.7817404  -1.0903273  -1.048161\n",
            " -0.870015   -0.7155011  -0.5950537  -0.81222093 -0.83538985 -0.9622025\n",
            " -0.72735584 -0.6547338  -0.7129596  -0.70829785 -0.81298816 -0.0814513\n",
            " -0.7034146 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.758437   -0.59615564 -0.7244942  -0.84315753 -1.191167   -0.9148731\n",
            " -0.6571554  -0.711476   -0.7073718  -0.6217665  -0.7177302  -0.784701\n",
            " -0.63006103 -0.6243939  -0.6084539  -0.62270516 -0.9381362  -0.04952784\n",
            " -0.63227516]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.65632284 -0.7388576  -0.38785857 -0.7874239  -1.0664246  -1.0970329\n",
            " -0.7751035  -0.826783   -0.57327944 -0.83274424 -0.6669193  -0.8759687\n",
            " -0.80301183 -0.60187423 -0.5094898  -0.6734835  -0.8959868  -0.11509608\n",
            " -0.69253236]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.76799273 -0.5909158  -0.6588156  -0.72906613 -0.91272247 -0.8071983\n",
            " -0.7144056  -0.58617747 -0.586896   -0.56453156 -0.6494087  -0.67576677\n",
            " -0.5368643  -0.67371833 -0.5697893  -0.5808255  -0.78792894 -0.06220969\n",
            " -0.5778307 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7863922  -0.6228677  -0.46306795 -0.7903975  -1.2124245  -0.8522098\n",
            " -0.8703429  -0.6976648  -0.5876794  -0.70408094 -0.70105827 -0.808737\n",
            " -0.824494   -0.54874253 -0.5764844  -0.5616961  -0.7197983  -0.10405501\n",
            " -0.742211  ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8216535  -0.65337664 -0.66284764 -0.74015826 -1.0515759  -0.80058795\n",
            " -0.6108775  -0.5618451  -0.6248841  -0.6002538  -0.7453649  -0.720408\n",
            " -0.5184805  -0.6310673  -0.6297996  -0.62053555 -0.8056185  -0.05512521\n",
            " -0.63422346]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.87528473 -0.5177051  -0.6460663  -0.79784846 -1.2312636  -0.87785214\n",
            " -0.67087096 -0.59185714 -0.59776133 -0.68913007 -0.79483944 -0.88496506\n",
            " -0.56831354 -0.5941248  -0.7325685  -0.5528751  -0.7215166  -0.06579416\n",
            " -0.6705613 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.9291736  -0.56209564 -0.7663198  -0.87216705 -1.1062238  -0.9401667\n",
            " -0.6391901  -0.7223745  -0.7587728  -0.63737917 -0.69200104 -0.851526\n",
            " -0.6719568  -0.61278546 -0.66508317 -0.6093281  -0.94085526  0.00444676\n",
            " -0.6665663 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8958606  -0.6267231  -0.49225098 -0.8663685  -1.3850303  -0.8433115\n",
            " -0.7680899  -0.7195081  -0.59904766 -0.7354059  -0.8152366  -0.81625473\n",
            " -0.76684374 -0.72351605 -0.6389421  -0.5832068  -0.79877543 -0.070645\n",
            " -0.73638463]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.74953204 -0.55192804 -0.67959994 -0.7666173  -1.0907953  -0.8091916\n",
            " -0.7143953  -0.63502026 -0.5690001  -0.5482927  -0.6470637  -0.75327235\n",
            " -0.6004163  -0.56559634 -0.5206253  -0.50611484 -0.8061038   0.04289736\n",
            " -0.606104  ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6874744  -0.57144773 -0.6799476  -0.7510457  -0.9959812  -0.9564165\n",
            " -0.78369325 -0.6382587  -0.5619555  -0.6975883  -0.75500965 -0.89163333\n",
            " -0.7083329  -0.5591022  -0.6090299  -0.6083189  -0.76554394 -0.08223979\n",
            " -0.6564995 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.63327295 -0.5638956  -0.63255215 -0.72364914 -1.1404889  -0.84358734\n",
            " -0.7150286  -0.62731266 -0.5579426  -0.54963875 -0.6599313  -0.68909526\n",
            " -0.59228194 -0.58920795 -0.49153274 -0.53703946 -0.78387225 -0.00127805\n",
            " -0.57167   ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8521933  -0.64310133 -0.5307276  -0.901591   -1.3356572  -0.95266694\n",
            " -0.81777453 -0.7599729  -0.6846161  -0.81375515 -0.7451547  -0.96331406\n",
            " -0.7378791  -0.75486624 -0.55457175 -0.65934694 -0.85940313 -0.08601967\n",
            " -0.6873445 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.88266873 -0.58683    -0.7021448  -0.79932654 -1.2562094  -0.8240131\n",
            " -0.62584376 -0.68750125 -0.6530602  -0.67396593 -0.7204983  -0.7985985\n",
            " -0.5964468  -0.65398365 -0.58702785 -0.5936585  -0.85703874  0.00379771\n",
            " -0.6653974 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.84761703 -0.50891876 -0.6174139  -0.8022379  -1.2930791  -0.7373479\n",
            " -0.71441984 -0.6166134  -0.57386225 -0.69781125 -0.82097995 -0.8542362\n",
            " -0.56970453 -0.66261774 -0.5952018  -0.4919352  -0.74155784 -0.0714431\n",
            " -0.6831751 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-8.8377559e-01 -5.4479098e-01 -7.4868822e-01 -8.0977064e-01\n",
            " -1.1385257e+00 -8.6890739e-01 -6.3971102e-01 -6.9103581e-01\n",
            " -6.1676055e-01 -5.6928384e-01 -6.6209936e-01 -7.7994448e-01\n",
            " -6.1628151e-01 -6.0580117e-01 -5.8727747e-01 -5.2635986e-01\n",
            " -8.4842086e-01  5.9731118e-04 -5.7646447e-01]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6512613  -0.57540786 -0.62955415 -0.7492989  -0.9804216  -0.91743547\n",
            " -0.73245543 -0.60496765 -0.52038395 -0.67826915 -0.7517863  -0.8646926\n",
            " -0.6655918  -0.53481704 -0.580732   -0.58922553 -0.7542912  -0.05324144\n",
            " -0.645628  ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8624108  -0.60768795 -0.74239993 -0.7978969  -0.9940445  -0.7799266\n",
            " -0.5499825  -0.60765845 -0.66525793 -0.60939795 -0.6948894  -0.8503685\n",
            " -0.64793116 -0.57868886 -0.5699688  -0.58030224 -0.7917007  -0.01194748\n",
            " -0.64735806]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6876831  -0.6870694  -0.42322975 -0.7414467  -1.0734468  -0.9770859\n",
            " -0.8258739  -0.72165376 -0.5390721  -0.6968968  -0.73025167 -0.74903536\n",
            " -0.80365723 -0.5938525  -0.5393887  -0.6200074  -0.7905351  -0.04248257\n",
            " -0.7048273 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8976002  -0.62303495 -0.5614289  -0.9364461  -0.9228715  -1.0227944\n",
            " -0.75913835 -0.81828964 -0.860019   -0.67828107 -0.6363824  -0.87116975\n",
            " -1.0404336  -0.6384483  -0.55925614 -0.6747179  -0.8687128   0.0728241\n",
            " -0.7176851 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8726475  -0.5064547  -0.62903917 -0.8124378  -1.2846901  -0.7633055\n",
            " -0.6045405  -0.5874849  -0.57065576 -0.6557039  -0.7808408  -0.84259003\n",
            " -0.55863553 -0.6357411  -0.6520821  -0.500047   -0.74284256 -0.01978071\n",
            " -0.65891516]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8646424  -0.5490343  -0.8236009  -0.8052342  -1.0996125  -0.8407996\n",
            " -0.6826718  -0.6876494  -0.636966   -0.59374243 -0.6223616  -0.843392\n",
            " -0.6496102  -0.54903007 -0.5227962  -0.5259364  -0.79064643 -0.00570099\n",
            " -0.5706411 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.68337256 -0.69864166 -0.5009974  -0.7319051  -0.9901025  -1.0161395\n",
            " -0.88646317 -0.7375011  -0.62327844 -0.7570791  -0.6375507  -0.8753236\n",
            " -0.7908293  -0.6056475  -0.45793638 -0.67204565 -0.7992941  -0.02370258\n",
            " -0.6447288 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.85346365 -0.57365495 -0.6590874  -0.881903   -0.87566173 -0.8582026\n",
            " -0.58581483 -0.6494014  -0.66812867 -0.63453203 -0.73484534 -0.93174773\n",
            " -0.690959   -0.61092323 -0.61869407 -0.6213797  -0.80212474 -0.01550082\n",
            " -0.6474993 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.67279017 -0.560308   -0.5969875  -0.73992866 -0.90863734 -0.93521357\n",
            " -0.7898987  -0.6356448  -0.5099927  -0.6906148  -0.69602907 -0.8204112\n",
            " -0.65489745 -0.57680297 -0.53553176 -0.5685868  -0.75771046 -0.08527779\n",
            " -0.67472076]\n",
            "Epsilon reduced to 0.022876792454961013\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8778845  -0.59222925 -0.7587218  -0.87110084 -1.222719   -0.88663787\n",
            " -0.83666456 -0.6563169  -0.74802417 -0.6233778  -0.9143055  -0.84619445\n",
            " -0.6423481  -0.72615427 -0.6584092  -0.65819806 -0.7094796  -0.08933125\n",
            " -0.6360981 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.696659   -0.61514604 -0.35721916 -0.78039885 -0.7791372  -0.83110607\n",
            " -0.69505596 -0.63853157 -0.6254516  -0.6140356  -0.69767404 -0.6815191\n",
            " -0.57788706 -0.6023084  -0.614242   -0.605087   -0.7536459  -0.11901175\n",
            " -0.549667  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.86751395 -0.50835955 -0.67130744 -0.77909    -0.94134736 -0.8789674\n",
            " -0.59143245 -0.55202806 -0.78510714 -0.5607805  -0.8039125  -0.72069687\n",
            " -0.5938642  -0.64688647 -0.64441633 -0.4822936  -0.66420865 -0.06089569\n",
            " -0.66975623]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7309361  -0.5789946  -0.32833773 -0.72944504 -0.9250367  -0.7689104\n",
            " -0.70145786 -0.598288   -0.5054997  -0.6428137  -0.74158424 -0.6995463\n",
            " -0.7401504  -0.62384546 -0.53858125 -0.4937837  -0.6598036  -0.05251462\n",
            " -0.6671098 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9545885  -0.5259278  -0.6650759  -0.727923   -0.88940966 -0.90173507\n",
            " -0.67818546 -0.56241006 -0.8261002  -0.52983165 -0.7374612  -0.7474337\n",
            " -0.60279673 -0.6176354  -0.63728046 -0.48542178 -0.66711164 -0.03644644\n",
            " -0.62474495]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7076842  -0.5951562  -0.4296599  -0.7605458  -0.6797125  -0.82061654\n",
            " -0.6534455  -0.62552184 -0.55203843 -0.5925096  -0.7661577  -0.6191676\n",
            " -0.65113866 -0.6214603  -0.5476968  -0.6072473  -0.8090416  -0.16121332\n",
            " -0.61245465]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.81951714 -0.54305613 -0.57562184 -0.7652174  -0.75953376 -0.9147825\n",
            " -0.75119674 -0.6070773  -0.72747076 -0.6248853  -0.78108037 -0.7172284\n",
            " -0.6414813  -0.7205457  -0.58044964 -0.5253016  -0.6248423  -0.12223974\n",
            " -0.65533   ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8010778  -0.5915197  -0.31530613 -0.7906354  -0.8803078  -0.87096894\n",
            " -0.67566794 -0.62688035 -0.61399466 -0.69785845 -0.8217171  -0.79550767\n",
            " -0.79803425 -0.6636895  -0.57852983 -0.5239095  -0.71008337 -0.05248267\n",
            " -0.68271834]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.76401126 -0.5730367  -0.4054643  -0.6421241  -0.6150095  -0.70602024\n",
            " -0.54923296 -0.5380388  -0.60004777 -0.5680608  -0.566037   -0.62280583\n",
            " -0.63211346 -0.6596208  -0.42925757 -0.49409688 -0.6268003  -0.16095921\n",
            " -0.53537256]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7954436  -0.5760678  -0.37865916 -0.8305336  -0.861478   -0.84523654\n",
            " -0.61981994 -0.6714382  -0.59025544 -0.65407836 -0.7299163  -0.6652945\n",
            " -0.69486046 -0.685321   -0.6039507  -0.51168925 -0.69608736 -0.17758831\n",
            " -0.68750554]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8786359  -0.5253626  -0.6431608  -0.70561415 -0.88015074 -0.8547512\n",
            " -0.75710726 -0.5526076  -0.79267013 -0.5295296  -0.7403758  -0.73434407\n",
            " -0.60178006 -0.6155696  -0.58068955 -0.48237348 -0.64809334 -0.05503367\n",
            " -0.6140025 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6955103  -0.62878793 -0.44251543 -0.7407436  -0.59204173 -0.8121075\n",
            " -0.67972976 -0.6138136  -0.6136452  -0.59586215 -0.71489656 -0.6698668\n",
            " -0.590543   -0.5983048  -0.5541067  -0.65852237 -0.81354547 -0.07809375\n",
            " -0.56897354]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.75782967 -0.62007195 -0.4775389  -0.6434943  -0.77295274 -0.7001925\n",
            " -0.53959036 -0.5299675  -0.54686815 -0.5871345  -0.603996   -0.65717953\n",
            " -0.6556904  -0.68986845 -0.474272   -0.5791476  -0.6130767  -0.22088932\n",
            " -0.6223047 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8258292  -0.5751371  -0.50944865 -0.8087327  -0.61108583 -0.84164566\n",
            " -0.56639653 -0.65036154 -0.5868334  -0.63207793 -0.75371736 -0.63964987\n",
            " -0.6651583  -0.6465013  -0.6111704  -0.6099964  -0.8158753  -0.20267265\n",
            " -0.63334936]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8677523  -0.52409315 -0.57523566 -0.68571067 -0.87663376 -0.8033737\n",
            " -0.5674594  -0.5671992  -0.65960467 -0.58893466 -0.7044893  -0.6708962\n",
            " -0.59796727 -0.6814774  -0.63012815 -0.48214    -0.61634904 -0.13437088\n",
            " -0.6426266 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.70887595 -0.5766492  -0.365129   -0.73761404 -0.8470825  -0.77390903\n",
            " -0.62594414 -0.5936069  -0.5790997  -0.6734432  -0.82689494 -0.7157429\n",
            " -0.6954321  -0.57864106 -0.51337516 -0.5133019  -0.73034275 -0.18360008\n",
            " -0.62016296]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8327661  -0.6066086  -0.56519866 -0.6866242  -0.61082137 -0.75173163\n",
            " -0.60582507 -0.5600625  -0.6486495  -0.6009362  -0.55796564 -0.7042641\n",
            " -0.6891997  -0.68113434 -0.4637617  -0.55256456 -0.6239579  -0.21030225\n",
            " -0.6128464 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.63163394 -0.5963839  -0.3425318  -0.68947387 -0.5795546  -0.7928226\n",
            " -0.6481468  -0.5782939  -0.56824315 -0.5719175  -0.6714752  -0.65104616\n",
            " -0.6020578  -0.6005079  -0.58601433 -0.6176116  -0.71120155 -0.06624725\n",
            " -0.5617567 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8431839  -0.64881027 -0.43976653 -0.67482996 -0.7742622  -0.73681027\n",
            " -0.5470919  -0.5910091  -0.6123616  -0.6416502  -0.6287012  -0.72463167\n",
            " -0.70431    -0.74256957 -0.5236505  -0.58792627 -0.6628008  -0.21881264\n",
            " -0.67114747]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7137581  -0.59864    -0.37902448 -0.7264984  -0.75990045 -0.81481594\n",
            " -0.6517853  -0.6092256  -0.5497798  -0.62884444 -0.7569756  -0.73580265\n",
            " -0.760083   -0.6079885  -0.47102824 -0.5675047  -0.77164173  0.02681531\n",
            " -0.6532502 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8852762  -0.5577322  -0.5499282  -0.6977228  -0.8198991  -0.81717694\n",
            " -0.6948693  -0.5089793  -0.6575937  -0.54483384 -0.70572436 -0.67411673\n",
            " -0.6196269  -0.70232433 -0.59365153 -0.46342915 -0.6150111  -0.12842995\n",
            " -0.6215526 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.701943   -0.5809475  -0.4584157  -0.7484963  -0.62803763 -0.7882479\n",
            " -0.61883515 -0.63294244 -0.5528765  -0.5918746  -0.7302564  -0.59568423\n",
            " -0.623592   -0.59652996 -0.53018975 -0.5828444  -0.78667855 -0.15829058\n",
            " -0.5599339 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.83833855 -0.7105273  -0.45774657 -0.65066546 -0.5091939  -0.7974756\n",
            " -0.47695112 -0.53169674 -0.5482462  -0.51936114 -0.51833904 -0.68357867\n",
            " -0.6587523  -0.6004219  -0.5485728  -0.57081234 -0.7194456  -0.10788611\n",
            " -0.58315593]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.73839164 -0.6488629  -0.41005084 -0.73414373 -0.7472093  -0.8149968\n",
            " -0.574669   -0.58145916 -0.61151433 -0.6617305  -0.8075934  -0.76963663\n",
            " -0.66081524 -0.6030737  -0.50921386 -0.6728507  -0.8323289  -0.02174193\n",
            " -0.6377123 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.70119625 -0.5624026  -0.4212575  -0.59304345 -0.6821934  -0.69091815\n",
            " -0.35626358 -0.4753106  -0.5264633  -0.49261323 -0.51387215 -0.6038268\n",
            " -0.5794325  -0.5799471  -0.46287948 -0.5330354  -0.57889605 -0.19202228\n",
            " -0.57439524]\n",
            "Epsilon reduced to 0.020589113209464913\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6721948  -0.6076124  -0.5004605  -0.6864859  -0.77400017 -0.90596926\n",
            " -0.61917347 -0.6361266  -0.56277823 -0.67883456 -0.6322007  -0.7520825\n",
            " -0.6572998  -0.56438345 -0.6145387  -0.48489213 -0.63075274 -0.267909\n",
            " -0.61408186]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.71605426 -0.5545559  -0.74255806 -0.6729302  -1.0227809  -0.9978086\n",
            " -0.89674133 -0.67999864 -0.7301371  -0.59262776 -0.76576626 -0.81273234\n",
            " -0.6456315  -0.6305804  -0.6766812  -0.55751365 -0.67633533  0.01052734\n",
            " -0.6214212 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.81112856 -0.6494825  -0.6012989  -0.7497665  -0.63634205 -0.9914086\n",
            " -0.52616173 -0.6507627  -0.5867102  -0.6778281  -0.70899373 -0.7809702\n",
            " -0.6540323  -0.6458808  -0.59103465 -0.53786534 -0.77456605 -0.20290403\n",
            " -0.65986174]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6881728  -0.5869309  -0.63104635 -0.76386106 -1.0054324  -1.0457054\n",
            " -0.7888569  -0.6419486  -0.7041232  -0.58521116 -0.8144574  -0.8033213\n",
            " -0.6082695  -0.63536155 -0.68940145 -0.54608876 -0.6863065  -0.00537786\n",
            " -0.62401825]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7428513  -0.62004447 -0.58114195 -0.7188604  -0.7955128  -0.7521836\n",
            " -0.39724272 -0.59355617 -0.5786497  -0.6528897  -0.74443406 -0.7420775\n",
            " -0.5481461  -0.59403753 -0.60548794 -0.5544703  -0.72707784 -0.25386018\n",
            " -0.6416454 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6379471  -0.69701755 -0.6257111  -0.63370556 -0.765571   -0.8238306\n",
            " -0.7247903  -0.5899225  -0.56093967 -0.6313595  -0.67807674 -0.79328835\n",
            " -0.53443366 -0.5807698  -0.62821007 -0.5866306  -0.60077703  0.0155298\n",
            " -0.57542795]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.58347076 -0.6083001  -0.43532476 -0.657645   -0.78008735 -0.8202391\n",
            " -0.5445928  -0.5715093  -0.4878912  -0.61838895 -0.6309464  -0.6858948\n",
            " -0.5336516  -0.5064168  -0.589871   -0.4530776  -0.613554   -0.21257733\n",
            " -0.57756996]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-6.7127454e-01 -6.8142629e-01 -5.1334918e-01 -6.2217724e-01\n",
            " -8.4488690e-01 -8.3389986e-01 -5.6182718e-01 -6.0075235e-01\n",
            " -5.4189992e-01 -5.3517413e-01 -5.8946472e-01 -7.2145289e-01\n",
            " -5.2554077e-01 -6.1818784e-01 -5.7111931e-01 -5.9541607e-01\n",
            " -6.4906108e-01 -6.6370197e-04 -5.7049698e-01]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6707163  -0.6275738  -0.53977066 -0.6441552  -0.63750076 -0.8101491\n",
            " -0.42893234 -0.5596033  -0.48040915 -0.58252716 -0.6588659  -0.6828775\n",
            " -0.46812278 -0.53170097 -0.5410462  -0.48872644 -0.69450355 -0.14604363\n",
            " -0.6001495 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5876383  -0.53276193 -0.69827306 -0.62312865 -0.9279121  -0.8985832\n",
            " -0.8462679  -0.56426567 -0.6305175  -0.58095956 -0.7624349  -0.7632555\n",
            " -0.5136009  -0.47991008 -0.6659907  -0.4649238  -0.6268702   0.02105915\n",
            " -0.57339036]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.666291   -0.6150738  -0.5344392  -0.61497694 -0.7302183  -0.6774731\n",
            " -0.36900705 -0.5196348  -0.5207888  -0.57710385 -0.66748047 -0.6621248\n",
            " -0.42893273 -0.55679226 -0.5618691  -0.5086846  -0.6486086  -0.17264473\n",
            " -0.59582037]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.66727453 -0.5394582  -0.7594133  -0.678918   -0.8867831  -0.96998835\n",
            " -0.9126934  -0.5863916  -0.6774765  -0.60035396 -0.7750805  -0.7934611\n",
            " -0.5311236  -0.54836595 -0.6816367  -0.5169289  -0.6708832  -0.00885228\n",
            " -0.5923269 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7248501  -0.57104576 -0.5116659  -0.68308914 -0.7117985  -0.8641648\n",
            " -0.4947946  -0.61353254 -0.55532503 -0.6260356  -0.6574145  -0.7226021\n",
            " -0.55678594 -0.6172732  -0.59581554 -0.45059407 -0.5992936  -0.22889793\n",
            " -0.5865534 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.61593944 -0.684381   -0.47649974 -0.66960275 -0.87495244 -0.886544\n",
            " -0.6317274  -0.64154106 -0.55343604 -0.5826475  -0.66374516 -0.7407461\n",
            " -0.5522729  -0.6394235  -0.5492065  -0.6117131  -0.705529   -0.0319464\n",
            " -0.58587116]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7144811  -0.5934013  -0.49456805 -0.6744448  -0.7901001  -0.7503971\n",
            " -0.34470132 -0.57141525 -0.52636963 -0.6044471  -0.67942244 -0.73440194\n",
            " -0.50446373 -0.57530266 -0.59546447 -0.5400218  -0.6773108  -0.14626247\n",
            " -0.629008  ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6371015  -0.6467769  -0.48568904 -0.5945086  -0.818104   -0.8028006\n",
            " -0.6175991  -0.6453924  -0.5507294  -0.59141153 -0.6572491  -0.7473477\n",
            " -0.50852156 -0.51993155 -0.58551073 -0.557886   -0.6258172  -0.0684936\n",
            " -0.53751427]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6629694  -0.6265528  -0.5365423  -0.6233603  -0.4330916  -0.8324463\n",
            " -0.3171954  -0.56767625 -0.48825276 -0.57031524 -0.5408827  -0.67573804\n",
            " -0.41339144 -0.515152   -0.55703557 -0.527726   -0.67384815 -0.02790493\n",
            " -0.53722376]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7174655  -0.53984725 -0.75424606 -0.6436067  -0.79509926 -0.9308211\n",
            " -0.74926025 -0.54028314 -0.6809119  -0.5653981  -0.7290398  -0.7343262\n",
            " -0.45325795 -0.5208237  -0.7439617  -0.499857   -0.65840805 -0.00961828\n",
            " -0.58866554]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5903977  -0.6386088  -0.4604918  -0.5988275  -0.6007987  -0.84199375\n",
            " -0.304556   -0.5951756  -0.45777    -0.5632683  -0.56433827 -0.6747469\n",
            " -0.39770797 -0.48958856 -0.5473196  -0.5516329  -0.7015587  -0.04447448\n",
            " -0.51256573]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7448615  -0.62299454 -0.523579   -0.6209361  -0.8449378  -0.7010637\n",
            " -0.56515753 -0.5102064  -0.5417328  -0.574461   -0.618788   -0.6916381\n",
            " -0.43403608 -0.56411636 -0.62778217 -0.50930274 -0.56967294 -0.04461841\n",
            " -0.5481248 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.66437626 -0.6348308  -0.410937   -0.6753963  -0.6575     -0.8665058\n",
            " -0.35101312 -0.64795583 -0.5644841  -0.60100317 -0.535817   -0.7867859\n",
            " -0.5466987  -0.5664276  -0.5325132  -0.60266083 -0.7169323  -0.04934626\n",
            " -0.55530053]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.56777334 -0.6289535  -0.5538384  -0.56153405 -0.7663032  -0.790577\n",
            " -0.6371518  -0.5562816  -0.5203942  -0.5869105  -0.5764669  -0.7125837\n",
            " -0.4941266  -0.5307143  -0.5357087  -0.55228335 -0.61722577 -0.01515141\n",
            " -0.5468104 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.63591826 -0.5822003  -0.522045   -0.61933494 -0.6172056  -0.7922996\n",
            " -0.40093535 -0.55314136 -0.5062623  -0.54682815 -0.63116294 -0.6628816\n",
            " -0.4562528  -0.57934105 -0.49829584 -0.46810752 -0.6296945  -0.13162947\n",
            " -0.582636  ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.63552654 -0.6532414  -0.551318   -0.61287475 -0.7067486  -0.8481309\n",
            " -0.6724838  -0.5586572  -0.55515695 -0.5996226  -0.6284384  -0.7390385\n",
            " -0.49438676 -0.5732179  -0.55439353 -0.54984504 -0.6346271  -0.02679543\n",
            " -0.56915563]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6133264  -0.63466    -0.38764942 -0.65461975 -0.7413683  -0.8074933\n",
            " -0.323373   -0.6297487  -0.42395198 -0.5390423  -0.55225784 -0.7171431\n",
            " -0.47899148 -0.5488194  -0.5072055  -0.54999167 -0.6837853  -0.0270476\n",
            " -0.46821296]\n",
            "Epsilon reduced to 0.01853020188851842\n",
            "Total reward: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLE963GfTvH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2e28720-6b53-4afd-f275-b379543eeac4"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=50, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=8, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_26(env), max_epsilon=0.1, epsilon_decay=0.9)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0827 07:48:33.117767 140127216002944 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0827 07:48:33.134547 140127216002944 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0827 07:48:33.146409 140127216002944 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0827 07:48:33.196032 140127216002944 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_1 (Reshape)          (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 187, 3)            12        \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 561)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 19)                10678     \n",
            "=================================================================\n",
            "Total params: 10,690\n",
            "Trainable params: 10,690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |████████████----------------------------------------------------------------------------------------| 12.5% \r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0827 07:48:45.049633 140127216002944 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0827 07:48:45.052090 140127216002944 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10309277  0.5984335   0.2602821   0.2871314   0.23329741 -0.30147278\n",
            " -0.10267039 -0.6985116  -0.87149316  0.15684015 -0.10640725  0.5068413\n",
            "  0.56695336  0.3792713  -0.56987524 -0.34771022 -0.3119429  -0.27110922\n",
            "  0.7924583 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.02578343 -0.19632375  0.02845808  0.05576573  0.3348491  -0.42152908\n",
            "  0.22095078  0.15249772 -0.88207215  0.14279985 -0.24246047  0.39012823\n",
            "  0.2450839   0.06260139 -0.51347774  1.1625648   0.34878966 -0.3526837\n",
            "  0.38910627]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.02284673  0.07801144  0.52321166  0.35037735  0.59441    -0.24195401\n",
            "  0.11019468  0.0686993  -0.58738154  0.5661438  -0.21393275  0.41433498\n",
            "  0.58088833  0.44216388 -0.5736586   1.1248904   0.31310713 -0.5120343\n",
            "  1.060168  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.0681624   0.2695218   0.29690626  0.75136817  0.45407754 -0.11254655\n",
            "  0.22631831  0.53353244 -0.9373065   0.01763123 -0.5736214   0.09701397\n",
            "  0.70851004  0.38407123 -0.62604946  1.0731214   0.16609864 -0.14974058\n",
            "  1.4710702 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5197679   0.03885453  0.7473523   0.30799654  0.2883933  -0.15980418\n",
            "  0.28705513 -0.10709891 -0.73321337  0.35290894 -0.34672934  0.19798027\n",
            "  0.6836842   0.1400034  -0.6311912   0.57871574  0.05848012 -0.3247341\n",
            "  1.5462828 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.16682135 -0.41423953  1.2284714   0.44458148  0.57198155 -0.5912231\n",
            " -0.00197969 -0.11967289 -0.90976465  0.37268716 -0.13426745  0.06903678\n",
            "  0.44767737 -0.04571014 -0.62620634  0.17430738  0.13568404 -0.5269051\n",
            "  1.6012273 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.28047183 -0.28212875  1.1776702   0.63998336  0.01738738 -0.6382774\n",
            "  0.0766708   0.09505562 -0.47904414  0.26144293 -0.20792991 -0.09796636\n",
            "  0.7414614  -0.04058193 -0.32552657  0.01880549  0.18832818 -0.5681596\n",
            "  0.88713294]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10123049  0.54753405  0.60618436 -0.04929936  0.01019817  0.22586781\n",
            "  0.16331005  0.05618334 -0.4516348  -0.64929104 -0.13419494  0.26367196\n",
            "  0.4597929   0.13945575 -0.5438485  -0.07718559  0.216504   -0.26180863\n",
            "  1.5748411 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.1287046  -0.21823889  1.1484394  -0.02913232  0.16043673 -0.7359766\n",
            "  0.2703511   0.08618735 -0.7372346  -0.37486896  0.34329915 -0.08247529\n",
            "  0.5710844   0.26849687 -0.4400847  -0.02483763  0.03814933 -0.39337096\n",
            "  0.18781888]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.07304712  0.25577614  0.7696906  -0.10159862  0.32994413 -0.18422325\n",
            " -0.05604991 -0.1046804  -0.2929011   0.01185233 -0.27058375  0.44285572\n",
            "  0.21648729  0.02182376 -0.376302   -1.0006067   0.36731526 -0.2019974\n",
            "  1.5872482 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.00975106 -0.04075722  1.1509856  -0.15276766 -0.02827019 -0.7116777\n",
            "  0.24443264 -0.2642091  -0.59982395 -0.2172286   0.3810901  -0.23426124\n",
            "  0.82564336 -0.01275436 -0.25565684 -0.263989    0.32354498 -0.6108615\n",
            "  0.23222412]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.21361554 -0.32406393  1.1883564   0.05854152  0.52887475 -0.9185926\n",
            "  0.1621168   0.06983348 -0.78921074  0.11655505  0.23279633  0.22731113\n",
            "  0.281854   -0.01093146 -0.55835813 -0.9932727   0.08375216 -0.64998233\n",
            " -0.00754277]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.07913616 -0.20200917  0.8355982  -0.03154985 -0.08473014 -0.41789323\n",
            "  0.21752574 -0.34613964 -0.4685254   0.06133015 -0.20920482  0.35779577\n",
            "  0.27354017 -0.1923537  -0.11559785 -0.8779228   0.18499334 -0.0693463\n",
            " -0.02082144]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.15261638  0.48625147  0.6753912   0.10121583  0.33691356 -0.16057852\n",
            "  0.02517034 -0.35488975 -0.60398537 -0.23105282 -0.1269094   0.51288974\n",
            "  0.24088725  0.03551403 -0.53808737 -1.6319352  -0.19841751 -0.25744832\n",
            "  0.17466474]\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.1936871   0.01477417  0.56873924 -0.12054396  0.1728006  -0.97304\n",
            "  0.20623319 -0.12527768 -0.06574933 -0.3336037   0.22390729  0.2831551\n",
            "  0.38879797 -0.22634327 -0.19411978 -0.5691484  -0.04499853 -0.37659103\n",
            " -0.79948646]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.09384604  0.04354106  0.18273742 -0.15494835  0.28241792 -0.35585135\n",
            "  0.3456669   0.03429083 -0.13409571 -0.1909657   0.11193293  0.13004363\n",
            " -0.00967561  0.17033029 -0.23977712 -0.66001374 -0.2532066  -0.14309947\n",
            " -0.54875094]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.22765872 -0.20811683 -0.11889551 -0.2229943  -0.04384078 -0.6058109\n",
            "  0.20186219 -0.16560563  0.09874378  0.07798804 -0.33005825  0.10597268\n",
            "  0.07903612  0.12886417 -0.18077856 -0.56941175 -0.2885666  -0.33086672\n",
            " -0.23990658]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.24787536 -0.11675199  0.18489635 -0.04159399  0.08411336 -0.09041694\n",
            " -0.12786928 -0.08910503 -0.09688689 -0.1382697   0.00374802  0.06654932\n",
            "  0.06059979 -0.3109079  -0.24390529 -0.18897405 -0.27029356 -0.09551072\n",
            " -0.45657772]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.17631325  0.13457641 -0.04722765 -0.15850484  0.48001814 -0.02444366\n",
            " -0.19500998  0.1705396  -0.09758694 -0.14939821 -0.12000382 -0.21795107\n",
            "  0.13227719 -0.19293374 -0.24805826 -0.44205186 -0.13118297  0.0949091\n",
            " -0.92638236]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.21529186 -0.15800816 -0.06857049 -0.06083737 -0.0567929  -0.21819769\n",
            " -0.2255254  -0.13178638  0.13787079 -0.03293803 -0.1912441   0.05355369\n",
            "  0.11880054 -0.21795107 -0.07495236 -0.30537498 -0.30508336 -0.12133196\n",
            " -0.41653755]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 14\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.44500104 -0.07280955  0.14905539 -0.30455753  0.03336415  0.14878044\n",
            " -0.4695289   0.15461016 -0.4459721   0.22350955 -0.24918553  0.07238764\n",
            "  0.1649419   0.05448893 -0.13295805  0.08956434 -0.43300807 -0.07274716\n",
            "  0.10473514]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.5730192   0.18368012 -0.19747314 -0.01157411  0.35638788 -0.00775354\n",
            " -0.683833    0.1858258  -0.4331801  -0.12796475 -0.33780813 -0.53733844\n",
            "  0.11882768  0.23311764 -0.15556453 -0.1489918  -0.32033464  0.10255435\n",
            " -1.1246737 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 4.3811819e-01  1.9643556e-01  6.9483794e-02  7.1209565e-02\n",
            " -5.8205798e-04 -2.1662898e-02 -6.4772022e-01 -2.1544348e-02\n",
            " -4.3706357e-01 -1.2732470e-01 -3.2065547e-01 -2.2500545e-01\n",
            "  1.8256260e-01  9.0480879e-02 -3.9094341e-01  1.4568712e-01\n",
            " -5.4392099e-01 -1.8186659e-02 -7.7737677e-01]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.2727939   0.18634306 -0.37399974  0.08820783  0.4055767  -0.07641587\n",
            " -0.657599   -0.12652001 -0.47277114 -0.03635527  0.16345897 -0.5525963\n",
            "  0.24535464  0.2639421  -0.44227588 -0.5591233  -0.08790064  0.15358764\n",
            " -0.6370516 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.15569092  0.41422787 -0.21411328  0.36973277  0.2746787   0.15958637\n",
            " -0.9187647   0.15564968 -0.42323107  0.08833365  0.06194963 -0.5742745\n",
            "  0.42368257  0.16751163 -0.69596314 -0.28758103 -0.34859562  0.3386407\n",
            " -0.9468915 ]\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 26 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3985242  -0.09275643 -0.32365036  0.25706786 -0.03347353 -0.2072836\n",
            " -0.6554702  -0.1959376  -0.40350613 -0.07546781  0.48301435 -0.9261155\n",
            "  0.20611055  0.46991628 -0.2649132  -0.9576473  -0.00637776  0.14109671\n",
            " -0.8385879 ]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.35838303  0.27361608 -0.04326362  0.32384083  0.0046033   0.1414554\n",
            " -0.899826    0.04916072 -0.7614691  -0.02659554 -0.07487741 -0.7533215\n",
            " -0.04559571  0.5632902  -0.6212521  -0.15419899 -0.41144076  0.11823738\n",
            " -0.88106024]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.40051067  0.06684858 -0.29156873  0.01652988 -0.1769212  -0.02154951\n",
            " -0.61755216 -0.19533783 -0.3329569   0.00247495  0.17073213 -0.5023782\n",
            " -0.03168347  0.29414195 -0.6036836  -0.32740057 -0.3235783   0.07128593\n",
            " -0.6812896 ]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.85981435  0.16076255  0.11597105 -0.10284138  0.01573773  0.2165551\n",
            " -0.9109262   0.29455495 -0.5738554   0.4201501  -0.04919066 -0.8069887\n",
            " -0.44168508 -0.17722332 -0.54401135 -0.1397336  -0.25254232  0.04022405\n",
            " -0.83075887]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.58517957 -0.02787877 -0.12599593 -0.09895685 -0.4359899  -0.10273729\n",
            " -0.48669478 -0.13780154 -0.2936604   0.1331886  -0.04941073 -0.2608218\n",
            " -0.06348307 -0.3124343  -0.42907846 -0.13214222 -0.03233381 -0.2721487\n",
            " -0.50765634]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.18526781e+00  1.72875851e-01  2.74569273e-01  2.23530456e-04\n",
            " -2.32784271e-01  8.25205743e-02 -8.08752000e-01 -1.71835482e-01\n",
            " -3.90652388e-01  3.54944110e-01  5.06325901e-01 -6.82313621e-01\n",
            " -5.16699612e-01 -1.41436309e-01 -6.64693832e-01 -4.13139388e-02\n",
            " -1.31075755e-02 -3.92444193e-01 -6.60726190e-01]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6919908   0.00245455  0.00695056  0.13698311 -0.27251178  0.01191846\n",
            " -0.37582627 -0.14574844 -0.12470059  0.30584946  0.12018026 -0.36290622\n",
            " -0.18466684 -0.38997748 -0.44786933 -0.10930862  0.07562113 -0.26566675\n",
            " -0.43610093]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.3685776   0.30721134  0.21559519  0.02260048 -0.17822686  0.13584961\n",
            " -0.7379953  -0.22610323 -0.25451458  0.5296403   0.7990048  -0.754227\n",
            " -0.6471744  -0.4301447  -0.71021855 -0.06828295 -0.02767156 -0.2809854\n",
            " -0.7845817 ]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.0801773   0.16473904 -0.20192954 -0.2644545  -0.42069086  0.00785223\n",
            " -0.76783836 -0.04911996 -0.2941702   0.79193956  0.24493946 -0.67741704\n",
            " -0.6537379  -0.8063234  -0.61452156 -0.28067902 -0.09068918 -0.12577789\n",
            " -0.6489106 ]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.2083315   0.05801542  0.347596   -0.00157901 -0.04922     0.2493266\n",
            " -0.75033945  0.07674307 -0.23193273  1.3634043   1.0306813  -0.88353086\n",
            " -0.8757252  -0.70541865 -0.6076491  -0.61208135 -0.10152756 -0.3220067\n",
            " -0.7656114 ]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.9309485  -0.01020353 -0.3586992  -0.22828017 -0.49052832 -0.14782624\n",
            " -0.48547092 -0.11763261 -0.2975637   0.922073    0.74131155 -0.70807946\n",
            " -0.6743574  -0.6952997  -0.579386    0.02870041 -0.18664621 -0.20237952\n",
            " -0.6691028 ]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.5896045   0.13494113  0.13923323  0.0981209  -0.37767684  0.16111311\n",
            " -0.8250534  -0.25460142 -0.42249584  1.4926997   2.102534   -0.9712087\n",
            " -1.1403388  -0.79856926 -0.833113   -0.22745107 -0.13452731 -0.39236152\n",
            " -0.8216365 ]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.5275193  -0.12278251 -0.0632648  -0.15156554 -0.34321392  0.04429937\n",
            " -0.75435597 -0.29651707 -0.47285655  1.6275305   1.5504656  -0.88138735\n",
            " -0.90123326 -0.64908516 -1.0080011   0.1378384  -0.16598429 -0.2184751\n",
            " -0.5954803 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.8029133   0.22952664  0.07549693  0.0990797  -0.39573666  0.06154172\n",
            " -0.83355516 -0.35388708 -0.39068505  1.7966791   1.5364687  -0.717125\n",
            " -0.72165984 -0.92848825 -0.9892499  -0.14300036 -0.1322432   0.02126739\n",
            " -0.7961875 ]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.2596362   0.09322514  0.01237337  0.15920895 -0.5377468   0.14406538\n",
            " -0.66673374 -0.14715058 -0.44053972  1.7352298   1.2049106  -0.7410819\n",
            " -0.92654186 -0.9612901  -0.6168946  -0.2679132  -0.03722036 -0.04986004\n",
            " -0.64597833]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.2152532   0.19441378 -0.07245432 -0.05103454 -0.68042254  0.29339644\n",
            " -0.8496141  -0.05173517 -0.36178324  1.9121659   1.4834328  -0.70172614\n",
            " -0.8438226  -0.9885264  -0.5777498  -0.44504863 -0.01838202  0.02586594\n",
            " -0.62833816]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.6772529   0.20932734 -0.13890108 -0.2280224  -0.5346979   0.6903858\n",
            " -0.9884085  -0.17243892 -0.00598728  2.20497     2.0287733  -1.0620058\n",
            " -1.0198168  -1.1461655  -0.6008674  -0.44185075  0.0697429  -0.09449147\n",
            " -0.65992177]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.1896880e+00  5.8408033e-02 -1.5259340e-01  9.2257559e-04\n",
            " -7.0184708e-01  3.5753986e-01 -6.6743004e-01 -3.1559977e-01\n",
            " -2.8320393e-01  1.6117719e+00  1.3027216e+00 -4.5527604e-01\n",
            " -7.1006292e-01 -9.6352243e-01 -5.9102440e-01 -2.4633490e-01\n",
            "  2.0911840e-01 -1.3574129e-01 -5.8815032e-01]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.7757185   0.32996058 -0.05363755  0.05672963 -0.5628508   0.77534276\n",
            " -1.1760223  -0.16847454 -0.53326267  2.3512652   2.126547   -0.9437389\n",
            " -1.4631602  -1.5185125  -0.8522768  -0.19136888  0.01573692  0.08946279\n",
            " -1.0851572 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.7573358   0.18950462  0.3559881  -0.01270736 -0.53395826  1.0134786\n",
            " -0.9572518  -0.14142744 -0.20384005  2.0432687   2.8815293  -1.0245169\n",
            " -1.4622386  -1.0505468  -0.8675498  -0.14266588  0.05182002 -0.107907\n",
            " -0.88167703]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.8929085  -0.02998938 -0.08550557 -0.01841327 -0.30915657  0.6321716\n",
            " -1.0730704  -0.16263212 -0.199455    2.2161088   1.5141113  -1.2812171\n",
            " -1.0833218  -0.8671502  -0.8972922  -0.64746326  0.09360075 -0.16832961\n",
            " -0.9731343 ]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.7088822e+00  5.2837241e-01  8.9087790e-01  1.5549578e-01\n",
            " -7.3962402e-01  9.9007952e-01 -1.2648695e+00  3.0210614e-04\n",
            " -4.3733349e-01  2.4562047e+00  2.1122801e+00 -1.1039090e+00\n",
            " -1.4739587e+00 -1.6119266e+00 -3.8016108e-01 -6.1350334e-01\n",
            "  2.8456587e-01 -1.9889392e-01 -9.4195569e-01]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.8333420e+00  3.4508139e-01  5.9924972e-01 -1.5342307e-01\n",
            " -4.5432615e-01  1.3163003e+00 -1.1272546e+00  2.1430328e-01\n",
            " -3.7109861e-01  2.1706412e+00  2.3333011e+00 -9.9666297e-01\n",
            " -1.6644007e+00 -1.2669265e+00 -7.8236574e-01 -1.7237242e-01\n",
            " -2.2706948e-03 -3.4545373e-02 -1.1673584e+00]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.2129066   0.09699696  0.4273619   0.02462926 -0.6645488   0.60426915\n",
            " -0.5996737  -0.24789265 -0.31219974  1.5290293   0.9705038  -0.5291608\n",
            " -0.72266155 -0.94885653 -0.50800914 -0.24265712  0.17820878 -0.02776649\n",
            " -0.5249431 ]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-1.664633    0.06331486  0.73540235 -0.00206947 -0.21000622  1.1293103\n",
            " -0.8178171  -0.20378754 -0.09306034  1.4742486   2.028427   -1.2872692\n",
            " -1.2908844  -0.7293174  -0.8948529  -0.55243486  0.03645971 -0.17570244\n",
            " -0.85440964]\n",
            "Epsilon reduced to 0.09000000000000001\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.4692991  -0.2691143   0.27625477  0.31093353 -0.914365    0.781107\n",
            " -1.1390812  -0.03198686 -0.16591892  1.6908915   1.377751   -1.1294729\n",
            " -1.058596   -0.8515725  -0.88588816 -1.1433299   0.11538366 -0.2131483\n",
            " -1.406412  ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.4517219   0.15334193  0.05277616  0.10898779 -1.0953058   0.8830364\n",
            " -1.1006377  -0.01012892 -0.25619298  1.3155415   0.8567347  -1.0516361\n",
            " -1.054104   -0.7877786  -0.79826474 -1.2882876  -0.04324343 -0.13094908\n",
            " -1.1385883 ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.4819214  -0.05827659  0.36363882  0.06807293 -0.86423606  0.8435412\n",
            " -0.86847234  0.01050114 -0.05979548  1.0765039   0.49542633 -1.114454\n",
            " -0.85202736 -0.6505729  -0.7492379  -1.4105414   0.07691427 -0.03022052\n",
            " -0.8916893 ]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.932425   -0.06652157  0.06208135  0.12107739 -0.46013516  0.8642605\n",
            " -0.836009   -0.16830201 -0.02441989  0.6033135   0.65149224 -0.9676552\n",
            " -0.8473215  -0.5064686  -0.89115334 -0.83206785  0.2698963   0.06280524\n",
            " -0.99734473]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.956132    0.00224007 -0.03661036  0.07164331 -0.5395047   0.31120026\n",
            " -0.65442544 -0.10201027 -0.12724574  0.477616    0.6677435  -0.79873854\n",
            " -1.000186   -0.6757834  -0.56261563 -0.6760447   0.03543694  0.04105156\n",
            " -0.75338876]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.82381827  0.1660246   0.3219487   0.10729403 -0.52290213  0.2639864\n",
            " -0.66033286 -0.01979037 -0.22763826  0.0816942   0.4325079  -0.7243106\n",
            " -0.9032031  -0.36536446 -0.49203065 -0.581918    0.03270248  0.21015231\n",
            " -0.9527198 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5046561  -0.11866947  0.04970179 -0.01917335 -0.36719248  0.18582381\n",
            " -0.5825053  -0.18412273 -0.02120067 -0.2100546   0.3910511  -0.69238967\n",
            " -0.63183    -0.28151327 -0.6542063  -0.74513     0.05256468 -0.05500655\n",
            " -0.91420937]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6534216  -0.02592155  0.18748513  0.2091078  -0.46780026 -0.01476467\n",
            " -0.38934514 -0.07173719  0.0116153   0.21335083  0.23396003 -0.47252825\n",
            " -0.5604288  -0.57143706 -0.3554552  -0.306457    0.05374961  0.07239306\n",
            " -0.4671834 ]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 9 reward=-2 new_state=[0 1 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-9.06622767e-01 -6.74603060e-02  4.01397228e-01  1.19922616e-01\n",
            " -2.65275002e-01  1.31246984e-01 -4.72145587e-01 -4.04710695e-03\n",
            " -8.52988511e-02 -5.66313505e-01  2.29555637e-01 -5.24618804e-01\n",
            " -5.94459116e-01 -2.57161915e-01 -6.39985025e-01 -9.14253235e-01\n",
            "  1.28057197e-01  9.00030136e-05 -7.50874341e-01]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 10 reward=-3 new_state=[0 1 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5996863  -0.05303347  0.05144672 -0.13861291 -0.2993045  -0.06033283\n",
            " -0.2777983  -0.1142543  -0.21040606 -0.20003739 -0.18682641 -0.3420437\n",
            " -0.1958431  -0.08306859 -0.16526225 -0.36875063 -0.03391661 -0.03042309\n",
            " -0.28121358]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.4603048  -0.13550022  0.1035508  -0.08622392 -0.43826833 -0.09421678\n",
            " -0.47595975 -0.02399371 -0.11479306  0.0797231   0.08420166 -0.43326578\n",
            " -0.41531494 -0.2396654  -0.2330949  -0.40226296 -0.03402014 -0.0980497\n",
            " -0.3991899 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.35767552  0.0304647  -0.18841565 -0.0812341  -0.4772106  -0.1407198\n",
            " -0.50934005  0.03003001 -0.16433963 -0.40753782 -0.13229023 -0.4326457\n",
            " -0.25294787 -0.26066786 -0.34469926 -0.55834764  0.08696623 -0.02142737\n",
            " -0.5237094 ]\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 13 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.45983526  0.0462009  -0.0163702  -0.16176724 -0.33876684  0.04024212\n",
            " -0.489544   -0.25980163 -0.08805461 -0.15271826  0.14648736 -0.6111826\n",
            " -0.3476104  -0.29924825 -0.3790663  -0.3966285   0.06989587 -0.04164051\n",
            " -0.43341133]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 14 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.74677694  0.07359811 -0.30107212 -0.30714428 -0.399745   -0.3001752\n",
            " -0.43840742 -0.05200978 -0.20266032 -0.563395   -0.29949227 -0.60121703\n",
            " -0.5353713  -0.12375682 -0.15927333 -0.5272005  -0.05630995  0.02826447\n",
            " -0.49994355]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 15 reward=-3 new_state=[1 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5630754   0.01434416  0.08401711 -0.12352356 -0.36699048 -0.05377785\n",
            " -0.36299723 -0.00748944 -0.39281774 -0.3369872  -0.1621219  -0.50821257\n",
            " -0.4021375  -0.26643306 -0.29200697 -0.52264804  0.09814697 -0.03921666\n",
            " -0.35634482]\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 16 reward=-3 new_state=[1 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.24724849 -0.09207353  0.03777588 -0.09815699 -0.17719385  0.05459092\n",
            " -0.12596247 -0.05820588 -0.11398841  0.16408941  0.00044566 -0.19236732\n",
            " -0.16963439 -0.07511627 -0.11106378 -0.05531375 -0.0569088  -0.03172598\n",
            " -0.12879722]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 17 reward=-2 new_state=[1 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.33142376 -0.03183156 -0.12521039 -0.1979398  -0.18443102  0.06801947\n",
            " -0.25025374 -0.0304469  -0.07046416  0.09165861 -0.00793008 -0.32545125\n",
            " -0.1988819  -0.20040376 -0.05537882 -0.1706707  -0.02795695 -0.03774145\n",
            " -0.15182342]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 18 reward=-2 new_state=[1 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.59000844 -0.1189604   0.10770695  0.01232981 -0.4279302  -0.10253919\n",
            " -0.44019634  0.04529434 -0.4129189  -0.4561006  -0.33704865 -0.3595959\n",
            " -0.41275916 -0.22460225 -0.35590702 -0.44204473 -0.14093304 -0.10040852\n",
            " -0.49517518]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 19 reward=-2 new_state=[1 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4788619  -0.08542725 -0.06395729 -0.14038773 -0.22436655  0.05169135\n",
            " -0.2374319  -0.11747196 -0.39284465 -0.16429096  0.04351899 -0.39043027\n",
            " -0.31683168 -0.171411   -0.32738155 -0.28104717 -0.09818299 -0.00344507\n",
            " -0.31893533]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 20 reward=-2 new_state=[1 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.25972912 -0.18040888 -0.2172708  -0.24704266 -0.18746649 -0.02070328\n",
            " -0.27662748 -0.07172724 -0.12039801 -0.10759158 -0.15752637 -0.2674151\n",
            " -0.09398944 -0.03919532 -0.15574308 -0.17118853 -0.07629461 -0.10944752\n",
            " -0.16977948]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 21 reward=-1 new_state=[1 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.26746464 -0.27452624 -0.21429554 -0.1406661  -0.2209892   0.01222796\n",
            " -0.29702365 -0.01805141 -0.31512922 -0.18047825 -0.42086977 -0.24492289\n",
            " -0.16507341 -0.10880631 -0.17647843 -0.28912753 -0.29986677  0.10442107\n",
            " -0.16526756]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[1 0 1 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.45667452 -0.16174138 -0.44443244 -0.34642917 -0.3908441  -0.35082468\n",
            " -0.38830295  0.11227316 -0.39196622 -0.11066194 -0.33577484 -0.43000033\n",
            " -0.3704168  -0.39207196 -0.2689167  -0.28391773 -0.32550165  0.16602752\n",
            " -0.49585843]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[1 0 1 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.37294278 -0.21238989 -0.28031474 -0.23876965 -0.16161299  0.10223722\n",
            " -0.2680047  -0.06167563 -0.23971978 -0.08177767 -0.13595532 -0.28774783\n",
            " -0.34845483 -0.19812241 -0.28823295 -0.19229355 -0.28858685 -0.09606872\n",
            " -0.20086616]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 24 reward=0 new_state=[1 0 1 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.3680882  -0.18987088 -0.23146075 -0.3072652  -0.11864105 -0.10093194\n",
            " -0.20772615 -0.03240844 -0.2881884  -0.17131059 -0.33198315 -0.31219998\n",
            " -0.31161398 -0.21923554 -0.29103827 -0.28666967 -0.26181656 -0.09974822\n",
            " -0.21736234]\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 25 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.35849828 -0.21749562 -0.16871162 -0.19850793 -0.21543345 -0.0740478\n",
            " -0.11625646 -0.04184993 -0.25152522 -0.12625079 -0.17435461 -0.22930405\n",
            " -0.3379552  -0.27644125 -0.23953049 -0.31207186 -0.25251073 -0.06786046\n",
            " -0.21637636]\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 26 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.41874743 -0.14104512 -0.29544482 -0.1902429  -0.23817565 -0.10979806\n",
            " -0.23369116 -0.04146386 -0.14853051  0.01735388 -0.10275501 -0.20516868\n",
            " -0.28064704 -0.2861757  -0.06898947 -0.14480723 -0.20865166  0.00666437\n",
            " -0.18646002]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 27 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.52662516 -0.36217803 -0.4126851  -0.35702765 -0.21641704 -0.22557172\n",
            " -0.38102224 -0.16117235 -0.22313353 -0.2635786  -0.51961076 -0.35561174\n",
            " -0.49664617 -0.28214115 -0.2409583  -0.35246122 -0.30685675 -0.05156814\n",
            " -0.25395373]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 28 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.37969154 -0.2951559  -0.34068227 -0.29386833 -0.18745068 -0.15523604\n",
            " -0.25531444 -0.22954476 -0.16089384 -0.21530256 -0.40955842 -0.12874603\n",
            " -0.32418132 -0.2364823  -0.24565695 -0.2409754  -0.31012037 -0.02154117\n",
            " -0.15063046]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 29 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.43927094 -0.2282159  -0.46277538 -0.2838687  -0.25213897 -0.28217146\n",
            " -0.27244177 -0.03947522 -0.27967077 -0.2304835  -0.32164502 -0.31539938\n",
            " -0.3071701  -0.18766549 -0.13416517 -0.232172   -0.32838345 -0.11783306\n",
            " -0.27811447]\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 30 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.47306168 -0.20198539 -0.3860675  -0.2945717  -0.36105755 -0.21263199\n",
            " -0.38711113 -0.10538953 -0.25346217 -0.16877046 -0.17822313 -0.31517246\n",
            " -0.27962482 -0.21331143 -0.1766966  -0.13506508 -0.21841587 -0.13360736\n",
            " -0.32135332]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 31 reward=-2 new_state=[1 0 1 1 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.47038206 -0.39321968 -0.5796215  -0.37033144 -0.4068966  -0.55204326\n",
            " -0.46841174 -0.32041577 -0.32944474 -0.2720604  -0.4683954  -0.35904527\n",
            " -0.27851453 -0.33604556 -0.13781476 -0.28346762 -0.59506065 -0.23886621\n",
            " -0.25700888]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 32 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.46305865 -0.5404147  -0.5175512  -0.32651728 -0.37051868 -0.24332675\n",
            " -0.43026635 -0.40266475 -0.25439197 -0.09238858 -0.48286617 -0.28145847\n",
            " -0.49107954 -0.37255388 -0.17514777 -0.35436666 -0.28442106 -0.413115\n",
            " -0.2521395 ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 33 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.43651134 -0.30455175 -0.3276548  -0.381216   -0.18381745 -0.23001021\n",
            " -0.20310396 -0.50407696 -0.371621   -0.22884502 -0.34103006 -0.33173445\n",
            " -0.44296953 -0.13210668 -0.317953   -0.28977516 -0.34326816 -0.3562321\n",
            " -0.26785526]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 34 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.46986672 -0.41003144 -0.51654685 -0.41858822 -0.19010577 -0.18559796\n",
            " -0.3187855  -0.49239883 -0.2368937  -0.32083806 -0.5389553  -0.29190752\n",
            " -0.4569197  -0.30348712 -0.32090855 -0.30692065 -0.48176432 -0.52401626\n",
            " -0.24370193]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 35 reward=0 new_state=[1 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.5236279  -0.3788488  -0.42589992 -0.35343358 -0.2936642  -0.10989709\n",
            " -0.3622201  -0.5544375  -0.32337305 -0.18287016 -0.2662045  -0.3524127\n",
            " -0.51988804 -0.3954287  -0.28084344 -0.36764932 -0.24510095 -0.39809078\n",
            " -0.23122373]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 36 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.42397642 -0.30935508 -0.33062357 -0.27716208 -0.29303652 -0.24402846\n",
            " -0.2316614  -0.56966764 -0.30722156 -0.1332347  -0.19633882 -0.31421727\n",
            " -0.413967   -0.41306219 -0.31353164 -0.30591416 -0.456433   -0.4781236\n",
            " -0.26696804]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 37 reward=-1 new_state=[1 0 1 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.46015298 -0.48486757 -0.48544323 -0.4125655  -0.2627464  -0.4728588\n",
            " -0.3140517  -0.87518436 -0.41858566 -0.50823337 -0.7642242  -0.36490244\n",
            " -0.64779913 -0.5263333  -0.48832998 -0.53643167 -0.67148125 -0.862098\n",
            " -0.3506169 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 38 reward=0 new_state=[1 0 0 0 1 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.429689   -0.24554552 -0.44361636 -0.23250696 -0.24129577 -0.25098035\n",
            " -0.3494652  -0.62886584 -0.19967449 -0.24516326 -0.2675339  -0.41452575\n",
            " -0.3652189  -0.40568173 -0.36631727 -0.27797836 -0.500378   -0.4190646\n",
            " -0.23097208]\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 39 reward=-1 new_state=[1 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5719594  -0.58277184 -0.5398356  -0.37002206 -0.32780015 -0.36416075\n",
            " -0.32363188 -0.7535118  -0.41656643 -0.36805284 -0.28436267 -0.5592067\n",
            " -0.4807917  -0.5453194  -0.5261862  -0.57762766 -0.63040996 -0.5907445\n",
            " -0.331289  ]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 40 reward=-1 new_state=[1 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.5442889  -0.41589305 -0.52571446 -0.29930377 -0.06591932 -0.32258958\n",
            " -0.38702008 -0.75650513 -0.39865556 -0.43771142 -0.3978509  -0.27320185\n",
            " -0.45508075 -0.45719764 -0.5310877  -0.37673193 -0.6726921  -0.68231004\n",
            " -0.25264588]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 41 reward=-1 new_state=[1 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.48753828 -0.420096   -0.4705323  -0.3084809  -0.13928297 -0.36663643\n",
            " -0.31019256 -0.74807125 -0.3603046  -0.2143066  -0.36336094 -0.19626068\n",
            " -0.4309162  -0.45626026 -0.48679063 -0.40160143 -0.616432   -0.6817794\n",
            " -0.30013135]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 42 reward=-1 new_state=[1 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.52228487 -0.5371108  -0.4862845  -0.29065207 -0.21001701 -0.36943802\n",
            " -0.43227255 -0.8706889  -0.523389   -0.31731933 -0.58157164 -0.49379689\n",
            " -0.59540045 -0.6146899  -0.7493481  -0.45881876 -0.7236003  -0.8995231\n",
            " -0.21113299]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 43 reward=-1 new_state=[1 0 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.6410845  -0.5753806  -0.7395747  -0.37496158 -0.48313707 -0.6508825\n",
            " -0.5332924  -1.0943878  -0.6689827  -0.42060974 -0.5759891  -0.4293523\n",
            " -0.60088414 -0.7208643  -0.6294185  -0.53644645 -0.57199043 -0.79638046\n",
            " -0.3783008 ]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 44 reward=-2 new_state=[1 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.55786675 -0.4076372  -0.67210716 -0.48115787 -0.19230661 -0.6823751\n",
            " -0.49816418 -1.0126414  -0.49598813 -0.45912665 -0.5341788  -0.5169866\n",
            " -0.54549193 -0.5233952  -0.69495857 -0.5168881  -0.7156121  -0.88384664\n",
            " -0.36084098]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 45 reward=-2 new_state=[1 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.58848876 -0.41248965 -0.49491107 -0.49856067 -0.17161381 -0.5738209\n",
            " -0.41625255 -0.9964034  -0.4891837  -0.38341856 -0.6968589  -0.50254554\n",
            " -0.61754066 -0.5509114  -0.55816364 -0.5844022  -0.80207074 -0.99277526\n",
            " -0.32555804]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 46 reward=-2 new_state=[1 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.68122536 -0.32637167 -0.52586925 -0.556141   -0.5777466  -0.704502\n",
            " -0.32918552 -1.1838499  -0.7731668  -0.3326233  -0.6463909  -0.46443844\n",
            " -0.6419941  -0.83006495 -0.59408027 -0.8282997  -0.59777516 -0.77279425\n",
            " -0.379026  ]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 47 reward=-2 new_state=[1 1 0 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.727583   -0.6234384  -0.919022   -0.8136169  -0.6898052  -0.84928536\n",
            " -0.5984179  -1.3704369  -1.1773981  -0.9408343  -0.85474753 -0.7742951\n",
            " -0.9397513  -0.9636258  -1.0161085  -0.98516154 -0.7524098  -0.9942502\n",
            " -0.6973988 ]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 48 reward=-1 new_state=[1 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.7422862  -0.7638813  -1.0764031  -0.9279453  -0.5642037  -1.0389957\n",
            " -0.6539984  -1.4720917  -1.1557353  -0.68055826 -1.1731818  -0.78725684\n",
            " -1.071261   -0.71057844 -0.87139505 -0.7264253  -1.1035806  -1.502284\n",
            " -0.8329611 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 49 reward=-1 new_state=[1 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.7344981  -0.701757   -0.77328414 -0.85760593 -0.7788788  -0.6329588\n",
            " -0.5959016  -1.2389655  -1.1566943  -0.39373684 -0.766891   -0.6213317\n",
            " -0.80494463 -0.76954716 -0.7975898  -0.90319157 -1.0736167  -1.0180248\n",
            " -0.70027137]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 50 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.78453654 -0.7710099  -0.59978217 -0.7679148  -0.88158214 -0.48562825\n",
            " -0.48275238 -1.3419114  -1.1972331  -0.7389017  -0.64756846 -0.35110113\n",
            " -0.6993225  -0.82257557 -0.90859276 -0.9125831  -1.1856667  -1.2090545\n",
            " -0.48924336]\n",
            "Epsilon reduced to 0.08100000000000002\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.2744688  -1.2824006  -1.1075     -0.7048703  -1.398463   -1.0069054\n",
            " -1.0484811  -1.780189   -1.8107183  -1.4843242  -1.2713304  -0.829192\n",
            " -1.3170592  -1.0311947  -1.4071242  -0.80029714 -1.706532   -1.5689806\n",
            " -1.882999  ]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 1 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5115702  -0.97535545 -0.5405352  -0.93824893 -1.2178702  -0.76602864\n",
            " -0.7405495  -1.1998407  -1.14873    -0.10732909 -0.7419954  -0.7761497\n",
            " -0.7785418  -1.0799668  -0.80035585 -0.58172566 -1.0096223  -0.9887535\n",
            " -0.67345184]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7309832  -1.3360397  -0.87138987 -1.0040922  -1.5447931  -1.1668088\n",
            " -1.0522226  -1.4322034  -1.2118108  -0.8436077  -1.3458952  -0.8770888\n",
            " -0.7734572  -1.1193103  -1.3515544  -1.0039947  -1.6394615  -1.4014108\n",
            " -1.1523926 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.76312274 -1.1374896  -0.91347367 -1.282499   -1.609902   -0.76555014\n",
            " -0.9228045  -1.6259902  -1.5472151  -1.3516858  -0.88932335 -0.8996348\n",
            " -0.89245665 -0.85208464 -1.189572   -1.1218095  -2.0496976  -1.3299134\n",
            " -1.1090869 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7967217 -1.3753407 -1.006635  -1.3522413 -1.8607845 -1.6763735\n",
            " -1.1440934 -1.9959093 -1.6904588 -1.1076452 -1.3007872 -1.1657767\n",
            " -1.1565673 -1.356217  -1.4572749 -1.5445913 -2.2610514 -2.0153918\n",
            " -1.1371881]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.8868661  -1.6745061  -1.025656   -1.5724535  -1.9039723  -1.0190042\n",
            " -1.2048786  -2.1197479  -2.2527578  -1.1723404  -0.9387175  -0.73286235\n",
            " -1.0321507  -1.2849855  -1.4714164  -0.99687773 -2.1369736  -1.6904309\n",
            " -1.4954301 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7671143 -1.7430876 -0.9951585 -1.5176821 -1.9333885 -1.3955567\n",
            " -1.0075465 -1.9853851 -2.1495218 -1.0815145 -1.1209197 -0.8011834\n",
            " -1.3308907 -1.5259882 -1.4456545 -1.2867869 -2.0847583 -1.8880413\n",
            " -1.459197 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7605222 -1.685391  -1.1482102 -2.0394816 -1.9612027 -1.1524323\n",
            " -1.250145  -2.3274019 -2.1151502 -1.1566257 -1.2536913 -0.9681484\n",
            " -1.3965782 -1.7754077 -1.1736406 -1.4230677 -2.1921704 -2.0122526\n",
            " -1.6638392]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.08332568 -1.5316973  -1.0681317  -1.4355744  -2.5679784  -1.497435\n",
            " -1.3902802  -1.7411615  -1.3725067  -0.9965156  -1.5312648  -1.1476749\n",
            " -0.9509081  -1.1347773  -1.5504334  -1.2581836  -2.5708325  -1.7793005\n",
            " -1.4119307 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.42699558 -1.975199   -1.0866598  -2.0663025  -2.3668165  -1.213508\n",
            " -1.2045567  -2.26426    -1.9168755  -1.3630177  -1.2854103  -0.7977552\n",
            " -1.1411315  -1.4566023  -1.3329105  -0.9349354  -1.9023086  -1.7449425\n",
            " -1.7436311 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.05978625 -1.3383654  -0.91092896 -1.6660972  -2.1690085  -1.7936488\n",
            " -1.391616   -1.8592238  -1.8049533  -1.3424561  -1.8665274  -1.0471884\n",
            " -0.96191674 -1.2965542  -1.5455701  -1.4906874  -2.2742958  -1.8909618\n",
            " -1.3025093 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.8879995e-03 -1.7000763e+00 -1.1530795e+00 -1.9421307e+00\n",
            " -2.3293393e+00 -7.6505983e-01 -1.2672143e+00 -2.1029119e+00\n",
            " -1.9519304e+00 -9.8109341e-01 -1.3809417e+00 -1.1925375e+00\n",
            " -1.1676432e+00 -1.5186337e+00 -1.2883902e+00 -1.1575861e+00\n",
            " -2.0563605e+00 -1.6497281e+00 -1.4982262e+00]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.14497565 -1.9812886  -1.3671727  -1.9690413  -2.4777799  -1.2769103\n",
            " -1.4142799  -1.8538935  -2.025702   -1.452185   -1.0283113  -0.95560986\n",
            " -0.93359715 -1.2274251  -1.5631783  -1.2382264  -2.2663937  -1.5025165\n",
            " -1.5339371 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.38920343 -1.9993255  -1.2399021  -2.4009826  -2.3634264  -0.90212595\n",
            " -1.4732884  -2.3272057  -2.0390131  -1.0239561  -1.3175901  -1.1632781\n",
            " -1.3028708  -1.6144974  -1.5447279  -1.2118449  -1.7409724  -1.6924472\n",
            " -1.4571122 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.4276935  -1.7703826  -1.4358692  -2.6389458  -2.387138   -1.664339\n",
            " -1.4664497  -2.2575629  -2.6463113  -1.973879   -1.0536088  -0.97215164\n",
            " -1.1082687  -1.4776762  -1.6005657  -1.4186543  -2.3063674  -2.1267407\n",
            " -1.5394292 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.47197503 -2.2048285  -1.2242719  -2.6288185  -2.8672302  -1.296323\n",
            " -1.6453729  -2.2011144  -1.7813714  -0.85942173 -1.310313   -0.91997916\n",
            " -1.4665351  -1.9065806  -1.3879662  -1.2238104  -1.6842583  -2.0942812\n",
            " -1.52858   ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.5926903 -1.8179601 -1.196143  -2.578317  -2.346928  -1.6250464\n",
            " -1.3536587 -2.3124747 -2.6339552 -1.7587767 -1.0061612 -1.0522853\n",
            " -1.1371459 -1.5035634 -1.5415704 -1.4818561 -2.2920198 -2.0724773\n",
            " -1.5211295]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.404861   -1.9256226  -0.98533076 -2.5147886  -2.906283   -1.7222708\n",
            " -1.6833153  -2.214991   -1.7673708  -0.755467   -1.6245403  -1.328103\n",
            " -1.4347728  -1.689521   -1.486419   -1.4553689  -2.0100825  -2.0890958\n",
            " -1.5006183 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.22805357 -1.9360912  -1.2503904  -2.595139   -2.539751   -1.4313611\n",
            " -1.4242617  -2.0791326  -2.7510962  -1.4838889  -1.1955391  -0.8039573\n",
            " -1.0497557  -1.4810934  -1.7524862  -1.3925035  -2.3830724  -1.8818834\n",
            " -1.7303226 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.3812528 -1.9898587 -1.1484194 -2.3083584 -2.9215488 -1.5048939\n",
            " -1.6560766 -2.1066403 -1.5543029 -1.3090539 -1.884561  -1.432417\n",
            " -1.283635  -1.6431121 -1.6273226 -1.6244015 -2.405975  -1.8904079\n",
            " -1.7083061]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.63948154 -1.6978585  -1.4162862  -2.6436996  -2.5124993  -1.239785\n",
            " -1.509215   -2.1797543  -2.5519874  -1.7440147  -1.2414758  -1.109085\n",
            " -1.3310969  -1.3109778  -1.5775144  -1.5209919  -2.7762887  -2.043369\n",
            " -1.5763242 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.7503278  -2.0333273  -1.0339417  -2.4495049  -2.6341414  -0.94222283\n",
            " -1.5697526  -2.0432823  -2.0517542  -1.4444344  -1.2224752  -1.1520369\n",
            " -1.083836   -1.3138921  -1.519295   -1.3505373  -1.8168617  -1.4970777\n",
            " -1.610044  ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.6730932  -2.0170457  -0.99197704 -3.2176151  -2.361029   -1.2039008\n",
            " -1.5823343  -2.2957835  -2.5608253  -1.4832487  -1.2933918  -0.74984175\n",
            " -1.1146468  -1.4354879  -1.6184001  -1.2448076  -2.4467595  -1.8725176\n",
            " -1.7986835 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.48996097 -1.9347081  -1.3572245  -2.8747358  -2.5779026  -1.215936\n",
            " -1.5809793  -2.3320973  -2.6864166  -1.8751621  -1.259109   -1.1156366\n",
            " -1.3224938  -1.7312967  -1.6777648  -1.5003687  -2.1171014  -1.9907849\n",
            " -1.53961   ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.6489409 -1.9731494 -1.3366525 -2.5771835 -2.5944698 -1.1024047\n",
            " -1.4007058 -2.386766  -2.3599625 -1.4326644 -1.2420787 -1.0329181\n",
            " -1.3026644 -1.5489745 -1.2598137 -1.3360592 -2.2524285 -2.078513\n",
            " -1.5930873]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.40155706 -2.4313784  -1.3634771  -2.7168887  -2.9529343  -1.4928237\n",
            " -1.487914   -2.4053884  -2.1042645  -1.7033598  -1.3136477  -1.2158271\n",
            " -1.3344562  -1.6070784  -1.644925   -1.2561296  -2.410761   -2.1146631\n",
            " -1.8272005 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.5206013 -1.9980136 -1.3774191 -2.7574208 -2.7771976 -1.2680354\n",
            " -1.4071473 -2.6268468 -2.2415566 -1.5098482 -1.3494025 -1.027508\n",
            " -1.5171388 -1.8148609 -1.2045211 -1.3638062 -2.26329   -2.3426657\n",
            " -1.6865393]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.12509115 -2.1119802  -1.4733909  -2.838231   -2.6984346  -1.5306859\n",
            " -1.4415435  -2.2920437  -2.5171723  -1.6371971  -1.4495814  -0.9211378\n",
            " -1.4872574  -1.6579069  -1.5736933  -1.3842854  -2.193934   -2.0814378\n",
            " -1.771192  ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [ 0.22385001 -2.1981695  -1.0490482  -2.818736   -2.709046   -1.01638\n",
            " -1.2687562  -2.4558067  -2.1732671  -1.1629654  -1.4434335  -0.75326467\n",
            " -1.1649195  -1.5717585  -1.2774516  -1.1885041  -1.9264103  -1.7045691\n",
            " -1.7179006 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.9862384 -1.6004782 -1.1959163 -2.2424526 -2.9663997 -1.7928808\n",
            " -1.8007315 -1.993286  -2.0086286 -1.2305005 -1.717654  -1.0686072\n",
            " -1.134905  -1.3928251 -1.7503744 -1.5527222 -2.6149492 -2.3645592\n",
            " -1.5237529]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.17239524 -1.8062626  -0.848226   -2.4216268  -2.7467775  -0.68513906\n",
            " -1.4710369  -1.9168297  -1.8480908  -0.6493059  -1.1090038  -0.95143104\n",
            " -1.2211136  -1.5807141  -1.2935768  -1.0748838  -1.4797543  -1.3537315\n",
            " -1.2215725 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.57415557 -1.704454   -0.9944112  -2.2856195  -2.466256   -0.8491048\n",
            " -1.1823661  -1.6917788  -2.0262415  -0.6177603  -0.8581085  -0.55212057\n",
            " -0.9553144  -1.2599094  -1.326886   -0.9957437  -1.4557192  -1.1524737\n",
            " -1.2501141 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.5443875  -1.9298375  -0.8368957  -2.2615018  -2.6959558  -0.90633315\n",
            " -1.3668326  -1.9172872  -1.4931695  -0.7428576  -0.90698415 -0.34184763\n",
            " -0.97253    -1.4633443  -1.1347613  -1.073106   -1.3838243  -1.3457876\n",
            " -0.8875724 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.4240249  -1.8894296  -0.8436815  -1.8462145  -2.535647   -0.68114996\n",
            " -1.1745653  -1.7618626  -1.5893707  -0.22223926 -1.0496972  -0.36722696\n",
            " -1.0079457  -1.3242161  -1.0652775  -0.92545027 -1.2144535  -1.2133541\n",
            " -0.9575503 ]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.1661675  -1.5776291  -0.7130244  -1.8065081  -2.376688   -0.6280328\n",
            " -1.1731836  -1.4962717  -1.3143809  -0.81466216 -0.8628583  -0.26491562\n",
            " -0.95402724 -1.1098278  -0.99618375 -0.5431796  -1.1026243  -1.3123264\n",
            " -0.93802756]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 10\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 0 0 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-7.6404011e-01 -1.5063028e+00 -8.4825844e-01 -1.6516159e+00\n",
            " -2.0968659e+00 -6.3747644e-01 -1.0035964e+00 -1.6494510e+00\n",
            " -1.0691395e+00 -4.9206936e-01 -6.5588820e-01  1.2700520e-03\n",
            " -6.8015826e-01 -1.1671143e+00 -1.0005634e+00 -3.9754802e-01\n",
            " -8.4154779e-01 -1.3215014e+00 -8.4793782e-01]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.3978382  -1.7937379  -0.95683265 -2.1556928  -2.6303334  -0.7630553\n",
            " -1.265831   -1.8919584  -1.60994    -0.31030843 -1.1727802  -0.09984549\n",
            " -1.2691225  -1.6225028  -1.0342407  -0.5820941  -1.3653058  -1.38285\n",
            " -1.091009  ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.9963663  -1.2321435  -0.79196876 -1.6636391  -2.1171196  -0.6546164\n",
            " -0.98908234 -1.4113685  -1.1535708  -0.6620472  -1.1235058   0.20685248\n",
            " -0.7745258  -1.2011174  -0.7833904  -0.508451   -1.3491138  -1.1032468\n",
            " -0.98253334]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.0889844  -1.6025323  -0.68805534 -1.7379285  -2.190712   -0.6103985\n",
            " -1.1384642  -1.559243   -1.1712956  -0.8395144  -0.9339211   0.06363395\n",
            " -0.77581507 -1.1537467  -1.1512085  -0.35601327 -1.0354319  -1.1261375\n",
            " -0.8819115 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.8818999  -1.6288774  -0.79641205 -1.9078255  -2.3794706  -0.5755022\n",
            " -1.1894032  -1.5586941  -1.499158   -0.20139387 -1.0319506   0.30453172\n",
            " -1.001574   -1.1237898  -1.137838   -0.5116567  -1.3366933  -1.0874299\n",
            " -1.0691166 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.9147439  -1.489179   -0.9015713  -1.5463706  -2.1891456  -0.8126047\n",
            " -0.9444842  -1.4993571  -1.0944499  -0.25349733 -0.9126176   0.20407762\n",
            " -0.867177   -1.1810054  -0.90275955 -0.39725977 -1.0981002  -1.1264552\n",
            " -0.8911207 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.0643864  -1.3064402  -0.67492914 -1.7211976  -2.0887315  -0.5129463\n",
            " -1.0970091  -1.2507077  -1.1561794  -0.7616133  -1.029663    0.19084264\n",
            " -0.690353   -1.1500783  -1.0048246  -0.21699664 -1.0207982  -1.1609014\n",
            " -0.77612853]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 43 reward=-1 new_state=[0 1 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.1447546  -1.5272353  -0.8582548  -1.7686738  -2.3305104  -0.78387564\n",
            " -1.022412   -1.6724894  -1.4218714  -0.38071293 -1.013374    0.2436907\n",
            " -0.91213876 -1.4616187  -1.0828285  -0.42419592 -1.257026   -1.2484428\n",
            " -0.9248873 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 44 reward=-1 new_state=[0 1 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.2388791e+00 -1.3664658e+00 -1.0706985e+00 -1.9625851e+00\n",
            " -2.3847570e+00 -9.5287913e-01 -1.3462510e+00 -1.8296651e+00\n",
            " -1.9632881e+00 -5.9238708e-01 -1.0595334e+00 -2.0909999e-03\n",
            " -1.1423458e+00 -1.5305879e+00 -1.3255750e+00 -5.8722794e-01\n",
            " -1.2851626e+00 -1.1908424e+00 -1.0364586e+00]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 45 reward=-1 new_state=[0 1 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.2788404  -1.4972981  -1.0098058  -1.9783814  -2.2518911  -0.87320083\n",
            " -1.44736    -1.6527234  -1.85799    -0.3145784  -1.3351892  -0.18983997\n",
            " -1.2326652  -1.154419   -1.2505286  -0.47223318 -1.0247611  -1.3507482\n",
            " -1.2268193 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 46 reward=-1 new_state=[0 1 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.75022787 -1.6109424  -0.8474496  -1.6038722  -2.0384324  -0.7913431\n",
            " -1.2710619  -1.4057378  -1.101964   -0.80155534 -1.5044824  -0.034178\n",
            " -0.8457628  -1.2821975  -1.0631781  -0.4145658  -0.9967419  -1.4073315\n",
            " -1.0532491 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 47 reward=-1 new_state=[0 1 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.9377873  -1.2612084  -0.868814   -1.5153939  -2.1564662  -0.99342275\n",
            " -1.0443834  -1.608987   -1.316405   -0.49909306 -1.0190963  -0.18391949\n",
            " -0.96972835 -1.2773958  -0.77954787 -0.39431402 -1.0552919  -1.1139718\n",
            " -0.95062935]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 1 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.0441837  -1.348804   -0.7177348  -1.4001489  -1.9252443  -0.86916345\n",
            " -1.1284811  -1.5953254  -1.3686253  -0.17209473 -0.98244786 -0.25168803\n",
            " -1.2059563  -1.1449724  -0.9210608  -0.34060848 -0.79944074 -1.0795939\n",
            " -0.8853421 ]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 1 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.89379835 -1.8571424  -0.7872411  -1.6584398  -2.038112   -0.71793354\n",
            " -1.460597   -1.5821887  -1.5027246  -0.9173614  -1.6740551  -0.49683842\n",
            " -1.0806855  -1.2730224  -1.4475654  -0.4497643  -1.2468357  -1.4308252\n",
            " -1.2848029 ]\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 1 0 0 1 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.66915    -1.2721169  -0.556207   -1.050682   -1.676658   -0.7825354\n",
            " -0.85294217 -1.3864392  -1.0208883  -0.471587   -0.87874377 -0.3553649\n",
            " -0.62456995 -0.9854487  -0.78468543 -0.37263647 -1.0357794  -0.8389753\n",
            " -0.7970365 ]\n",
            "Epsilon reduced to 0.07290000000000002\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3393168 -1.6565441 -1.0016267 -1.888868  -2.1040115 -1.2770311\n",
            " -1.1643159 -1.6641548 -1.4402931 -1.6281569 -1.197891  -0.705556\n",
            " -0.7255198 -1.1578095 -1.0231811 -0.2851932 -1.8493724 -1.2064389\n",
            " -1.2032349]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6877508  -1.3476598  -1.0217532  -1.6168867  -1.892142   -0.5919694\n",
            " -1.2732756  -1.4749132  -1.8071986  -1.0969247  -1.6135163  -0.5309355\n",
            " -0.90614027 -0.9989005  -1.2297825  -0.1819259  -1.4645586  -1.0617782\n",
            " -1.1189646 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.76251036 -1.2367826  -1.0480478  -1.2145634  -1.8450713  -0.89943033\n",
            " -0.8663564  -1.3125653  -1.2653749  -0.6775164  -1.3773252  -0.84085506\n",
            " -0.8526602  -0.8445888  -0.785873   -0.48777956 -1.0780202  -1.1729548\n",
            " -0.9061309 ]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7977272  -1.047658   -0.70373863 -1.0033829  -1.5676602  -0.54651654\n",
            " -0.8700312  -1.084247   -0.9181417  -0.5699342  -0.64388794 -0.27478442\n",
            " -0.50564337 -0.73480546 -0.59459877 -0.40671664 -0.873496   -0.77731264\n",
            " -0.6983977 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6925276  -0.78349257 -0.644547   -0.7691384  -1.5833249  -0.6666471\n",
            " -0.7880647  -0.82565695 -0.7842688  -0.8679187  -0.7040763  -0.2349948\n",
            " -0.4545178  -0.60278356 -0.45272207 -0.5363387  -0.8046002  -0.6694249\n",
            " -0.65043294]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.7520536  -1.5103872  -1.0314517  -1.4540002  -1.9636593  -0.97018707\n",
            " -1.090219   -1.5351913  -1.4026209  -1.0215765  -1.6617116  -1.1623769\n",
            " -0.98578715 -1.0752456  -0.73049504 -0.94925225 -1.198626   -1.505069\n",
            " -1.078052  ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.59459984 -1.0985526  -0.7430377  -1.1298811  -1.765029   -0.6416309\n",
            " -0.9628867  -1.1736615  -1.1086252  -0.38847184 -0.8027023  -0.64694124\n",
            " -0.7240603  -1.0514841  -0.48672736 -0.58707035 -0.9088665  -1.086596\n",
            " -0.76659817]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 8 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.7928036  -0.8552041  -0.5926525  -1.0656979  -1.9848559  -0.5958588\n",
            " -0.9888334  -1.0383134  -0.89770967 -0.6356553  -0.7583637  -0.51002\n",
            " -0.5986455  -0.8717776  -0.3795953  -0.7460942  -0.8877409  -1.1388192\n",
            " -0.8254818 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12 from 6\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.45278952 -0.9898765  -0.31735674 -1.3693576  -1.4425833  -0.46400076\n",
            " -0.7598487  -1.0704985  -1.0914146  -0.5623145  -0.5348169  -0.81249446\n",
            " -0.54507965 -0.7524977  -0.28911394 -0.48633134 -0.76588404 -0.73060817\n",
            " -0.90567166]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 10 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5490737  -0.83100986 -0.4190669  -1.0101709  -1.4465004  -0.12412753\n",
            " -0.73326886 -0.973607   -1.0838194  -0.28559703 -0.6256473  -0.8859218\n",
            " -0.6507218  -0.5625743  -0.19119307 -0.5921824  -0.8854747  -1.0222299\n",
            " -0.768029  ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.48443374 -0.89150196 -0.22768241 -1.185045   -1.3245475  -0.40792024\n",
            " -0.49051142 -0.9832021  -0.9055522  -0.45189005 -0.51292145 -0.9475572\n",
            " -0.52739155 -0.6964149  -0.02883847 -0.5459665  -0.6860235  -0.70154583\n",
            " -0.81542313]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.41482124 -0.7045272  -0.3953876  -0.67376053 -1.2053735  -0.09902847\n",
            " -0.4272724  -0.79650784 -0.81191885 -0.3460547  -0.567749   -0.75610757\n",
            " -0.5180304  -0.39417934  0.20535776 -0.65191686 -0.7501418  -0.8640982\n",
            " -0.60415447]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.13558012 -0.5413614  -0.30848378 -0.84399956 -1.1641811  -0.28242674\n",
            " -0.45273852 -0.8596889  -0.8549932  -0.6082298  -0.38986254 -0.65534514\n",
            " -0.5381281  -0.5375745   0.2831715  -0.69793963 -0.61648595 -0.7137945\n",
            " -0.66229033]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.48578724 -0.6581908  -0.45543605 -0.67682785 -1.3521872  -0.290539\n",
            " -0.34966534 -0.79524136 -0.6222299  -0.30136943 -0.47679186 -0.7566827\n",
            " -0.4890063  -0.438037    0.20292908 -0.51821387 -0.7327031  -0.86877584\n",
            " -0.5807472 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4558878  -0.4366392  -0.33343792 -0.66244924 -1.2609237  -0.47626483\n",
            " -0.34005585 -0.7555728  -0.5777637  -0.42477727 -0.63386065 -0.22350398\n",
            " -0.4966839  -0.43018904  0.35596246 -0.7769367  -0.7554305  -0.78436923\n",
            " -0.5407875 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6334502  -0.6566253  -0.28715712 -0.69415206 -1.0376221  -0.15416355\n",
            " -0.19671677 -0.7285739  -0.6597557  -0.12660515 -0.46870792 -0.6232815\n",
            " -0.3571589  -0.28868768  0.3880337  -0.50003576 -0.5665356  -0.62087196\n",
            " -0.5202141 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.42255068 -0.5977323  -0.32650054 -0.8995394  -1.1041384  -0.3379383\n",
            " -0.1911163  -0.76559275 -0.8006518  -0.51806223 -0.40300488 -0.7761129\n",
            " -0.46855617 -0.34806874  0.8801544  -0.600976   -0.5014485  -0.54320264\n",
            " -0.5839439 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5798002  -0.7366133  -0.370845   -0.72372645 -1.0680871  -0.29483622\n",
            " -0.22040944 -0.72034055 -0.6824245  -0.06379628 -0.54695237 -0.7402222\n",
            " -0.4717983  -0.3081436   0.5364826  -0.6186657  -0.49788073 -0.6135225\n",
            " -0.48301443]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.37412256 -0.68382645 -0.29050398 -0.9317447  -1.1384956  -0.41333473\n",
            " -0.17095214 -0.78959256 -0.83532095 -0.47231877 -0.40331635 -0.96709186\n",
            " -0.45642954 -0.35265836  1.2045515  -0.58080804 -0.5847801  -0.5887824\n",
            " -0.4986359 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 8\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4548553  -0.79469997 -0.46609902 -0.677799   -1.2807896  -0.39286533\n",
            " -0.26333386 -0.8453474  -0.70697975 -0.29479408 -0.5179375  -0.7458729\n",
            " -0.5417265  -0.32615882  0.9973867  -0.84417945 -0.78650844 -0.84305835\n",
            " -0.19161311]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 21 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.2551379  -0.58038217 -0.30396748 -0.69826657 -1.1483338  -0.328117\n",
            " -0.14762731 -0.71338034 -0.6973349  -0.3443406  -0.35496217 -1.022963\n",
            " -0.53964525 -0.27633965  1.409334   -0.48820144 -0.6161838  -0.6692083\n",
            " -0.28211796]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 22 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.267454   -0.2589343  -0.21164466 -0.36909807 -0.5762623  -0.14145681\n",
            " -0.07857525 -0.38864434 -0.28604457 -0.10127188 -0.23679851 -0.4090416\n",
            " -0.2374832  -0.16853087  0.5408438  -0.3063022  -0.27384153 -0.43180507\n",
            " -0.1441753 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 23 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.439426   -0.47818607 -0.40366578 -0.5796204  -1.0534834  -0.58822536\n",
            " -0.21987326 -0.5599986  -0.45459214 -0.4003246  -0.6057936  -0.45401654\n",
            " -0.4533962  -0.32107124  0.79552317 -0.62782764 -0.65437573 -0.732252\n",
            " -0.2779178 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 24 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.35169446 -0.49030918 -0.11790777 -0.5781008  -0.7549295  -0.2866431\n",
            " -0.07283349 -0.5672488  -0.30816472 -0.12788875 -0.41487578 -0.6299382\n",
            " -0.2733718  -0.2412847   1.1222484  -0.58972985 -0.43555218 -0.4351426\n",
            " -0.27373645]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 25 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.18402386 -0.2218139  -0.21842252 -0.4087748  -0.57767457 -0.17945874\n",
            " -0.0432687  -0.39943093 -0.26191014 -0.28419507 -0.20464303 -0.58609164\n",
            " -0.2528928  -0.09727514  0.7615166  -0.24933213 -0.23124237 -0.37962556\n",
            " -0.18454048]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 26 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.2072447  -0.19806671 -0.22270021 -0.47642696 -0.60664773 -0.06803859\n",
            " -0.05645235 -0.44661462 -0.24771172 -0.23567526 -0.27544707 -0.5291036\n",
            " -0.28569925 -0.083362    0.7767391  -0.3384483  -0.32207477 -0.4516663\n",
            " -0.20291995]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 27 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.14688376 -0.23305777 -0.3045566  -0.49088243 -0.7394583  -0.17229256\n",
            " -0.07612889 -0.5010114  -0.23495242 -0.34851247 -0.39212587 -0.55302936\n",
            " -0.38995764 -0.22412822  0.9195529  -0.53264594 -0.4502321  -0.5650453\n",
            " -0.206191  ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 28 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3141774  -0.20163074 -0.424488   -0.45346206 -0.8999107  -0.42102396\n",
            " -0.09305743 -0.50676084 -0.20970237 -0.4154281  -0.48664814 -0.36824602\n",
            " -0.3478332  -0.11250791  0.7141896  -0.3962936  -0.4998871  -0.57501256\n",
            " -0.14336744]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 29 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.22944868 -0.19448343 -0.215665   -0.31801164 -0.5404102  -0.18044364\n",
            " -0.07346111 -0.39085495 -0.12927076 -0.16548285 -0.25949717 -0.40820825\n",
            " -0.24086794 -0.12567648  0.45829988 -0.31834227 -0.19466546 -0.36416012\n",
            " -0.15576157]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 30 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.17259061 -0.09174912 -0.20285276 -0.26445976 -0.54470587 -0.22739828\n",
            " -0.05046898 -0.33101785 -0.11082464 -0.18203525 -0.2929046  -0.25183895\n",
            " -0.18826506 -0.1215046   0.54687434 -0.32050413 -0.30778182 -0.3538544\n",
            " -0.05546983]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 31 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.08814695 -0.17286423 -0.23021418 -0.287283   -0.55699384 -0.22479725\n",
            " -0.03173801 -0.37035978 -0.14441274 -0.25050685 -0.2612927  -0.3692991\n",
            " -0.24757436 -0.13304278  0.5450508  -0.44519073 -0.3552815  -0.38521653\n",
            " -0.1409053 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 32 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.1859977  -0.11470675 -0.22818619 -0.26329035 -0.46724388 -0.09908754\n",
            " -0.03142938 -0.3613354  -0.09990892 -0.2706694  -0.19844519 -0.4245218\n",
            " -0.2148987  -0.084502    0.41500866 -0.27846062 -0.21883635 -0.3234424\n",
            " -0.10311349]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 33 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.17597255 -0.13238156 -0.18977228 -0.2198631  -0.3937989  -0.23704341\n",
            " -0.00206872 -0.21325308 -0.03811779 -0.12101733 -0.23303854 -0.22737327\n",
            " -0.1507024  -0.07657917  0.38265708 -0.25074235 -0.22814311 -0.20500687\n",
            " -0.10909638]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 34 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.13864802 -0.1403842  -0.16521297 -0.17110452 -0.3644793  -0.07974012\n",
            " -0.05867661 -0.23801008 -0.07308809 -0.10884479 -0.19901054 -0.24441007\n",
            " -0.15775388 -0.09248078  0.3182323  -0.2512832  -0.17771919 -0.22366208\n",
            " -0.07481617]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 35 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.13747218 -0.07274482 -0.23223606 -0.19892198 -0.40328735 -0.19787648\n",
            " -0.06791933 -0.2474417  -0.09356834 -0.18526727 -0.2233925  -0.23601276\n",
            " -0.18288437 -0.0962114   0.31719512 -0.32130614 -0.24403864 -0.23417987\n",
            " -0.07455201]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 36 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.06500496 -0.10136347 -0.20947608 -0.1709315  -0.35105234 -0.15693381\n",
            " -0.00850986 -0.23620485 -0.07396695 -0.15635292 -0.18005088 -0.22839153\n",
            " -0.1388591  -0.0522147   0.27525246 -0.25485712 -0.2214843  -0.21465182\n",
            " -0.0511514 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 37 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.16398215 -0.13047577 -0.16364573 -0.2084896  -0.36957264 -0.12226466\n",
            " -0.01541529 -0.2479443  -0.04029297 -0.17521542 -0.17529404 -0.29632676\n",
            " -0.15618086 -0.06673257  0.37366688 -0.21388388 -0.14982578 -0.20986801\n",
            " -0.09458639]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 38 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.07696997 -0.08422847 -0.18841946 -0.17094375 -0.3454386  -0.13491906\n",
            " -0.00109322 -0.2233461  -0.06400627 -0.15240361 -0.16174059 -0.25912744\n",
            " -0.15971464 -0.06284191  0.34759992 -0.25253433 -0.19115725 -0.20824407\n",
            " -0.06649907]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 39 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10871444 -0.11325437 -0.18464568 -0.1770775  -0.33362633 -0.13010432\n",
            " -0.02012765 -0.21152896 -0.05754793 -0.1615664  -0.1696236  -0.2682729\n",
            " -0.14055033 -0.04759963  0.2721373  -0.2177451  -0.16583806 -0.20814435\n",
            " -0.07075951]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 40 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.11601774 -0.04728952 -0.1771576  -0.16857305 -0.35143536 -0.12525603\n",
            " -0.0071169  -0.21145013 -0.03789829 -0.14233337 -0.17159116 -0.21287544\n",
            " -0.15397498 -0.06713668  0.29964435 -0.22148487 -0.18040738 -0.21219584\n",
            " -0.03768417]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 41 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.11365949 -0.09165856 -0.14860846 -0.1509119  -0.31867552 -0.09039864\n",
            " -0.01711433 -0.20595789 -0.03858766 -0.1073853  -0.15678513 -0.23050982\n",
            " -0.14220329 -0.06337161  0.25917903 -0.1992908  -0.1525446  -0.18067859\n",
            " -0.04613857]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 42 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.12832063 -0.1054597  -0.15370205 -0.16475633 -0.3137112  -0.10235271\n",
            " -0.01949714 -0.20506343 -0.03085369 -0.1409792  -0.14868188 -0.25636035\n",
            " -0.13920245 -0.06718739  0.2862568  -0.20573638 -0.14590724 -0.18636736\n",
            " -0.06170315]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 43 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.09736472 -0.07221819 -0.16643786 -0.15437321 -0.3132385  -0.09120496\n",
            " -0.00607485 -0.21011767 -0.0442006  -0.13451102 -0.15076613 -0.23292395\n",
            " -0.13855189 -0.05297858  0.29994762 -0.22026685 -0.16316058 -0.18531589\n",
            " -0.03821799]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 44 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.09968393 -0.09248441 -0.15916456 -0.14117885 -0.28655493 -0.09268254\n",
            " -0.01305688 -0.18422881 -0.03360057 -0.12091072 -0.14227132 -0.22484308\n",
            " -0.12516426 -0.04224734  0.22904432 -0.18645974 -0.13687694 -0.17024548\n",
            " -0.05205969]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 45 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.08867823 -0.07030569 -0.15419467 -0.14674902 -0.29317504 -0.08423592\n",
            " -0.00790328 -0.19626898 -0.03188042 -0.12588316 -0.13846947 -0.22957791\n",
            " -0.13686664 -0.04762338  0.2752989  -0.19710827 -0.1425163  -0.17287326\n",
            " -0.04075879]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 46 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.11121149 -0.08997787 -0.15772216 -0.13263308 -0.282896   -0.09204264\n",
            " -0.02083745 -0.18003415 -0.03149851 -0.10611098 -0.14802147 -0.20843342\n",
            " -0.1261195  -0.05588277  0.20909545 -0.18909495 -0.13030791 -0.16791977\n",
            " -0.0477792 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 47 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.09387768 -0.07566249 -0.15106192 -0.1354549  -0.284264   -0.08435763\n",
            " -0.0101671  -0.18546718 -0.02949432 -0.11924946 -0.13651352 -0.21926281\n",
            " -0.13033089 -0.04849511  0.23919092 -0.1910865  -0.13661017 -0.16712138\n",
            " -0.0421017 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 48 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.09862314 -0.08313792 -0.15230341 -0.12754229 -0.27474174 -0.088351\n",
            " -0.0154921  -0.17850085 -0.03331172 -0.11137539 -0.13912204 -0.20431443\n",
            " -0.12224008 -0.04984873  0.21419977 -0.1886451  -0.13388802 -0.15935637\n",
            " -0.03823631]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 49 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10575207 -0.08631815 -0.14968692 -0.12967865 -0.27375284 -0.08515626\n",
            " -0.01601523 -0.17746468 -0.03153494 -0.11642014 -0.13434634 -0.2144598\n",
            " -0.12195804 -0.04867786  0.2142783  -0.18564673 -0.12965308 -0.15800026\n",
            " -0.04326859]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 50 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10002384 -0.0825054  -0.15105903 -0.1261784  -0.27168643 -0.08491639\n",
            " -0.01371634 -0.17538396 -0.03115344 -0.11007584 -0.13521422 -0.20623691\n",
            " -0.12095502 -0.04572194  0.20990726 -0.18560849 -0.1303734  -0.15835065\n",
            " -0.03928621]\n",
            "Epsilon reduced to 0.06561000000000002\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10154325 -0.08396968 -0.15093717 -0.12365371 -0.26744193 -0.08642088\n",
            " -0.01727474 -0.17142701 -0.03192996 -0.10846076 -0.13627313 -0.2002681\n",
            " -0.11778769 -0.04505637  0.19656415 -0.18452254 -0.13002184 -0.15431485\n",
            " -0.03871949]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068478 -0.08424521 -0.14906211 -0.12171704 -0.26382682 -0.08355744\n",
            " -0.01628254 -0.16912718 -0.03059251 -0.10625066 -0.1332841  -0.19958997\n",
            " -0.11707269 -0.04479904  0.18753642 -0.18244033 -0.12699181 -0.153517\n",
            " -0.03854699]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068481 -0.08399519 -0.14906213 -0.12170784 -0.26382717 -0.08355748\n",
            " -0.01616541 -0.16912723 -0.03027203 -0.10626473 -0.13328479 -0.1996995\n",
            " -0.11707269 -0.04471086  0.17882916 -0.18249854 -0.12699182 -0.15351704\n",
            " -0.03831946]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068483 -0.08376956 -0.14906214 -0.12169955 -0.26382747 -0.08355751\n",
            " -0.01605972 -0.16912727 -0.0299828  -0.10627744 -0.13328542 -0.19979836\n",
            " -0.1170727  -0.04463127  0.16762483 -0.18255109 -0.12699184 -0.15351708\n",
            " -0.03811412]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068485 -0.08356594 -0.14906216 -0.12169206 -0.26382774 -0.08355754\n",
            " -0.01596433 -0.16912732 -0.02972179 -0.1062889  -0.13328598 -0.19988757\n",
            " -0.11707271 -0.04455945  0.15434483 -0.1825985  -0.12699185 -0.15351713\n",
            " -0.03792882]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068487 -0.08338217 -0.14906217 -0.1216853  -0.263828   -0.08355756\n",
            " -0.01587825 -0.16912735 -0.02948625 -0.10629925 -0.13328649 -0.19996808\n",
            " -0.11707272 -0.04449464  0.13933891 -0.1826413  -0.12699187 -0.15351716\n",
            " -0.0377616 ]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068488 -0.08321635 -0.14906219 -0.12167921 -0.26382825 -0.08355758\n",
            " -0.01580057 -0.16912737 -0.02927369 -0.10630859 -0.13328695 -0.20004074\n",
            " -0.11707272 -0.04443615  0.12289919 -0.1826799  -0.12699187 -0.15351719\n",
            " -0.03761069]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1006849  -0.0830667  -0.1490622  -0.12167371 -0.26382846 -0.08355761\n",
            " -0.01573047 -0.1691274  -0.02908187 -0.10631701 -0.13328737 -0.20010631\n",
            " -0.11707273 -0.04438337  0.10527094 -0.18271475 -0.12699187 -0.15351722\n",
            " -0.03747451]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068491 -0.08293166 -0.1490622  -0.12166875 -0.26382864 -0.08355763\n",
            " -0.01566721 -0.16912743 -0.02890877 -0.10632461 -0.13328774 -0.20016548\n",
            " -0.11707274 -0.04433573  0.08666103 -0.18274619 -0.12699187 -0.15351725\n",
            " -0.03735162]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 10 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068493 -0.0828098  -0.1490622  -0.12166427 -0.2638288  -0.08355764\n",
            " -0.01561013 -0.16912746 -0.02875257 -0.10633148 -0.13328809 -0.20021887\n",
            " -0.11707274 -0.04429275  0.0672445  -0.18277456 -0.12699187 -0.15351728\n",
            " -0.03724072]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 11 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068493 -0.08269984 -0.1490622  -0.12166023 -0.26382896 -0.08355766\n",
            " -0.01555862 -0.16912748 -0.02861161 -0.10633767 -0.13328838 -0.20026705\n",
            " -0.11707274 -0.04425396  0.04716998 -0.18280016 -0.12699187 -0.15351729\n",
            " -0.03714065]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 12 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068494 -0.08260061 -0.1490622  -0.12165657 -0.2638291  -0.08355767\n",
            " -0.01551213 -0.1691275  -0.02848442 -0.10634325 -0.13328865 -0.20031053\n",
            " -0.11707274 -0.04421896  0.02656392 -0.18282327 -0.12699187 -0.1535173\n",
            " -0.03705035]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068495 -0.08251107 -0.1490622  -0.12165328 -0.26382923 -0.08355769\n",
            " -0.01547019 -0.16912751 -0.02836964 -0.1063483  -0.1332889  -0.20034976\n",
            " -0.11707274 -0.04418738  0.00553411 -0.18284412 -0.12699187 -0.15351732\n",
            " -0.03696886]\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068496 -0.08243027 -0.1490622  -0.12165031 -0.26382935 -0.0835577\n",
            " -0.01543234 -0.16912752 -0.02826607 -0.10635285 -0.13328913 -0.20038517\n",
            " -0.11707274 -0.04415888 -0.01582741 -0.18286294 -0.12699187 -0.15351734\n",
            " -0.03689533]\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068496 -0.08235737 -0.1490622  -0.12164763 -0.26382944 -0.08355771\n",
            " -0.02128065 -0.16912754 -0.02817262 -0.10635696 -0.13328932 -0.20041712\n",
            " -0.11707274 -0.04413317 -0.03510285 -0.18287991 -0.12699187 -0.15351735\n",
            " -0.03682899]\n",
            "\n",
            "Taking action 6 from 8\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068497 -0.08229158 -0.1490622  -0.1216452  -0.26382953 -0.08355772\n",
            " -0.03156251 -0.16912755 -0.02808829 -0.10636066 -0.1332895  -0.20044594\n",
            " -0.11707274 -0.04410996 -0.05249573 -0.18289523 -0.12699187 -0.15351737\n",
            " -0.03676912]\n",
            "\n",
            "Taking action 6 from 8\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068498 -0.08223222 -0.1490622  -0.12164302 -0.26382962 -0.08355772\n",
            " -0.04084009 -0.16912757 -0.03149568 -0.106364   -0.13328966 -0.20047194\n",
            " -0.11707274 -0.04408903 -0.06818976 -0.18290906 -0.12699187 -0.15351738\n",
            " -0.0367151 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068499 -0.08217866 -0.1490622  -0.12164105 -0.26382968 -0.08355773\n",
            " -0.04921139 -0.16912758 -0.03786243 -0.10636702 -0.13328981 -0.2004954\n",
            " -0.11707274 -0.04407014 -0.0823507  -0.18292153 -0.12699187 -0.1535174\n",
            " -0.03666636]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068499 -0.08213033 -0.1490622  -0.12163928 -0.26382974 -0.08355774\n",
            " -0.05676485 -0.1691276  -0.04360718 -0.10636974 -0.13328995 -0.20051658\n",
            " -0.11707274 -0.04405309 -0.09512819 -0.18293278 -0.12699187 -0.15351741\n",
            " -0.04234533]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.100685   -0.08208672 -0.1490622  -0.12163768 -0.2638298  -0.08355775\n",
            " -0.0635803  -0.16912761 -0.04879064 -0.1063722  -0.13329007 -0.20053568\n",
            " -0.11707274 -0.04403771 -0.10665727 -0.18294293 -0.12699187 -0.15351743\n",
            " -0.05239152]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08204737 -0.1490622  -0.12163623 -0.26382986 -0.08355775\n",
            " -0.0697298  -0.16912763 -0.05346762 -0.10637441 -0.13329017 -0.20055293\n",
            " -0.11707274 -0.04397573 -0.11705981 -0.18295209 -0.12699187 -0.15351743\n",
            " -0.06145606]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08201187 -0.1490622  -0.12163493 -0.26382992 -0.08355776\n",
            " -0.07527834 -0.16912763 -0.05768754 -0.10637641 -0.13329028 -0.20056848\n",
            " -0.11707274 -0.04387164 -0.12644577 -0.18296036 -0.12699187 -0.15351743\n",
            " -0.06963478]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08197984 -0.1490622  -0.12163375 -0.26382995 -0.08355777\n",
            " -0.08028462 -0.16912763 -0.06149504 -0.10637821 -0.13329037 -0.20058252\n",
            " -0.11707274 -0.04372957 -0.13491443 -0.18296783 -0.12699187 -0.15351743\n",
            " -0.07701418]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08195094 -0.1490622  -0.12163269 -0.26382998 -0.08355778\n",
            " -0.08480157 -0.16912763 -0.06493039 -0.10637984 -0.13329044 -0.20059519\n",
            " -0.11707274 -0.04355326 -0.14255534 -0.18297456 -0.12699187 -0.15351743\n",
            " -0.0836723 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08192486 -0.1490622  -0.12163173 -0.26383    -0.08355778\n",
            " -0.08887699 -0.16912763 -0.06802994 -0.1063813  -0.13329051 -0.20060661\n",
            " -0.11707274 -0.04334613 -0.14944935 -0.18298063 -0.12699187 -0.15351743\n",
            " -0.08967959]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08190133 -0.1490622  -0.12163086 -0.26383004 -0.08355778\n",
            " -0.092554   -0.16912763 -0.07082648 -0.10638263 -0.13329057 -0.20061693\n",
            " -0.11707274 -0.0431113  -0.1556694  -0.18298611 -0.12699187 -0.15351743\n",
            " -0.09509961]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08188011 -0.1490622  -0.12163008 -0.26383007 -0.08355778\n",
            " -0.09587152 -0.16912763 -0.0733496  -0.10638382 -0.13329063 -0.20062622\n",
            " -0.11707274 -0.04285163 -0.16128135 -0.18299106 -0.12699187 -0.15351743\n",
            " -0.09998973]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08186096 -0.1490622  -0.12162937 -0.2638301  -0.08355778\n",
            " -0.09886467 -0.16912763 -0.07562602 -0.1063849  -0.1332907  -0.20063461\n",
            " -0.11707274 -0.04256972 -0.16634457 -0.18299551 -0.12699187 -0.15351743\n",
            " -0.10440172]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08184368 -0.1490622  -0.12162874 -0.26383013 -0.08355778\n",
            " -0.10156512 -0.16912763 -0.07767984 -0.10638588 -0.13329074 -0.20064218\n",
            " -0.11707274 -0.04226794 -0.17091268 -0.18299954 -0.12699187 -0.15351743\n",
            " -0.10838226]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.0818281  -0.1490622  -0.12162817 -0.26383016 -0.08355778\n",
            " -0.10400148 -0.16912763 -0.07953281 -0.10638676 -0.13329078 -0.20064901\n",
            " -0.11707274 -0.04194847 -0.17503406 -0.18300317 -0.12699187 -0.15351743\n",
            " -0.11197354]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08181403 -0.1490622  -0.12162765 -0.26383018 -0.08355778\n",
            " -0.10619957 -0.16912763 -0.08120455 -0.10638755 -0.13329083 -0.20065518\n",
            " -0.11707274 -0.04161327 -0.17875235 -0.18300645 -0.12699187 -0.15351743\n",
            " -0.11521357]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08180134 -0.1490622  -0.12162718 -0.2638302  -0.08355778\n",
            " -0.10818265 -0.16912763 -0.08271278 -0.10638826 -0.13329086 -0.20066074\n",
            " -0.11707274 -0.04126415 -0.18210694 -0.1830094  -0.12699187 -0.15351743\n",
            " -0.1181367 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.0817899  -0.1490622  -0.12162676 -0.26383024 -0.08355778\n",
            " -0.10997175 -0.16912763 -0.08407347 -0.10638891 -0.13329089 -0.20066576\n",
            " -0.11707274 -0.04090276 -0.1851334  -0.18301207 -0.12699187 -0.15351743\n",
            " -0.12077388]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08177957 -0.1490622  -0.12162638 -0.26383024 -0.08355778\n",
            " -0.11158583 -0.16912763 -0.08530105 -0.10638949 -0.13329092 -0.20067029\n",
            " -0.11707274 -0.04053058 -0.18786378 -0.18301447 -0.12699187 -0.15351743\n",
            " -0.12315308]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08177026 -0.1490622  -0.12162604 -0.26383024 -0.08355778\n",
            " -0.113042   -0.16912763 -0.08640853 -0.10639001 -0.13329095 -0.20067437\n",
            " -0.11707274 -0.040149   -0.19032703 -0.18301664 -0.12699187 -0.15351743\n",
            " -0.1252995 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08176185 -0.1490622  -0.12162574 -0.26383024 -0.08355778\n",
            " -0.11435568 -0.16912763 -0.08740764 -0.10639049 -0.13329098 -0.20067805\n",
            " -0.11707274 -0.03975926 -0.19254927 -0.1830186  -0.12699187 -0.15351743\n",
            " -0.1272359 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08175427 -0.1490622  -0.12162546 -0.26383024 -0.08355778\n",
            " -0.11554082 -0.16912763 -0.08830899 -0.10639092 -0.13329099 -0.20068137\n",
            " -0.11707274 -0.03936249 -0.19455406 -0.18302035 -0.12699187 -0.15351743\n",
            " -0.12898283]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08174743 -0.1490622  -0.12162521 -0.26383024 -0.08355778\n",
            " -0.11660998 -0.16912763 -0.08912213 -0.1063913  -0.133291   -0.20068437\n",
            " -0.11707274 -0.03895973 -0.19636266 -0.18302195 -0.12699187 -0.15351743\n",
            " -0.13055879]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08174126 -0.1490622  -0.12162498 -0.26383024 -0.08355778\n",
            " -0.1175745  -0.16912763 -0.08985569 -0.10639165 -0.13329102 -0.20068707\n",
            " -0.11707274 -0.03855192 -0.19799425 -0.18302338 -0.12699187 -0.15351743\n",
            " -0.13198052]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08173569 -0.1490622  -0.12162478 -0.26383024 -0.08355778\n",
            " -0.11844461 -0.16912763 -0.09051745 -0.10639197 -0.13329104 -0.20068951\n",
            " -0.11707274 -0.03813993 -0.19946614 -0.18302467 -0.12699187 -0.15351743\n",
            " -0.1332631 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08173067 -0.1490622  -0.1216246  -0.26383024 -0.08355778\n",
            " -0.11922956 -0.16912763 -0.09111444 -0.10639225 -0.13329105 -0.20069171\n",
            " -0.11707274 -0.03772452 -0.20079395 -0.18302584 -0.12699187 -0.15351743\n",
            " -0.13442013]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08172614 -0.1490622  -0.12162443 -0.26383024 -0.08355778\n",
            " -0.11993767 -0.16912763 -0.09165298 -0.1063925  -0.13329107 -0.2006937\n",
            " -0.11707274 -0.03730643 -0.20199178 -0.1830269  -0.12699187 -0.15351743\n",
            " -0.1354639 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08172205 -0.1490622  -0.12162428 -0.26383024 -0.08355778\n",
            " -0.12057645 -0.16912763 -0.0921388  -0.10639273 -0.13329108 -0.20069548\n",
            " -0.11707274 -0.03688629 -0.20307235 -0.18302785 -0.12699187 -0.15351743\n",
            " -0.13640548]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08171836 -0.1490622  -0.12162415 -0.26383024 -0.08355778\n",
            " -0.12115269 -0.16912763 -0.09257706 -0.10639294 -0.1332911  -0.2006971\n",
            " -0.11707274 -0.03646468 -0.20404713 -0.18302871 -0.12699187 -0.15351743\n",
            " -0.13725488]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08171504 -0.1490622  -0.12162403 -0.26383024 -0.08355778\n",
            " -0.12167251 -0.16912763 -0.09297241 -0.10639313 -0.13329111 -0.20069855\n",
            " -0.11707274 -0.03604216 -0.20492646 -0.18302949 -0.12699187 -0.15351743\n",
            " -0.13802111]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08171204 -0.1490622  -0.12162392 -0.26383024 -0.08355778\n",
            " -0.12214143 -0.16912763 -0.09332903 -0.1063933  -0.13329113 -0.20069987\n",
            " -0.11707274 -0.0356192  -0.20571968 -0.18303019 -0.12699187 -0.15351743\n",
            " -0.1387123 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08170933 -0.1490622  -0.12162382 -0.26383024 -0.08355778\n",
            " -0.12256442 -0.16912763 -0.09365074 -0.10639345 -0.13329113 -0.20070106\n",
            " -0.11707274 -0.03519625 -0.20643522 -0.18303081 -0.12699187 -0.15351743\n",
            " -0.13933581]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08170689 -0.1490622  -0.12162373 -0.26383024 -0.08355778\n",
            " -0.12294599 -0.16912763 -0.09394094 -0.10639358 -0.13329113 -0.20070213\n",
            " -0.11707274 -0.0347737  -0.20708068 -0.18303138 -0.12699187 -0.15351743\n",
            " -0.13989826]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08170468 -0.1490622  -0.12162365 -0.26383024 -0.08355778\n",
            " -0.12329018 -0.16912763 -0.09420272 -0.10639371 -0.13329113 -0.2007031\n",
            " -0.11707274 -0.03435192 -0.20766293 -0.18303189 -0.12699187 -0.15351743\n",
            " -0.14040561]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08170269 -0.1490622  -0.12162358 -0.26383024 -0.08355778\n",
            " -0.12360066 -0.16912763 -0.09443885 -0.10639382 -0.13329113 -0.20070396\n",
            " -0.11707274 -0.03393124 -0.20818813 -0.18303235 -0.12699187 -0.15351743\n",
            " -0.14086327]\n",
            "Epsilon reduced to 0.05904900000000002\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.0817009  -0.1490622  -0.12162351 -0.26383024 -0.08355778\n",
            " -0.12388073 -0.16912763 -0.09465186 -0.10639393 -0.13329113 -0.20070475\n",
            " -0.11707274 -0.03335276 -0.2086619  -0.18303277 -0.12699187 -0.15351743\n",
            " -0.14127609]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08169928 -0.1490622  -0.12162345 -0.26383024 -0.08355778\n",
            " -0.12413336 -0.16912763 -0.09484399 -0.10639402 -0.13329113 -0.20070547\n",
            " -0.11707274 -0.03279172 -0.20908925 -0.18303314 -0.12699187 -0.15351743\n",
            " -0.14164847]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08169782 -0.1490622  -0.1216234  -0.26383024 -0.08355778\n",
            " -0.12436123 -0.16912763 -0.0950173  -0.1063941  -0.13329113 -0.20070611\n",
            " -0.11707274 -0.03224701 -0.20947473 -0.18303348 -0.12699187 -0.15351743\n",
            " -0.14198437]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.0816965  -0.1490622  -0.12162335 -0.26383024 -0.08355778\n",
            " -0.12456678 -0.16912763 -0.09517363 -0.10639417 -0.13329113 -0.20070669\n",
            " -0.11707274 -0.03171758 -0.20982243 -0.1830338  -0.12699187 -0.15351743\n",
            " -0.14228736]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08169532 -0.1490622  -0.12162331 -0.26383024 -0.08355778\n",
            " -0.12475219 -0.16912763 -0.09531464 -0.10639424 -0.13329113 -0.20070721\n",
            " -0.11707274 -0.03120248 -0.21013607 -0.18303408 -0.12699187 -0.15351743\n",
            " -0.14256066]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08169425 -0.1490622  -0.12162327 -0.26383024 -0.08355778\n",
            " -0.12491942 -0.16912763 -0.09544183 -0.1063943  -0.13329113 -0.20070767\n",
            " -0.11707274 -0.03070084 -0.21041897 -0.18303433 -0.12699187 -0.15351743\n",
            " -0.14280717]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08169328 -0.1490622  -0.12162323 -0.26383024 -0.08355778\n",
            " -0.12507027 -0.16912763 -0.09555656 -0.10639435 -0.13329113 -0.20070809\n",
            " -0.11707274 -0.03021186 -0.21067415 -0.18303455 -0.12699187 -0.15351743\n",
            " -0.14302953]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08169241 -0.1490622  -0.1216232  -0.26383024 -0.08355778\n",
            " -0.12520634 -0.16912763 -0.09566005 -0.1063944  -0.13329113 -0.20070848\n",
            " -0.11707274 -0.02973479 -0.21090432 -0.18303476 -0.12699187 -0.15351743\n",
            " -0.1432301 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08169162 -0.1490622  -0.12162317 -0.26383024 -0.08355778\n",
            " -0.12532906 -0.16912763 -0.09575339 -0.10639445 -0.13329113 -0.20070882\n",
            " -0.11707274 -0.02926898 -0.21111192 -0.18303494 -0.12699187 -0.15351743\n",
            " -0.143411  ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08169091 -0.1490622  -0.12162315 -0.26383024 -0.08355778\n",
            " -0.12543976 -0.16912763 -0.09583758 -0.10639448 -0.13329113 -0.20070913\n",
            " -0.11707274 -0.02881379 -0.21129918 -0.1830351  -0.12699187 -0.15351743\n",
            " -0.14357416]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08169027 -0.1490622  -0.12162313 -0.26383024 -0.08355778\n",
            " -0.12553962 -0.16912763 -0.09591351 -0.10639452 -0.13329113 -0.20070942\n",
            " -0.11707274 -0.02836866 -0.21146809 -0.18303525 -0.12699187 -0.15351743\n",
            " -0.14372134]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.0816897  -0.1490622  -0.12162311 -0.26383024 -0.08355778\n",
            " -0.12562968 -0.16912763 -0.09598201 -0.10639455 -0.13329113 -0.20070967\n",
            " -0.11707274 -0.02793306 -0.21162044 -0.18303539 -0.12699187 -0.15351743\n",
            " -0.1438541 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168918 -0.1490622  -0.12162308 -0.26383024 -0.08355778\n",
            " -0.1257109  -0.16912763 -0.09604379 -0.10639458 -0.13329113 -0.2007099\n",
            " -0.11707274 -0.02750652 -0.21175784 -0.18303551 -0.12699187 -0.15351743\n",
            " -0.14397383]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168871 -0.1490622  -0.12162307 -0.26383024 -0.08355778\n",
            " -0.12578417 -0.16912763 -0.09609951 -0.10639461 -0.13329113 -0.2007101\n",
            " -0.11707274 -0.02708858 -0.21188177 -0.18303561 -0.12699187 -0.15351743\n",
            " -0.14408182]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168828 -0.1490622  -0.12162305 -0.26383024 -0.08355778\n",
            " -0.12585026 -0.16912763 -0.09614976 -0.10639463 -0.13329113 -0.20071028\n",
            " -0.11707274 -0.02667885 -0.21199356 -0.18303572 -0.12699187 -0.15351743\n",
            " -0.14417922]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.0816879  -0.1490622  -0.12162304 -0.26383024 -0.08355778\n",
            " -0.12590986 -0.16912763 -0.09619509 -0.10639466 -0.13329113 -0.20071045\n",
            " -0.11707274 -0.02627695 -0.21209438 -0.1830358  -0.12699187 -0.15351743\n",
            " -0.14426708]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168756 -0.1490622  -0.12162302 -0.26383024 -0.08355778\n",
            " -0.12596361 -0.16912763 -0.09623598 -0.10639468 -0.13329113 -0.2007106\n",
            " -0.11707274 -0.02588253 -0.21218531 -0.18303588 -0.12699187 -0.15351743\n",
            " -0.14434633]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 8\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168725 -0.1490622  -0.12162301 -0.26383024 -0.08355778\n",
            " -0.1260121  -0.16912763 -0.09627285 -0.10639469 -0.13329113 -0.20071073\n",
            " -0.11707274 -0.02549528 -0.21226732 -0.18303595 -0.12699187 -0.15351743\n",
            " -0.1444178 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168697 -0.1490622  -0.121623   -0.26383024 -0.08355778\n",
            " -0.12605584 -0.16912763 -0.09601518 -0.10639471 -0.13329113 -0.20071085\n",
            " -0.11707274 -0.025146   -0.2123413  -0.18303601 -0.12699187 -0.15351743\n",
            " -0.14448225]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168671 -0.1490622  -0.12162299 -0.26383024 -0.08355778\n",
            " -0.12609528 -0.16912763 -0.09578278 -0.10639472 -0.13329113 -0.20071095\n",
            " -0.11707274 -0.02480023 -0.212408   -0.18303607 -0.12699187 -0.15351743\n",
            " -0.14454038]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168648 -0.1490622  -0.12162299 -0.26383024 -0.08355778\n",
            " -0.12613085 -0.16912763 -0.09557318 -0.10639474 -0.13329113 -0.20071106\n",
            " -0.11707274 -0.02445798 -0.21246818 -0.18303613 -0.12699187 -0.15351743\n",
            " -0.14459282]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168627 -0.1490622  -0.12162298 -0.26383024 -0.08355778\n",
            " -0.12616293 -0.16912763 -0.09538414 -0.10639475 -0.13329113 -0.20071115\n",
            " -0.11707274 -0.02414931 -0.21252245 -0.18303618 -0.12699187 -0.15351743\n",
            " -0.14388709]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168609 -0.1490622  -0.12162297 -0.26383024 -0.08355778\n",
            " -0.12619187 -0.16912763 -0.09521365 -0.10639476 -0.13329113 -0.20071122\n",
            " -0.11707274 -0.0238412  -0.2125714  -0.18303622 -0.12699187 -0.15351743\n",
            " -0.14325058]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168592 -0.1490622  -0.12162296 -0.26383024 -0.08355778\n",
            " -0.12621796 -0.16912763 -0.09505989 -0.10639477 -0.13329113 -0.2007113\n",
            " -0.11707274 -0.02353391 -0.21261553 -0.18303627 -0.12699187 -0.15351743\n",
            " -0.14267653]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168577 -0.1490622  -0.12162296 -0.26383024 -0.08355778\n",
            " -0.12624149 -0.15981378 -0.09492121 -0.10639478 -0.13329113 -0.20071135\n",
            " -0.11707274 -0.02325677 -0.21265535 -0.1830363  -0.12699187 -0.15351743\n",
            " -0.1421588 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168564 -0.1490622  -0.12162295 -0.26383024 -0.08355778\n",
            " -0.12626271 -0.15141383 -0.09479614 -0.10639478 -0.13329113 -0.20071141\n",
            " -0.11707274 -0.01725062 -0.21269126 -0.18303633 -0.12699187 -0.15351743\n",
            " -0.14169188]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168552 -0.1490622  -0.12162294 -0.26383024 -0.08355778\n",
            " -0.12628186 -0.14383817 -0.09468334 -0.10639479 -0.13329113 -0.20071147\n",
            " -0.11707274 -0.00675731 -0.21272364 -0.18303636 -0.12699187 -0.15351743\n",
            " -0.14127077]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168541 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12629913 -0.13700594 -0.09458161 -0.1063948  -0.13329113 -0.20071152\n",
            " -0.11707274  0.00729536 -0.21275285 -0.18303639 -0.12699187 -0.15351743\n",
            " -0.14089099]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168531 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1263147  -0.13084424 -0.09448987 -0.10639481 -0.13329113 -0.20071156\n",
            " -0.11707274  0.02419436 -0.21277918 -0.18303642 -0.12699187 -0.15351743\n",
            " -0.14054848]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168522 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12632874 -0.12528728 -0.09440713 -0.10639481 -0.13329113 -0.20071161\n",
            " -0.11707274  0.04337991 -0.21280293 -0.18303643 -0.12699187 -0.15351743\n",
            " -0.14023958]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168514 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1263414  -0.12027575 -0.09433251 -0.10639482 -0.13329113 -0.20071164\n",
            " -0.11707274  0.06440528 -0.21282436 -0.18303645 -0.12699187 -0.15351743\n",
            " -0.139961  ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168507 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12635282 -0.11575615 -0.09426522 -0.10639483 -0.13329113 -0.20071167\n",
            " -0.11707274  0.08690935 -0.21284367 -0.18303646 -0.12699187 -0.15351743\n",
            " -0.13970977]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.081685   -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12636311 -0.1116802  -0.09420453 -0.10639483 -0.13329113 -0.2007117\n",
            " -0.11707274  0.11059729 -0.21286109 -0.18303648 -0.12699187 -0.15351743\n",
            " -0.1394832 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168494 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1263724  -0.10800439 -0.0941498  -0.10639483 -0.13329113 -0.20071173\n",
            " -0.11707274  0.13522638 -0.2128768  -0.18303649 -0.12699187 -0.15351743\n",
            " -0.13927887]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 35 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168489 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12638077 -0.09674166 -0.09410044 -0.10639483 -0.13329113 -0.20071176\n",
            " -0.11707274  0.15743752 -0.21289097 -0.1830365  -0.12699187 -0.15351743\n",
            " -0.1390946 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 36 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168484 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12638833 -0.08658472 -0.09405593 -0.10639483 -0.13329113 -0.20071177\n",
            " -0.11707274  0.17746794 -0.21290375 -0.18303652 -0.12699187 -0.13963205\n",
            " -0.13892843]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 37 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.0816848  -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12639514 -0.07742504 -0.09401578 -0.10639483 -0.13329113 -0.20071179\n",
            " -0.11707274  0.20071164 -0.21291527 -0.18303654 -0.12699187 -0.12711003\n",
            " -0.13877857]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 38 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168476 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12640128 -0.06916477 -0.09397958 -0.10639483 -0.13329113 -0.2007118\n",
            " -0.11707274  0.22631572 -0.21292566 -0.18303655 -0.12699187 -0.11581758\n",
            " -0.13864343]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 39 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168472 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12640682 -0.06171563 -0.09394693 -0.10639483 -0.13329113 -0.20071182\n",
            " -0.11707274  0.25368395 -0.21293503 -0.18303657 -0.12699187 -0.105634\n",
            " -0.13852155]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 40 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168469 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12641181 -0.05499802 -0.0939175  -0.10639483 -0.13329113 -0.20071183\n",
            " -0.11707274  0.2823774  -0.21294348 -0.18303657 -0.12699187 -0.09645049\n",
            " -0.13841164]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 41 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168466 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12641631 -0.04894013 -0.09389095 -0.10639483 -0.13329113 -0.20071185\n",
            " -0.11707274  0.31206194 -0.2129511  -0.18303657 -0.12699187 -0.08816886\n",
            " -0.13831253]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 42 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168464 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12642038 -0.04346913 -0.09386697 -0.10639483 -0.13329113 -0.20071186\n",
            " -0.11707274  0.3425222  -0.21295798 -0.18303657 -0.12699187 -0.08068956\n",
            " -0.13822302]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 43 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168462 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12642404 -0.03854277 -0.09384538 -0.10639483 -0.13329113 -0.20071188\n",
            " -0.11707274  0.3734621  -0.21296418 -0.18303657 -0.12699187 -0.07395482\n",
            " -0.13814242]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 12\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.0816846  -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12642735 -0.03410029 -0.09382591 -0.10639483 -0.13329113 -0.20071189\n",
            " -0.11707274  0.40476155 -0.21296977 -0.18303657 -0.12699187 -0.06788159\n",
            " -0.13806973]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 45 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168457 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12643033 -0.03009419 -0.09380836 -0.10639483 -0.13329113 -0.2007119\n",
            " -0.10873565  0.4329865  -0.2129748  -0.18303657 -0.12699187 -0.06240493\n",
            " -0.13800418]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 46 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168456 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12643301 -0.02648162 -0.09379253 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.10121755  0.4618473  -0.21297935 -0.18303657 -0.12699187 -0.05746626\n",
            " -0.13794509]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 47 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168454 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12643544 -0.02322396 -0.09377825 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.09443802  0.491175   -0.21298344 -0.18303657 -0.12699187 -0.05301277\n",
            " -0.13789178]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 48 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168453 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12643763 -0.02028634 -0.09376538 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.08832455  0.5208312  -0.21298714 -0.18303657 -0.12699187 -0.04899681\n",
            " -0.13784373]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 49 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168451 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1264396  -0.01763734 -0.09375377 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.08281171  0.5507023  -0.21299048 -0.18303657 -0.12699187 -0.04537541\n",
            " -0.1378004 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 50 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.0816845  -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12644137 -0.01524861 -0.0937433  -0.10639483 -0.13329113 -0.20071192\n",
            " -0.07784054  0.5806944  -0.21299349 -0.18303657 -0.12699187 -0.04210983\n",
            " -0.13776131]\n",
            "Epsilon reduced to 0.05314410000000002\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168449 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12644298 -0.0130946  -0.09373386 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.07335784  0.6101544  -0.2129962  -0.18303657 -0.12699187 -0.03916512\n",
            " -0.13772607]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168449 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12644443 -0.01115226 -0.09372535 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.06931562  0.636472   -0.21299864 -0.18303657 -0.12699187 -0.03650978\n",
            " -0.13769428]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168448 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12644573 -0.00940078 -0.09371768 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.06567063  0.6599449  -0.21300085 -0.18303657 -0.12699187 -0.03411537\n",
            " -0.13766563]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168447 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1264469  -0.00782144 -0.09371076 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.06238385  0.6808428  -0.21300283 -0.18303657 -0.12699187 -0.03195628\n",
            " -0.13763979]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168446 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12644796 -0.0063973  -0.09370451 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.05942009  0.6994099  -0.21300462 -0.18303657 -0.12699187 -0.03000937\n",
            " -0.13761649]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168446 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12644891 -0.00511314 -0.09369889 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.05674763  0.71586734 -0.21300623 -0.18303657 -0.12699187 -0.02825382\n",
            " -0.13759547]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168445 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12644978 -0.0039552  -0.09369382 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.05433783  0.73041546 -0.21300769 -0.18303657 -0.12699187 -0.02667082\n",
            " -0.13757654]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168444 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645055 -0.00291107 -0.09368924 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.05216491  0.74323565 -0.213009   -0.18303657 -0.12699187 -0.02524341\n",
            " -0.13755946]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645125 -0.00196958 -0.09368511 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.05020558  0.75449216 -0.21301018 -0.18303657 -0.12699187 -0.02395632\n",
            " -0.13754405]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645188 -0.00112065 -0.0936814  -0.10639483 -0.13329113 -0.20071192\n",
            " -0.04843885  0.7643337  -0.21301125 -0.18303657 -0.12699187 -0.02279576\n",
            " -0.13753016]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.00685008e-01 -8.16844255e-02 -1.49062201e-01 -1.21622935e-01\n",
            " -2.63830245e-01 -8.35577771e-02 -1.26452446e-01 -3.55166034e-04\n",
            " -9.36780423e-02 -1.06394827e-01 -1.33291125e-01 -2.00711921e-01\n",
            " -4.68458161e-02  7.72895098e-01 -2.13012218e-01 -1.83036566e-01\n",
            " -1.26991868e-01 -2.17492823e-02 -1.37517646e-01]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-1.00685008e-01 -8.16844255e-02 -1.49062201e-01 -1.21622935e-01\n",
            " -2.63830245e-01 -8.35577771e-02 -1.26452953e-01  3.35058256e-04\n",
            " -9.36750174e-02 -1.06394827e-01 -1.33291125e-01 -2.00711921e-01\n",
            " -4.54093926e-02  7.80298233e-01 -2.13013083e-01 -1.83036566e-01\n",
            " -1.26991868e-01 -2.08056904e-02 -1.37506351e-01]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645341  0.00095742 -0.09367229 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.04411419  0.7866536  -0.21301387 -0.18303657 -0.12699187 -0.01995487\n",
            " -0.13749617]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645383  0.00151859 -0.09366983 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.04294634  0.79206115 -0.21301457 -0.18303657 -0.12699187 -0.0191877\n",
            " -0.137487  ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1264542   0.00202459 -0.09366761 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.04189332  0.7966115  -0.21301521 -0.18303657 -0.12699187 -0.01849597\n",
            " -0.13747871]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645455  0.00248083 -0.09366561 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.04094384  0.8003866  -0.2130158  -0.18303657 -0.12699187 -0.01787225\n",
            " -0.13747124]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645486  0.0028922  -0.09366381 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.04008773  0.8034608  -0.21301632 -0.18303657 -0.12699187 -0.01730987\n",
            " -0.13746451]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645514  0.00326312 -0.09366219 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.0393158   0.80590117 -0.21301678 -0.18303657 -0.12699187 -0.01680279\n",
            " -0.13745844]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1264554   0.00359756 -0.09366072 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.0386198   0.8077687  -0.2130172  -0.18303657 -0.12699187 -0.01634558\n",
            " -0.13745297]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645562  0.00389911 -0.0936594  -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03799224  0.8091185  -0.21301757 -0.18303657 -0.12699187 -0.01593334\n",
            " -0.13744804]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645583  0.00417101 -0.09365821 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03742641  0.8100005  -0.21301791 -0.18303657 -0.12699187 -0.01556164\n",
            " -0.13744359]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.126456    0.00441616 -0.09365714 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03691623  0.8104598  -0.21301822 -0.18303657 -0.12699187 -0.0152265\n",
            " -0.13743958]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645617  0.00463719 -0.09365617 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03645623  0.8105374  -0.2130185  -0.18303657 -0.12699187 -0.01492433\n",
            " -0.13743596]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645632  0.00483648 -0.0936553  -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03604148  0.8102703  -0.21301876 -0.18303657 -0.12699187 -0.01465187\n",
            " -0.1374327 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645645  0.00501617 -0.09365451 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03566753  0.8096921  -0.21301898 -0.18303657 -0.12699187 -0.01440622\n",
            " -0.13742976]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645657  0.00517819 -0.0936538  -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03533036  0.80883306 -0.21301919 -0.18303657 -0.12699187 -0.01418474\n",
            " -0.1374271 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645668  0.00532426 -0.09365316 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03502637  0.8077208  -0.21301937 -0.18303657 -0.12699187 -0.01398505\n",
            " -0.13742472]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645678  0.00545596 -0.09365258 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03475228  0.8063801  -0.21301953 -0.18303657 -0.12699187 -0.013805\n",
            " -0.13742256]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645687  0.00557471 -0.09365206 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03450516  0.8048336  -0.21301968 -0.18303657 -0.12699187 -0.01364266\n",
            " -0.13742062]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645695  0.00568177 -0.09365159 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03428236  0.80310166 -0.21301982 -0.18303657 -0.12699187 -0.0134963\n",
            " -0.13741887]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645702  0.00577829 -0.09365117 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03408147  0.8012028  -0.21301994 -0.18303657 -0.12699187 -0.01336434\n",
            " -0.13741729]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645708  0.00586532 -0.09365079 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03390036  0.7991536  -0.21302004 -0.18303657 -0.12699187 -0.01324537\n",
            " -0.13741586]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645714  0.00594379 -0.09365045 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03373706  0.7969694  -0.21302015 -0.18303657 -0.12699187 -0.0131381\n",
            " -0.13741457]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1264572   0.00601453 -0.09365013 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03358984  0.79466385 -0.21302024 -0.18303657 -0.12699187 -0.01304139\n",
            " -0.13741341]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645724  0.00607831 -0.09364985 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.0334571   0.7922493  -0.21302031 -0.18303657 -0.12699187 -0.0129542\n",
            " -0.13741237]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645729  0.00613582 -0.0936496  -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03333743  0.7897371  -0.21302038 -0.18303657 -0.12699187 -0.01287558\n",
            " -0.13741143]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645733  0.00618766 -0.09364937 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03322954  0.7871374  -0.21302044 -0.18303657 -0.12699187 -0.01280471\n",
            " -0.13741058]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645736  0.0062344  -0.09364917 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03313226  0.7844594  -0.2130205  -0.18303657 -0.12699187 -0.01274081\n",
            " -0.13740982]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1264574   0.00627655 -0.09364899 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03304456  0.78171146 -0.21302056 -0.18303657 -0.12699187 -0.0126832\n",
            " -0.13740914]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645742  0.00631454 -0.09364882 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03296549  0.77890116 -0.21302061 -0.18303657 -0.12699187 -0.01263126\n",
            " -0.13740851]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645745  0.00634879 -0.09364867 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03289421  0.7760353  -0.21302065 -0.18303657 -0.12699187 -0.01258443\n",
            " -0.13740794]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645748  0.00637967 -0.09364854 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03282994  0.7731201  -0.2130207  -0.18303657 -0.12699187 -0.01254221\n",
            " -0.13740744]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1264575   0.00640751 -0.09364842 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.032772    0.77016115 -0.21302073 -0.18303657 -0.12699187 -0.01250415\n",
            " -0.13740698]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645751  0.00643261 -0.09364831 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03271976  0.7671636  -0.21302076 -0.18303657 -0.12699187 -0.01246984\n",
            " -0.13740656]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645753  0.00645524 -0.09364821 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03267267  0.76413196 -0.21302079 -0.18303657 -0.12699187 -0.0124389\n",
            " -0.13740619]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645754  0.00647565 -0.09364812 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03263021  0.76107043 -0.21302082 -0.18303657 -0.12699187 -0.01241101\n",
            " -0.13740586]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645756  0.00649404 -0.09364804 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03259194  0.7579828  -0.21302085 -0.18303657 -0.12699187 -0.01238587\n",
            " -0.13740556]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645757  0.00651062 -0.09364796 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03255743  0.75487244 -0.21302086 -0.18303657 -0.12699187 -0.0123632\n",
            " -0.13740529]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645759  0.00652557 -0.0936479  -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03252632  0.7517425  -0.21302088 -0.18303657 -0.12699187 -0.01234276\n",
            " -0.13740505]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.1264576   0.00653905 -0.09364784 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03249827  0.7485958  -0.21302089 -0.18303657 -0.12699187 -0.01232434\n",
            " -0.13740483]\n",
            "Epsilon reduced to 0.04782969000000002\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645762  0.0065512  -0.09364779 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03247299  0.74415517 -0.2130209  -0.18303657 -0.12699187 -0.01230773\n",
            " -0.13740464]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00656215 -0.09364774 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03245019  0.73983073 -0.21302092 -0.18303657 -0.12699187 -0.01229275\n",
            " -0.13740446]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00657202 -0.0936477  -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03242964  0.7356124  -0.21302094 -0.18303657 -0.12699187 -0.01227925\n",
            " -0.1374043 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00658093 -0.09364766 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03241112  0.731491   -0.21302095 -0.18303657 -0.12699187 -0.01226708\n",
            " -0.13740414]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00658895 -0.09364762 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03239442  0.7274584  -0.21302097 -0.18303657 -0.12699187 -0.01225611\n",
            " -0.13740401]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00659619 -0.09364759 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03237936  0.72350705 -0.21302098 -0.18303657 -0.12699187 -0.01224622\n",
            " -0.13740389]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00660271 -0.09364756 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03236578  0.7196302  -0.213021   -0.18303657 -0.12699187 -0.01223731\n",
            " -0.13740379]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00660859 -0.09364754 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03235355  0.7158217  -0.213021   -0.18303657 -0.12699187 -0.01222927\n",
            " -0.1374037 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00661389 -0.09364752 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03234252  0.71207607 -0.213021   -0.18303657 -0.12699187 -0.01222202\n",
            " -0.1374036 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00661867 -0.09364749 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03233257  0.70838827 -0.213021   -0.18303657 -0.12699187 -0.01221549\n",
            " -0.13740353]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00662298 -0.09364747 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.0323236   0.7047537  -0.213021   -0.18303657 -0.12699187 -0.0122096\n",
            " -0.13740346]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00662686 -0.09364746 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03231552  0.7011683  -0.213021   -0.18303657 -0.12699187 -0.01220429\n",
            " -0.1374034 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00663036 -0.09364744 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03230824  0.6976283  -0.213021   -0.18303657 -0.12699187 -0.0121995\n",
            " -0.13740334]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00663352 -0.09364743 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03230167  0.6941304  -0.213021   -0.18303657 -0.12699187 -0.01219519\n",
            " -0.1374033 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00663636 -0.09364741 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03229575  0.6906715  -0.213021   -0.18303657 -0.12699187 -0.0121913\n",
            " -0.13740325]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00663893 -0.0936474  -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03229041  0.6872488  -0.213021   -0.18303657 -0.12699187 -0.01218779\n",
            " -0.1374032 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00664124 -0.09364739 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.0322856   0.6838598  -0.213021   -0.18303657 -0.12699187 -0.01218463\n",
            " -0.13740316]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00664332 -0.09364738 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03228126  0.6805023  -0.213021   -0.18303657 -0.12699187 -0.01218178\n",
            " -0.13740313]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.0066452  -0.09364738 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03227735  0.67717415 -0.213021   -0.18303657 -0.12699187 -0.01217921\n",
            " -0.1374031 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.0066469  -0.09364737 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03227383  0.67387354 -0.213021   -0.18303657 -0.12699187 -0.0121769\n",
            " -0.13740307]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00664842 -0.09364736 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03227065  0.6708982  -0.213021   -0.18303657 -0.12699187 -0.01217481\n",
            " -0.13269494]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.0066498  -0.09364735 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03226778  0.66791743 -0.213021   -0.18303657 -0.12699187 -0.01217293\n",
            " -0.12845078]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665104 -0.09364735 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.0322652   0.6649327  -0.213021   -0.18303657 -0.12699187 -0.01217123\n",
            " -0.12462488]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665216 -0.09364734 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03226287  0.66194534 -0.213021   -0.18303657 -0.12699187 -0.01216971\n",
            " -0.12117603]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665317 -0.09364733 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03226078  0.65895647 -0.213021   -0.18303657 -0.12699187 -0.01216833\n",
            " -0.11806708]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665407 -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03225888  0.6559672  -0.213021   -0.18303657 -0.12699187 -0.01216708\n",
            " -0.11526454]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665489 -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03225718  0.65297836 -0.213021   -0.18303657 -0.12699187 -0.01216596\n",
            " -0.11273822]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665563 -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03225564  0.649991   -0.213021   -0.18303657 -0.12699187 -0.01216496\n",
            " -0.11046091]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.0066563  -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03225425  0.64700574 -0.213021   -0.18303657 -0.12699187 -0.01216405\n",
            " -0.10840806]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.0066569  -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.032253    0.6440234  -0.213021   -0.18303657 -0.12699187 -0.01216322\n",
            " -0.10655757]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665744 -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03225188  0.6410446  -0.213021   -0.18303657 -0.12699187 -0.01216249\n",
            " -0.10488949]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665793 -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03225087  0.6380699  -0.213021   -0.18303657 -0.12699187 -0.01216182\n",
            " -0.10338584]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665837 -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03224995  0.6350999  -0.213021   -0.18303657 -0.12699187 -0.01216122\n",
            " -0.10203043]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665876 -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03224913  0.632135   -0.213021   -0.18303657 -0.12699187 -0.01216068\n",
            " -0.10080863]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665912 -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03224838  0.6291756  -0.213021   -0.18303657 -0.12699187 -0.01216019\n",
            " -0.09970728]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665944 -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03224771  0.6262222  -0.213021   -0.18303657 -0.12699187 -0.01215975\n",
            " -0.09871452]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665973 -0.09364732 -0.10639483 -0.13329113 -0.20071192\n",
            " -0.03224711  0.6232751  -0.213021   -0.18303657 -0.12699187 -0.01215935\n",
            " -0.09781963]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00665999 -0.09364732 -0.09961925 -0.13329113 -0.20071192\n",
            " -0.03224656  0.6206186  -0.213021   -0.18303657 -0.12699187 -0.01215899\n",
            " -0.09701297]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00666023 -0.09364732 -0.09351172 -0.13329113 -0.20071192\n",
            " -0.03224607  0.6202273  -0.213021   -0.18303657 -0.12699187 -0.01215867\n",
            " -0.09628586]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00666044 -0.09364732 -0.08800641 -0.13329113 -0.20071192\n",
            " -0.03224563  0.6218564  -0.213021   -0.18303657 -0.12699187 -0.01215838\n",
            " -0.09563043]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00666063 -0.09364732 -0.08304394 -0.13329113 -0.20071192\n",
            " -0.03224523  0.6252861  -0.213021   -0.18303657 -0.12699187 -0.01215812\n",
            " -0.09503963]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00666081 -0.09084104 -0.07857081 -0.13329113 -0.20071192\n",
            " -0.03224487  0.6283776  -0.213021   -0.18303657 -0.12699187 -0.01215788\n",
            " -0.09450709]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00666096 -0.08831149 -0.07453877 -0.13329113 -0.20071192\n",
            " -0.03224455  0.63088036 -0.213021   -0.18303657 -0.12699187 -0.01215767\n",
            " -0.09402706]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.0066611  -0.08603138 -0.07090434 -0.13329113 -0.20071192\n",
            " -0.03224426  0.6328509  -0.213021   -0.18303657 -0.12699187 -0.01215748\n",
            " -0.09359437]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00666123 -0.08397612 -0.06762832 -0.13329113 -0.20071192\n",
            " -0.03224399  0.6343404  -0.213021   -0.18303657 -0.12699187 -0.0121573\n",
            " -0.09320435]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00666134 -0.08212355 -0.06467538 -0.13329113 -0.20071192\n",
            " -0.03224375  0.63539535 -0.213021   -0.18303657 -0.12699187 -0.01215715\n",
            " -0.09285279]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00666145 -0.08045368 -0.06201366 -0.13329113 -0.20071192\n",
            " -0.03224354  0.6360578  -0.213021   -0.18303657 -0.12699187 -0.01215701\n",
            " -0.09253591]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00666154 -0.0789485  -0.05961445 -0.13329113 -0.20071192\n",
            " -0.03224335  0.6363657  -0.213021   -0.18303657 -0.12699187 -0.01215688\n",
            " -0.09225027]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.00666162 -0.07759177 -0.05745186 -0.13329113 -0.20071192\n",
            " -0.03224317  0.6363536  -0.213021   -0.18303657 -0.12699187 -0.01215677\n",
            " -0.09199281]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10068501 -0.08168443 -0.1490622  -0.12162293 -0.26383024 -0.08355778\n",
            " -0.12645763  0.0066617  -0.07636885 -0.05550257 -0.13329113 -0.20071192\n",
            " -0.03224302  0.63605267 -0.213021   -0.18303657 -0.12699187 -0.01215666\n",
            " -0.09176074]\n",
            "Epsilon reduced to 0.043046721000000024\n",
            "Total reward: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFxlAl2a45yN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b1cbf16-32b7-4c78-e2b2-915b3aa0d599"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=100, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=10, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_26(env), max_epsilon=0.1, epsilon_decay=0.9)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            " -0.15976723 -0.11257976  0.09704254 -0.1426222  -0.20286576 -0.02509947\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4847052\n",
            " -0.04898446]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 83 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09974854 -0.26154268 -0.03680118 -0.07103981  0.20459686 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09711615 -0.14155217 -0.20286576 -0.02509653\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4861475\n",
            " -0.03931977]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 84 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09834614 -0.26154268 -0.03679752 -0.07103981  0.20912416 -0.08601997\n",
            " -0.15976723 -0.11257976  0.0971825  -0.14058761 -0.20286576 -0.02509387\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4866655\n",
            " -0.03060786]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 85 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09708198 -0.26154268 -0.03679423 -0.07103981  0.21320514 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09724232 -0.13971816 -0.20286576 -0.02509148\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4863503\n",
            " -0.02275481]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 86 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09594246 -0.26154268 -0.03679125 -0.07103981  0.21688378 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09729624 -0.13893442 -0.20286576 -0.02508932\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4852844\n",
            " -0.01567599]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 87 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09491529 -0.26154268 -0.03678858 -0.07103981  0.22019973 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09734484 -0.13822795 -0.20286576 -0.02508738\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4835427\n",
            " -0.00929509]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 88 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09398939 -0.26154268 -0.03678616 -0.07103981  0.22318874 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09738865 -0.13477817 -0.20286576 -0.02508562\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4819726\n",
            " -0.00354331]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 89 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09315478 -0.26154268 -0.03678399 -0.07103981  0.22588305 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09742814 -0.13166852 -0.20286576 -0.02508404\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4797767\n",
            "  0.00164135]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 90 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09240247 -0.26154268 -0.03678202 -0.07103981  0.2283117  -0.08601997\n",
            " -0.15976723 -0.11257976  0.09746373 -0.1288655  -0.20286576 -0.02508262\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4770181\n",
            "  0.0063148 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 91 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09172434 -0.26154268 -0.03678025 -0.07103981  0.23050088 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09749582 -0.12633885 -0.20286576 -0.02508134\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4737537\n",
            "  0.01052743]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 92 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09111307 -0.26154268 -0.03677866 -0.07103981  0.23247418 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09752475 -0.12406136 -0.20286576 -0.02508018\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.470035\n",
            "  0.01432467]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 93 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09056208 -0.26154268 -0.03677722 -0.07103981  0.2342529  -0.08601997\n",
            " -0.15976723 -0.11257976  0.09755082 -0.12200845 -0.20286576 -0.02507914\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4659085\n",
            "  0.01774746]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 94 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09006543 -0.26154268 -0.03677592 -0.07103981  0.2358562  -0.08601997\n",
            " -0.15976723 -0.11257976  0.09757432 -0.12015798 -0.20286576 -0.0250782\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4614167\n",
            "  0.02083272]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 95 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08961775 -0.26154268 -0.03677476 -0.07103981  0.23730141 -0.08601997\n",
            " -0.15976723 -0.11257976  0.0975955  -0.11849    -0.20286576 -0.02507735\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4565978\n",
            "  0.02361372]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 96 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08921423 -0.26154268 -0.0367737  -0.07103981  0.23860408 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09761459 -0.11698651 -0.20286576 -0.02507658\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4514862\n",
            "  0.02612047]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 97 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.0888505  -0.26154268 -0.03677275 -0.07103981  0.2397783  -0.08601997\n",
            " -0.15976723 -0.11257976  0.0976318  -0.11563129 -0.20286576 -0.0250759\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4461135\n",
            "  0.02838   ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 98 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08852264 -0.26154268 -0.0367719  -0.07103981  0.2408367  -0.08601997\n",
            " -0.15976723 -0.11257976  0.09764732 -0.11440973 -0.20286576 -0.02507528\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4405077\n",
            "  0.03041669]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 99 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08822712 -0.26154268 -0.03677113 -0.07103981  0.24179071 -0.08601997\n",
            " -0.15976723 -0.11257976  0.0976613  -0.11330865 -0.20286576 -0.02507472\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4346945\n",
            "  0.0322525 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 100 reward=0 new_state=[0 0 0 0 1 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08796075 -0.26154268 -0.03677043 -0.07103981  0.24265063 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09767391 -0.11231618 -0.20286576 -0.02507421\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4286973\n",
            "  0.03390725]\n",
            "Epsilon reduced to 0.06561000000000002\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08772065 -0.26154268 -0.0367698  -0.07103981  0.24342573 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09768527 -0.11142159 -0.20286576 -0.02507376\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4196864\n",
            "  0.03539879]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08750423 -0.26154268 -0.03676924 -0.07103981  0.24412438 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09769551 -0.11061523 -0.20286576 -0.02507335\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4108313\n",
            "  0.0367432 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08730916 -0.26154268 -0.03676873 -0.07103981  0.24475412 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09770474 -0.10988842 -0.20286576 -0.02507298\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.4021205\n",
            "  0.03795501]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08713333 -0.26154268 -0.03676827 -0.07103981  0.24532174 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09771306 -0.1092333  -0.20286576 -0.02507265\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.3935436\n",
            "  0.03904728]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08697484 -0.26154268 -0.03676786 -0.07103981  0.24583337 -0.08601997\n",
            " -0.15976723 -0.11257976  0.09772056 -0.1086428  -0.20286576 -0.02507235\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.3850913\n",
            "  0.04003181]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08683199 -0.26154268 -0.03676748 -0.07103981  0.24629453 -0.08447028\n",
            " -0.15976723 -0.11257976  0.09772731 -0.10811055 -0.20286576 -0.02507208\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.3774728\n",
            "  0.04091922]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08670323 -0.26154268 -0.03676715 -0.07103981  0.2467102  -0.08307347\n",
            " -0.15976723 -0.11257976  0.09773341 -0.10763081 -0.20286576 -0.02507183\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.367432\n",
            "  0.04171908]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08658718 -0.26154268 -0.03676685 -0.07103981  0.24708486 -0.08181445\n",
            " -0.15976723 -0.11257976  0.0977389  -0.10719839 -0.20286576 -0.02507161\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.3552907\n",
            "  0.04244004]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 9 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08648257 -0.26154268 -0.03676657 -0.07103981  0.24742255 -0.08067964\n",
            " -0.15976723 -0.11257976  0.09774385 -0.10680864 -0.20286576 -0.02507141\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.3413306\n",
            "  0.04308987]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 10 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08638828 -0.26154268 -0.03676633 -0.07103981  0.24772693 -0.07965679\n",
            " -0.15976723 -0.11257976  0.09774831 -0.10645734 -0.20286576 -0.02507124\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.3257992\n",
            "  0.04367559]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 11 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0863033  -0.26154268 -0.0367661  -0.07103981  0.24800128 -0.07873484\n",
            " -0.15976723 -0.11257976  0.09775233 -0.1061407  -0.20286576 -0.02507108\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.3089134\n",
            "  0.04420353]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 12 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0862267  -0.26154268 -0.0367659  -0.07103981  0.24824856 -0.07790387\n",
            " -0.15976723 -0.11257976  0.09775595 -0.10585529 -0.20286576 -0.02507093\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.2908634\n",
            "  0.04467937]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 13 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08615766 -0.26154268 -0.03676572 -0.07103981  0.24847145 -0.07715487\n",
            " -0.15976723 -0.11257976  0.09775922 -0.10559805 -0.20286576 -0.0250708\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.2718165\n",
            "  0.04510827]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08609543 -0.26154268 -0.03676556 -0.07103981  0.24867235 -0.07647979\n",
            " -0.15976723 -0.11257976  0.09776216 -0.10536619 -0.20286576 -0.02507068\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.2519194\n",
            "  0.04549485]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08603934 -0.26154268 -0.03676542 -0.07103981  0.24885342 -0.07587131\n",
            " -0.15976723 -0.11257976  0.09776481 -0.1051572  -0.20286576 -0.02507058\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.2313008\n",
            "  0.04584328]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08598879 -0.26154268 -0.03676528 -0.07103981  0.24901661 -0.07532287\n",
            " -0.15976723 -0.11257976  0.0977672  -0.10496884 -0.20286576 -0.02507048\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.2100741\n",
            "  0.04615734]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08594322 -0.26154268 -0.03676517 -0.07103981  0.24916372 -0.07482855\n",
            " -0.15976723 -0.11257976  0.09776936 -0.10479906 -0.20286576 -0.0250704\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.1883385\n",
            "  0.0464404 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08590215 -0.26154268 -0.03676506 -0.07103981  0.24929631 -0.07438301\n",
            " -0.15976723 -0.11257976  0.0977713  -0.10464604 -0.20286576 -0.02507032\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.1661812\n",
            "  0.04669554]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08586514 -0.26154268 -0.03676496 -0.07103981  0.24941581 -0.07398143\n",
            " -0.15976723 -0.11257976  0.09777305 -0.10450812 -0.20286576 -0.02507025\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.1436788\n",
            "  0.04692549]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08583178 -0.26154268 -0.03676488 -0.07103981  0.24952352 -0.07361948\n",
            " -0.15976723 -0.11257976  0.09777463 -0.1043838  -0.20286576 -0.02507018\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.1208984\n",
            "  0.04713276]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08580171 -0.26154268 -0.0367648  -0.07103981  0.2496206  -0.07329325\n",
            " -0.15976723 -0.11257976  0.09777606 -0.10427176 -0.20286576 -0.02507013\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.0978988\n",
            "  0.04731956]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0857746  -0.26154268 -0.03676473 -0.07103981  0.2497081  -0.07299922\n",
            " -0.15976723 -0.11257976  0.09777734 -0.10417078 -0.20286576 -0.02507007\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.0747317\n",
            "  0.04748794]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08575017 -0.26154268 -0.03676466 -0.07103981  0.24978696 -0.0727342\n",
            " -0.15976723 -0.11257976  0.09777849 -0.10407975 -0.20286576 -0.02507003\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.0514421\n",
            "  0.04763969]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08572815 -0.26154268 -0.03676461 -0.07103981  0.24985804 -0.07249534\n",
            " -0.15976723 -0.11257976  0.09777953 -0.10399771 -0.20286576 -0.02506999\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.0280695\n",
            "  0.04777647]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08570831 -0.26154268 -0.03676455 -0.07103981  0.2499221  -0.07228006\n",
            " -0.15976723 -0.11257976  0.09778047 -0.10392378 -0.20286576 -0.02506995\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   1.0046483\n",
            "  0.04789975]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08569042 -0.26154268 -0.03676451 -0.07103981  0.24997984 -0.07208602\n",
            " -0.15976723 -0.11257976  0.09778132 -0.10385713 -0.20286576 -0.02506991\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.9812087\n",
            "  0.04801086]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08567429 -0.26154268 -0.03676447 -0.07103981  0.2500319  -0.07191113\n",
            " -0.15976723 -0.11257976  0.09778208 -0.10379706 -0.20286576 -0.02506988\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.95777667\n",
            "  0.04811101]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08565976 -0.26154268 -0.03676443 -0.07103981  0.2500788  -0.07175351\n",
            " -0.15976723 -0.11257976  0.09778277 -0.10374293 -0.20286576 -0.02506986\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.93437505\n",
            "  0.04820127]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 29 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08564667 -0.26154268 -0.03676439 -0.07103981  0.2501211  -0.07161144\n",
            " -0.15976723 -0.11257976  0.09778339 -0.10369413 -0.20286576 -0.02506983\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.9132833\n",
            "  0.04462591]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 30 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08563486 -0.26154268 -0.03676436 -0.07103981  0.2501592  -0.0714834\n",
            " -0.15976723 -0.11257976  0.09778395 -0.10365016 -0.20286576 -0.02506981\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.8919998\n",
            "  0.04140346]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 31 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08562423 -0.26154268 -0.03676434 -0.07103981  0.25019354 -0.07136799\n",
            " -0.15976723 -0.11257976  0.09778445 -0.10361052 -0.20286576 -0.02506979\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.87056684\n",
            "  0.0384991 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 32 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08561464 -0.26154268 -0.03676431 -0.07103981  0.2502245  -0.07126398\n",
            " -0.15976723 -0.11257976  0.09778491 -0.1035748  -0.20286576 -0.02506977\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.84902173\n",
            "  0.03588144]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 33 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08560599 -0.26154268 -0.03676429 -0.07103981  0.2502524  -0.07117024\n",
            " -0.15976723 -0.11257976  0.09778532 -0.1035426  -0.20286576 -0.02506975\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.8273972\n",
            "  0.03352218]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 34 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08559821 -0.26154268 -0.03676427 -0.07103981  0.25027755 -0.07108575\n",
            " -0.15976723 -0.11257976  0.09778568 -0.10351358 -0.20286576 -0.02506974\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.8057221\n",
            "  0.03139582]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 35 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08559119 -0.26154268 -0.03676425 -0.07103981  0.2503002  -0.0710096\n",
            " -0.15976723 -0.11257976  0.09778602 -0.10348743 -0.20286576 -0.02506973\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.7840217\n",
            "  0.02947938]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08558486 -0.26154268 -0.03676424 -0.07103981  0.2503206  -0.07094096\n",
            " -0.15976723 -0.11257976  0.09778631 -0.10346386 -0.20286576 -0.02506972\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.7623181\n",
            "  0.02775213]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 37 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08557916 -0.26154268 -0.03676422 -0.07103981  0.25033903 -0.07087911\n",
            " -0.15976723 -0.11257976  0.09778658 -0.10344262 -0.20286576 -0.0250697\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.74063075\n",
            "  0.0261954 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 38 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08557402 -0.26154268 -0.03676421 -0.07103981  0.25035563 -0.07082336\n",
            " -0.15976723 -0.11257976  0.09778683 -0.10342347 -0.20286576 -0.02506969\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.71897656\n",
            "  0.02479236]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 39 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08556939 -0.26154268 -0.0367642  -0.07103981  0.2503706  -0.07077311\n",
            " -0.15976723 -0.11257976  0.09778704 -0.10340621 -0.20286576 -0.02506969\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.69737023\n",
            "  0.02352784]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 40 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08556522 -0.26154268 -0.03676419 -0.07103981  0.25038406 -0.07072783\n",
            " -0.15976723 -0.11257976  0.09778725 -0.10339066 -0.20286576 -0.02506968\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.67582464\n",
            "  0.02238817]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 41 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08556145 -0.26154268 -0.03676417 -0.07103981  0.25039622 -0.07068701\n",
            " -0.15976723 -0.11257976  0.09778742 -0.10337665 -0.20286576 -0.02506967\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.6543508\n",
            "  0.02136102]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 42 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08555806 -0.26154268 -0.03676417 -0.07103981  0.25040716 -0.07065023\n",
            " -0.15976723 -0.11257976  0.09778759 -0.10336401 -0.20286576 -0.02506967\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.63295835\n",
            "  0.02043529]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 43 reward=-1 new_state=[0 0 1 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.08555501 -0.26154268 -0.03676416 -0.07103981  0.25041702 -0.07061707\n",
            " -0.15976723 -0.11257976  0.09778773 -0.10335263 -0.20286576 -0.02506966\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.6116555\n",
            "  0.01960097]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08555225 -0.26154268 -0.03676415 -0.07103981  0.2504259  -0.0705872\n",
            " -0.15976723 -0.11257976  0.09778786 -0.10334237 -0.20286576 -0.02506965\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.5904492\n",
            "  0.01884903]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08554977 -0.26154268 -0.03676414 -0.07103981  0.25043392 -0.07056027\n",
            " -0.15976723 -0.11257976  0.09778798 -0.10333312 -0.20286576 -0.02213699\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.5713369\n",
            "  0.01817133]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08554754 -0.26154268 -0.03676414 -0.07103981  0.25044113 -0.070536\n",
            " -0.15976723 -0.11257976  0.09778808 -0.10332479 -0.20286576 -0.01949391\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.55389565\n",
            "  0.01756056]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08554552 -0.26154268 -0.03676413 -0.07103981  0.25044763 -0.07051413\n",
            " -0.15976723 -0.11257976  0.09778818 -0.10331728 -0.20286576 -0.01711183\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.53796667\n",
            "  0.0170101 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0855437  -0.26154268 -0.03676413 -0.07103981  0.2504535  -0.07049441\n",
            " -0.15976723 -0.11257976  0.09778827 -0.1033105  -0.20286576 -0.01496498\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.5234063\n",
            "  0.01651399]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08554206 -0.26154268 -0.03676413 -0.07103981  0.25045878 -0.07047665\n",
            " -0.15976723 -0.11257976  0.09778834 -0.1033044  -0.20286576 -0.01303014\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.5100847\n",
            "  0.01606688]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08554059 -0.26154268 -0.03676412 -0.07103981  0.25046355 -0.07046064\n",
            " -0.15976723 -0.11257976  0.09778841 -0.1032989  -0.20286576 -0.01128636\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.49788418\n",
            "  0.01566392]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 51 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553925 -0.26154268 -0.03676412 -0.07103981  0.25046784 -0.07044621\n",
            " -0.15976723 -0.11257976  0.09778847 -0.10329395 -0.20286576 -0.0097148\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.48669845\n",
            "  0.01530076]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 52 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553805 -0.26154268 -0.03676412 -0.07103981  0.2504717  -0.0704332\n",
            " -0.15976723 -0.11257976  0.09778853 -0.10328948 -0.20286576 -0.00829844\n",
            " -0.09681693 -0.09237857 -0.09958698 -0.05768096 -0.0555824   0.47643128\n",
            "  0.01497346]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 53 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553697 -0.26154268 -0.03676411 -0.07103981  0.2504752  -0.07042148\n",
            " -0.15976723 -0.11257976  0.09778858 -0.10328545 -0.20286576 -0.00702196\n",
            " -0.09681693 -0.08040863 -0.09958698 -0.05768096 -0.0555824   0.4671781\n",
            "  0.01467848]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 54 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.085536   -0.26154268 -0.03676411 -0.07103981  0.25047833 -0.07041091\n",
            " -0.15976723 -0.11257976  0.09778862 -0.10328183 -0.20286576 -0.00587155\n",
            " -0.09681693 -0.06962086 -0.09958698 -0.05768096 -0.0555824   0.4606089\n",
            "  0.01441264]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 55 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553512 -0.26154268 -0.0367641  -0.07103981  0.25048116 -0.07040139\n",
            " -0.15976723 -0.11257976  0.09778867 -0.10327855 -0.20286576 -0.00483476\n",
            " -0.09681693 -0.05989855 -0.09958698 -0.05768096 -0.0555824   0.4564462\n",
            "  0.01417305]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 56 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553433 -0.26154268 -0.0367641  -0.07103981  0.25048372 -0.07039281\n",
            " -0.15976723 -0.11257976  0.09778871 -0.10327561 -0.20286576 -0.00390037\n",
            " -0.09681693 -0.05113649 -0.09958698 -0.05768096 -0.0555824   0.45444018\n",
            "  0.01395713]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 57 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553361 -0.26154268 -0.0367641  -0.07103981  0.25048602 -0.07038508\n",
            " -0.15976723 -0.11257976  0.09778874 -0.10327296 -0.20286576 -0.00305827\n",
            " -0.09681693 -0.04323984 -0.09958698 -0.05768096 -0.0555824   0.45436585\n",
            "  0.01376253]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 58 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553297 -0.26154268 -0.0367641  -0.07103981  0.2504881  -0.07037811\n",
            " -0.15976723 -0.11257976  0.09778877 -0.10327057 -0.20286576 -0.00229934\n",
            " -0.09681693 -0.03612316 -0.09958698 -0.05768096 -0.0555824   0.45602074\n",
            "  0.01358716]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 59 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553239 -0.26154268 -0.0367641  -0.07103981  0.25048998 -0.07037183\n",
            " -0.15976723 -0.11257976  0.0977888  -0.10326841 -0.20286576 -0.00161538\n",
            " -0.09681693 -0.02970941 -0.09958698 -0.05768096 -0.0555824   0.45922258\n",
            "  0.0134291 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 60 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553187 -0.26154268 -0.0367641  -0.07103981  0.25049168 -0.07036617\n",
            " -0.15976723 -0.11257976  0.09778883 -0.10326646 -0.20286576 -0.00099898\n",
            " -0.09681693 -0.02392919 -0.09958698 -0.05768096 -0.0555824   0.46380734\n",
            "  0.01328666]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 61 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-8.55313987e-02 -2.61542678e-01 -3.67641002e-02 -7.10398108e-02\n",
            "  2.50493199e-01 -7.03610629e-02 -1.59767225e-01 -1.12579755e-01\n",
            "  9.77888480e-02 -1.03264712e-01 -2.02865764e-01 -4.43457509e-04\n",
            " -9.68169272e-02 -1.87199209e-02 -9.95869786e-02 -5.76809645e-02\n",
            " -5.55824041e-02  4.69627440e-01  1.31582879e-02]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 62 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-8.55309740e-02 -2.61542678e-01 -3.67641002e-02 -7.10398108e-02\n",
            "  2.50494570e-01 -7.03564659e-02 -1.59767225e-01 -1.12579755e-01\n",
            "  9.77888703e-02 -1.03263132e-01 -2.02865764e-01  5.71865821e-05\n",
            " -9.68169272e-02 -1.40252225e-02 -9.95869786e-02 -5.76809645e-02\n",
            " -5.55824041e-02  4.76550102e-01  1.30425962e-02]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 63 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553059 -0.26154268 -0.0367641  -0.07103981  0.2504958  -0.07035232\n",
            " -0.15976723 -0.11257976  0.09778889 -0.10326171 -0.20286576  0.00050838\n",
            " -0.09681693 -0.00979428 -0.09958698 -0.05768096 -0.0555824   0.48445588\n",
            "  0.01293833]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 64 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08553025 -0.26154268 -0.0367641  -0.07103981  0.2504969  -0.07034859\n",
            " -0.15976723 -0.11257976  0.0977889  -0.10326043 -0.20286576  0.000915\n",
            " -0.09681693 -0.00598128 -0.09958698 -0.05768096 -0.0555824   0.49323738\n",
            "  0.01284437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 65 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552994 -0.26154268 -0.0367641  -0.07103981  0.2504979  -0.07034522\n",
            " -0.15976723 -0.11257976  0.09778892 -0.10325927 -0.20286576  0.00128145\n",
            " -0.09681693 -0.00254495 -0.09958698 -0.05768096 -0.0555824   0.50279796\n",
            "  0.01275969]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 66 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552966 -0.26154268 -0.0367641  -0.07103981  0.2504988  -0.07034219\n",
            " -0.15976723 -0.11257976  0.09778893 -0.10325823 -0.20286576  0.0016117\n",
            " -0.09681693  0.00055191 -0.09958698 -0.05768096 -0.0555824   0.51305073\n",
            "  0.01268337]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 67 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0855294  -0.26154268 -0.0367641  -0.07103981  0.2504996  -0.07033946\n",
            " -0.15976723 -0.11257976  0.09778894 -0.10325729 -0.20286576  0.00190932\n",
            " -0.09681693  0.00334283 -0.09958698 -0.05768096 -0.0555824   0.52391756\n",
            "  0.0126146 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 68 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552918 -0.26154268 -0.0367641  -0.07103981  0.25050035 -0.07033699\n",
            " -0.15976723 -0.11257976  0.09778895 -0.10325644 -0.20286576  0.00217754\n",
            " -0.09681693  0.00585803 -0.09958698 -0.05768096 -0.0555824   0.53532827\n",
            "  0.01255261]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 69 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552898 -0.26154268 -0.0367641  -0.07103981  0.250501   -0.07033477\n",
            " -0.15976723 -0.11257976  0.09778896 -0.10325568 -0.20286576  0.00241927\n",
            " -0.09681693  0.00812475 -0.09958698 -0.05768096 -0.0555824   0.5472197\n",
            "  0.01249675]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 70 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552879 -0.26154268 -0.0367641  -0.07103981  0.2505016  -0.07033277\n",
            " -0.15976723 -0.11257976  0.09778897 -0.103255   -0.20286576  0.00263711\n",
            " -0.09681693  0.01016752 -0.09958698 -0.05768096 -0.0555824   0.55953515\n",
            "  0.01244641]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 71 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552863 -0.26154268 -0.0367641  -0.07103981  0.25050214 -0.07033097\n",
            " -0.15976723 -0.11257976  0.09778897 -0.10325438 -0.20286576  0.00283343\n",
            " -0.09681693  0.01200847 -0.08637346 -0.05768096 -0.0555824   0.5706339\n",
            "  0.01240105]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 72 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552848 -0.26154268 -0.0367641  -0.07103981  0.25050262 -0.07032935\n",
            " -0.15976723 -0.11257976  0.09778898 -0.10325382 -0.20286576  0.00301035\n",
            " -0.09681693  0.01366753 -0.07446545 -0.05768096 -0.0555824   0.582228\n",
            "  0.01236016]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 73 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552834 -0.26154268 -0.0367641  -0.07103981  0.25050306 -0.07032789\n",
            " -0.15976723 -0.11257976  0.09778899 -0.10325332 -0.20286576  0.00316979\n",
            " -0.09681693  0.01516267 -0.06373397 -0.05768096 -0.0555824   0.5942596\n",
            "  0.01232332]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 74 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552822 -0.26154268 -0.0367641  -0.07103981  0.25050345 -0.07032657\n",
            " -0.15976723 -0.11257976  0.097789   -0.10325287 -0.20286576  0.00331348\n",
            " -0.09681693  0.01651009 -0.0540628  -0.05768096 -0.0555824   0.60667664\n",
            "  0.01229011]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 75 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552811 -0.26154268 -0.0367641  -0.07103981  0.2505038  -0.07032537\n",
            " -0.15976723 -0.11257976  0.097789   -0.10325246 -0.20286576  0.00344297\n",
            " -0.09681693  0.01772438 -0.0453472  -0.05768096 -0.0555824   0.6194324\n",
            "  0.01226019]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 76 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552802 -0.26154268 -0.0367641  -0.07103981  0.25050414 -0.0703243\n",
            " -0.15976723 -0.11257976  0.09778901 -0.10325209 -0.20286576  0.00355967\n",
            " -0.09681693  0.01881868 -0.03749277 -0.05768096 -0.0555824   0.6324848\n",
            "  0.01223322]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 77 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552793 -0.26154268 -0.0367641  -0.07103981  0.25050443 -0.07032333\n",
            " -0.15976723 -0.11257976  0.09778902 -0.10325176 -0.20286576  0.00366484\n",
            " -0.09681693  0.01980486 -0.03041444 -0.05768096 -0.0555824   0.6457961\n",
            "  0.01220892]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 78 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552784 -0.26154268 -0.0367641  -0.07103981  0.2505047  -0.07032246\n",
            " -0.15976723 -0.11257976  0.09778903 -0.10325146 -0.20286576  0.00375961\n",
            " -0.09681693  0.02069359 -0.02403553 -0.05768096 -0.0555824   0.65933245\n",
            "  0.01218702]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 79 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552777 -0.26154268 -0.0367641  -0.07103981  0.25050494 -0.07032168\n",
            " -0.15976723 -0.11257976  0.09778903 -0.10325119 -0.20286576  0.00384502\n",
            " -0.09681693  0.0214945  -0.01828695 -0.05768096 -0.0555824   0.6730634\n",
            "  0.01216729]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 80 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0855277  -0.26154268 -0.0367641  -0.07103981  0.25050515 -0.07032097\n",
            " -0.15976723 -0.11257976  0.09778903 -0.10325094 -0.20286576  0.00392199\n",
            " -0.09681693  0.02221626 -0.01310642 -0.05768096 -0.0555824   0.68696165\n",
            "  0.0121495 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 81 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552764 -0.26154268 -0.0367641  -0.07103981  0.25050533 -0.07032034\n",
            " -0.15976723 -0.11257976  0.09778903 -0.10325073 -0.20286576  0.00399135\n",
            " -0.09681693  0.02286671 -0.00843781 -0.05768096 -0.0555824   0.70100266\n",
            "  0.01213347]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 82 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552759 -0.26154268 -0.0367641  -0.07103981  0.2505055  -0.07031976\n",
            " -0.15976723 -0.11257976  0.09778903 -0.10325053 -0.20286576  0.00405386\n",
            " -0.09681693  0.02345288 -0.00423056 -0.05768096 -0.0555824   0.7151645\n",
            "  0.01211902]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 83 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-8.55275467e-02 -2.61542678e-01 -3.67641002e-02 -7.10398108e-02\n",
            "  2.50505656e-01 -7.03192502e-02 -1.43482417e-01 -1.12579755e-01\n",
            "  9.77890268e-02 -1.03250355e-01 -2.02865764e-01  4.11019521e-03\n",
            " -9.68169272e-02  2.39811204e-02 -4.39072028e-04 -5.76809645e-02\n",
            " -5.55824041e-02  7.27926791e-01  1.21060079e-02]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 84 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0855275  -0.26154268 -0.0367641  -0.07103981  0.2505058  -0.07031878\n",
            " -0.12880693 -0.11257976  0.09778903 -0.1032502  -0.20286576  0.00416096\n",
            " -0.09681693  0.02445716  0.00297773 -0.05768096 -0.0555824   0.74093\n",
            "  0.01209428]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 85 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552746 -0.26154268 -0.0367641  -0.07103981  0.25050592 -0.07031836\n",
            " -0.11558176 -0.11257976  0.09778903 -0.10325006 -0.20286576  0.00420671\n",
            " -0.09681693  0.02488616  0.00605686 -0.05768096 -0.0555824   0.7541427\n",
            "  0.01208371]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 86 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552743 -0.26154268 -0.0367641  -0.07103981  0.25050604 -0.07031798\n",
            " -0.10366359 -0.11257976  0.09778903 -0.10324993 -0.20286576  0.00424794\n",
            " -0.09681693  0.02527275  0.00883169 -0.05768096 -0.0555824   0.7675365\n",
            "  0.01207418]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 87 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0855274  -0.26154268 -0.0367641  -0.07103981  0.25050613 -0.07031764\n",
            " -0.09292328 -0.11257976  0.09778903 -0.10324981 -0.20286576  0.00428509\n",
            " -0.09681693  0.02562114  0.01133228 -0.05768096 -0.0555824   0.78108615\n",
            "  0.01206559]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 88 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552737 -0.26154268 -0.0299289  -0.07103981  0.25050622 -0.07031734\n",
            " -0.08324444 -0.11257976  0.09778903 -0.10324971 -0.20286576  0.00431857\n",
            " -0.09681693  0.0259351   0.01358574 -0.05768096 -0.0555824   0.79329664\n",
            "  0.01205786]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 89 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552735 -0.26154268 -0.02376924 -0.07103981  0.2505063  -0.07031706\n",
            " -0.0745222  -0.11257976  0.09778903 -0.10324961 -0.20286576  0.00434874\n",
            " -0.09681693  0.02621803  0.01561648 -0.05768096 -0.0555824   0.8057738\n",
            "  0.01205089]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 90 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552732 -0.26154268 -0.01821837 -0.07103981  0.25050637 -0.07031681\n",
            " -0.06666201 -0.11257976  0.09778903 -0.10324952 -0.20286576  0.00437593\n",
            " -0.09681693  0.026473    0.01744652 -0.05768096 -0.0555824   0.81848395\n",
            "  0.0120446 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 91 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0855273  -0.26154268 -0.01321613 -0.07103981  0.25050643 -0.07031658\n",
            " -0.0595787  -0.11257976  0.09778903 -0.10324945 -0.20286576  0.00440043\n",
            " -0.09681693  0.02670277  0.01909568 -0.05768096 -0.0555824   0.83139694\n",
            "  0.01203894]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 92 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552728 -0.26154268 -0.0087083  -0.07103981  0.2505065  -0.07031638\n",
            " -0.0531955  -0.11257976  0.09778903 -0.10324938 -0.20286576  0.00442251\n",
            " -0.09681693  0.02690982  0.02058184 -0.05768096 -0.0555824   0.8444856\n",
            "  0.01203384]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 93 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552726 -0.26154268 -0.00464602 -0.07103981  0.25050655 -0.0703162\n",
            " -0.0474432  -0.11257976  0.09778903 -0.10324932 -0.20286576  0.00444241\n",
            " -0.09681693  0.02709641  0.02192111 -0.05768096 -0.0555824   0.8577255\n",
            "  0.01202924]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 94 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552725 -0.26154268 -0.00098527 -0.07103981  0.2505066  -0.07031603\n",
            " -0.04225948 -0.11257976  0.09778903 -0.10324926 -0.20286576  0.00446034\n",
            " -0.09681693  0.02726456  0.023128   -0.05768096 -0.0555824   0.87109476\n",
            "  0.0120251 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 95 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552723 -0.26154268  0.00231363 -0.07103981  0.25050664 -0.07031588\n",
            " -0.03758814 -0.11257976  0.09778903 -0.10324921 -0.20286576  0.0044765\n",
            " -0.09681693  0.02741609  0.0242156  -0.05768096 -0.0555824   0.8845737\n",
            "  0.01202136]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 96 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552722 -0.26154268  0.00528646 -0.07103981  0.25050667 -0.07031575\n",
            " -0.03337854 -0.11257976  0.09778903 -0.10324916 -0.20286576  0.00449106\n",
            " -0.09681693  0.02755264  0.02519569 -0.05768096 -0.0555824   0.8981446\n",
            "  0.012018  ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 97 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552721 -0.26154268  0.00796543 -0.07103981  0.2505067  -0.07031563\n",
            " -0.02958504 -0.11257976  0.09778903 -0.10324912 -0.20286576  0.00450418\n",
            " -0.09681693  0.02767569  0.02607891 -0.05768096 -0.0555824   0.9117917\n",
            "  0.01201497]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 98 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0855272  -0.26154268  0.01037958 -0.07103981  0.25050673 -0.07031552\n",
            " -0.02616654 -0.11257976  0.09778903 -0.10324908 -0.20286576  0.00451601\n",
            " -0.09681693  0.02778658  0.02687482 -0.05768096 -0.0555824   0.92550075\n",
            "  0.01201223]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 99 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0855272  -0.26154268  0.0125551  -0.07103981  0.25050676 -0.07031542\n",
            " -0.02308595 -0.11257976  0.09778903 -0.10324904 -0.20286576  0.00452667\n",
            " -0.09681693  0.02788651  0.02759205 -0.05768096 -0.0555824   0.93925905\n",
            "  0.01200977]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 100 reward=1 new_state=[0 0 1 0 0 1 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.08552719 -0.26154268  0.01451555 -0.07103981  0.2505068  -0.07031533\n",
            " -0.02030989 -0.11257976  0.09778903 -0.10324901 -0.20286576  0.00453627\n",
            " -0.09681693  0.02797656  0.02823838 -0.05768096 -0.0555824   0.95305514\n",
            "  0.01200755]\n",
            "Epsilon reduced to 0.05904900000000002\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552718 -0.26154268  0.01628221 -0.07103981  0.25050682 -0.07031525\n",
            " -0.01780826 -0.11257976  0.09778903 -0.10324898 -0.20286576  0.00454492\n",
            " -0.09681693  0.02805771  0.02882082 -0.05768096 -0.0555824   0.9655701\n",
            "  0.01200555]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552717 -0.26154268  0.01787422 -0.07103981  0.25050685 -0.07031517\n",
            " -0.01555393 -0.11257976  0.09778903 -0.10324896 -0.20286576  0.00455272\n",
            " -0.09681693  0.02813083  0.02934568 -0.05768096 -0.0555824   0.97650534\n",
            "  0.01200375]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552717 -0.26154268  0.01930885 -0.07103981  0.25050688 -0.07031511\n",
            " -0.01352245 -0.11257976  0.09778903 -0.10324894 -0.20286576  0.00455975\n",
            " -0.09681693  0.02819673  0.02981866 -0.05768096 -0.0555824   0.98601294\n",
            "  0.01200213]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552716 -0.26154268  0.02060165 -0.07103981  0.2505069  -0.07031505\n",
            " -0.01169181 -0.11257976  0.09778903 -0.10324892 -0.20286576  0.00456608\n",
            " -0.09681693  0.02825611  0.03024487 -0.05768096 -0.0555824   0.9942304\n",
            "  0.01200066]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552715 -0.26154268  0.02176664 -0.07103981  0.25050694 -0.070315\n",
            " -0.01004215 -0.11257976  0.09778903 -0.1032489  -0.20286576  0.00457179\n",
            " -0.09681693  0.02830962  0.03062895 -0.05768096 -0.0555824   1.0012821\n",
            "  0.01199934]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.02281646 -0.07103981  0.25050694 -0.07031495\n",
            " -0.00855558 -0.11257976  0.09778903 -0.10324889 -0.20286576  0.00457693\n",
            " -0.09681693  0.02835784  0.03097506 -0.05768096 -0.0555824   1.0072806\n",
            "  0.01199816]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.02376249 -0.07103981  0.25050694 -0.07031491\n",
            " -0.00721598 -0.11257976  0.09778903 -0.10324887 -0.20286576  0.00458156\n",
            " -0.09681693  0.0284013   0.03128695 -0.05768096 -0.0555824   1.0123277\n",
            "  0.01199708]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.02461499 -0.07103981  0.25050694 -0.07031487\n",
            " -0.00600882 -0.11257976  0.09778903 -0.10324886 -0.20286576  0.00458574\n",
            " -0.09681693  0.02844045  0.031568   -0.05768096 -0.0555824   1.0165153\n",
            "  0.01199612]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.0253832  -0.07103981  0.25050694 -0.07031483\n",
            " -0.00492102 -0.11257976  0.09778903 -0.10324884 -0.20286576  0.0045895\n",
            " -0.09681693  0.02847574  0.03182127 -0.05768096 -0.0555824   1.0199265\n",
            "  0.01199525]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.02607546 -0.07103981  0.25050694 -0.0703148\n",
            " -0.00394076 -0.11257976  0.09778903 -0.10324883 -0.20286576  0.00459289\n",
            " -0.09681693  0.02850754  0.0320495  -0.05768096 -0.0555824   1.0226368\n",
            "  0.01199447]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.02669927 -0.07103981  0.25050694 -0.07031477\n",
            " -0.00305742 -0.11257976  0.09778903 -0.10324883 -0.20286576  0.00459595\n",
            " -0.09681693  0.02853619  0.03225516 -0.05768096 -0.0555824   1.024714\n",
            "  0.01199376]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.0272614  -0.07103981  0.25050694 -0.07031475\n",
            " -0.00226143 -0.11257976  0.09778903 -0.10324882 -0.20286576  0.0045987\n",
            " -0.09681693  0.02856201  0.03244048 -0.05768096 -0.0555824   1.0262197\n",
            "  0.01199312]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.02776796 -0.07103981  0.25050694 -0.07031473\n",
            " -0.00154413 -0.11257976  0.09778903 -0.10324881 -0.20286576  0.00460118\n",
            " -0.09681693  0.02858528  0.03260748 -0.05768096 -0.0555824   1.0272098\n",
            "  0.01199255]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-8.55271369e-02 -2.61542678e-01  2.82244254e-02 -7.10398108e-02\n",
            "  2.50506938e-01 -7.03147054e-02 -8.97763122e-04 -1.12579755e-01\n",
            "  9.77890268e-02 -1.03248805e-01 -2.02865764e-01  4.60341806e-03\n",
            " -9.68169272e-02  2.86062434e-02  3.27579752e-02 -5.76809645e-02\n",
            " -5.55824041e-02  1.02773428e+00  1.19920336e-02]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-8.55271369e-02 -2.61542678e-01  2.86357570e-02 -7.10398108e-02\n",
            "  2.50506938e-01 -7.03146905e-02 -3.15305777e-04 -1.12579755e-01\n",
            "  9.77890268e-02 -1.03248797e-01 -2.02865764e-01  4.60543297e-03\n",
            " -9.68169272e-02  2.86251362e-02  3.28935832e-02 -5.76809645e-02\n",
            " -5.55824041e-02  1.02783883e+00  1.19915679e-02]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-8.55271369e-02 -2.61542678e-01  2.90064160e-02 -7.10398108e-02\n",
            "  2.50506938e-01 -7.03146756e-02  2.09557882e-04 -1.12579755e-01\n",
            "  9.77890268e-02 -1.03248790e-01 -2.02865764e-01  4.60724859e-03\n",
            " -9.68169272e-02  2.86421608e-02  3.30157839e-02 -5.76809645e-02\n",
            " -5.55824041e-02  1.02756453e+00  1.19911488e-02]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-8.55271369e-02 -2.61542678e-01  2.93404236e-02 -7.10398108e-02\n",
            "  2.50506938e-01 -7.03146607e-02  6.82521961e-04 -1.12579755e-01\n",
            "  9.77890268e-02 -1.03248782e-01 -2.02865764e-01  4.60888445e-03\n",
            " -9.68169272e-02  2.86575034e-02  3.31258997e-02 -5.76809645e-02\n",
            " -5.55824041e-02  1.02694869e+00  1.19907707e-02]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.0296414  -0.07103981  0.25050694 -0.07031465\n",
            "  0.00110872 -0.11257976  0.09778903 -0.10324878 -0.20286576  0.00461036\n",
            " -0.09681693  0.02867133  0.03322513 -0.05768096 -0.0555824   1.0260249\n",
            "  0.01199043]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.02991262 -0.07103981  0.25050694 -0.07031463\n",
            "  0.00149277 -0.11257976  0.09778903 -0.10324877 -0.20286576  0.00461169\n",
            " -0.09681693  0.02868379  0.03331454 -0.05768096 -0.0555824   1.0248237\n",
            "  0.01199012]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.03015702 -0.07103981  0.25050694 -0.07031462\n",
            "  0.00183884 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00461288\n",
            " -0.09681693  0.02869501  0.03339512 -0.05768096 -0.0555824   1.0233724\n",
            "  0.01198985]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.03037725 -0.07103981  0.25050694 -0.07031462\n",
            "  0.00215069 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00461396\n",
            " -0.09681693  0.02870513  0.03346772 -0.05768096 -0.0555824   1.0216961\n",
            "  0.0119896 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.0305757  -0.07103981  0.25050694 -0.07031461\n",
            "  0.0024317  -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00461494\n",
            " -0.09681693  0.02871424  0.03353315 -0.05768096 -0.0555824   1.0198171\n",
            "  0.01198937]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.03075453 -0.07103981  0.25050694 -0.0703146\n",
            "  0.00268493 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00461581\n",
            " -0.09681693  0.02872246  0.0335921  -0.05768096 -0.0555824   1.017756\n",
            "  0.01198917]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.03091567 -0.07103981  0.25050694 -0.07031459\n",
            "  0.00291311 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.0046166\n",
            " -0.09681693  0.02872986  0.03364523 -0.05768096 -0.0555824   1.015531\n",
            "  0.01198899]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.03106087 -0.07103981  0.25050694 -0.07031459\n",
            "  0.00311872 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00461731\n",
            " -0.09681693  0.02873653  0.0336931  -0.05768096 -0.0555824   1.0131588\n",
            "  0.01198882]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.03119172 -0.07103981  0.25050694 -0.07031458\n",
            "  0.003304   -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00461795\n",
            " -0.09681693  0.02874254  0.03373624 -0.05768096 -0.0555824   1.0106544\n",
            "  0.01198868]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.03130962 -0.07103981  0.25050694 -0.07031457\n",
            "  0.00347096 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00461853\n",
            " -0.09681693  0.02874796  0.03377511 -0.05768096 -0.0555824   1.0080316\n",
            "  0.01198854]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.03141586 -0.07103981  0.25050694 -0.07031456\n",
            "  0.0036214  -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00461905\n",
            " -0.09681693  0.02875284  0.03381013 -0.05768096 -0.0555824   1.0053025\n",
            "  0.01198842]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.0315116  -0.07103981  0.25050694 -0.07031456\n",
            "  0.00375696 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00461952\n",
            " -0.09681693  0.02875724  0.0338417  -0.05768096 -0.0555824   1.0024784\n",
            "  0.01198831]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 30 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26154268  0.03159787 -0.07103981  0.25050694 -0.07031455\n",
            "  0.00387912 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00461994\n",
            " -0.09681693  0.0287612   0.03387014 -0.05768096 -0.0555824   0.9995692\n",
            "  0.01198822]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 31 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26099527  0.0316756  -0.07103981  0.25050694 -0.07031455\n",
            "  0.0039892  -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462032\n",
            " -0.09681693  0.02876477  0.03389577 -0.05768096 -0.0555824   0.99694777\n",
            "  0.01198813]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 32 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.260502    0.03174565 -0.07103981  0.25050694 -0.07031455\n",
            "  0.00408838 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462067\n",
            " -0.09681693  0.02876798  0.03391886 -0.05768096 -0.0555824   0.9924245\n",
            "  0.01198805]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 33 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.26005754  0.03180877 -0.07103981  0.25050694 -0.07031455\n",
            "  0.00208418 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462097\n",
            " -0.09681693  0.02877088  0.03393967 -0.05768096 -0.0555824   0.9883486\n",
            "  0.01198798]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 34 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-8.55271369e-02 -2.59657025e-01  3.18656415e-02 -7.10398108e-02\n",
            "  2.50506938e-01 -7.03145489e-02  2.78217136e-04 -1.12579755e-01\n",
            "  9.77890268e-02 -1.03248760e-01 -2.02865764e-01  4.62125335e-03\n",
            " -9.68169272e-02  2.87734959e-02  3.39584202e-02 -5.76809645e-02\n",
            " -5.55824041e-02  9.82532203e-01  1.19879134e-02]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 35 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25929612  0.03191689 -0.07103981  0.25050694 -0.07031455\n",
            " -0.00134912 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.0046215\n",
            " -0.09681693  0.02877585  0.03397532 -0.05768096 -0.0555824   0.9751677\n",
            "  0.01198786]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 36 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25897092  0.03196307 -0.07103981  0.25050694 -0.07031455\n",
            " -0.00281549 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462173\n",
            " -0.09681693  0.02877797  0.03399054 -0.05768096 -0.0555824   0.9664275\n",
            "  0.0119878 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 37 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25867787  0.03200468 -0.07103981  0.25050694 -0.07031455\n",
            " -0.00413682 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462193\n",
            " -0.09681693  0.02877988  0.03400426 -0.05768096 -0.0555824   0.9564663\n",
            "  0.01198776]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 38 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25841382  0.03204218 -0.07103981  0.25050694 -0.07031455\n",
            " -0.00532745 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462212\n",
            " -0.09681693  0.02878161  0.03401662 -0.05768096 -0.0555824   0.9454228\n",
            "  0.01198771]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 39 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25817588  0.03207597 -0.07103981  0.25050694 -0.07031455\n",
            " -0.0064003  -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462228\n",
            " -0.09681693  0.02878316  0.03402776 -0.05768096 -0.0555824   0.93342125\n",
            "  0.01198767]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 40 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25796148  0.03210641 -0.08307704  0.25050694 -0.07031455\n",
            " -0.00736703 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462243\n",
            " -0.09681693  0.02878456  0.0340378  -0.05768096 -0.0555824   0.9226069\n",
            "  0.01198764]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 41 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.2577683   0.03213385 -0.09392355  0.25050694 -0.07031455\n",
            " -0.00823813 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462257\n",
            " -0.09681693  0.02878582  0.03404684 -0.05768096 -0.0555824   0.90922695\n",
            "  0.01198761]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 42 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25759423  0.03215857 -0.10369711  0.25050694 -0.07031455\n",
            " -0.00902306 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462269\n",
            " -0.09681693  0.02878695  0.03405499 -0.05768096 -0.0555824   0.89363265\n",
            "  0.01198758]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 43 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25743738  0.03218084 -0.11250384  0.25050694 -0.07031455\n",
            " -0.00973035 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.0046228\n",
            " -0.09681693  0.02878798  0.03406234 -0.05768096 -0.0555824   0.8761305\n",
            "  0.01198756]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 44 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25729603  0.03220091 -0.12043937  0.25050694 -0.07031455\n",
            " -0.01036766 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.0046229\n",
            " -0.09681693  0.0287889   0.03406895 -0.05768096 -0.0555824   0.85698813\n",
            "  0.01198753]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 45 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25716868  0.032219   -0.12758987  0.25050694 -0.07031455\n",
            " -0.01094193 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462298\n",
            " -0.09681693  0.02878973  0.03407492 -0.05768096 -0.0555824   0.8364393\n",
            "  0.01198751]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 46 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.2570539   0.0322353  -0.13403298  0.25050694 -0.07031455\n",
            " -0.01145939 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462306\n",
            " -0.09681693  0.02879048  0.03408029 -0.05768096 -0.0555824   0.81468844\n",
            "  0.0119875 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 47 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.2569505   0.03224998 -0.13983868  0.25050694 -0.07031455\n",
            " -0.01192565 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462314\n",
            " -0.09681693  0.02879115  0.03408513 -0.05768096 -0.0555824   0.7919143\n",
            "  0.01198748]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 48 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25685734  0.03226322 -0.14507002  0.25050694 -0.07031455\n",
            " -0.01234579 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.0046232\n",
            " -0.09681693  0.02879176  0.03408949 -0.05768096 -0.0555824   0.7682732\n",
            "  0.01198746]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 49 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25677338  0.03227514 -0.14978382  0.25050694 -0.07031455\n",
            " -0.01272436 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462326\n",
            " -0.09681693  0.02879231  0.03409342 -0.05768096 -0.0555824   0.7439019\n",
            "  0.01198745]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 50 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25669774  0.03228588 -0.15403126  0.25050694 -0.07031455\n",
            " -0.01306548 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462331\n",
            " -0.09681693  0.0287928   0.03409697 -0.05768096 -0.0555824   0.71892023\n",
            "  0.01198744]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 51 reward=-2 new_state=[1 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.2566296   0.03229556 -0.15785849  0.25050694 -0.07031455\n",
            " -0.01337285 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462336\n",
            " -0.09681693  0.02879325  0.03410016 -0.05768096 -0.0555824   0.6934328\n",
            "  0.01198743]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 52 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08552714 -0.25656816  0.03230428 -0.16130707  0.25050694 -0.07031455\n",
            " -0.01364981 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.0046234\n",
            " -0.09681693  0.02879365  0.03410304 -0.05768096 -0.0555824   0.66753125\n",
            "  0.01198742]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 53 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08670744 -0.25651282  0.03231214 -0.16441445  0.25050694 -0.07031455\n",
            " -0.01389937 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462344\n",
            " -0.09681693  0.02879401  0.03410563 -0.05768096 -0.0555824   0.64419234\n",
            "  0.01198741]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 54 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08777097 -0.25646296  0.03231922 -0.1672144   0.25050694 -0.07031455\n",
            " -0.01412424 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462347\n",
            " -0.09681693  0.02879433  0.03410796 -0.05768096 -0.0555824   0.6215371\n",
            "  0.0119874 ]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 55 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08872927 -0.25641802  0.0323256  -0.16973731  0.25050694 -0.07031455\n",
            " -0.01432686 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462351\n",
            " -0.09681693  0.02879463  0.03411007 -0.05768096 -0.0555824   0.59950536\n",
            "  0.01198739]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 56 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.08959275 -0.25637752  0.03233135 -0.1720106   0.25050694 -0.07031455\n",
            " -0.01450943 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462353\n",
            " -0.09681693  0.02879489  0.03411197 -0.05768096 -0.0555824   0.5780429\n",
            "  0.01198739]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12 from 14\n",
            "\n",
            "Step 57 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.0903708  -0.25634104  0.03233653 -0.17405896  0.25050694 -0.07031455\n",
            " -0.01467394 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462356\n",
            " -0.09681693  0.02879513  0.03411367 -0.05768096 -0.0555824   0.55710083\n",
            "  0.01198738]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 58 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09107187 -0.25630817  0.0323412  -0.17590465  0.25050694 -0.07031455\n",
            " -0.01482217 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462358\n",
            " -0.09681693  0.02879534  0.02870873 -0.05768096 -0.0555824   0.53823084\n",
            "  0.01198737]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 59 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09170356 -0.25627854  0.03234541 -0.1775677   0.25050694 -0.07031455\n",
            " -0.01495573 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.0046236\n",
            " -0.09681693  0.02879554  0.02383859 -0.05768096 -0.0555824   0.5196294\n",
            "  0.01198737]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 60 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09227276 -0.25625184  0.0323492  -0.17906621  0.25050694 -0.07031455\n",
            " -0.01507608 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462362\n",
            " -0.09681693  0.02879571  0.01945033 -0.05768096 -0.0555824   0.50127745\n",
            "  0.01198736]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 61 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09278563 -0.2562278   0.03235261 -0.18041644  0.25050694 -0.07031455\n",
            " -0.01518452 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462364\n",
            " -0.09681693  0.02879587  0.01549629 -0.05768096 -0.0555824   0.48315763\n",
            "  0.01198736]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 62 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09324775 -0.25620613  0.03235569 -0.18163306  0.25050694 -0.07031455\n",
            " -0.01528223 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462365\n",
            " -0.09681693  0.02879601  0.01193351 -0.05768096 -0.0555824   0.46525428\n",
            "  0.01198736]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 63 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09366415 -0.2561866   0.03235846 -0.18272929  0.25050694 -0.07031455\n",
            " -0.01537027 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462367\n",
            " -0.09681693  0.02879613  0.00872327 -0.05768096 -0.0555824   0.4475532\n",
            "  0.01198735]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 64 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09403934 -0.25616902  0.03236096 -0.18371704  0.25050694 -0.07031455\n",
            " -0.0154496  -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462368\n",
            " -0.09681693  0.02879625  0.00583069 -0.05768096 -0.0555824   0.4300415\n",
            "  0.01198735]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 65 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.0943774  -0.25615317  0.03236321 -0.18460706  0.25050694 -0.07031455\n",
            " -0.01552108 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462369\n",
            " -0.09681693  0.02879635  0.00322435 -0.05768096 -0.0555824   0.4127075\n",
            "  0.01198735]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 66 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09468201 -0.2561389   0.03236524 -0.18540901  0.25050694 -0.07031455\n",
            " -0.01558548 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.0046237\n",
            " -0.09681693  0.02879645  0.00087592 -0.05768096 -0.0555824   0.39554068\n",
            "  0.01198735]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 67 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09495647 -0.25612602  0.03236707 -0.1861316   0.25050694 -0.07031455\n",
            " -0.01564351 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462371\n",
            " -0.09681693  0.02879653 -0.00124011 -0.05768096 -0.0555824   0.37853137\n",
            "  0.01198735]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 68 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09520378 -0.25611442  0.03236871 -0.18678267  0.25050694 -0.07031455\n",
            " -0.0156958  -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462372\n",
            " -0.09681693  0.02879661 -0.00314675 -0.05768096 -0.0555824   0.36167082\n",
            "  0.01198734]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 69 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09542661 -0.25610396  0.03237019 -0.18736932  0.25050694 -0.07031455\n",
            " -0.01574292 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462372\n",
            " -0.09681693  0.02879668 -0.0048647  -0.05768096 -0.0555824   0.34495112\n",
            "  0.01198734]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 70 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09562739 -0.25609455  0.03237153 -0.1878979   0.25050694 -0.07031455\n",
            " -0.01578537 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462373\n",
            " -0.09681693  0.02879674 -0.00641264 -0.05768096 -0.0555824   0.32836503\n",
            "  0.01198734]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 71 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.0958083  -0.25608605  0.03237274 -0.18837419  0.25050694 -0.07031455\n",
            " -0.01582362 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462374\n",
            " -0.09681693  0.02879679 -0.0078074  -0.05768096 -0.0555824   0.31190598\n",
            "  0.01198734]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 72 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.0959713  -0.25607842  0.03237382 -0.18880334  0.25050694 -0.07031455\n",
            " -0.01585809 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462374\n",
            " -0.09681693  0.02879684 -0.00906412 -0.05768096 -0.0555824   0.29556796\n",
            "  0.01198734]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 73 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09611817 -0.25607154  0.0323748  -0.18919002  0.25050694 -0.07031455\n",
            " -0.01588914 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462375\n",
            " -0.09681693  0.02879689 -0.01019647 -0.05768096 -0.0555824   0.27934548\n",
            "  0.01198734]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 74 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09625051 -0.25606534  0.03237568 -0.18953842  0.25050694 -0.07031455\n",
            " -0.01591712 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462375\n",
            " -0.09681693  0.02879693 -0.01121675 -0.05768096 -0.0555824   0.26323354\n",
            "  0.01198734]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 75 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09636975 -0.25605974  0.03237648 -0.18985234  0.25050694 -0.07031455\n",
            " -0.01594234 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462375\n",
            " -0.09681693  0.02879697 -0.01213606 -0.05768096 -0.0555824   0.24722758\n",
            "  0.01198734]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 76 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09647719 -0.2560547   0.03237719 -0.1901352   0.24208325 -0.07031455\n",
            " -0.01596505 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462376\n",
            " -0.09681693  0.028797   -0.01296439 -0.05768096 -0.0555824   0.23280571\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 77 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09657399 -0.25605017  0.03237784 -0.19039007  0.22723037 -0.07031455\n",
            " -0.01598552 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462376\n",
            " -0.09681693  0.02879703 -0.01371073 -0.05768096 -0.0555824   0.2198112\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 78 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09666122 -0.2560461   0.03237842 -0.1906197   0.20738085 -0.07031455\n",
            " -0.01600396 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462376\n",
            " -0.09681693  0.02879706 -0.01438321 -0.05768096 -0.0555824   0.20810278\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 79 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09673981 -0.2560424   0.03237894 -0.19082662  0.1894959  -0.07031455\n",
            " -0.01602058 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462377\n",
            " -0.09681693  0.02879708 -0.01498913 -0.05768096 -0.0555824   0.1960664\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 80 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09681063 -0.25603908  0.03237941 -0.19101305  0.17338112 -0.07031455\n",
            " -0.01603555 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462377\n",
            " -0.09681693  0.0287971  -0.01553508 -0.05768096 -0.0555824   0.18374075\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 81 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09687444 -0.2560361   0.03237984 -0.19118103  0.15886131 -0.07031455\n",
            " -0.01604904 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462377\n",
            " -0.09681693  0.02879712 -0.016027   -0.05768096 -0.0555824   0.17116065\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 82 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09693193 -0.25603342  0.03238022 -0.19133238  0.14577864 -0.07031455\n",
            " -0.0160612  -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462377\n",
            " -0.09681693  0.02879714 -0.01647022 -0.05768096 -0.0555824   0.1583574\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 83 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09698372 -0.25603098  0.03238057 -0.19146876  0.13399088 -0.07031455\n",
            " -0.01607215 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462377\n",
            " -0.09681693  0.02879716 -0.01686958 -0.05768096 -0.0555824   0.14535911\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 84 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09703039 -0.2560288   0.03238088 -0.19159164  0.12336987 -0.07031455\n",
            " -0.01608202 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462377\n",
            " -0.09681693  0.02879717 -0.0172294  -0.05768096 -0.0555824   0.13219109\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 85 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09707244 -0.25602683  0.03238115 -0.19170235  0.11380015 -0.07031455\n",
            " -0.01609091 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879718 -0.01755361 -0.05768096 -0.0555824   0.11887607\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 86 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09711034 -0.25602505  0.03238141 -0.1918021   0.10517766 -0.07031455\n",
            " -0.01609892 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879719 -0.01784573 -0.05768096 -0.0555824   0.10543447\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 6 from 8\n",
            "\n",
            "Step 87 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09714448 -0.25602344  0.03238164 -0.19189198  0.09740865 -0.07031455\n",
            " -0.01610614 -0.11257976  0.09778903 -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879721 -0.01810894 -0.05768096 -0.0555824   0.09188464\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 88 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09717524 -0.256022    0.03238184 -0.19197297  0.09040867 -0.07031455\n",
            " -0.01611264 -0.11257976  0.08470642 -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879721 -0.01834609 -0.05768096 -0.0555824   0.07967603\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 89 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09720296 -0.2560207   0.03238203 -0.19204594  0.07762161 -0.07031455\n",
            " -0.0161185  -0.11257976  0.0729188  -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879722 -0.01855977 -0.05768096 -0.0555824   0.06867592\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 6 from 8\n",
            "\n",
            "Step 90 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09722793 -0.25601953  0.03238219 -0.19211169  0.06017678 -0.07031455\n",
            " -0.01612378 -0.11257976  0.06229802 -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879723 -0.01875229 -0.05768096 -0.0555824   0.05876468\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 91 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09725043 -0.2560185   0.03238234 -0.19217092  0.04445879 -0.07031455\n",
            " -0.01612854 -0.11257976  0.04306341 -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879724 -0.01892576 -0.05768096 -0.0555824   0.04983455\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 15 from 17\n",
            "\n",
            "Step 92 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.0972707  -0.25601754  0.03238248 -0.1922243   0.03029673 -0.07031455\n",
            " -0.01613283 -0.11257976  0.02573283 -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879724 -0.01908206 -0.05768096 -0.0555824   0.04035065\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 93 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09728897 -0.25601667  0.0323826  -0.19227238  0.0175366  -0.07031455\n",
            " -0.01613669 -0.11257976  0.01011783 -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879725 -0.01922288 -0.05768096 -0.0555824   0.03037339\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 94 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09730543 -0.2560159   0.03235245 -0.19231571  0.00603963 -0.07031455\n",
            " -0.01614017 -0.11257976 -0.0039514  -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879726 -0.01934976 -0.05768096 -0.0555824   0.02138381\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 95 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09732026 -0.2560152   0.03229502 -0.19235475 -0.00431922 -0.07031455\n",
            " -0.0161433  -0.11257976 -0.01662785 -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879726 -0.01946408 -0.05768096 -0.0555824   0.01328414\n",
            "  0.01198733]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 96 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09733362 -0.2560146   0.03221303 -0.19238994 -0.01365258 -0.07031455\n",
            " -0.01614613 -0.11257976 -0.0280494  -0.10324876 -0.20286576  0.00462378\n",
            " -0.09681693  0.02879726 -0.01956709 -0.05768096 -0.0555824   0.0059863\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 97 reward=-1 new_state=[1 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09734567 -0.26261058  0.03213911 -0.19242164 -0.02206714 -0.07031455\n",
            " -0.01614868 -0.11257976 -0.03834656 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02879727 -0.01965995 -0.05768096 -0.0555824  -0.0005931\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 98 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09735651 -0.26854995  0.02746384 -0.1924502  -0.02964402 -0.07031455\n",
            " -0.01615097 -0.11257976 -0.04761865 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02879727 -0.01974357 -0.05768096 -0.0555824  -0.00651753\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 99 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09736629 -0.2739013   0.02325141 -0.19247593 -0.0364708  -0.07031455\n",
            " -0.01615303 -0.11257976 -0.0559728  -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02874128 -0.01981892 -0.05768096 -0.0555824  -0.01185545\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 100 reward=0 new_state=[1 0 0 0 0 1 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.09737509 -0.27872288  0.01945601 -0.19249912 -0.04262173 -0.07031455\n",
            " -0.01615489 -0.11257976 -0.0634999  -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02863489 -0.0198868  -0.05768096 -0.0555824  -0.01666491\n",
            "  0.01198733]\n",
            "Epsilon reduced to 0.05314410000000002\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09738303 -0.28306714  0.01603637 -0.19252001 -0.04816371 -0.07031455\n",
            " -0.01615657 -0.11257976 -0.0702818  -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02826005 -0.01994796 -0.05768096 -0.0555824  -0.02099823\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09739017 -0.28698128  0.01295527 -0.19253883 -0.05315702 -0.07031455\n",
            " -0.01615808 -0.11257976 -0.07639228 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.0278672  -0.02000307 -0.05768096 -0.0555824  -0.02490254\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09739661 -0.2905079   0.01017922 -0.19255579 -0.05765598 -0.07031455\n",
            " -0.01615945 -0.11257976 -0.0818978  -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02745882 -0.02005272 -0.05768096 -0.0555824  -0.02842031\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09740242 -0.2936854   0.00767801 -0.19257106 -0.06170951 -0.07031455\n",
            " -0.01616067 -0.11257976 -0.08685824 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.0270372  -0.02009745 -0.05768096 -0.0555824  -0.0315898\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09740765 -0.2965483   0.00542443 -0.19258483 -0.06536172 -0.07031455\n",
            " -0.01616178 -0.11257976 -0.09132756 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.0266044  -0.02013776 -0.05768096 -0.0555824  -0.03444549\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09741236 -0.29912776  0.00339398 -0.19259723 -0.06865233 -0.07031455\n",
            " -0.01616277 -0.11257976 -0.09535439 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02616233 -0.02017407 -0.05768096 -0.0555824  -0.03701845\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0974166  -0.3014518   0.00156456 -0.1926084  -0.07161715 -0.07031455\n",
            " -0.01616367 -0.11257976 -0.09898254 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02571271 -0.02020679 -0.05768096 -0.0555824  -0.03933667\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-9.74204242e-02 -3.03545743e-01 -8.37353291e-05 -1.92618474e-01\n",
            " -7.42884204e-02 -7.03145489e-02 -1.61644798e-02 -1.12579755e-01\n",
            " -1.02251455e-01 -1.03248760e-01 -2.02865764e-01  4.62378748e-03\n",
            " -9.68169272e-02  2.52571199e-02 -2.02362742e-02 -5.76809645e-02\n",
            " -5.55824041e-02 -4.14253548e-02  1.19873295e-02]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09742387 -0.30543238 -0.00156883 -0.19262755 -0.0766952  -0.07031455\n",
            " -0.01616521 -0.11257976 -0.10519671 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02479699 -0.02026284 -0.05768096 -0.0555824  -0.04330724\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09742697 -0.3071322  -0.00290688 -0.19263572 -0.07886369 -0.07031455\n",
            " -0.01616586 -0.11257976 -0.10785036 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02433362 -0.02028677 -0.05768096 -0.0555824  -0.04500279\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09742977 -0.30866373 -0.00411244 -0.19264308 -0.08081746 -0.07031455\n",
            " -0.01616645 -0.11257976 -0.11024126 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02386819 -0.02030833 -0.05768096 -0.0555824  -0.04653046\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09743229 -0.3100436  -0.00519864 -0.1926497  -0.08257778 -0.07031455\n",
            " -0.01616699 -0.11257976 -0.11239542 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02340177 -0.02032776 -0.05768096 -0.0555824  -0.04790687\n",
            "  0.01198733]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09743456 -0.31128687 -0.00617729 -0.19265568 -0.0841638  -0.07031455\n",
            " -0.01616747 -0.11257976 -0.11433629 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02293534 -0.02034526 -0.05768096 -0.0555824  -0.04914699\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09705175 -0.31240702 -0.00705903 -0.19266106 -0.08559278 -0.07031455\n",
            " -0.0161679  -0.11257976 -0.11608498 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02251509 -0.02036103 -0.05768096 -0.0555824  -0.05026432\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09670684 -0.31341624 -0.00785347 -0.19266592 -0.08688026 -0.07031455\n",
            " -0.01616829 -0.11257976 -0.11766052 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.0220919  -0.02037524 -0.05768096 -0.0555824  -0.05127102\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09639608 -0.31432554 -0.00856924 -0.19267029 -0.08804026 -0.07031455\n",
            " -0.01616864 -0.11257976 -0.11908004 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02166685 -0.02038804 -0.05768096 -0.0555824  -0.05217804\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0961161  -0.3151448  -0.00921413 -0.19267422 -0.08908539 -0.07031455\n",
            " -0.01616896 -0.11257976 -0.12035901 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02124093 -0.02039957 -0.05768096 -0.0555824  -0.05299524\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09586383 -0.31588295 -0.00979517 -0.19267777 -0.09002703 -0.07031455\n",
            " -0.01616924 -0.11257976 -0.12151133 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02081502 -0.02040997 -0.05768096 -0.0555824  -0.05373152\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09563655 -0.316548   -0.01031867 -0.19268097 -0.09087543 -0.07031455\n",
            " -0.0161695  -0.11257976 -0.12254955 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.02038992 -0.02041933 -0.05768096 -0.0555824  -0.05439489\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09543178 -0.31714717 -0.01079033 -0.19268385 -0.09163982 -0.07031455\n",
            " -0.01616973 -0.11257976 -0.12348495 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.01996635 -0.02042777 -0.05768096 -0.0555824  -0.05499258\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09524728 -0.317687   -0.01121529 -0.19268644 -0.09232852 -0.07031455\n",
            " -0.01616994 -0.11257976 -0.12432773 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.01954496 -0.02043537 -0.05768096 -0.0555824  -0.05553107\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09508105 -0.3181734  -0.01159816 -0.19268878 -0.09294902 -0.07031455\n",
            " -0.01617013 -0.11257976 -0.12508705 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.01912633 -0.02044221 -0.05768096 -0.0555824  -0.05601624\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0949312  -0.3186119  -0.01194332 -0.19269088 -0.09350839 -0.07031455\n",
            " -0.01617029 -0.11257976 -0.12577158 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.01871074 -0.02044839 -0.05768096 -0.0555824  -0.05645362\n",
            "  0.01198733]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 15\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 0 0 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09479626 -0.3190067  -0.01225412 -0.19269277 -0.09401208 -0.07031455\n",
            " -0.01617045 -0.11257976 -0.12638797 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.01829914 -0.02045394 -0.05768096 -0.0555824  -0.05684746\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 0 0 0 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09467469 -0.31936243 -0.01253414 -0.19269449 -0.09446589 -0.07031455\n",
            " -0.01617059 -0.11257976 -0.1269433  -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.0179283  -0.02045895 -0.06225859 -0.0555824  -0.0572023\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 0 0 0 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09456516 -0.31968293 -0.01278643 -0.19269602 -0.09487475 -0.07031455\n",
            " -0.01617071 -0.11257976 -0.12744366 -0.10324876 -0.20286576  0.00462379\n",
            " -0.09681693  0.00851805 -0.02046347 -0.06638289 -0.0555824  -0.057522\n",
            "  0.01198733]\n",
            "\n",
            "Taking action 7 from 11\n",
            "\n",
            "Step 27 reward=-2 new_state=[0 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-9.44664776e-02 -3.19971681e-01 -1.30137336e-02 -1.92697406e-01\n",
            " -9.52431336e-02 -7.03145489e-02 -1.61708202e-02 -1.12579755e-01\n",
            " -1.27894446e-01 -1.03248760e-01 -2.02865764e-01  4.62378748e-03\n",
            " -9.68169272e-02  3.97097319e-05 -2.04675328e-02 -7.00987577e-02\n",
            " -5.55824041e-02 -5.78100309e-02 -6.27988018e-04]\n",
            "\n",
            "Taking action 7 from 11\n",
            "\n",
            "Step 28 reward=-2 new_state=[0 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09437752 -0.320232   -0.01321865 -0.19269866 -0.09557522 -0.07031455\n",
            " -0.01617092 -0.11257976 -0.12830083 -0.10324876 -0.20286576 -0.00755314\n",
            " -0.09681693 -0.00760337 -0.0204712  -0.07344855 -0.0555824  -0.05806969\n",
            " -0.01200049]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 29 reward=-2 new_state=[0 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09429741 -0.3204664  -0.01340316 -0.19269979 -0.09587425 -0.07031455\n",
            " -0.01617101 -0.11257976 -0.12866676 -0.10324876 -0.20286576 -0.0277077\n",
            " -0.09681693 -0.01448559 -0.0204745  -0.07646487 -0.0555824  -0.0583035\n",
            " -0.02224086]\n",
            "\n",
            "Taking action 16 from 6\n",
            "\n",
            "Step 30 reward=-2 new_state=[0 0 0 1 1 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09422524 -0.32067758 -0.02223274 -0.1927008  -0.09614366 -0.07031455\n",
            " -0.01617109 -0.11257976 -0.12899645 -0.10324876 -0.20286576 -0.04586625\n",
            " -0.09681693 -0.02068622 -0.02047747 -0.07918247 -0.0555824  -0.05851416\n",
            " -0.03146707]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 31 reward=-1 new_state=[0 0 0 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09416021 -0.32086784 -0.03018788 -0.19270171 -0.0963864  -0.07031455\n",
            " -0.03167185 -0.11257976 -0.12929349 -0.10324876 -0.20286576 -0.06222644\n",
            " -0.09681693 -0.02627277 -0.02048015 -0.08163093 -0.0555824  -0.05870395\n",
            " -0.03977956]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 32 reward=-1 new_state=[0 0 0 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09410162 -0.32103926 -0.03735515 -0.19270253 -0.09660508 -0.07031455\n",
            " -0.04563746 -0.11257976 -0.12956111 -0.10324876 -0.20286576 -0.07696637\n",
            " -0.09681693 -0.03130605 -0.02951404 -0.0838369  -0.0555824  -0.05887495\n",
            " -0.0472688 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 33 reward=-1 new_state=[0 0 0 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09404884 -0.3211937  -0.0438126  -0.19270328 -0.09680212 -0.07031455\n",
            " -0.05821993 -0.11257976 -0.12980223 -0.10324876 -0.20286576 -0.09024648\n",
            " -0.09681693 -0.03584083 -0.04535657 -0.08582439 -0.0555824  -0.05902901\n",
            " -0.05401632]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 34 reward=-1 new_state=[0 0 0 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09400129 -0.32133284 -0.0496305  -0.19270395 -0.09697963 -0.07031455\n",
            " -0.06955625 -0.11257976 -0.13001946 -0.10324876 -0.20286576 -0.10221133\n",
            " -0.09681693 -0.04798811 -0.05963007 -0.08761505 -0.0555824  -0.05916782\n",
            " -0.06009557]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 35 reward=-1 new_state=[0 0 0 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09395844 -0.32145822 -0.0548722  -0.19270454 -0.09713957 -0.07031455\n",
            " -0.07976981 -0.11257976 -0.13021518 -0.10324876 -0.20286576 -0.11299118\n",
            " -0.09681693 -0.06600413 -0.07248992 -0.08922835 -0.0555824  -0.05929288\n",
            " -0.06557273]\n",
            "\n",
            "Taking action 10 from 16\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 0 1 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09391984 -0.32157117 -0.06374447 -0.19270508 -0.09728366 -0.07031455\n",
            " -0.08897181 -0.11257976 -0.13039152 -0.10324876 -0.20286576 -0.12270338\n",
            " -0.09681693 -0.08223583 -0.0840761  -0.09068188 -0.0555824  -0.05940555\n",
            " -0.07050742]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09388503 -0.321673   -0.0717425  -0.19270557 -0.09741356 -0.07031455\n",
            " -0.09726708 -0.11257976 -0.13055049 -0.10324876 -0.20286576 -0.13145858\n",
            " -0.09681693 -0.09686811 -0.09452062 -0.09199218 -0.07388042 -0.05950712\n",
            " -0.07495587]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0938537  -0.3217647  -0.07894433 -0.19270602 -0.09753053 -0.07031455\n",
            " -0.10473655 -0.11257976 -0.13069363 -0.10324876 -0.20286576 -0.1393422\n",
            " -0.09681693 -0.11004376 -0.1039254  -0.09317204 -0.09035687 -0.05958056\n",
            " -0.07896147]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09382547 -0.32184732 -0.08543285 -0.19270642 -0.09763591 -0.07031455\n",
            " -0.11146621 -0.11257976 -0.13082258 -0.10324876 -0.20286576 -0.14644499\n",
            " -0.09681693 -0.12191442 -0.11239868 -0.09423503 -0.10520142 -0.05962869\n",
            " -0.08257034]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09380003 -0.32192174 -0.09127871 -0.19270678 -0.09773085 -0.07031455\n",
            " -0.11752933 -0.11257976 -0.13093877 -0.10324876 -0.20286576 -0.15284428\n",
            " -0.09681693 -0.13260934 -0.11948432 -0.09519275 -0.11857568 -0.05967205\n",
            " -0.08582176]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09377711 -0.3219888  -0.09654555 -0.1927071  -0.09781639 -0.07031455\n",
            " -0.12299192 -0.11257976 -0.13104345 -0.10324876 -0.20286576 -0.15860973\n",
            " -0.09681693 -0.14224496 -0.12586814 -0.0960556  -0.13062525 -0.05969299\n",
            " -0.08875114]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09375647 -0.3220492  -0.10129073 -0.19270739 -0.09789346 -0.07031455\n",
            " -0.12791345 -0.11257976 -0.13113776 -0.10324876 -0.20286576 -0.16380413\n",
            " -0.09681693 -0.1509262  -0.13161966 -0.09683299 -0.14148134 -0.05969372\n",
            " -0.09139038]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09373786 -0.32210362 -0.10556591 -0.19270766 -0.09796289 -0.07031455\n",
            " -0.1323475  -0.11257976 -0.13122272 -0.10324876 -0.20286576 -0.16848403\n",
            " -0.09681693 -0.15874757 -0.1368015  -0.09753338 -0.15126215 -0.05967622\n",
            " -0.0937682 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09372111 -0.32215264 -0.10941762 -0.1927079  -0.09802545 -0.07031455\n",
            " -0.13634235 -0.11257976 -0.13129927 -0.10324876 -0.20286576 -0.17270039\n",
            " -0.09681693 -0.16579422 -0.14147007 -0.0981644  -0.16007416 -0.05964228\n",
            " -0.0959105 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09370601 -0.3221968  -0.11288782 -0.1927081  -0.09808181 -0.07031455\n",
            " -0.13994151 -0.11257976 -0.13136825 -0.10324876 -0.20286576 -0.17649911\n",
            " -0.09681693 -0.17214291 -0.14567623 -0.09873292 -0.16801333 -0.05959351\n",
            " -0.0978406 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09369241 -0.32223663 -0.11601429 -0.1927083  -0.09813259 -0.07031455\n",
            " -0.14318417 -0.11257976 -0.13143039 -0.10324876 -0.20286576 -0.17992157\n",
            " -0.09681693 -0.17786273 -0.14946574 -0.09924512 -0.17516612 -0.05953139\n",
            " -0.09957952]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09368015 -0.32227248 -0.11883107 -0.19270848 -0.09817833 -0.07031455\n",
            " -0.14610563 -0.11257976 -0.13148637 -0.10324876 -0.20286576 -0.183005\n",
            " -0.09681693 -0.183016   -0.1528799  -0.09970659 -0.18161039 -0.05945724\n",
            " -0.10114619]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09366911 -0.3223048  -0.12136883 -0.19270863 -0.09821955 -0.07031455\n",
            " -0.14873771 -0.11257976 -0.13153681 -0.10324876 -0.20286576 -0.18578301\n",
            " -0.09681693 -0.18765882 -0.15595587 -0.10012235 -0.18741633 -0.05937224\n",
            " -0.10255768]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09365916 -0.3223339  -0.12365522 -0.19270876 -0.09825668 -0.07031455\n",
            " -0.15110907 -0.11257976 -0.13158226 -0.10324876 -0.20286576 -0.18828586\n",
            " -0.09681693 -0.19184174 -0.15872714 -0.10049692 -0.19264716 -0.05927749\n",
            " -0.10382935]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0936502  -0.32236013 -0.12571514 -0.19270888 -0.09829014 -0.07031455\n",
            " -0.15324552 -0.11257976 -0.1316232  -0.10324876 -0.20286576 -0.19054078\n",
            " -0.09681693 -0.1956103  -0.1612239  -0.10083438 -0.19735985 -0.05917395\n",
            " -0.10497506]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 51 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09364212 -0.32238376 -0.127571   -0.192709   -0.09832028 -0.07031455\n",
            " -0.15517035 -0.11257976 -0.13166007 -0.10324876 -0.20286576 -0.19257233\n",
            " -0.09681693 -0.19900556 -0.16347334 -0.10113843 -0.2016057  -0.05906251\n",
            " -0.10600728]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 52 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09363485 -0.32240504 -0.12924302 -0.1927091  -0.09834744 -0.07031455\n",
            " -0.1569045  -0.11257976 -0.1316933  -0.10324876 -0.20286576 -0.19440264\n",
            " -0.09681693 -0.2020645  -0.16549996 -0.10141235 -0.20543098 -0.05894398\n",
            " -0.10693724]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 53 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09362829 -0.32242423 -0.1307494  -0.1927092  -0.09837191 -0.07031455\n",
            " -0.15846688 -0.11257976 -0.13172324 -0.10324876 -0.20286576 -0.19605163\n",
            " -0.09681693 -0.20482042 -0.16732581 -0.10165913 -0.20887733 -0.05881906\n",
            " -0.10777508]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 54 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09362239 -0.32244152 -0.13210657 -0.19270928 -0.09839395 -0.07031455\n",
            " -0.15987448 -0.11257976 -0.13175021 -0.10324876 -0.20286576 -0.19753727\n",
            " -0.09681693 -0.20730335 -0.1689708  -0.10188147 -0.21198227 -0.05868842\n",
            " -0.10852993]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 55 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09361707 -0.32245708 -0.1333293  -0.19270936 -0.0984138  -0.07031455\n",
            " -0.16114265 -0.11257976 -0.13177451 -0.10324876 -0.20286576 -0.19887576\n",
            " -0.09681693 -0.20954031 -0.17045283 -0.10208179 -0.21477963 -0.05855264\n",
            " -0.10921   ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 56 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09361228 -0.3224711  -0.1344309  -0.19270943 -0.09843169 -0.07031455\n",
            " -0.16228518 -0.11257976 -0.1317964  -0.10324876 -0.20286576 -0.20008165\n",
            " -0.09681693 -0.21155566 -0.17178805 -0.10226227 -0.21729988 -0.05841225\n",
            " -0.1098227 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 57 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09360795 -0.32248375 -0.13542338 -0.19270949 -0.09844781 -0.07031455\n",
            " -0.16331454 -0.11257976 -0.13181613 -0.10324876 -0.20286576 -0.20116808\n",
            " -0.09681693 -0.21337137 -0.17299101 -0.10242486 -0.21957047 -0.05826773\n",
            " -0.1103747 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 58 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09360407 -0.32249513 -0.13631752 -0.19270955 -0.09846233 -0.07031455\n",
            " -0.16424191 -0.11257976 -0.13183391 -0.10324876 -0.20286576 -0.20214687\n",
            " -0.09681693 -0.2150072  -0.17407478 -0.10257135 -0.22161612 -0.05811952\n",
            " -0.11087202]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 59 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09360056 -0.32250538 -0.1371231  -0.1927096  -0.09847541 -0.07031455\n",
            " -0.16507742 -0.11257976 -0.13184991 -0.10324876 -0.20286576 -0.20302871\n",
            " -0.09681693 -0.21648099 -0.1750512  -0.10270332 -0.22345912 -0.05796802\n",
            " -0.11132008]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 60 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935974  -0.32251462 -0.13784885 -0.19270964 -0.0984872  -0.07031455\n",
            " -0.16583015 -0.11257976 -0.13186434 -0.10324876 -0.20286576 -0.20382318\n",
            " -0.09681693 -0.21780877 -0.17593089 -0.10282222 -0.22511955 -0.05781357\n",
            " -0.11172374]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 61 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09359456 -0.32252294 -0.13850272 -0.19270968 -0.09849782 -0.07031455\n",
            " -0.16650832 -0.11257976 -0.13187733 -0.10324876 -0.20286576 -0.20453894\n",
            " -0.09681693 -0.219005   -0.17672342 -0.10292935 -0.22661547 -0.0576565\n",
            " -0.11208742]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 62 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.093592   -0.32253045 -0.1390918  -0.19270971 -0.09850738 -0.07031455\n",
            " -0.1671193  -0.11257976 -0.13188905 -0.10324876 -0.20286576 -0.2051838\n",
            " -0.09681693 -0.22008273 -0.17743744 -0.10302585 -0.2279632  -0.0574971\n",
            " -0.11241507]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 63 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09358969 -0.3225372  -0.13962254 -0.19270974 -0.098516   -0.07031455\n",
            " -0.16766974 -0.11257976 -0.1318996  -0.10324876 -0.20286576 -0.20576477\n",
            " -0.09681693 -0.22105369 -0.17808072 -0.1031128  -0.2291774  -0.05733563\n",
            " -0.11271025]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 64 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09358761 -0.3225433  -0.14010069 -0.19270977 -0.09852377 -0.07031455\n",
            " -0.16816567 -0.11257976 -0.1319091  -0.10324876 -0.20286576 -0.20628819\n",
            " -0.09681693 -0.22192846 -0.17866027 -0.10319114 -0.23027131 -0.05717232\n",
            " -0.11297619]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 65 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09358573 -0.32254878 -0.14053147 -0.1927098  -0.09853076 -0.07031455\n",
            " -0.16861245 -0.11257976 -0.13191767 -0.10324876 -0.20286576 -0.20675975\n",
            " -0.09681693 -0.22271657 -0.17918241 -0.10326171 -0.23125684 -0.0570074\n",
            " -0.11321579]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 66 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09358404 -0.32255372 -0.14091957 -0.19270983 -0.09853707 -0.07031455\n",
            " -0.16901498 -0.11257976 -0.13192539 -0.10324876 -0.20286576 -0.2071846\n",
            " -0.09681693 -0.2234266  -0.17965281 -0.10332529 -0.23214474 -0.05684105\n",
            " -0.11343165]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 67 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09358252 -0.32255816 -0.14126922 -0.19270985 -0.09854274 -0.07031455\n",
            " -0.16937762 -0.11257976 -0.13193233 -0.10324876 -0.20286576 -0.20756735\n",
            " -0.09681693 -0.22406627 -0.18007661 -0.10338257 -0.23294468 -0.05667346\n",
            " -0.11362612]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 68 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09358115 -0.3225622  -0.14158423 -0.19270986 -0.09854786 -0.07031455\n",
            " -0.16970433 -0.11257976 -0.13193859 -0.10324876 -0.20286576 -0.20791218\n",
            " -0.09681693 -0.22464257 -0.18045843 -0.10343418 -0.23366536 -0.05650477\n",
            " -0.11380133]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 69 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357991 -0.3225658  -0.14186803 -0.19270988 -0.09855247 -0.07031455\n",
            " -0.16999868 -0.11257976 -0.13194424 -0.10324876 -0.20286576 -0.20822284\n",
            " -0.09681693 -0.22516178 -0.1808024  -0.10348067 -0.23431464 -0.05633513\n",
            " -0.11395918]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 70 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935788  -0.32256904 -0.1421237  -0.1927099  -0.09855662 -0.07031455\n",
            " -0.17026386 -0.11257976 -0.13194932 -0.10324876 -0.20286576 -0.20850272\n",
            " -0.09681693 -0.22562954 -0.1811123  -0.10352256 -0.23489958 -0.05616467\n",
            " -0.11410139]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 71 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357779 -0.32257196 -0.14235404 -0.19270991 -0.09856036 -0.07031455\n",
            " -0.17050277 -0.11257976 -0.1319539  -0.10324876 -0.20286576 -0.20875488\n",
            " -0.09681693 -0.22605096 -0.1813915  -0.1035603  -0.23542657 -0.05599351\n",
            " -0.11422951]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 72 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357689 -0.32257462 -0.14256157 -0.19270992 -0.09856373 -0.07031455\n",
            " -0.170718   -0.11257976 -0.13195802 -0.10324876 -0.20286576 -0.20898205\n",
            " -0.09681693 -0.22643062 -0.18164304 -0.1035943  -0.23590136 -0.05582175\n",
            " -0.11434493]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 73 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357608 -0.322577   -0.14274853 -0.19270994 -0.09856677 -0.07031455\n",
            " -0.17091191 -0.11257976 -0.13196173 -0.10324876 -0.20286576 -0.20918672\n",
            " -0.09681693 -0.22677267 -0.18186966 -0.10362493 -0.2363291  -0.05564949\n",
            " -0.11444892]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 74 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357535 -0.32257915 -0.14291698 -0.19270995 -0.0985695  -0.07031455\n",
            " -0.17108661 -0.11257976 -0.13196509 -0.10324876 -0.20286576 -0.2093711\n",
            " -0.09681693 -0.22708082 -0.18207382 -0.10365252 -0.23671445 -0.05547681\n",
            " -0.1145426 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 75 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357469 -0.32258108 -0.14306873 -0.19270997 -0.09857197 -0.07031455\n",
            " -0.171244   -0.11257976 -0.1319681  -0.10324876 -0.20286576 -0.20953722\n",
            " -0.09681693 -0.22735845 -0.18225776 -0.10367738 -0.23706163 -0.05530379\n",
            " -0.114627  ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 76 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357409 -0.3225828  -0.14320545 -0.19270998 -0.09857419 -0.07031455\n",
            " -0.1713858  -0.11257976 -0.13197081 -0.10324876 -0.20286576 -0.20968688\n",
            " -0.09681693 -0.22760856 -0.18242347 -0.10369978 -0.23737441 -0.05513051\n",
            " -0.11470304]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 77 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357356 -0.3225844  -0.14332862 -0.19271    -0.09857619 -0.07031455\n",
            " -0.17151354 -0.11257976 -0.13197325 -0.10324876 -0.20286576 -0.2098217\n",
            " -0.09681693 -0.2278339  -0.18257277 -0.10371996 -0.23765619 -0.05495702\n",
            " -0.11477155]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 78 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357307 -0.3225858  -0.14343959 -0.19271    -0.09857799 -0.07031455\n",
            " -0.17162862 -0.11257976 -0.13197546 -0.10324876 -0.20286576 -0.20994318\n",
            " -0.09681693 -0.22803691 -0.18270727 -0.10373814 -0.23791006 -0.05478339\n",
            " -0.11483327]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 79 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357264 -0.32258707 -0.14353956 -0.19271    -0.09857962 -0.07031455\n",
            " -0.1717323  -0.11257976 -0.13197744 -0.10324876 -0.20286576 -0.21005261\n",
            " -0.09681693 -0.2282198  -0.18282844 -0.10375451 -0.23813878 -0.05460966\n",
            " -0.11488888]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 80 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357224 -0.3225882  -0.14362963 -0.19271    -0.09858108 -0.07031455\n",
            " -0.17182572 -0.11257976 -0.13197923 -0.10324876 -0.20286576 -0.2101512\n",
            " -0.09681693 -0.22838458 -0.1829376  -0.10376927 -0.23834483 -0.05443588\n",
            " -0.11493897]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 81 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357189 -0.32258925 -0.14371076 -0.19271    -0.09858239 -0.07031455\n",
            " -0.17190988 -0.11257976 -0.13198084 -0.10324876 -0.20286576 -0.21024002\n",
            " -0.09681693 -0.22853303 -0.18303595 -0.10378256 -0.23853047 -0.05426209\n",
            " -0.1149841 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 82 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357157 -0.32259017 -0.14378387 -0.19271    -0.09858358 -0.07031455\n",
            " -0.1719857  -0.11257976 -0.13198228 -0.10324876 -0.20286576 -0.21032004\n",
            " -0.09681693 -0.22866677 -0.18312456 -0.10379453 -0.23869771 -0.05408834\n",
            " -0.11502475]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 83 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357129 -0.322591   -0.14384973 -0.19271    -0.09858465 -0.07031455\n",
            " -0.17205401 -0.11257976 -0.1319836  -0.10324876 -0.20286576 -0.21039213\n",
            " -0.09681693 -0.22878726 -0.18320438 -0.10380532 -0.23884837 -0.05391466\n",
            " -0.11506138]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 84 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357103 -0.32259175 -0.14390907 -0.19271    -0.09858561 -0.07031455\n",
            " -0.17211555 -0.11257976 -0.13198477 -0.10324876 -0.20286576 -0.21045709\n",
            " -0.09681693 -0.2288958  -0.1832763  -0.10381504 -0.23898411 -0.05374108\n",
            " -0.11509438]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 85 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935708  -0.32259244 -0.14396252 -0.19271    -0.09858648 -0.07031455\n",
            " -0.17217098 -0.11257976 -0.13198583 -0.10324876 -0.20286576 -0.2105156\n",
            " -0.09681693 -0.2289936  -0.18334109 -0.1038238  -0.2391064  -0.05356762\n",
            " -0.11512411]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 86 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357059 -0.32259306 -0.14401068 -0.19271    -0.09858727 -0.07031455\n",
            " -0.17222093 -0.11257976 -0.13198678 -0.10324876 -0.20286576 -0.21056832\n",
            " -0.09681693 -0.22908169 -0.18339945 -0.10383169 -0.23921657 -0.05339431\n",
            " -0.11515089]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 87 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935704  -0.32259363 -0.14405406 -0.19271    -0.09858797 -0.07031455\n",
            " -0.17226593 -0.11257976 -0.13198765 -0.10324876 -0.20286576 -0.21061581\n",
            " -0.09681693 -0.22916105 -0.18345204 -0.10383879 -0.23931582 -0.05322118\n",
            " -0.11517502]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 88 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357023 -0.32259414 -0.14409314 -0.19271    -0.09858861 -0.07031455\n",
            " -0.17230646 -0.11257976 -0.13198842 -0.10324876 -0.20286576 -0.2106586\n",
            " -0.09681693 -0.22923256 -0.18349941 -0.10384519 -0.23940524 -0.05304825\n",
            " -0.11519676]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 89 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09357008 -0.32259458 -0.14412835 -0.19271    -0.09858918 -0.07031455\n",
            " -0.17234299 -0.11257976 -0.13198912 -0.10324876 -0.20286576 -0.21069714\n",
            " -0.09681693 -0.22929698 -0.18354209 -0.10385096 -0.2394858  -0.05287553\n",
            " -0.11521635]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 90 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09356993 -0.322595   -0.14416008 -0.19271    -0.0985897  -0.07031455\n",
            " -0.17237589 -0.11257976 -0.13198975 -0.10324876 -0.20286576 -0.21073186\n",
            " -0.09681693 -0.22935502 -0.18358053 -0.10385616 -0.23955837 -0.05270304\n",
            " -0.115234  ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 91 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09356981 -0.32259536 -0.14418866 -0.19271    -0.09859016 -0.07031455\n",
            " -0.17240553 -0.11257976 -0.13199031 -0.10324876 -0.20286576 -0.21076314\n",
            " -0.09681693 -0.22940731 -0.18361518 -0.10386084 -0.23962376 -0.0525308\n",
            " -0.11524989]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 92 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935697  -0.3225957  -0.1442144  -0.19271    -0.09859058 -0.07031455\n",
            " -0.17243223 -0.11257976 -0.13199082 -0.10324876 -0.20286576 -0.21079132\n",
            " -0.09681693 -0.22945441 -0.18364638 -0.10386506 -0.23968266 -0.05235882\n",
            " -0.11526421]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 93 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09356959 -0.32259598 -0.14423761 -0.19271    -0.09859096 -0.07031455\n",
            " -0.17245628 -0.11257976 -0.13199128 -0.10324876 -0.20286576 -0.21081671\n",
            " -0.09681693 -0.22949685 -0.1836745  -0.10386886 -0.23973572 -0.05218711\n",
            " -0.11527711]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 94 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935695  -0.32259625 -0.1442585  -0.19271    -0.0985913  -0.07031455\n",
            " -0.17247795 -0.11257976 -0.1319917  -0.10324876 -0.20286576 -0.21083958\n",
            " -0.09681693 -0.22953509 -0.18369983 -0.10387228 -0.23978353 -0.0520157\n",
            " -0.11528873]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 95 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09356942 -0.3225965  -0.14427732 -0.19271    -0.0985916  -0.07031455\n",
            " -0.17249747 -0.11257976 -0.13199207 -0.10324876 -0.20286576 -0.2108602\n",
            " -0.09681693 -0.22956952 -0.18372265 -0.10387536 -0.23982659 -0.05184457\n",
            " -0.1152992 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 96 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09356935 -0.3225967  -0.14429428 -0.19271    -0.09859188 -0.07031455\n",
            " -0.17251505 -0.11257976 -0.13199241 -0.10324876 -0.20286576 -0.21087876\n",
            " -0.09681693 -0.22960055 -0.18374321 -0.10387814 -0.23986539 -0.05167375\n",
            " -0.11530863]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 97 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09356928 -0.3225969  -0.14430955 -0.19271    -0.09859212 -0.07031455\n",
            " -0.17253089 -0.11257976 -0.13199271 -0.10324876 -0.20286576 -0.21089548\n",
            " -0.09681693 -0.2296285  -0.18376173 -0.10388064 -0.23990035 -0.05150324\n",
            " -0.11531714]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 98 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09356922 -0.3225971  -0.14432332 -0.19271    -0.09859235 -0.07031455\n",
            " -0.17254516 -0.11257976 -0.13199298 -0.10324876 -0.20286576 -0.21091054\n",
            " -0.09681693 -0.22965369 -0.18377842 -0.1038829  -0.23993184 -0.05133306\n",
            " -0.1153248 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 99 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09356917 -0.32259724 -0.14433572 -0.19271    -0.09859255 -0.07031455\n",
            " -0.17255802 -0.11257976 -0.13199323 -0.10324876 -0.20286576 -0.21092412\n",
            " -0.09681693 -0.22967638 -0.18379346 -0.10388494 -0.23996021 -0.0511632\n",
            " -0.11533169]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 100 reward=0 new_state=[0 0 0 1 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.09356911 -0.32259738 -0.1443469  -0.19271    -0.09859273 -0.07031455\n",
            " -0.17256962 -0.11257976 -0.13199346 -0.10324876 -0.20286576 -0.21093635\n",
            " -0.09681693 -0.22969683 -0.183807   -0.10388677 -0.23998576 -0.05099367\n",
            " -0.11533791]\n",
            "Epsilon reduced to 0.04782969000000002\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356907 -0.3225975  -0.14435695 -0.19271    -0.09859289 -0.07031455\n",
            " -0.17258006 -0.11257976 -0.13199365 -0.10324876 -0.20286576 -0.21094736\n",
            " -0.09681693 -0.22971524 -0.1838192  -0.10388841 -0.24000879 -0.05075863\n",
            " -0.1153435 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356903 -0.32259762 -0.14436601 -0.19271    -0.09859304 -0.07031455\n",
            " -0.17258947 -0.11257976 -0.13199383 -0.10324876 -0.20286576 -0.21095729\n",
            " -0.09681693 -0.22973183 -0.1838302  -0.1038899  -0.24002953 -0.05053048\n",
            " -0.11534855]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.093569   -0.3225977  -0.14437418 -0.19271    -0.09859318 -0.07031455\n",
            " -0.17259794 -0.11257976 -0.131994   -0.10324876 -0.20286576 -0.21096623\n",
            " -0.09681693 -0.22974677 -0.1838401  -0.10389124 -0.24004821 -0.0503086\n",
            " -0.11535309]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356897 -0.3225978  -0.14438154 -0.19271    -0.09859329 -0.07031455\n",
            " -0.17260557 -0.11257976 -0.13199414 -0.10324876 -0.20286576 -0.21097429\n",
            " -0.09681693 -0.22976024 -0.18384902 -0.10389245 -0.24006505 -0.05009242\n",
            " -0.11535718]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356894 -0.3225979  -0.14438817 -0.19271    -0.0985934  -0.07031455\n",
            " -0.17261244 -0.11257976 -0.13199428 -0.10324876 -0.20286576 -0.21098155\n",
            " -0.09681693 -0.22977237 -0.18385705 -0.10389353 -0.24008022 -0.04988142\n",
            " -0.11536087]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356891 -0.32259798 -0.14439414 -0.19271    -0.0985935  -0.07031455\n",
            " -0.17261864 -0.11257976 -0.1319944  -0.10324876 -0.20286576 -0.21098809\n",
            " -0.09681693 -0.2297833  -0.1838643  -0.10389451 -0.24009389 -0.04967516\n",
            " -0.11536419]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356889 -0.32259804 -0.14439952 -0.19271    -0.09859359 -0.07031455\n",
            " -0.17262423 -0.11257976 -0.1319945  -0.10324876 -0.20286576 -0.21099398\n",
            " -0.09681693 -0.22979315 -0.18387082 -0.10389539 -0.2401062  -0.04947321\n",
            " -0.11536719]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356887 -0.3225981  -0.14440437 -0.19271    -0.09859367 -0.07031455\n",
            " -0.17262925 -0.11257976 -0.13199459 -0.10324876 -0.20161484 -0.21099928\n",
            " -0.09681693 -0.22980201 -0.1838767  -0.10389619 -0.24011728 -0.04929128\n",
            " -0.11536989]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356885 -0.32259816 -0.14440873 -0.19271    -0.09859374 -0.07031455\n",
            " -0.17263378 -0.11257976 -0.13199468 -0.10324876 -0.20048791 -0.21100406\n",
            " -0.09681693 -0.22981    -0.18388198 -0.1038969  -0.24012727 -0.04911133\n",
            " -0.11537232]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356883 -0.32259822 -0.14441267 -0.19271    -0.09859381 -0.07031455\n",
            " -0.17263786 -0.11257976 -0.13199475 -0.10324876 -0.19947267 -0.21100837\n",
            " -0.09681693 -0.2298172  -0.18388675 -0.10389755 -0.24013627 -0.04893323\n",
            " -0.11537451]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356882 -0.32259828 -0.14441621 -0.19271    -0.09859387 -0.07031455\n",
            " -0.17264155 -0.11257976 -0.13199483 -0.10324876 -0.19855805 -0.21101224\n",
            " -0.09681693 -0.22982368 -0.18389104 -0.10389813 -0.24014437 -0.04875682\n",
            " -0.11537648]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11 from 11\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935688  -0.3225983  -0.1444194  -0.19271    -0.09859392 -0.07031455\n",
            " -0.17264485 -0.11257976 -0.13199489 -0.10324876 -0.19773409 -0.21101575\n",
            " -0.09681693 -0.22982952 -0.18389492 -0.10389865 -0.24015167 -0.04858198\n",
            " -0.11537825]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356879 -0.32259834 -0.14442228 -0.19271    -0.09859397 -0.07031455\n",
            " -0.17264783 -0.11257976 -0.13199495 -0.10324876 -0.19699179 -0.20452787\n",
            " -0.09681693 -0.22983478 -0.1838984  -0.10389912 -0.24015826 -0.04842448\n",
            " -0.11537986]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356877 -0.32259837 -0.14442487 -0.19271    -0.09859401 -0.07031455\n",
            " -0.17265052 -0.11257976 -0.13199499 -0.10324876 -0.19632307 -0.19868304\n",
            " -0.09681693 -0.22983952 -0.18390155 -0.10389955 -0.24016419 -0.04663651\n",
            " -0.11538129]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356876 -0.3225984  -0.14442721 -0.19271    -0.09859405 -0.07031455\n",
            " -0.17265293 -0.11257976 -0.13199504 -0.10324876 -0.19572063 -0.19341753\n",
            " -0.09681693 -0.2298438  -0.18390438 -0.10389993 -0.24016953 -0.0433869\n",
            " -0.11538259]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356876 -0.32259843 -0.14442931 -0.19271    -0.09859408 -0.07031455\n",
            " -0.1726551  -0.11257976 -0.13199508 -0.10324876 -0.1951779  -0.18867394\n",
            " -0.09681693 -0.22984764 -0.18390693 -0.10390027 -0.24017434 -0.03882774\n",
            " -0.11538376]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356875 -0.32259846 -0.1444312  -0.19271    -0.09859411 -0.07031455\n",
            " -0.17265707 -0.11257976 -0.13199513 -0.10324876 -0.19468896 -0.18440053\n",
            " -0.09681693 -0.22985111 -0.18390922 -0.10390058 -0.24017867 -0.03309606\n",
            " -0.11538481]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356874 -0.3225985  -0.14443292 -0.19271    -0.09859414 -0.07031455\n",
            " -0.17265885 -0.11257976 -0.13199516 -0.10324876 -0.19424848 -0.1805507\n",
            " -0.09681693 -0.22985424 -0.1839113  -0.10390086 -0.24018258 -0.02631526\n",
            " -0.11538576]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356873 -0.32259852 -0.14443445 -0.19271    -0.09859417 -0.07031455\n",
            " -0.17266044 -0.11257976 -0.13199519 -0.10324876 -0.19385166 -0.17708245\n",
            " -0.09681693 -0.22985706 -0.18391316 -0.10390112 -0.2401861  -0.0185965\n",
            " -0.11538661]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356873 -0.32259855 -0.14443584 -0.19271    -0.09859419 -0.07031455\n",
            " -0.17266187 -0.11257976 -0.13199522 -0.10324876 -0.19349419 -0.17395799\n",
            " -0.09681693 -0.22985959 -0.18391484 -0.10390134 -0.24018927 -0.0100399\n",
            " -0.11538738]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356872 -0.32259858 -0.14443709 -0.19271    -0.09859421 -0.07031455\n",
            " -0.17266317 -0.11257976 -0.13199525 -0.10324876 -0.19317214 -0.17114323\n",
            " -0.09681693 -0.22986187 -0.18391636 -0.10390154 -0.24019213 -0.0007356\n",
            " -0.11538807]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356871 -0.32259858 -0.1444382  -0.19271    -0.09859423 -0.07031455\n",
            " -0.17266433 -0.11257976 -0.13199526 -0.10324876 -0.19288202 -0.16860749\n",
            " -0.09681693 -0.22986393 -0.18391772 -0.10390173 -0.24019471  0.00923522\n",
            " -0.1153887 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 6\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.09356871 -0.32259858 -0.14443922 -0.19271    -0.09859424 -0.07031455\n",
            " -0.17266537 -0.11257976 -0.13199528 -0.10324876 -0.19262065 -0.1663231\n",
            " -0.09681693 -0.22986577 -0.18391894 -0.10390189 -0.24019702  0.01979949\n",
            " -0.11538927]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444013 -0.19271    -0.09859426 -0.07031455\n",
            " -0.16340643 -0.11257976 -0.13199529 -0.10324876 -0.1923852  -0.16426514\n",
            " -0.09681693 -0.22986744 -0.18392004 -0.10390204 -0.2401991   0.02931657\n",
            " -0.11538977]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444095 -0.19271    -0.09859427 -0.07031455\n",
            " -0.1550653  -0.11257976 -0.1319953  -0.10324876 -0.19217308 -0.16241118\n",
            " -0.09681693 -0.22986895 -0.18392104 -0.10390218 -0.24020098  0.03946599\n",
            " -0.11539023]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.1444417  -0.19271    -0.09859429 -0.07031455\n",
            " -0.14755099 -0.11257976 -0.13199532 -0.10324876 -0.19198199 -0.160741\n",
            " -0.09681693 -0.2298703  -0.18392193 -0.1039023  -0.24020268  0.05017821\n",
            " -0.11539064]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444236 -0.19271    -0.09859429 -0.07031455\n",
            " -0.14078155 -0.11257976 -0.13199534 -0.10324876 -0.19180983 -0.15923637\n",
            " -0.09681693 -0.22987153 -0.18392274 -0.10390241 -0.2402042   0.06139059\n",
            " -0.11539101]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444296 -0.19271    -0.0985943  -0.07031455\n",
            " -0.13468316 -0.11257976 -0.13199535 -0.10324876 -0.19165476 -0.1578809\n",
            " -0.09681693 -0.22987263 -0.18392347 -0.1039025  -0.24020557  0.07304678\n",
            " -0.11539134]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.1444435  -0.19271    -0.09859431 -0.07031455\n",
            " -0.1291893  -0.11257976 -0.13199537 -0.10324876 -0.19151504 -0.1566598\n",
            " -0.09681693 -0.22987361 -0.18392412 -0.10390259 -0.24020681  0.08509604\n",
            " -0.11539164]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444399 -0.19271    -0.09859432 -0.07031455\n",
            " -0.12424003 -0.11257976 -0.13199538 -0.10324876 -0.19138919 -0.15555973\n",
            " -0.09681693 -0.2298745  -0.18392472 -0.10390268 -0.24020793  0.09749269\n",
            " -0.11539191]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444442 -0.19271    -0.09859432 -0.07031455\n",
            " -0.11978139 -0.11257976 -0.1319954  -0.10324876 -0.1912758  -0.15456872\n",
            " -0.09681693 -0.22987531 -0.18392526 -0.10390275 -0.24020892  0.11019567\n",
            " -0.11539216]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444482 -0.19271    -0.09859433 -0.07031455\n",
            " -0.11576474 -0.11257976 -0.13199541 -0.10324876 -0.19117366 -0.15367594\n",
            " -0.09681693 -0.22987604 -0.18392573 -0.10390282 -0.24020983  0.12316798\n",
            " -0.11539238]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444518 -0.19271    -0.09859434 -0.07031455\n",
            " -0.11214627 -0.11257976 -0.13199541 -0.10324876 -0.19108164 -0.15287168\n",
            " -0.09681693 -0.2298767  -0.18392617 -0.10390288 -0.24021065  0.1363764\n",
            " -0.11539258]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444551 -0.19271    -0.09859435 -0.07031455\n",
            " -0.1088865  -0.11257976 -0.13199541 -0.10324876 -0.19099875 -0.15214714\n",
            " -0.09681693 -0.22987728 -0.18392655 -0.10390293 -0.24021138  0.14979097\n",
            " -0.11539276]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444579 -0.19271    -0.09859435 -0.07031455\n",
            " -0.10594989 -0.11257976 -0.13199541 -0.10324876 -0.19092406 -0.15149443\n",
            " -0.09681693 -0.22987781 -0.18392691 -0.10390297 -0.24021204  0.16338484\n",
            " -0.11539292]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444605 -0.19271    -0.09859436 -0.07031455\n",
            " -0.1033044  -0.11257976 -0.13199541 -0.10324876 -0.19085678 -0.15090643\n",
            " -0.09681693 -0.22987829 -0.18392722 -0.10390302 -0.24021263  0.1771338\n",
            " -0.11539306]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444628 -0.19271    -0.09859437 -0.07031455\n",
            " -0.10092118 -0.11257976 -0.13199541 -0.10324876 -0.19079618 -0.15037672\n",
            " -0.09681693 -0.22987872 -0.1839275  -0.10390306 -0.24021317  0.19101611\n",
            " -0.1153932 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444649 -0.19271    -0.09859437 -0.07031455\n",
            " -0.09877421 -0.11257976 -0.13199541 -0.10324876 -0.19074158 -0.14989953\n",
            " -0.09681693 -0.22987911 -0.18392776 -0.10390309 -0.24021365  0.20501225\n",
            " -0.11539332]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444669 -0.19271    -0.09859437 -0.07031455\n",
            " -0.09684009 -0.11257976 -0.13199541 -0.10324876 -0.1906924  -0.14946963\n",
            " -0.09681693 -0.22987945 -0.183928   -0.10390312 -0.24021408  0.21910466\n",
            " -0.11539342]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444686 -0.19271    -0.09859437 -0.07031455\n",
            " -0.09509771 -0.11257976 -0.13199541 -0.10324876 -0.19064808 -0.14908236\n",
            " -0.09681693 -0.22987977 -0.1839282  -0.10390315 -0.24021447  0.23327759\n",
            " -0.11539352]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444701 -0.19271    -0.09859437 -0.07031455\n",
            " -0.09352807 -0.11257976 -0.13199541 -0.10324876 -0.19060816 -0.14873348\n",
            " -0.09681693 -0.22988005 -0.1839284  -0.10390317 -0.24021482  0.24751689\n",
            " -0.11539361]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444715 -0.19271    -0.09859437 -0.07031455\n",
            " -0.09211404 -0.11257976 -0.13199541 -0.10324876 -0.1905722  -0.14841919\n",
            " -0.09681693 -0.2298803  -0.18392856 -0.1039032  -0.24021514  0.26180983\n",
            " -0.11539368]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444727 -0.19271    -0.09859437 -0.07031455\n",
            " -0.0908402  -0.11257976 -0.13199541 -0.10324876 -0.1905398  -0.14813605\n",
            " -0.09681693 -0.22988053 -0.18392871 -0.10390322 -0.24021542  0.27614504\n",
            " -0.11539375]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444739 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08969264 -0.11257976 -0.13199541 -0.10324876 -0.19051063 -0.14788099\n",
            " -0.09681693 -0.22988074 -0.18392885 -0.10390323 -0.24021567  0.29051232\n",
            " -0.11539381]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 45 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444749 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08865886 -0.11257976 -0.13199541 -0.10324876 -0.19048434 -0.14765121\n",
            " -0.09681693 -0.22988093 -0.18392897 -0.10390325 -0.24021591  0.3049025\n",
            " -0.11539387]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444758 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08772757 -0.11257976 -0.13199541 -0.10324876 -0.19046067 -0.14744422\n",
            " -0.09681693 -0.2298811  -0.18392907 -0.10390326 -0.24021612  0.3193074\n",
            " -0.11539392]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444767 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08688861 -0.11257976 -0.13199541 -0.10324876 -0.19043933 -0.14725775\n",
            " -0.09681693 -0.22988124 -0.18392918 -0.10390328 -0.24021631  0.33371964\n",
            " -0.11539397]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444774 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08613283 -0.11257976 -0.13199541 -0.10324876 -0.1904201  -0.14708976\n",
            " -0.09681693 -0.22988138 -0.18392926 -0.10390329 -0.24021648  0.34813267\n",
            " -0.11539401]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 49 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.32259858 -0.14444782 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08545198 -0.11257976 -0.13199541 -0.10324876 -0.19040279 -0.14693843\n",
            " -0.09681693 -0.2298815  -0.18392934 -0.1039033  -0.24021663  0.36254063\n",
            " -0.11539405]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 50 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.3162751  -0.14444788 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08483864 -0.11257976 -0.13199541 -0.10324876 -0.19038719 -0.1468021\n",
            " -0.09681693 -0.2298816  -0.18392941 -0.10390331 -0.24021676  0.3755201\n",
            " -0.11539409]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 51 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.31057855 -0.14444794 -0.19271    -0.09859437 -0.07031455\n",
            " -0.0842861  -0.11257976 -0.13199541 -0.10324876 -0.19037314 -0.14667928\n",
            " -0.09681693 -0.2298817  -0.18392947 -0.10390332 -0.24021688  0.3870953\n",
            " -0.11539412]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 52 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.3054468  -0.14444798 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08378835 -0.11257976 -0.13199541 -0.10324876 -0.19036049 -0.14656864\n",
            " -0.09681693 -0.2298818  -0.18392953 -0.10390332 -0.240217    0.39740172\n",
            " -0.11539415]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 53 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.30082387 -0.14444803 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08333995 -0.11257976 -0.13199541 -0.10324876 -0.19034909 -0.14646898\n",
            " -0.09681693 -0.22988187 -0.18392959 -0.10390333 -0.2402171   0.40656182\n",
            " -0.11539417]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 54 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2966593  -0.14444807 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08293601 -0.11257976 -0.13199541 -0.10324876 -0.19033882 -0.1463792\n",
            " -0.09681693 -0.22988194 -0.18392964 -0.10390334 -0.2402172   0.4146863\n",
            " -0.11539419]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 55 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.29290763 -0.1444481  -0.19271    -0.09859437 -0.07031455\n",
            " -0.08257212 -0.11257976 -0.13199541 -0.10324876 -0.19032957 -0.14629832\n",
            " -0.09681693 -0.229882   -0.18392968 -0.10390335 -0.24021728  0.42187515\n",
            " -0.11539421]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 56 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.28952792 -0.14444813 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08224431 -0.11257976 -0.13199541 -0.10324876 -0.19032124 -0.14622545\n",
            " -0.09681693 -0.22988206 -0.18392973 -0.10390335 -0.24021736  0.42821875\n",
            " -0.11539423]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 57 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.28648332 -0.14444816 -0.19271    -0.09859437 -0.07031455\n",
            " -0.081949   -0.11257976 -0.13199541 -0.10324876 -0.19031373 -0.14615981\n",
            " -0.09681693 -0.22988212 -0.18392976 -0.10390336 -0.24021742  0.43379885\n",
            " -0.11539424]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 58 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2837406  -0.14444819 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08168297 -0.11257976 -0.13199541 -0.10324876 -0.19030696 -0.14610069\n",
            " -0.09681693 -0.22988217 -0.18392979 -0.10390337 -0.24021748  0.43868923\n",
            " -0.11539426]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 59 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.28126985 -0.14444822 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08144331 -0.11257976 -0.13199541 -0.10324876 -0.19030087 -0.14604741\n",
            " -0.09681693 -0.22988221 -0.18392982 -0.10390338 -0.24021754  0.44295663\n",
            " -0.11539427]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 60 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.27904406 -0.14444824 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08122742 -0.11257976 -0.13199541 -0.10324876 -0.19029538 -0.14599943\n",
            " -0.09681693 -0.22988226 -0.18392985 -0.10390338 -0.24021758  0.44666135\n",
            " -0.11539429]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 61 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.27703896 -0.14444825 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08103294 -0.11257976 -0.13199541 -0.10324876 -0.19029044 -0.1459562\n",
            " -0.09681693 -0.22988228 -0.18392988 -0.10390338 -0.24021763  0.44985792\n",
            " -0.11539429]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 62 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.27523267 -0.14444827 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08085774 -0.11257976 -0.13199541 -0.10324876 -0.19028598 -0.14591727\n",
            " -0.09681693 -0.22988231 -0.18392989 -0.10390338 -0.24021767  0.45259556\n",
            " -0.1153943 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 63 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2736055  -0.14444828 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08069991 -0.11257976 -0.13199541 -0.10324876 -0.19028197 -0.14588219\n",
            " -0.09681693 -0.22988234 -0.1839299  -0.10390338 -0.2402177   0.4549188\n",
            " -0.11539431]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 64 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.27213967 -0.1444483  -0.19271    -0.09859437 -0.07031455\n",
            " -0.08055773 -0.11257976 -0.13199541 -0.10324876 -0.19027835 -0.14585058\n",
            " -0.09681693 -0.22988237 -0.18392992 -0.10390338 -0.24021773  0.45686787\n",
            " -0.11539432]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 65 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2708192  -0.14444831 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08042965 -0.11257976 -0.13199541 -0.10324876 -0.19027509 -0.14582212\n",
            " -0.09681693 -0.2298824  -0.18392994 -0.10390338 -0.24021776  0.45847914\n",
            " -0.11539432]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 66 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26962963 -0.14444833 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08031427 -0.11257976 -0.13199541 -0.10324876 -0.19027215 -0.14579648\n",
            " -0.09681693 -0.22988242 -0.18392995 -0.10390338 -0.24021779  0.45978543\n",
            " -0.11539433]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 67 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26855803 -0.14444834 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08021034 -0.11257976 -0.13199541 -0.10324876 -0.19026951 -0.14577338\n",
            " -0.09681693 -0.22988243 -0.18392996 -0.10390338 -0.24021782  0.46081647\n",
            " -0.11539434]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 68 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2675927  -0.14444835 -0.19271    -0.09859437 -0.07031455\n",
            " -0.0801167  -0.11257976 -0.13199541 -0.10324876 -0.19026713 -0.14575256\n",
            " -0.09681693 -0.22988245 -0.18392998 -0.10390338 -0.24021783  0.46159908\n",
            " -0.11539435]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 69 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2667231  -0.14444837 -0.19271    -0.09859437 -0.07031455\n",
            " -0.08003236 -0.11257976 -0.13199541 -0.10324876 -0.19026498 -0.14573382\n",
            " -0.09681693 -0.22988246 -0.18393    -0.10390338 -0.24021785  0.46215755\n",
            " -0.11539435]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 70 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2659397  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07995638 -0.11257976 -0.13199541 -0.10324876 -0.19026305 -0.14571694\n",
            " -0.09267043 -0.22988248 -0.18393001 -0.10390338 -0.24021786  0.4626606\n",
            " -0.11539436]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 71 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26523402 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07988793 -0.11257976 -0.13199541 -0.10324876 -0.1902613  -0.14570172\n",
            " -0.0889351  -0.2298825  -0.18393002 -0.10390338 -0.24021788  0.46296662\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 72 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2645983  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07982627 -0.11257976 -0.13199541 -0.10324876 -0.19025974 -0.14568801\n",
            " -0.08557017 -0.22988251 -0.18393002 -0.10390338 -0.2402179   0.46309492\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 73 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26402563 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07977072 -0.11257976 -0.13199541 -0.10324876 -0.19025832 -0.14567566\n",
            " -0.08253892 -0.22988252 -0.18393002 -0.10390338 -0.24021791  0.4630629\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 74 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26350972 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07972068 -0.11257976 -0.13199541 -0.10324876 -0.19025706 -0.14566454\n",
            " -0.07980826 -0.22988254 -0.18393002 -0.10390338 -0.24021792  0.4628864\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 75 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26304498 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07967561 -0.11257976 -0.13199541 -0.10324876 -0.19025591 -0.14565453\n",
            " -0.07734838 -0.22988255 -0.18393002 -0.10390338 -0.24021794  0.4625796\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 76 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26262632 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.079635   -0.11257976 -0.13199541 -0.10324876 -0.19025488 -0.1456455\n",
            " -0.07513244 -0.22988255 -0.18393002 -0.10390338 -0.24021795  0.46215546\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 77 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26224917 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07959842 -0.11257976 -0.13199541 -0.10324876 -0.19025396 -0.14563736\n",
            " -0.07313624 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.46162558\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 78 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26190943 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07956547 -0.11257976 -0.13199541 -0.10324876 -0.19025312 -0.14563003\n",
            " -0.071338   -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4610005\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 79 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2616034  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07953578 -0.11257976 -0.13199541 -0.10324876 -0.19025236 -0.14562343\n",
            " -0.06971808 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.46028972\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 80 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26132768 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07950904 -0.11257976 -0.13199541 -0.10324876 -0.19025168 -0.14561749\n",
            " -0.0682588  -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.45950183\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 81 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26107934 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07948495 -0.11257976 -0.13199541 -0.10324876 -0.19025107 -0.14561214\n",
            " -0.06694424 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4586446\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 82 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26085562 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07946325 -0.11257976 -0.13199541 -0.10324876 -0.19025052 -0.14560731\n",
            " -0.06576005 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.45772505\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 83 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26065406 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.0794437  -0.11257976 -0.13199541 -0.10324876 -0.19025002 -0.14560296\n",
            " -0.06469329 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.45674953\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 84 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2604725  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07942609 -0.11257976 -0.13199541 -0.10324876 -0.19024958 -0.14559904\n",
            " -0.06373232 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.45572376\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 85 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.26030895 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07941023 -0.11257976 -0.13199541 -0.10324876 -0.19024917 -0.1455955\n",
            " -0.06286665 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.45465297\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 86 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2601616  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07939593 -0.11257976 -0.13199541 -0.10324876 -0.19024882 -0.14559233\n",
            " -0.06208683 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4535418\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 87 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2600289  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07938306 -0.11257976 -0.13199541 -0.10324876 -0.19024849 -0.14558947\n",
            " -0.06138435 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.45239446\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 88 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25990933 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07937147 -0.11257976 -0.13199541 -0.10324876 -0.19024819 -0.1455869\n",
            " -0.06075153 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.45121482\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 89 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25980163 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07936102 -0.11257976 -0.13199541 -0.10324876 -0.19024792 -0.14558457\n",
            " -0.06018148 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4500063\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 90 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25970462 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07935161 -0.11257976 -0.13199541 -0.10324876 -0.19024768 -0.14558248\n",
            " -0.05966796 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.44877207\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 91 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2596172  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07934313 -0.11257976 -0.13199541 -0.10324876 -0.19024748 -0.1455806\n",
            " -0.05920537 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.44751492\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 92 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25953847 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.0793355  -0.11257976 -0.13199541 -0.10324876 -0.19024728 -0.1455789\n",
            " -0.05878865 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.44623742\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 93 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25946754 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07932862 -0.11257976 -0.13199541 -0.10324876 -0.1902471  -0.14557737\n",
            " -0.05841327 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.44494185\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 94 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25940365 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07932242 -0.11257976 -0.13199541 -0.10324876 -0.19024694 -0.145576\n",
            " -0.05807511 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4436303\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 95 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2593461  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07931684 -0.11257976 -0.13199541 -0.10324876 -0.19024679 -0.14557476\n",
            " -0.05777049 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.44230467\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 96 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25929424 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07931181 -0.11257976 -0.13199541 -0.10324876 -0.19024666 -0.14557365\n",
            " -0.05749608 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.44096664\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 97 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25924754 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07930728 -0.11257976 -0.13199541 -0.10324876 -0.19024654 -0.14557263\n",
            " -0.05724889 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.43961775\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 98 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25920546 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.0793032  -0.11257976 -0.13199541 -0.10324876 -0.19024643 -0.14557172\n",
            " -0.05702621 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4382594\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 99 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25916755 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07929952 -0.11257976 -0.13199541 -0.10324876 -0.19024634 -0.1455709\n",
            " -0.05682562 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.43689284\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 100 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25913343 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07929622 -0.11257976 -0.13199541 -0.10324876 -0.19024625 -0.14557017\n",
            " -0.05664493 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4355192\n",
            " -0.11539437]\n",
            "Epsilon reduced to 0.043046721000000024\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25910267 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07929324 -0.11257976 -0.13199541 -0.10324876 -0.19024618 -0.14556952\n",
            " -0.05648215 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4335711\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25907496 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07929055 -0.11257976 -0.13199541 -0.10324876 -0.1902461  -0.14556892\n",
            " -0.05633552 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4316745\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25905    -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07928813 -0.11257976 -0.13199541 -0.10324876 -0.19024605 -0.14556839\n",
            " -0.05620344 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.42982483\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25902754 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07928594 -0.11257976 -0.13199541 -0.10324876 -0.19024599 -0.1455679\n",
            " -0.05608445 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.42801785\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25900728 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07928398 -0.11257976 -0.13199541 -0.10324876 -0.19024594 -0.14556746\n",
            " -0.05597722 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.42624912\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25898904 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07928221 -0.11257976 -0.13199541 -0.10324876 -0.1902459  -0.14556707\n",
            " -0.05588067 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.42451665\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25897262 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07928061 -0.11257976 -0.13199541 -0.10324876 -0.19024585 -0.14556672\n",
            " -0.0557937  -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.42281663\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2589578  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927918 -0.11257976 -0.13199541 -0.10324876 -0.19024582 -0.1455664\n",
            " -0.05571534 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4211463\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25894448 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927788 -0.11257976 -0.13199541 -0.10324876 -0.19024579 -0.14556612\n",
            " -0.05564477 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.41950315\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25893247 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927672 -0.11257976 -0.13199541 -0.10324876 -0.19024576 -0.14556587\n",
            " -0.05558119 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4178849\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25892165 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927567 -0.11257976 -0.13199541 -0.10324876 -0.19024573 -0.14556563\n",
            " -0.05552392 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.41628945\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2589119  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927472 -0.11257976 -0.13199541 -0.10324876 -0.1902457  -0.14556542\n",
            " -0.05547233 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.41471496\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25890312 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927387 -0.11257976 -0.13199541 -0.10324876 -0.19024569 -0.14556523\n",
            " -0.05542585 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4131598\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25889522 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.0792731  -0.11257976 -0.13199541 -0.10324876 -0.19024567 -0.14556506\n",
            " -0.05538399 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.41162238\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588881  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927241 -0.11257976 -0.13199541 -0.10324876 -0.19024566 -0.14556491\n",
            " -0.05534628 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.41010135\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588817  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927179 -0.11257976 -0.13199541 -0.10324876 -0.19024564 -0.14556478\n",
            " -0.05531231 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.4085955\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588759  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927123 -0.11257976 -0.13199541 -0.10324876 -0.19024563 -0.14556466\n",
            " -0.05528171 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.40710366\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588707  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927072 -0.11257976 -0.13199541 -0.10324876 -0.19024561 -0.14556454\n",
            " -0.05525414 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.40562484\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.258866   -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07927027 -0.11257976 -0.13199541 -0.10324876 -0.1902456  -0.14556444\n",
            " -0.05522931 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.40415812\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25886178 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926986 -0.11257976 -0.13199541 -0.10324876 -0.19024558 -0.14556435\n",
            " -0.05520694 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.40270266\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25885797 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926948 -0.11257976 -0.13199541 -0.10324876 -0.19024557 -0.14556426\n",
            " -0.05518679 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.40125772\n",
            " -0.11539437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25885454 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926915 -0.11257976 -0.13199541 -0.10324876 -0.19024555 -0.14556418\n",
            " -0.05516864 -0.22988255 -0.18393002 -0.10390338 -0.24021797  0.39982262\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25885144 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926885 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556412\n",
            " -0.05515229 -0.22988255 -0.18393002 -0.09588395 -0.24021797  0.3985299\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25884867 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926858 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556406\n",
            " -0.05513756 -0.22988255 -0.18393002 -0.08866003 -0.24021797  0.39889845\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25884616 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926834 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.145564\n",
            " -0.05512429 -0.22988255 -0.18393002 -0.08215271 -0.24021797  0.40075845\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588439  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926812 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556396\n",
            " -0.05511234 -0.22988255 -0.18393002 -0.0762909  -0.24021797  0.4039567\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25884187 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926793 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556392\n",
            " -0.05510158 -0.22988255 -0.18393002 -0.07101057 -0.24021797  0.40835518\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25884005 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926775 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556387\n",
            " -0.05509188 -0.22988255 -0.18393002 -0.06625406 -0.24021797  0.41382933\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25883842 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926759 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556384\n",
            " -0.05508314 -0.22988255 -0.18393002 -0.06196938 -0.24021797  0.420267\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25883693 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926745 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556381\n",
            " -0.05507528 -0.22988255 -0.18393002 -0.05810975 -0.24021797  0.42756698\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25883558 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926732 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556378\n",
            " -0.05506819 -0.22988255 -0.18393002 -0.054633   -0.24021797  0.43563813\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588344  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.0792672  -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556375\n",
            " -0.0550618  -0.22988255 -0.18393002 -0.05150115 -0.24021797  0.44439828\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25883332 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.0792671  -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556372\n",
            " -0.05505605 -0.22988255 -0.18393002 -0.04867999 -0.24021797  0.4537734\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25883234 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.079267   -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.1455637\n",
            " -0.05505087 -0.22988255 -0.18393002 -0.04613869 -0.24021797  0.46369678\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25883144 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926692 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556369\n",
            " -0.0550462  -0.22988255 -0.18393002 -0.0438495  -0.24021797  0.47410825\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25883064 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926685 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556368\n",
            " -0.05504199 -0.22988255 -0.18393002 -0.04178741 -0.24021797  0.4849537\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882992 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926678 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556366\n",
            " -0.05503821 -0.22988255 -0.18393002 -0.03992989 -0.24021797  0.49618432\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882927 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926672 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556365\n",
            " -0.05503479 -0.22988255 -0.18393002 -0.03825664 -0.24021797  0.5077562\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588287  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926666 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556363\n",
            " -0.05503172 -0.22988255 -0.18393002 -0.03674939 -0.24021797  0.51962966\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882816 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926661 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556362\n",
            " -0.05502895 -0.22988255 -0.18393002 -0.03539167 -0.24021797  0.5317691\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588277  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926656 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.1455636\n",
            " -0.05502646 -0.22988255 -0.18393002 -0.03416865 -0.24021797  0.5441424\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882727 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926652 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556359\n",
            " -0.05502421 -0.22988255 -0.18393002 -0.03306695 -0.24021797  0.55672073\n",
            " -0.11539437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 43 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882688 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926648 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.14556357\n",
            " -0.05502219 -0.22988255 -0.18393002 -0.03207419 -0.24021797  0.56948286\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 44 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882652 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926645 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.13212319\n",
            " -0.05502037 -0.22988255 -0.18393002 -0.03118025 -0.24021797  0.5809745\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 45 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882623 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926642 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.12001622\n",
            " -0.05501873 -0.22988255 -0.18393002 -0.030375   -0.24021797  0.59429055\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 46 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882596 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926639 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.1091104\n",
            " -0.05501725 -0.22988255 -0.18393002 -0.02964963 -0.24021797  0.6092121\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 47 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882572 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926637 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.09928655\n",
            " -0.05501591 -0.22988255 -0.18393002 -0.02899624 -0.24021797  0.6255442\n",
            " -0.11539437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 48 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882548 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.07926635 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.09043734\n",
            " -0.05501471 -0.22988255 -0.18393002 -0.02840766 -0.24021797  0.6431131\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 49 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882527 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.06281291 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.08246608\n",
            " -0.05501363 -0.22988255 -0.18393002 -0.02787748 -0.24021797  0.65893894\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 50 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588251  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.04799186 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.07528567\n",
            " -0.05501266 -0.22988255 -0.18393002 -0.02739991 -0.24021797  0.6760297\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 51 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588249  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.03464125 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.06881765\n",
            " -0.05501179 -0.22988255 -0.18393002 -0.02696971 -0.24021797  0.6942284\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 52 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882477 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.02261522 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.06299135\n",
            " -0.055011   -0.22988255 -0.18393002 -0.02658219 -0.24021797  0.71339536\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 53 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882462 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.01178234 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.0577431\n",
            " -0.05501029 -0.22988255 -0.18393002 -0.02623312 -0.24021797  0.733406\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 54 reward=2 new_state=[0 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588245  -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            " -0.00202424 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.05301556\n",
            " -0.05500964 -0.22988255 -0.18393002 -0.02591869 -0.24021797  0.7541495\n",
            " -0.11539437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 55 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882438 -0.14444838 -0.19271    -0.09859437 -0.07031455\n",
            "  0.0067657  -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.04875707\n",
            " -0.05500907 -0.22988255 -0.18393002 -0.02563545 -0.24021797  0.775527\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 56 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588243  -0.14444838 -0.1752764  -0.09859437 -0.07031455\n",
            "  0.01468353 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.04492108\n",
            " -0.05500855 -0.22988255 -0.18393002 -0.02538032 -0.24021797  0.79478353\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 57 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588242  -0.14444838 -0.1595725  -0.09859437 -0.07031455\n",
            "  0.02181578 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.04146569\n",
            " -0.05500808 -0.22988255 -0.18393002 -0.02515049 -0.24021797  0.8133886\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 58 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588241  -0.14444838 -0.1454267  -0.09859437 -0.07031455\n",
            "  0.02824039 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.03835314\n",
            " -0.05500766 -0.22988255 -0.18393002 -0.02494347 -0.24021797  0.83140147\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 59 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882405 -0.14444838 -0.13268442 -0.09859437 -0.07031455\n",
            "  0.03402757 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.03554941\n",
            " -0.05500728 -0.22988255 -0.18393002 -0.02475699 -0.24021797  0.84887576\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 60 reward=1 new_state=[0 1 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.258824   -0.14444838 -0.12120642 -0.09859437 -0.07031455\n",
            "  0.03924054 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.03302386\n",
            " -0.05500693 -0.22988255 -0.18393002 -0.02458902 -0.24021797  0.8658598\n",
            " -0.11539437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 61 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882393 -0.14444838 -0.11086725 -0.09859437 -0.07031455\n",
            "  0.04393629 -0.11257976 -0.13199541 -0.10324876 -0.19024554 -0.03074889\n",
            " -0.05500662 -0.22988255 -0.18393002 -0.0244377  -0.24021797  0.88239723\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 62 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882387 -0.14444838 -0.10155052 -0.09859437 -0.07031455\n",
            "  0.04816768 -0.10514654 -0.13199541 -0.10324876 -0.19024554 -0.02869889\n",
            " -0.05500634 -0.22988255 -0.18393002 -0.02430136 -0.24021797  0.8972993\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 63 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588238  -0.14444838 -0.09316127 -0.09859437 -0.07031455\n",
            "  0.05197783 -0.0984533  -0.13199541 -0.10324876 -0.19024554 -0.02685297\n",
            " -0.05500609 -0.22988255 -0.18393002 -0.02417858 -0.24021797  0.9104431\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 64 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882378 -0.14444838 -0.08560441 -0.09859437 -0.07031455\n",
            "  0.05540994 -0.09242416 -0.13199541 -0.10324876 -0.19024554 -0.02519021\n",
            " -0.05500587 -0.22988255 -0.18393002 -0.02406799 -0.24021797  0.9220039\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 65 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882375 -0.14444838 -0.07879735 -0.09859437 -0.07031455\n",
            "  0.0585015  -0.08699325 -0.13199541 -0.10324876 -0.19024554 -0.02369243\n",
            " -0.05500566 -0.22988255 -0.18393002 -0.02396837 -0.24021797  0.9321351\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 66 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882372 -0.14444838 -0.0726657  -0.09859437 -0.07031455\n",
            "  0.06128632 -0.0821012  -0.13199541 -0.10324876 -0.19024554 -0.02234326\n",
            " -0.05500548 -0.22988255 -0.18393002 -0.02387863 -0.24021797  0.94097525\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 67 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588237  -0.14444838 -0.06714245 -0.09859437 -0.07031455\n",
            "  0.06379482 -0.07769455 -0.13199541 -0.10324876 -0.19024554 -0.02112796\n",
            " -0.05500532 -0.22988255 -0.18393002 -0.0237978  -0.24021797  0.9486496\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 68 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882366 -0.14444838 -0.06216723 -0.09859437 -0.07031455\n",
            "  0.06605442 -0.07372515 -0.13199541 -0.10324876 -0.19024554 -0.02003324\n",
            " -0.05500517 -0.22988255 -0.18393002 -0.02372499 -0.24021797  0.9552713\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 69 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882363 -0.14444838 -0.05768568 -0.09859437 -0.07031455\n",
            "  0.06808981 -0.0701496  -0.13199541 -0.10324876 -0.19024554 -0.01904715\n",
            " -0.05500503 -0.22988255 -0.18393002 -0.02365941 -0.24021797  0.9609426\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 70 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588236  -0.14444838 -0.0536488  -0.09859437 -0.07031455\n",
            "  0.06992324 -0.06692883 -0.13199541 -0.10324876 -0.19024554 -0.0181589\n",
            " -0.05500491 -0.22988255 -0.18393002 -0.02360033 -0.24021797  0.96575594\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 71 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882357 -0.14444838 -0.05001248 -0.09859437 -0.07031455\n",
            "  0.07157476 -0.06402765 -0.13199541 -0.10324876 -0.19024554 -0.01735878\n",
            " -0.05500481 -0.22988255 -0.18393002 -0.02354711 -0.24021797  0.9697947\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 72 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882354 -0.14444838 -0.04673697 -0.09859437 -0.07031455\n",
            "  0.07306239 -0.06141434 -0.13199541 -0.10324876 -0.19024554 -0.01663806\n",
            " -0.05500471 -0.22988255 -0.18393002 -0.02349917 -0.24021797  0.97313434\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 73 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.2588235  -0.14444838 -0.04378648 -0.09859437 -0.07031455\n",
            "  0.07440241 -0.05906034 -0.13199541 -0.10324876 -0.19024554 -0.01598885\n",
            " -0.05500462 -0.22988255 -0.18393002 -0.02345599 -0.24021797  0.9758431\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 74 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.04112876 -0.09859437 -0.07031455\n",
            "  0.07560948 -0.05693991 -0.13199541 -0.10324876 -0.19024554 -0.01540406\n",
            " -0.05500454 -0.22988255 -0.18393002 -0.0234171  -0.24021797  0.9779824\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 75 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.03873476 -0.09859437 -0.07031455\n",
            "  0.07669676 -0.05502988 -0.13199541 -0.10324876 -0.19024554 -0.0148773\n",
            " -0.05500447 -0.22988255 -0.18393002 -0.02338206 -0.24021797  0.979608\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 76 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0365783  -0.09859437 -0.07031455\n",
            "  0.07767616 -0.05330939 -0.13199541 -0.10324876 -0.19024554 -0.01440281\n",
            " -0.05500441 -0.22988255 -0.18393002 -0.0233505  -0.24021797  0.9807702\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 77 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.03463583 -0.09859437 -0.07031455\n",
            "  0.07855838 -0.05175962 -0.13199541 -0.10324876 -0.19024554 -0.0139754\n",
            " -0.05500435 -0.22988255 -0.18393002 -0.02332208 -0.24021797  0.9815143\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 78 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0328861  -0.09859437 -0.07031455\n",
            "  0.07935305 -0.05036362 -0.13199541 -0.10324876 -0.19024554 -0.0135904\n",
            " -0.05500429 -0.22988255 -0.18393002 -0.02329647 -0.24021797  0.9818814\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 79 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.03131    -0.09859437 -0.07031455\n",
            "  0.08006887 -0.04910615 -0.13199541 -0.10324876 -0.19024554 -0.0132436\n",
            " -0.05500425 -0.22988255 -0.18393002 -0.0232734  -0.24021797  0.9819086\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 80 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0298903  -0.09859437 -0.07031455\n",
            "  0.08071366 -0.04797347 -0.13199541 -0.10324876 -0.19024554 -0.01293122\n",
            " -0.05500421 -0.22988255 -0.18393002 -0.02325263 -0.24021797  0.98162943\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 81 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02861147 -0.09859437 -0.07031455\n",
            "  0.08129446 -0.04695317 -0.13199541 -0.10324876 -0.19024554 -0.01264983\n",
            " -0.05500417 -0.22988255 -0.18393002 -0.02323391 -0.24021797  0.98107415\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 82 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02745955 -0.09859437 -0.07031455\n",
            "  0.08181763 -0.04603412 -0.13199541 -0.10324876 -0.19024554 -0.01239637\n",
            " -0.05500413 -0.22988255 -0.18393002 -0.02321705 -0.24021797  0.9802701\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 83 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02642193 -0.09859437 -0.07031455\n",
            "  0.08228889 -0.04520627 -0.13199541 -0.10324876 -0.19024554 -0.01216806\n",
            " -0.0550041  -0.22988255 -0.18393002 -0.02320186 -0.24021797  0.97924197\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 84 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02548727 -0.09859437 -0.07031455\n",
            "  0.08271338 -0.04446057 -0.13199541 -0.10324876 -0.19024554 -0.0119624\n",
            " -0.05500408 -0.22988255 -0.18393002 -0.02318818 -0.24021797  0.97801214\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 85 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02464536 -0.09859437 -0.07031455\n",
            "  0.08309575 -0.04378887 -0.13199541 -0.10324876 -0.19024554 -0.01177716\n",
            " -0.05500405 -0.22988255 -0.18393002 -0.02317586 -0.24021797  0.9766008\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 86 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.023887   -0.09859437 -0.07031455\n",
            "  0.08344018 -0.04318382 -0.13199541 -0.10324876 -0.19024554 -0.01161029\n",
            " -0.05500403 -0.22988255 -0.18393002 -0.02316477 -0.24021797  0.9750262\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 87 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0232039  -0.09859437 -0.07031455\n",
            "  0.08375043 -0.04263882 -0.13199541 -0.10324876 -0.19024554 -0.01145998\n",
            " -0.05500401 -0.22988255 -0.18393002 -0.02315477 -0.24021797  0.9733047\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 88 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02258858 -0.09859437 -0.07031455\n",
            "  0.08402988 -0.04214789 -0.13199541 -0.10324876 -0.19024554 -0.01132459\n",
            " -0.05500399 -0.22988255 -0.18393002 -0.02314576 -0.24021797  0.9714513\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 89 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02203432 -0.09859437 -0.07031455\n",
            "  0.08428161 -0.04170569 -0.13199541 -0.10324876 -0.19024554 -0.01120264\n",
            " -0.05500398 -0.22988255 -0.18393002 -0.02313765 -0.24021797  0.9694793\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 90 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02153506 -0.09859437 -0.07031455\n",
            "  0.08450836 -0.04130736 -0.13199541 -0.10324876 -0.19024554 -0.01109278\n",
            " -0.05500396 -0.22988255 -0.18393002 -0.02313034 -0.24021797  0.967401\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 91 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02108535 -0.09859437 -0.07031455\n",
            "  0.0847126  -0.04094857 -0.13199541 -0.10324876 -0.19024554 -0.01099383\n",
            " -0.05500395 -0.22988255 -0.18393002 -0.02312376 -0.24021797  0.96522737\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 92 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02068027 -0.09859437 -0.07031455\n",
            "  0.08489658 -0.04062537 -0.13199541 -0.10324876 -0.19024554 -0.0109047\n",
            " -0.05500394 -0.22988255 -0.18393002 -0.02311783 -0.24021797  0.96296823\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 93 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.02031538 -0.09859437 -0.07031455\n",
            "  0.0850623  -0.04033426 -0.13199541 -0.10324876 -0.19024554 -0.01082441\n",
            " -0.05500393 -0.22988255 -0.18393002 -0.02311249 -0.24021797  0.9606326\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 94 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01998671 -0.09859437 -0.07031455\n",
            "  0.08521158 -0.04007203 -0.13199541 -0.10324876 -0.19024554 -0.01075209\n",
            " -0.05500391 -0.22988255 -0.18393002 -0.02310769 -0.24021797  0.9582286\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 95 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01969065 -0.09859437 -0.07031455\n",
            "  0.08534604 -0.03983583 -0.13199541 -0.10324876 -0.19024554 -0.01068695\n",
            " -0.05500391 -0.22988255 -0.18393002 -0.02310335 -0.24021797  0.95576346\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 96 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01942397 -0.09859437 -0.07031455\n",
            "  0.08546715 -0.03962306 -0.13199541 -0.10324876 -0.19024554 -0.01062827\n",
            " -0.0550039  -0.22988255 -0.18393002 -0.02309945 -0.24021797  0.95324385\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 97 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01918376 -0.09859437 -0.07031455\n",
            "  0.08557625 -0.03943141 -0.13199541 -0.10324876 -0.19024554 -0.01057542\n",
            " -0.05500389 -0.22988255 -0.18393002 -0.02309594 -0.24021797  0.9506757\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 98 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01896739 -0.09859437 -0.07031455\n",
            "  0.08567452 -0.03925878 -0.13199541 -0.10324876 -0.19024554 -0.01052781\n",
            " -0.05500389 -0.22988255 -0.18393002 -0.02309277 -0.24021797  0.9480645\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 99 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01877248 -0.09859437 -0.07031455\n",
            "  0.08576304 -0.03910328 -0.13199541 -0.10324876 -0.19024554 -0.01048492\n",
            " -0.05500388 -0.22988255 -0.18393002 -0.02308992 -0.24021797  0.945415\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 100 reward=0 new_state=[0 1 0 0 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01859692 -0.09859437 -0.07031455\n",
            "  0.08584278 -0.03896321 -0.13199541 -0.10324876 -0.19024554 -0.01044629\n",
            " -0.05500387 -0.22988255 -0.18393002 -0.02308735 -0.24021797  0.94273174\n",
            " -0.11539437]\n",
            "Epsilon reduced to 0.03874204890000002\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01843879 -0.09859437 -0.07031455\n",
            "  0.0859146  -0.03883705 -0.13199541 -0.10324876 -0.19024554 -0.0104115\n",
            " -0.05500387 -0.22988255 -0.18393002 -0.02308504 -0.24021797  0.9388401\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01829635 -0.09859437 -0.07031455\n",
            "  0.0859793  -0.0387234  -0.13199541 -0.10324876 -0.19024554 -0.01038016\n",
            " -0.05500387 -0.22988255 -0.18393002 -0.02308295 -0.24021797  0.9350401\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01816804 -0.09859437 -0.07031455\n",
            "  0.08603757 -0.03862103 -0.13199541 -0.10324876 -0.19024554 -0.01035192\n",
            " -0.05500386 -0.22988255 -0.18393002 -0.02308107 -0.24021797  0.93132365\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01805247 -0.09859437 -0.07031455\n",
            "  0.08609006 -0.03852882 -0.13199541 -0.10324876 -0.19024554 -0.01032649\n",
            " -0.05500386 -0.22988255 -0.18393002 -0.02307938 -0.24021797  0.92768335\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01794836 -0.09859437 -0.07031455\n",
            "  0.08613734 -0.03844576 -0.13199541 -0.10324876 -0.19024554 -0.01030359\n",
            " -0.05500386 -0.22988255 -0.18393002 -0.02307786 -0.24021797  0.92411256\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01785459 -0.09859437 -0.07031455\n",
            "  0.08617993 -0.03837095 -0.13199541 -0.10324876 -0.19024554 -0.01028296\n",
            " -0.05500385 -0.22988255 -0.18393002 -0.02307649 -0.24021797  0.9206053\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01777013 -0.09859437 -0.07031455\n",
            "  0.08621829 -0.03830356 -0.13199541 -0.10324876 -0.19024554 -0.01026437\n",
            " -0.05500385 -0.22988255 -0.18393002 -0.02307525 -0.24021797  0.9171561\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01769404 -0.09859437 -0.07031455\n",
            "  0.08625285 -0.03824286 -0.13199541 -0.10324876 -0.19024554 -0.01024763\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02307414 -0.24021797  0.9137601\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01762551 -0.09859437 -0.07031455\n",
            "  0.08628397 -0.03818818 -0.13199541 -0.10324876 -0.19024554 -0.01023255\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02307313 -0.24021797  0.9104129\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01756378 -0.09859437 -0.07031455\n",
            "  0.08631201 -0.03813893 -0.13199541 -0.10324876 -0.19024554 -0.01021897\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02307223 -0.24021797  0.90711045\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01750818 -0.09859437 -0.07031455\n",
            "  0.08633726 -0.03809457 -0.13199541 -0.10324876 -0.19024554 -0.01020673\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02307142 -0.24021797  0.9038491\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01745809 -0.09859437 -0.07031455\n",
            "  0.08636001 -0.0380546  -0.13199541 -0.10324876 -0.19024554 -0.01019571\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02307068 -0.24021797  0.90062565\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01741298 -0.09859437 -0.07031455\n",
            "  0.0863805  -0.03801861 -0.13199541 -0.10324876 -0.19024554 -0.01018579\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02307002 -0.24021797  0.89743704\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01737234 -0.09859437 -0.07031455\n",
            "  0.08639895 -0.03798619 -0.13199541 -0.10324876 -0.19024554 -0.01017684\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306943 -0.24021797  0.8942807\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01733574 -0.09859437 -0.07031455\n",
            "  0.08641557 -0.03795699 -0.13199541 -0.10324876 -0.19024554 -0.01016879\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306889 -0.24021797  0.8911541\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01730276 -0.09859437 -0.07031455\n",
            "  0.08643055 -0.03793068 -0.13199541 -0.10324876 -0.19024554 -0.01016154\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306841 -0.24021797  0.88805515\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01727307 -0.09859437 -0.07031455\n",
            "  0.08644404 -0.03790699 -0.13199541 -0.10324876 -0.19024554 -0.010155\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306798 -0.24021797  0.8849819\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01724632 -0.09859437 -0.07031455\n",
            "  0.08645619 -0.03788565 -0.13199541 -0.10324876 -0.19024554 -0.01014911\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306758 -0.24021797  0.88193244\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01722222 -0.09859437 -0.07031455\n",
            "  0.08646713 -0.03786642 -0.13199541 -0.10324876 -0.19024554 -0.01014381\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306723 -0.24021797  0.87890524\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01720052 -0.09859437 -0.07031455\n",
            "  0.08647699 -0.03784911 -0.13199541 -0.10324876 -0.19024554 -0.01013904\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306691 -0.24021797  0.87589884\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01718097 -0.09859437 -0.07031455\n",
            "  0.08648587 -0.03783351 -0.13199541 -0.10324876 -0.19024554 -0.01013474\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306663 -0.24021797  0.8729119\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01716336 -0.09859437 -0.07031455\n",
            "  0.08649386 -0.03781946 -0.13199541 -0.10324876 -0.19024554 -0.01013086\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306637 -0.24021797  0.8699432\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01714749 -0.09859437 -0.07031455\n",
            "  0.08650107 -0.03780681 -0.13199541 -0.10324876 -0.19024554 -0.01012737\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306614 -0.24021797  0.8669917\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01713321 -0.09859437 -0.07031455\n",
            "  0.08650756 -0.03779541 -0.13199541 -0.10324876 -0.19024554 -0.01012423\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306593 -0.24021797  0.8640564\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01712034 -0.09859437 -0.07031455\n",
            "  0.08651341 -0.03778514 -0.13199541 -0.10324876 -0.19024554 -0.01012139\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306574 -0.24021797  0.8611365\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01710874 -0.09859437 -0.07031455\n",
            "  0.08651868 -0.03777589 -0.13199541 -0.10324876 -0.19024554 -0.01011884\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306557 -0.24021797  0.85823107\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0170983  -0.09859437 -0.07031455\n",
            "  0.08652342 -0.03776756 -0.13199541 -0.10324876 -0.19024554 -0.01011655\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306542 -0.24021797  0.85533947\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0170889  -0.09859437 -0.07031455\n",
            "  0.08652769 -0.03776006 -0.13199541 -0.10324876 -0.19024554 -0.01011448\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306528 -0.24021797  0.85246104\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01708043 -0.09859437 -0.07031455\n",
            "  0.08653153 -0.0377533  -0.13199541 -0.10324876 -0.19024554 -0.01011261\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306516 -0.24021797  0.8495951\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01707279 -0.09859437 -0.07031455\n",
            "  0.086535   -0.03774721 -0.13199541 -0.10324876 -0.19024554 -0.01011093\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306504 -0.24021797  0.84674126\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01706592 -0.09859437 -0.07031455\n",
            "  0.08653812 -0.03774173 -0.13199541 -0.10324876 -0.19024554 -0.01010942\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306494 -0.24021797  0.84389895\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01705973 -0.09859437 -0.07031455\n",
            "  0.08654093 -0.03773679 -0.13199541 -0.10324876 -0.19024554 -0.01010806\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306485 -0.24021797  0.84106773\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01705415 -0.09859437 -0.07031455\n",
            "  0.08654346 -0.03773234 -0.13199541 -0.10324876 -0.19024554 -0.01010683\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306477 -0.24021797  0.8382472\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01704913 -0.09859437 -0.07031455\n",
            "  0.08654574 -0.03772833 -0.13199541 -0.10324876 -0.19024554 -0.01010572\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.0230647  -0.24021797  0.835437\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01704461 -0.09859437 -0.07031455\n",
            "  0.0865478  -0.03772472 -0.13199541 -0.10324876 -0.19024554 -0.01010473\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306463 -0.24021797  0.83263683\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01704053 -0.09859437 -0.07031455\n",
            "  0.08654965 -0.03772147 -0.13199541 -0.10324876 -0.19024554 -0.01010383\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306457 -0.24021797  0.8298464\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01703686 -0.09859437 -0.07031455\n",
            "  0.08655132 -0.03771854 -0.13199541 -0.10324876 -0.19024554 -0.01010302\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306452 -0.24021797  0.82706535\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01703355 -0.09859437 -0.07031455\n",
            "  0.08655282 -0.0377159  -0.13199541 -0.10324876 -0.19024554 -0.0101023\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306447 -0.24021797  0.8242935\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01703057 -0.09859437 -0.07031455\n",
            "  0.08655418 -0.03771353 -0.13199541 -0.10324876 -0.19024554 -0.01010164\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306443 -0.24021797  0.82152975\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01702789 -0.09859437 -0.07031455\n",
            "  0.0865554  -0.03771139 -0.13199541 -0.10324876 -0.19024554 -0.01010105\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306439 -0.24021797  0.81877565\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01702547 -0.09859437 -0.07031455\n",
            "  0.08655649 -0.03770946 -0.13199541 -0.10324876 -0.19024554 -0.01010052\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306435 -0.24021797  0.8160302\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0170233  -0.09859437 -0.07031455\n",
            "  0.08655749 -0.03770772 -0.13199541 -0.10324876 -0.19024554 -0.01010004\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306432 -0.24021797  0.81329316\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01702134 -0.09859437 -0.07031455\n",
            "  0.08655838 -0.03770616 -0.13199541 -0.10324876 -0.19024554 -0.01009961\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306429 -0.24021797  0.8105644\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01701957 -0.09859437 -0.07031455\n",
            "  0.08655918 -0.03770475 -0.13199541 -0.10324876 -0.19024554 -0.01009922\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306427 -0.24021797  0.80784374\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01701798 -0.09859437 -0.07031455\n",
            "  0.08655991 -0.03770348 -0.13199541 -0.10324876 -0.19024554 -0.01009887\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306424 -0.24021797  0.80513114\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01701655 -0.09859437 -0.07031455\n",
            "  0.08656055 -0.03770233 -0.13199541 -0.10324876 -0.19024554 -0.01009856\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306422 -0.24021797  0.8024264\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01701526 -0.09859437 -0.07031455\n",
            "  0.08656114 -0.03770131 -0.13199541 -0.10324876 -0.19024554 -0.01009827\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.0230642  -0.24021797  0.79972947\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01701409 -0.09859437 -0.07031455\n",
            "  0.08656167 -0.03770038 -0.13199541 -0.10324876 -0.19024554 -0.01009802\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306419 -0.24021797  0.7970402\n",
            " -0.11539437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 4\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01701305 -0.09859437 -0.07031455\n",
            "  0.08656215 -0.03769954 -0.13199541 -0.10324876 -0.19024554 -0.01009779\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306417 -0.24021797  0.7943586\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0170121  -0.09223422 -0.07031455\n",
            "  0.08656257 -0.03769879 -0.13199541 -0.10324876 -0.19024554 -0.01009758\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306416 -0.24021797  0.7919432\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 51 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01701126 -0.08650542 -0.07031455\n",
            "  0.08656296 -0.03769811 -0.13199541 -0.10324876 -0.19024554 -0.01009739\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306415 -0.24021797  0.7895094\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 52 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01701049 -0.08134529 -0.07031455\n",
            "  0.08656331 -0.0376975  -0.13199541 -0.10324876 -0.19024554 -0.01009722\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306413 -0.24021797  0.7870598\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 53 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0170098  -0.07669739 -0.07031455\n",
            "  0.08656362 -0.03769695 -0.13199541 -0.10324876 -0.19024554 -0.01009707\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306413 -0.24021797  0.7845964\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 54 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700918 -0.07251088 -0.07031455\n",
            "  0.08656391 -0.03769645 -0.13199541 -0.10324876 -0.19024554 -0.01009693\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306412 -0.24021797  0.78212124\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 55 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700862 -0.06873994 -0.07031455\n",
            "  0.08656416 -0.03769601 -0.13199541 -0.10324876 -0.19024554 -0.01009681\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306411 -0.24021797  0.77963614\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 56 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700812 -0.06534334 -0.07031455\n",
            "  0.08656439 -0.03769561 -0.13199541 -0.10324876 -0.19024554 -0.0100967\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.0230641  -0.24021797  0.7771427\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 57 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700766 -0.06228392 -0.07031455\n",
            "  0.0865646  -0.03769524 -0.13199541 -0.10324876 -0.19024554 -0.0100966\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306409 -0.24021797  0.77464235\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 58 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700726 -0.0595282  -0.07031455\n",
            "  0.08656479 -0.03769492 -0.13199541 -0.10324876 -0.19024554 -0.01009651\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306409 -0.24021797  0.7721364\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 59 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700689 -0.05704604 -0.07031455\n",
            "  0.08656495 -0.03769462 -0.13199541 -0.10324876 -0.19024554 -0.01009643\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306408 -0.24021797  0.769626\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 60 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700656 -0.05481028 -0.07031455\n",
            "  0.0865651  -0.03769436 -0.13199541 -0.10324876 -0.19024554 -0.01009636\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306408 -0.24021797  0.7671123\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 61 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700626 -0.05279646 -0.07031455\n",
            "  0.08656523 -0.03769412 -0.13199541 -0.10324876 -0.19024554 -0.01009629\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306407 -0.24021797  0.7645962\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 62 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700599 -0.05098256 -0.07031455\n",
            "  0.08656535 -0.0376939  -0.13199541 -0.10324876 -0.19024554 -0.01009623\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306407 -0.24021797  0.76207864\n",
            " -0.11539437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 63 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700575 -0.04934873 -0.07031455\n",
            "  0.08656546 -0.03769371 -0.13199541 -0.10324876 -0.19024554 -0.01009618\n",
            " -0.05500384 -0.22988255 -0.18393002 -0.02306407 -0.24021797  0.75956035\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 64 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700553 -0.04787709 -0.07031455\n",
            "  0.08656556 -0.03769353 -0.13199541 -0.10324876 -0.19024554 -0.01009613\n",
            " -0.05500384 -0.21449262 -0.18393002 -0.02306406 -0.24021797  0.75729203\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 65 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700533 -0.04655154 -0.07031455\n",
            "  0.08656565 -0.03769338 -0.13199541 -0.10324876 -0.19024554 -0.01009609\n",
            " -0.05500384 -0.20063047 -0.18393002 -0.02306406 -0.24021797  0.7566482\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 66 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700515 -0.04535758 -0.07031455\n",
            "  0.08656573 -0.03769324 -0.13199541 -0.10324876 -0.19024554 -0.01009605\n",
            " -0.05500384 -0.18814445 -0.18393002 -0.02306406 -0.24021797  0.7574644\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 67 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700499 -0.04428215 -0.07031455\n",
            "  0.08656581 -0.03769311 -0.13199541 -0.10324876 -0.19024554 -0.01009601\n",
            " -0.05500384 -0.17689794 -0.18393002 -0.02306405 -0.24021797  0.7595923\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 68 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700485 -0.04331348 -0.07031455\n",
            "  0.08656587 -0.03769299 -0.13199541 -0.10324876 -0.19024554 -0.01009598\n",
            " -0.05500384 -0.16676793 -0.18393002 -0.02306405 -0.24021797  0.7628981\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 69 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700472 -0.04244098 -0.07031455\n",
            "  0.08656593 -0.03769289 -0.13199541 -0.10324876 -0.19024554 -0.01009595\n",
            " -0.05500384 -0.15764356 -0.18393002 -0.02306405 -0.24021797  0.767261\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 70 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700461 -0.04165509 -0.07031455\n",
            "  0.08656599 -0.0376928  -0.13199541 -0.10324876 -0.19024554 -0.01009593\n",
            " -0.05500384 -0.149425   -0.18393002 -0.02306405 -0.24021797  0.7725723\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 71 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0170045  -0.04094722 -0.07031455\n",
            "  0.08656603 -0.03769271 -0.13199541 -0.10324876 -0.19024554 -0.0100959\n",
            " -0.05500384 -0.14202234 -0.18393002 -0.02306405 -0.24021797  0.7787336\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 72 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700441 -0.04030963 -0.07031455\n",
            "  0.08656608 -0.03769264 -0.13199541 -0.10324876 -0.19024554 -0.01009588\n",
            " -0.05500384 -0.13535458 -0.18393002 -0.02306405 -0.24021797  0.78565645\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 73 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700432 -0.03973533 -0.07031455\n",
            "  0.08656611 -0.03769257 -0.13199541 -0.10324876 -0.19024554 -0.01009586\n",
            " -0.05500384 -0.12934875 -0.18393002 -0.02306404 -0.24021797  0.79326093\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 74 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700424 -0.03921805 -0.07031455\n",
            "  0.08656615 -0.03769251 -0.13199541 -0.10324876 -0.19024554 -0.01009585\n",
            " -0.05500384 -0.12393917 -0.18393002 -0.02306404 -0.24021797  0.80147505\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 75 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700418 -0.03875212 -0.07031455\n",
            "  0.08656618 -0.03769246 -0.13199541 -0.10324876 -0.19024554 -0.01009583\n",
            " -0.05500384 -0.11906663 -0.18393002 -0.02306404 -0.24021797  0.81023383\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 76 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700411 -0.03833245 -0.07031455\n",
            "  0.08656621 -0.03769241 -0.13199541 -0.10324876 -0.19024554 -0.01009582\n",
            " -0.05500384 -0.11467782 -0.18393002 -0.02306404 -0.24021797  0.81947875\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 77 reward=1 new_state=[0 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700406 -0.03795444 -0.07031455\n",
            "  0.08656623 -0.03769236 -0.13199541 -0.10324876 -0.19024554 -0.0100958\n",
            " -0.05500384 -0.11072472 -0.18393002 -0.02306404 -0.24021797  0.829157\n",
            " -0.11539437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 15\n",
            "\n",
            "Step 78 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700401 -0.03761396 -0.07031455\n",
            "  0.08656625 -0.03769232 -0.13199541 -0.10324876 -0.19024554 -0.01009579\n",
            " -0.05500384 -0.10716407 -0.18393002 -0.02306404 -0.24021797  0.83922094\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 79 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700396 -0.03730728 -0.07031455\n",
            "  0.08656628 -0.03769229 -0.13199541 -0.10324876 -0.19024554 -0.01009578\n",
            " -0.05500384 -0.10395692 -0.18393002 -0.01867155 -0.24021797  0.84828573\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 80 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700392 -0.03703096 -0.07031455\n",
            "  0.08656629 -0.03769225 -0.13199541 -0.10324876 -0.19024554 -0.01009577\n",
            " -0.05500384 -0.1010673  -0.18393002 -0.01471395 -0.24021797  0.85617554\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 81 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700388 -0.03678215 -0.07031455\n",
            "  0.08656631 -0.03769222 -0.13199541 -0.10324876 -0.19024554 -0.01009577\n",
            " -0.05500384 -0.09846534 -0.18393002 -0.01115033 -0.24021797  0.8629998\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 82 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700385 -0.03655805 -0.07031455\n",
            "  0.08656632 -0.0376922  -0.13199541 -0.10324876 -0.19024554 -0.01009576\n",
            " -0.05500384 -0.0961217  -0.18393002 -0.00794051 -0.24021797  0.868864\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 83 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700382 -0.03635619 -0.07031455\n",
            "  0.08656634 -0.03769217 -0.13199541 -0.10324876 -0.19024554 -0.01009575\n",
            " -0.05500384 -0.09401073 -0.18393002 -0.00504936 -0.24021797  0.8738614\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 84 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0170038  -0.03617437 -0.07031455\n",
            "  0.08656635 -0.03769215 -0.13199541 -0.10324876 -0.19024554 -0.01009575\n",
            " -0.05500384 -0.09210934 -0.18393002 -0.00244524 -0.24021797  0.8780762\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 85 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-9.3568698e-02 -2.5882348e-01 -1.4444838e-01 -1.7003771e-02\n",
            " -3.6010604e-02 -7.0314549e-02  8.6566359e-02 -3.7692133e-02\n",
            " -1.3199541e-01 -1.0324876e-01 -1.9024554e-01 -1.0095741e-02\n",
            " -5.5003840e-02 -9.0396732e-02 -1.8393002e-01 -9.9654775e-05\n",
            " -2.4021797e-01  8.8158464e-01 -1.1539437e-01]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 86 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700375 -0.0358631  -0.07031455\n",
            "  0.08656637 -0.03769211 -0.13199541 -0.10324876 -0.19024554 -0.01009574\n",
            " -0.05500384 -0.08885415 -0.18393002  0.00201305 -0.24021797  0.8844555\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 87 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700373 -0.03573024 -0.07031455\n",
            "  0.08656637 -0.0376921  -0.13199541 -0.10324876 -0.19024554 -0.01009573\n",
            " -0.05500384 -0.08746471 -0.18393002  0.00391601 -0.24021797  0.88675094\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 88 reward=0 new_state=[0 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700371 -0.03561056 -0.07031455\n",
            "  0.08656638 -0.03769208 -0.13199541 -0.10324876 -0.19024554 -0.01009573\n",
            " -0.05500384 -0.08621322 -0.18393002  0.00563004 -0.24021797  0.88852715\n",
            " -0.11539437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 89 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700369 -0.03550277 -0.07031455\n",
            "  0.08656639 -0.03769207 -0.13199541 -0.10324876 -0.19024554 -0.01009572\n",
            " -0.05500384 -0.08508599 -0.18393002  0.00717389 -0.24021797  0.88983494\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 90 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700368 -0.03540568 -0.07031455\n",
            "  0.0865664  -0.03769206 -0.13199541 -0.10399251 -0.19024554 -0.01009572\n",
            " -0.05500384 -0.08407067 -0.18393002  0.00856446 -0.24021797  0.8910129\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 91 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700366 -0.03531823 -0.07031455\n",
            "  0.0865664  -0.03769205 -0.13199541 -0.10466243 -0.19024554 -0.01009572\n",
            " -0.05500384 -0.08315615 -0.18393002  0.00981698 -0.24021797  0.8901376\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 92 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700365 -0.03523947 -0.07031455\n",
            "  0.08656641 -0.03769204 -0.13199541 -0.10526583 -0.19024554 -0.01009572\n",
            " -0.05500384 -0.08233243 -0.18393002  0.01094514 -0.24021797  0.88742286\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 93 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700364 -0.03516852 -0.07031455\n",
            "  0.08656642 -0.03769203 -0.13199541 -0.10580932 -0.19024554 -0.01009571\n",
            " -0.05500384 -0.0815905  -0.18393002  0.01196129 -0.24021797  0.88306123\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 94 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700363 -0.03510461 -0.07031455\n",
            "  0.08656643 -0.03769203 -0.13199541 -0.10629885 -0.19024554 -0.01009571\n",
            " -0.05500384 -0.08092222 -0.18393002  0.01287655 -0.24021797  0.877226\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 95 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700362 -0.03504705 -0.07031455\n",
            "  0.08656643 -0.03769202 -0.13199541 -0.10673977 -0.19024554 -0.01009571\n",
            " -0.05500384 -0.0803203  -0.18393002  0.01370095 -0.24021797  0.8700732\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 96 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700362 -0.03499521 -0.07031455\n",
            "  0.08656643 -0.03769201 -0.13199541 -0.10713692 -0.19024554 -0.01009571\n",
            " -0.05500384 -0.07977813 -0.18393002  0.01444349 -0.24021797  0.86174333\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 97 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700361 -0.03494851 -0.07031455\n",
            "  0.08656643 -0.03769201 -0.13199541 -0.10749464 -0.19024554 -0.01009571\n",
            " -0.05500384 -0.0792898  -0.18393002  0.01511231 -0.24021797  0.8523628\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 98 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0170036  -0.03490645 -0.07031455\n",
            "  0.08656643 -0.037692   -0.13199541 -0.10781684 -0.19024554 -0.01009571\n",
            " -0.05500384 -0.07884995 -0.18393002  0.01571472 -0.24021797  0.84204537\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 99 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.0170036  -0.03486857 -0.07031455\n",
            "  0.08656643 -0.037692   -0.13199541 -0.10810705 -0.19024554 -0.0100957\n",
            " -0.05500384 -0.07845377 -0.18393002  0.01625732 -0.24021797  0.8308934\n",
            " -0.11539437]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 100 reward=-1 new_state=[0 0 1 0 1 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.0935687  -0.25882348 -0.14444838 -0.01700359 -0.03483445 -0.07031455\n",
            "  0.08656643 -0.037692   -0.13199541 -0.10836844 -0.19024554 -0.0100957\n",
            " -0.05500384 -0.07809693 -0.18393002  0.01674605 -0.24021797  0.818999\n",
            " -0.11539437]\n",
            "Epsilon reduced to 0.03486784401000002\n",
            "Total reward: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZWtLJClFBRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_conv_27(env):\n",
        "    input_shape = env.observation_shape()\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(input_shape + (1, ), input_shape=input_shape))\n",
        "    model.add(Conv1D(16, kernel_size=3, activation=\"relu\"))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(24))\n",
        "    model.add(Dense(env.action_space.n, activation=\"linear\"))\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(lr=0.01))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLFh4f7gtUsc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c67ff1a-8529-4faa-d890-2b042e56965f"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=100, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=4, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_27(env), max_epsilon=0.1, epsilon_decay=0.9)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_8 (Reshape)          (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 187, 16)           64        \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 2992)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 24)                71832     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 19)                475       \n",
            "=================================================================\n",
            "Total params: 72,371\n",
            "Trainable params: 72,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |█████████████████████████---------------------------------------------------------------------------| 25.0% \r\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 1 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.03119485 -0.32680216  0.01924852  0.10472694 -0.04424024  0.03546907\n",
            " -0.23689467 -0.21605127  0.03278229  0.01301541 -0.2947041   0.05111575\n",
            "  0.10863831 -0.17608246  0.04758864 -0.18282829 -0.22277927  0.08445218\n",
            "  0.01295167]\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 2 reward=-2 new_state=[1 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.07308722 -1.3432403  -0.10568012 -0.19469321 -0.07786173  0.19242252\n",
            " -0.07237534 -0.23197392 -0.11519413  0.20751972 -0.19536051 -0.08988319\n",
            "  0.16380462 -0.06308479  0.10228557 -0.06169416 -0.30645165 -0.02879115\n",
            "  0.23663063]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 3 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.1441254  -3.3546293   0.12721181 -0.9193655  -0.02709285  0.26771557\n",
            " -0.30533108 -0.28140736 -0.11092875  0.83679646 -0.04451457 -0.10354179\n",
            " -0.89167595  0.52654237  0.6049042  -0.50515336 -1.3420221   0.3872807\n",
            "  0.2958966 ]\n",
            "\n",
            "Taking action 10 from 14\n",
            "\n",
            "Step 4 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.05206349 -3.1628397   0.06231661 -0.93760407  0.00625443  0.5266914\n",
            " -0.17279965 -0.2662801  -0.26997966  0.40407676 -0.08977074 -0.09456496\n",
            " -0.8277539   0.36136264  0.5198337  -0.35503072 -1.3284906   0.08470502\n",
            "  0.24345975]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 5 reward=-2 new_state=[1 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.11545703 -1.7483308   0.50688803 -0.74472994 -0.03749695  0.18651032\n",
            " -0.25374103 -0.2026619  -0.05757796  0.19381048  0.04108428 -0.23323981\n",
            " -0.9547159  -0.05746632 -0.5302019  -0.18410714 -0.55317664  0.26110145\n",
            "  0.42075837]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 6 reward=-2 new_state=[1 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.27121234 -1.135357    0.6232188  -0.65256286 -0.00294761 -0.26922977\n",
            " -0.05281156 -0.07200586 -0.21545124 -0.12828723 -0.07432026 -0.12611282\n",
            " -1.052494    0.09028778 -0.24916865 -0.29411742 -0.7333989   0.02648746\n",
            "  0.17602783]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 7 reward=-2 new_state=[1 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [ 0.37797877 -2.3468637   0.65141493 -1.4735024   0.15228121 -0.7681858\n",
            " -0.40878657 -0.2817449  -0.11213049  0.12857902 -0.21764176  0.03807388\n",
            " -1.6227237  -0.01532319 -0.52205825 -0.39437944 -1.2919827   0.36141384\n",
            "  0.34276858]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 8 reward=-1 new_state=[1 0 1 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.48596266 -2.3577437   0.29757908 -2.0085666   0.5029093  -1.0772264\n",
            " -0.38465995 -0.23912148 -0.08320639  0.1310047  -0.27238423  0.20509014\n",
            " -1.8391688  -0.2904777  -0.81493074 -0.29992506 -1.2130419   0.5556068\n",
            "  0.42044395]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 9 reward=0 new_state=[1 0 1 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.39422268 -3.0759797  -0.17337538 -1.4274597   0.37469664 -0.74892247\n",
            "  0.02215202 -0.2937773  -0.2543543  -0.46892768 -0.66015226  0.08837783\n",
            " -0.66699874 -0.24225733 -0.35373914 -0.20778885 -1.1632092   0.06061525\n",
            "  0.48886445]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 10 reward=0 new_state=[1 0 1 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.42843884 -1.4959857  -0.08538221 -1.2327129   0.43622488 -0.68991345\n",
            "  0.01791574 -0.16513045 -0.3063681  -0.13166922 -0.27626774  0.06723106\n",
            " -1.3859305   0.20597343 -0.0683433  -0.40616748 -1.1683391   0.06170453\n",
            "  0.22754328]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 11 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.4978144  -1.7949342   0.01655755 -1.31607     0.41365674 -0.93494105\n",
            " -0.10035492 -0.22261526 -0.12262133  0.11625354 -0.2539551   0.17393073\n",
            " -1.5306387   0.28362545 -0.09490332 -0.34434497 -1.2134067   0.06592349\n",
            "  0.22097507]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 12 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.71109366 -1.5055027   0.10898733 -1.7451833   0.64669377 -1.9544297\n",
            " -0.07689286 -0.03481789  0.03539091  0.15354872 -0.39259407  0.44834885\n",
            " -1.703679    0.25034034 -0.0606382  -0.33149034 -1.2193371   0.00944813\n",
            "  0.03334777]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 13 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.5710981  -1.2757314  -0.24625541 -1.4212649   0.82617426 -0.9979512\n",
            "  0.13096908 -0.13190922 -0.27718154 -0.10291871 -0.2646528   0.30200592\n",
            " -1.6084514   0.33102188 -0.03080883 -0.34313068 -1.1093091  -0.02389604\n",
            "  0.04545482]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 14 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.84492934 -1.2862452   0.3462855  -2.2087479   1.0554591  -1.6236519\n",
            " -0.27911752 -0.34942776 -0.13773203  0.34548712 -0.08337422  0.36862686\n",
            " -3.0994973   0.4945015  -0.65112543 -0.39327377 -1.5665892   0.03902331\n",
            " -0.03940586]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 15 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.7553483  -1.3079456  -0.41869524 -1.9840368   1.3610578  -1.4711498\n",
            "  0.11519012 -0.25734633 -0.25012234  0.11420365 -0.29931438  0.42681262\n",
            " -2.0334399   0.5666662  -0.02140606 -0.2762954  -1.3849095  -0.14566323\n",
            " -0.0993334 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 16 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.52558017 -0.90999067 -0.20544451 -1.8036402   1.4477859  -1.0597464\n",
            " -0.1597058  -0.3058196  -0.08712617  0.17903513 -0.12213139  0.29789916\n",
            " -2.0639575   0.4486209  -0.51867837 -0.11422729 -1.2032075   0.1278161\n",
            "  0.00445023]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 17 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.78806555 -0.6489257  -0.10704069 -2.506823    2.1648023  -1.7224956\n",
            " -0.14824793 -0.29008934 -0.04258116  0.10454597 -0.24855554  0.45908272\n",
            " -2.610728    0.41331953 -0.909808   -0.03033795 -1.4446257   0.12163735\n",
            " -0.07778338]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 18 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3454318  -3.3515613  -1.674087   -3.5151052   4.0537705  -2.3564034\n",
            "  0.8353938  -0.3724089  -0.28787065 -0.7515147  -1.2915312   0.9295771\n",
            " -1.9423023   0.6837215  -0.253714    0.20743261 -2.1971865  -0.5973685\n",
            " -0.09858168]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 19 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.3646035  -1.782095   -0.77079743 -4.1185784   4.7777777  -2.7280133\n",
            "  0.2036816  -0.41225123  0.14189282 -0.07898553 -0.7465093   0.989455\n",
            " -3.432294    0.7422883  -1.1740203   0.4101532  -2.2778816  -0.01221247\n",
            " -0.00558623]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 20 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.0764779  -2.0485966  -1.419053   -3.0817125   4.1938996  -2.0025508\n",
            "  0.5964082  -0.33239943  0.05956261 -0.17442329 -0.855265    0.95801204\n",
            " -2.2911768   0.8551131  -0.03658342  0.18335262 -2.0068994  -0.21174543\n",
            "  0.17288692]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 21 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.6573339e+00 -4.0314660e+00 -1.7646693e+00 -4.2226162e+00\n",
            "  5.1802177e+00 -4.0825586e+00  7.1935278e-01 -2.1795480e-01\n",
            "  2.4702662e-01  1.4236942e-03 -1.4490768e+00  1.2705952e+00\n",
            " -2.6291325e+00  1.4068015e+00  2.5535497e-01  2.4276416e-01\n",
            " -2.9075413e+00 -4.4310534e-01  5.8877939e-01]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 22 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4992706  -1.997567   -0.77287674 -4.2869215   5.1400194  -3.2947443\n",
            "  0.17434673 -0.612852    0.54793346  0.589954   -0.80674934  1.1169012\n",
            " -4.2904305   1.0895594  -0.96925104  0.31793416 -2.5630116  -0.07354275\n",
            "  1.0470757 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 23 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4451878  -3.22539    -1.9373972  -4.545178    6.3856196  -2.7616582\n",
            "  1.1130183  -0.54733175  0.6112602  -0.54698324 -1.5574573   1.0692579\n",
            " -3.3018625   0.8022564  -1.1710956   0.76119363 -2.5244575   0.09054027\n",
            "  2.341745  ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 24 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.4249157  -2.441529   -1.43404    -4.762973    6.161782   -3.284724\n",
            "  0.47568798 -0.5957136   0.692842    0.02918272 -1.3124139   0.9780637\n",
            " -3.703035    0.75439024 -1.6687555   0.8132041  -2.502222    0.2337029\n",
            "  2.1573145 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 25 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.0591736  -2.3619533  -2.14305    -3.1272051   4.4016194  -2.4918084\n",
            "  0.7793322  -0.50649107  0.29432777 -0.06536931 -1.3942206   0.8483997\n",
            " -1.5655245   0.7160951  -0.12973073  0.4663284  -1.8565428  -0.06807423\n",
            "  1.6648178 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 26 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 2.1548853  -4.200265   -2.2764883  -5.140383    6.6211348  -5.8704104\n",
            "  1.3595626  -0.01823024  0.8075881  -0.30561224 -2.3786361   1.6266422\n",
            " -2.6512384   1.261977    0.02230465  0.59037524 -3.047062   -0.43635428\n",
            "  2.3978908 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 27 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.252314   -3.8580642  -2.6410944  -4.5606556   6.2771263  -2.861573\n",
            "  1.2846098  -0.67916924  0.8959167  -0.7160819  -2.1506948   0.78995633\n",
            " -2.5801783   0.48707202 -1.5647691   0.9968609  -2.354957    0.26962677\n",
            "  4.0373683 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 28 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.98781884 -1.6735679  -1.1258637  -2.9497495   3.934291   -2.0676813\n",
            "  0.5409027  -0.4522405   0.47890562 -0.0089458  -0.8991511   0.590519\n",
            " -2.4688365   0.6764528  -0.96947217  0.47570866 -1.5313046   0.06062407\n",
            "  1.9669789 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 29 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 1.1695676  -1.5032096  -0.9331701  -3.659639    4.48314    -2.9360766\n",
            "  0.47913134 -0.5518594   0.8950573   0.1486019  -1.2168885   0.66675997\n",
            " -3.108512    0.35223982 -1.7668182   0.74721724 -1.6926566   0.21407261\n",
            "  2.8993213 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 30 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.99628997 -3.134966   -2.1931078  -3.2188683   4.37231    -2.154497\n",
            "  1.1439924  -0.5831158   0.6880886  -0.4581153  -1.7399274   0.50562096\n",
            " -2.2391648   0.646143   -0.7511536   0.54744935 -1.9494547   0.03116134\n",
            "  3.5747097 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 31 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.6456722  -2.6327872  -1.7707369  -2.1389208   3.0693016  -1.3991808\n",
            "  0.9082998  -0.46492502  0.5269688  -0.5404362  -1.4516153   0.18075189\n",
            " -1.3749459   0.34862697 -0.8390926   0.48102826 -1.2841152   0.07469012\n",
            "  3.0100555 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 32 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.6831662  -0.74243104 -0.1840903  -1.3182225   2.3340364  -1.5433497\n",
            "  0.31311545 -0.29003343  0.69132483  0.16164467 -0.7187871   0.15170401\n",
            " -2.2061076   0.28677145 -1.1589824   0.42264065 -0.8877096   0.0652527\n",
            "  2.176244  ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 33 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.7935061  -2.4432023  -0.9019415  -1.3690057   2.381664   -1.8064885\n",
            "  0.700477   -0.36545044  0.5910721  -0.16010532 -1.1964447   0.12118532\n",
            " -1.9008306   0.5191127  -0.7794101   0.4214473  -1.2682794  -0.28047138\n",
            "  2.7165792 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 34 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.40546262 -1.6005036  -1.0672038  -0.8354528   1.686094   -0.82988125\n",
            "  0.78235805 -0.1871247   0.36526823 -0.49487486 -1.0439532  -0.04520731\n",
            " -0.8713267   0.21057792 -0.4701407   0.31875932 -0.69808674 -0.04656103\n",
            "  2.2108724 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 35 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.29444057 -0.788863   -0.7073871  -0.39777583  1.1825354  -0.7703372\n",
            "  0.4046102  -0.10443643  0.22545323 -0.19703263 -0.6596898   0.0779766\n",
            " -0.3562105   0.18597104 -0.12312778  0.24464847 -0.45700958 -0.06937759\n",
            "  1.1210808 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 36 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.698502   -2.2619462  -0.79214376 -0.59095865  1.6099833  -2.0567632\n",
            "  0.6053752   0.00319098  0.4654462  -0.40205565 -1.2740445   0.17757961\n",
            " -0.69909805  0.20928317 -0.4011981   0.34535682 -0.86224556 -0.3250162\n",
            "  1.9067081 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 37 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.44155115 -1.3438337  -0.48839542 -0.06708936  1.056433   -0.7476297\n",
            "  0.70503587 -0.14204043  0.29217744 -0.38145897 -0.821981   -0.2311205\n",
            " -1.062965    0.31638002 -0.41689897  0.18643723 -0.5618692  -0.28827947\n",
            "  1.9040262 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 38 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.2589034  -0.95220804 -0.4611262   0.06604411  0.6550112  -0.5713668\n",
            "  0.44876617 -0.06117291  0.19707078 -0.24758479 -0.6065954  -0.09934969\n",
            " -0.42458993  0.14661574 -0.19388004  0.19798301 -0.28681985 -0.15789843\n",
            "  1.2840463 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 39 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.28040424 -0.38036567  0.09223489  0.37185514  0.6326848  -0.56199664\n",
            "  0.18803617  0.01845375  0.3289866   0.08994068 -0.37081057 -0.16906707\n",
            " -1.0302176   0.233334   -0.33223015  0.12975758 -0.30455643 -0.15626529\n",
            "  0.9124471 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 40 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.39075574 -0.90644825 -0.47607848 -0.12323809  0.9601475  -0.6883189\n",
            "  0.4805422  -0.05871296  0.21778253 -0.14040223 -0.571721    0.03289165\n",
            " -0.7409904   0.30936104 -0.06851427  0.16388993 -0.47724348 -0.3045562\n",
            "  1.0149999 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 41 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.16105053 -0.7142029  -0.39106756  0.1547518   0.3856115  -0.38444224\n",
            "  0.32570943 -0.01514702  0.09786163 -0.27162623 -0.4725909  -0.0927076\n",
            " -0.13283879  0.06484538 -0.11016859  0.12919886 -0.16275962 -0.16728602\n",
            "  0.7705407 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 42 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.21815337 -0.29872316 -0.07282668  0.29196036  0.48863554 -0.47231364\n",
            "  0.15273207  0.04418859  0.1471813  -0.01695304 -0.30605724 -0.07028663\n",
            " -0.50303555  0.1811623  -0.11237688  0.08197762 -0.21245015 -0.180978\n",
            "  0.41472033]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 43 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.1786226  -0.8476633  -0.34570116  0.26988885  0.40005437 -0.42230946\n",
            "  0.30576697  0.01468261  0.10431965 -0.23604256 -0.4597628  -0.10497344\n",
            " -0.15871581  0.13359338 -0.07100129  0.15396048 -0.21537808 -0.23903424\n",
            "  0.6701071 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 44 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.15035316 -0.49374348 -0.3331385   0.09507185  0.46851307 -0.25393096\n",
            "  0.20153114  0.00265838  0.03629818 -0.07452933 -0.24344504 -0.03394565\n",
            " -0.36995187  0.2430605   0.03339111  0.06631475 -0.2519551  -0.17663261\n",
            "  0.32387248]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 45 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.13186495 -0.24441929 -0.04679154  0.39575937  0.23307317 -0.26770207\n",
            "  0.15149035  0.06156036  0.08398875 -0.08760545 -0.25123912 -0.14228967\n",
            " -0.30091882  0.12876898 -0.07554417  0.06278272 -0.09238951 -0.18644935\n",
            "  0.28770176]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 46 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.15385331 -0.45441017 -0.2883021   0.08775993  0.4750417  -0.23578647\n",
            "  0.20951177  0.02598136  0.01954244 -0.13102092 -0.21428718  0.01087373\n",
            " -0.28369635  0.22290464  0.06375888  0.0530773  -0.22898495 -0.19990526\n",
            "  0.17966914]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 47 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.12479931 -0.44596052 -0.23200892  0.22368467  0.3492022  -0.19026986\n",
            "  0.18858594  0.01819574  0.02337267 -0.11304387 -0.21310247 -0.09132704\n",
            " -0.29959577  0.21213762 -0.00355154  0.05202042 -0.19399396 -0.1914398\n",
            "  0.27583066]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 48 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.18999042 -0.60379905 -0.26294315  0.04264408  0.34600207 -0.5708531\n",
            "  0.16363329  0.05448497  0.05077644 -0.10834578 -0.29839227  0.01450913\n",
            " -0.20116158  0.20110688  0.06716242  0.03179547 -0.23414361 -0.19485638\n",
            "  0.2358828 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 49 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.09887946 -0.3767207  -0.26990813  0.16308892  0.3648766  -0.23096816\n",
            "  0.149668    0.02173152  0.03988133 -0.1097693  -0.21745104 -0.01671997\n",
            " -0.12904862  0.13663627  0.00999888  0.07604524 -0.15360838 -0.13151488\n",
            "  0.2186514 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 50 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.07280663 -0.2305809  -0.00055553  0.4313868   0.08069716 -0.08540173\n",
            "  0.13062418  0.04487149  0.06934533 -0.04457408 -0.12953319 -0.13801304\n",
            " -0.26680002  0.13795474 -0.02546718  0.04963256 -0.02577433 -0.16605651\n",
            "  0.23771843]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 51 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.15147142 -0.46016082 -0.19570404  0.09250998  0.26530296 -0.40962234\n",
            "  0.12331147  0.05152025  0.01939279 -0.09664712 -0.20688094 -0.01063238\n",
            " -0.19207376  0.18332104  0.04533095  0.01201634 -0.17343545 -0.19175327\n",
            "  0.10470068]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 52 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.06078403 -0.29537636 -0.21254861  0.25539166  0.24085644 -0.13596651\n",
            "  0.14441189  0.0251855   0.02490724 -0.12218537 -0.18413623 -0.0483408\n",
            " -0.03595455  0.09897453  0.00845918  0.08106925 -0.06640259 -0.12879838\n",
            "  0.18261251]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 53 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.06162099 -0.2197975  -0.09206553  0.30709204  0.20480973 -0.09602286\n",
            "  0.07345223  0.04283239  0.04687915 -0.02843631 -0.11223894 -0.08162426\n",
            " -0.230261    0.15144624 -0.0155659   0.04934885 -0.0855092  -0.13219632\n",
            "  0.10484398]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 54 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.05764402 -0.19363642 -0.1144506   0.25318632  0.20690691 -0.07481144\n",
            "  0.07945572  0.02520847  0.02704924 -0.02370724 -0.08847636 -0.07177682\n",
            " -0.25456637  0.16228676 -0.00032172  0.03174235 -0.09523054 -0.12576899\n",
            "  0.09641388]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 55 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.05811309 -0.23213387 -0.21210416  0.16572583  0.22273616 -0.13639031\n",
            "  0.11139067  0.02329175  0.0011903  -0.09856293 -0.14410852 -0.01516365\n",
            " -0.0418856   0.09973861  0.03215064  0.0576134  -0.06957291 -0.1263968\n",
            "  0.07800734]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 56 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.04174168 -0.2114127  -0.0820702   0.35930967  0.15527025 -0.06794148\n",
            "  0.08371847  0.0485818   0.04678264 -0.04965157 -0.11583061 -0.09319768\n",
            " -0.14165835  0.11888073 -0.0102133   0.06291804 -0.04612403 -0.1319389\n",
            "  0.11106981]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 57 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.05894831 -0.2676083  -0.1711286   0.20960478  0.20828459 -0.08642881\n",
            "  0.09109817  0.01316219 -0.00370656 -0.07659107 -0.11208393 -0.04287736\n",
            " -0.11343722  0.13939372  0.02610381  0.04465385 -0.09142068 -0.14979579\n",
            "  0.043248  ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 58 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.05577896 -0.26690057 -0.241701    0.12159485  0.24736843 -0.14596853\n",
            "  0.09213094  0.00449115  0.00968203 -0.08186924 -0.14018726 -0.0076002\n",
            " -0.06291209  0.10674849  0.01070701  0.05150869 -0.08788764 -0.10801695\n",
            "  0.07982651]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 59 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.04278143 -0.21205103 -0.1290552   0.28730792  0.19348088 -0.08451945\n",
            "  0.07346436  0.02659403  0.04118949 -0.04931381 -0.11789019 -0.06043265\n",
            " -0.12643903  0.10977904 -0.01551329  0.05909482 -0.06451096 -0.11107451\n",
            "  0.10858069]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 60 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.03961074 -0.23009771 -0.13990413  0.23520482  0.13932207 -0.09612805\n",
            "  0.07923734  0.0191226   0.01542904 -0.06390616 -0.11054557 -0.05242872\n",
            " -0.08591877  0.10431646  0.0070293   0.04615474 -0.04826399 -0.12638497\n",
            "  0.07498552]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 61 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.05075295 -0.2502196  -0.20491028  0.13766554  0.20481709 -0.10799621\n",
            "  0.09175255  0.00564077  0.00122432 -0.08377653 -0.11565318 -0.01828424\n",
            " -0.07582828  0.10761197  0.00984392  0.0472633  -0.07325735 -0.12313938\n",
            "  0.05713554]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 62 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.03938575 -0.2008027  -0.16362685  0.19490479  0.18206187 -0.12600979\n",
            "  0.07849779  0.02885413  0.0139549  -0.05380481 -0.10564842 -0.03685196\n",
            " -0.08683403  0.11795227  0.02173379  0.04051759 -0.06466683 -0.10628856\n",
            "  0.06984173]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 63 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.05232088 -0.27472252 -0.22198454  0.14887619  0.22425753 -0.12692314\n",
            "  0.09893259  0.01303577  0.00583438 -0.09060034 -0.13675164 -0.01775334\n",
            " -0.05240078  0.10346229  0.01197355  0.06226248 -0.08088981 -0.1260375\n",
            "  0.0734067 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 64 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 4.98764664e-02 -2.24065959e-01 -2.21683532e-01  1.21773459e-01\n",
            "  2.31947675e-01 -1.05424255e-01  9.35081318e-02  1.20472061e-02\n",
            "  5.27719967e-05 -7.62966722e-02 -1.11278698e-01 -5.26753627e-03\n",
            " -6.02664687e-02  1.10582560e-01  2.55454704e-02  5.15292436e-02\n",
            " -7.43238926e-02 -1.18207820e-01  3.78032029e-02]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 65 reward=0 new_state=[1 1 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [ 0.03452595 -0.17365317 -0.15967315  0.1996007   0.18389189 -0.08719297\n",
            "  0.05996778  0.01842042  0.01166583 -0.05542202 -0.10027321 -0.0373013\n",
            " -0.06440291  0.09917304  0.0030238   0.04986672 -0.05612502 -0.10587072\n",
            "  0.03428675]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 12\n",
            "\n",
            "Step 66 reward=-1 new_state=[1 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03977694 -0.2519635  -0.22452708  0.13337576  0.22538193 -0.1199903\n",
            "  0.08488709  0.00880724  0.0062345  -0.08098958 -0.11976231 -0.0072349\n",
            " -0.04424061  0.09947602  0.00394506  0.05899048 -0.07408515 -0.10305342\n",
            "  0.07038721]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 67 reward=-1 new_state=[1 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.04048138 -0.19050226 -0.1655865   0.17602399  0.19318593 -0.1015562\n",
            "  0.08074237  0.02059893  0.0111319  -0.06084024 -0.09810243 -0.02490007\n",
            " -0.09699546  0.11050626  0.01016345  0.04367445 -0.06187656 -0.10781182\n",
            "  0.05634515]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 14\n",
            "\n",
            "Step 68 reward=-1 new_state=[1 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03110314 -0.18127601 -0.15377265  0.19418773  0.17484942 -0.08702896\n",
            "  0.05904667  0.01591342  0.01351074 -0.05027742 -0.09076655 -0.03410346\n",
            " -0.08762848  0.10569163  0.00234287  0.04331017 -0.05701853 -0.09958538\n",
            "  0.04219613]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 69 reward=-1 new_state=[1 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03891869 -0.2213178  -0.20875563  0.12184466  0.21768606 -0.10942657\n",
            "  0.07655664  0.00568347  0.006784   -0.07419513 -0.10589603 -0.00248127\n",
            " -0.06605395  0.09866686 -0.00022535  0.04945645 -0.07437939 -0.0983703\n",
            "  0.05219692]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 70 reward=-1 new_state=[1 1 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02904545 -0.21533138 -0.19035572  0.11764154  0.18378052 -0.09487645\n",
            "  0.05170451  0.01100998 -0.00525675 -0.04847714 -0.07623753 -0.02454196\n",
            " -0.11863987  0.12409696 -0.0055741   0.02313878 -0.08409741 -0.09603174\n",
            "  0.02429699]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 7\n",
            "\n",
            "Step 71 reward=-2 new_state=[1 1 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02806146 -0.20922516 -0.1734429   0.13725762  0.16065167 -0.07994319\n",
            "  0.05546512  0.00095248  0.00159869 -0.059142   -0.08058819 -0.02399606\n",
            " -0.10445379  0.10112172 -0.02732191  0.03349465 -0.06414916 -0.09979622\n",
            "  0.04238527]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 72 reward=-2 new_state=[1 1 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03165602 -0.20297849 -0.19059323  0.11920446  0.16719596 -0.09360753\n",
            "  0.06453937 -0.02143401  0.00382771 -0.0726796  -0.09447043 -0.00873928\n",
            " -0.06771588  0.08001856 -0.03385855  0.0438775  -0.04911099 -0.09650017\n",
            "  0.04928669]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 73 reward=-2 new_state=[1 1 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02984075 -0.19883376 -0.19048166  0.11245927  0.15752643 -0.08938462\n",
            "  0.05732873 -0.04207223  0.00224695 -0.06807727 -0.08999749 -0.00647202\n",
            " -0.07165877  0.07775553 -0.04205353  0.04323434 -0.04777259 -0.09387095\n",
            "  0.04439985]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 74 reward=-2 new_state=[1 1 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03347381 -0.20344082 -0.18570718  0.10342778  0.13704956 -0.08257443\n",
            "  0.05493191 -0.06307988 -0.00298069 -0.06790704 -0.0869431  -0.01086676\n",
            " -0.08666307  0.08175585 -0.04825664  0.03422963 -0.05264922 -0.10381854\n",
            "  0.03649217]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 75 reward=-2 new_state=[1 1 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.02466798 -0.21056393 -0.1785914   0.12450482  0.08385079 -0.06960779\n",
            "  0.04876812 -0.07746207 -0.00421609 -0.07078996 -0.09098603 -0.02833115\n",
            " -0.07096916  0.06849894 -0.05896004  0.02896834 -0.03962381 -0.10655792\n",
            "  0.04043459]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 76 reward=-1 new_state=[0 1 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.03232769 -0.21510074 -0.19317606  0.09011923  0.08943041 -0.09012106\n",
            "  0.03905224 -0.10252136 -0.00464381 -0.05893561 -0.09199142 -0.01197585\n",
            " -0.07919058  0.06769834 -0.0642067   0.02985553 -0.04487072 -0.10164735\n",
            "  0.03781204]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 77 reward=-1 new_state=[0 1 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [ 0.00736115 -0.20820622 -0.17848155  0.09337402  0.05608976 -0.07123514\n",
            "  0.02970582 -0.1118286  -0.00392101 -0.05366056 -0.08176944 -0.02136453\n",
            " -0.08995925  0.06612431 -0.07179132  0.0243547  -0.04152124 -0.09882316\n",
            "  0.03752001]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 78 reward=-1 new_state=[0 1 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.01171187 -0.21219979 -0.17985426  0.08529146  0.03008005 -0.06620659\n",
            "  0.02554816 -0.12367764 -0.00616657 -0.05637113 -0.08134165 -0.02515127\n",
            " -0.08374344  0.05681105 -0.08457234  0.02247023 -0.0327421  -0.09871536\n",
            "  0.03863568]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 79 reward=-1 new_state=[0 1 0 1 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.02318477 -0.19590774 -0.18004528  0.06836127  0.02725586 -0.06053028\n",
            "  0.01914283 -0.13507622 -0.00967386 -0.05265494 -0.07490338 -0.02143953\n",
            " -0.08531593  0.05677684 -0.08717591  0.02045131 -0.03378987 -0.09602767\n",
            "  0.02748214]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 80 reward=0 new_state=[0 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.03165124 -0.20863879 -0.19458744  0.03808535  0.02891564 -0.0815512\n",
            "  0.01459282 -0.14777613 -0.00689352 -0.04660566 -0.08094667 -0.01385995\n",
            " -0.08690479  0.05521224 -0.08950847  0.02163367 -0.03884004 -0.09447908\n",
            "  0.02771022]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 81 reward=0 new_state=[0 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.05283652 -0.21553326 -0.18640107  0.0445726  -0.00262151 -0.0624735\n",
            "  0.00713933 -0.15559304 -0.00451703 -0.04912736 -0.08006398 -0.02603252\n",
            " -0.08262519  0.04270122 -0.10844034  0.02555489 -0.0335702  -0.09173879\n",
            "  0.03818532]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 82 reward=0 new_state=[0 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.06165002 -0.20874357 -0.18903875  0.02878855 -0.00469449 -0.0675108\n",
            "  0.00419905 -0.16310993 -0.00266121 -0.04258556 -0.07780787 -0.0205511\n",
            " -0.09199096  0.04307498 -0.10627706  0.02194401 -0.0359284  -0.08575837\n",
            "  0.04397213]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 83 reward=0 new_state=[0 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.0673756  -0.21581125 -0.19809408  0.00497482 -0.0113588  -0.06978405\n",
            "  0.00065183 -0.17246369 -0.01160343 -0.04728379 -0.07846458 -0.01886902\n",
            " -0.0875965   0.04083857 -0.1100373   0.01828986 -0.03987464 -0.09165417\n",
            "  0.03340369]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 84 reward=0 new_state=[0 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.08324119 -0.19733891 -0.19490984  0.00345888 -0.01693742 -0.05665633\n",
            " -0.00655275 -0.17364618 -0.00618153 -0.03691109 -0.07111868 -0.0240067\n",
            " -0.10389335  0.04220854 -0.11504208  0.01601921 -0.03960532 -0.08018419\n",
            "  0.03593004]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 85 reward=-1 new_state=[1 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.07842181 -0.19664541 -0.18564066 -0.01187207 -0.02869848 -0.06724476\n",
            "  0.00132757 -0.17611647 -0.01221561 -0.04819355 -0.07217427 -0.01848573\n",
            " -0.10142971  0.03683964 -0.11776531  0.01315116 -0.03250227 -0.09395634\n",
            "  0.02764315]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 86 reward=-1 new_state=[1 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.08858551 -0.21712694 -0.19451204 -0.02726876 -0.04127941 -0.07937101\n",
            " -0.01290621 -0.18650022 -0.01576846 -0.04656673 -0.0754095  -0.01814464\n",
            " -0.08736786  0.02701372 -0.12957469  0.01270606 -0.03553798 -0.08789052\n",
            "  0.03244882]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 87 reward=-1 new_state=[1 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.09628322 -0.21339095 -0.1823182  -0.04456868 -0.05610913 -0.0727827\n",
            " -0.02485889 -0.1831955  -0.0134656  -0.04059679 -0.0643857  -0.0154464\n",
            " -0.09725721  0.01297349 -0.13644654  0.01081368 -0.03180852 -0.08625974\n",
            "  0.01912072]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 88 reward=-1 new_state=[1 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10396892 -0.22988406 -0.18522519 -0.03384056 -0.05625458 -0.06712203\n",
            " -0.02016112 -0.19399153 -0.0075746  -0.04079462 -0.06980862 -0.01878669\n",
            " -0.09774731  0.0050624  -0.14172216  0.01583194 -0.03155342 -0.08601809\n",
            "  0.02730402]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 89 reward=-1 new_state=[1 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.11081509 -0.24668047 -0.18572064 -0.05751161 -0.07874066 -0.07668042\n",
            " -0.03348126 -0.19757485 -0.0110958  -0.04049418 -0.06920198 -0.01722552\n",
            " -0.10283761 -0.01394341 -0.1529341   0.00751166 -0.03288814 -0.0872528\n",
            "  0.02233644]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 90 reward=-1 new_state=[1 1 0 1 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.11650482 -0.25550672 -0.18431866 -0.06383634 -0.07513951 -0.06158634\n",
            " -0.03841646 -0.19852775 -0.01743491 -0.04549634 -0.05885655 -0.01237462\n",
            " -0.10796422 -0.01784417 -0.14970568  0.00653408 -0.03953669 -0.08846326\n",
            "  0.00177932]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 91 reward=0 new_state=[1 1 0 1 0 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.1149969  -0.24375084 -0.1736303  -0.04593084 -0.08816443 -0.05453552\n",
            " -0.03338403 -0.19661528 -0.02339133 -0.05009668 -0.05730566 -0.01841333\n",
            " -0.09937461 -0.02549639 -0.14850444  0.00681856 -0.02527607 -0.09577125\n",
            " -0.01113791]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 92 reward=-1 new_state=[1 1 0 1 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.0911369  -0.32015538 -0.21748051 -0.11019062 -0.01772687 -0.1262455\n",
            " -0.02736607 -0.22486097 -0.02698193 -0.05170205 -0.08222315  0.01506566\n",
            " -0.09877895 -0.00344993 -0.13602316  0.02760373 -0.06738314 -0.10486123\n",
            " -0.03082021]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 93 reward=-1 new_state=[1 1 0 1 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.12102121 -0.23234385 -0.14925984 -0.03923036 -0.10954151 -0.04353611\n",
            " -0.05218957 -0.1937716  -0.02847829 -0.03750721 -0.04471495 -0.04725955\n",
            " -0.11669826 -0.0313386  -0.1544427   0.02209168 -0.0209316  -0.09717093\n",
            " -0.05235351]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 94 reward=-1 new_state=[1 1 0 1 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.11872743 -0.34270492 -0.21163468 -0.09470904 -0.07002722 -0.04896937\n",
            " -0.04903059 -0.23668382 -0.03663986 -0.06045434 -0.06294321 -0.03679926\n",
            " -0.10980618 -0.03649488 -0.16216071 -0.00085288 -0.05321392 -0.10697794\n",
            " -0.05817509]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 95 reward=-1 new_state=[1 1 0 1 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.1127864  -0.27889013 -0.20554711 -0.0994233  -0.07283176 -0.08115949\n",
            " -0.0509236  -0.22148022 -0.04255706 -0.03026504 -0.06099238 -0.06695344\n",
            " -0.12307636 -0.01479661 -0.13460922 -0.03081874 -0.06084878 -0.10286647\n",
            " -0.06269392]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 96 reward=-1 new_state=[1 1 0 1 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.11728428 -0.30672702 -0.19533584 -0.0952514  -0.0844641  -0.08486599\n",
            " -0.06063812 -0.20657605 -0.04967449 -0.05846272 -0.061727   -0.0794294\n",
            " -0.10332657 -0.035334   -0.1431949  -0.07448523 -0.06281684 -0.10131542\n",
            " -0.07996145]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 97 reward=-1 new_state=[1 1 0 1 1 1 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.12760605 -0.3163887  -0.22887644 -0.11075196 -0.07554877 -0.07441989\n",
            " -0.07158227 -0.2330229  -0.05103808 -0.05709401 -0.06948627 -0.09394248\n",
            " -0.10883815 -0.0644374  -0.18013227 -0.10746922 -0.04900714 -0.08134209\n",
            " -0.07225649]\n",
            "\n",
            "Taking action 12 from 16\n",
            "\n",
            "Step 98 reward=-2 new_state=[1 1 0 1 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.11996342 -0.2879112  -0.18960625 -0.10011718 -0.10909332 -0.06809594\n",
            " -0.07869854 -0.20517524 -0.054938   -0.05165215 -0.0551741  -0.11420029\n",
            " -0.12620357 -0.05699684 -0.13808747 -0.15496533 -0.0691364  -0.09324608\n",
            " -0.09690804]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 99 reward=-2 new_state=[1 1 0 1 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.12769315 -0.3275478  -0.19130963 -0.0838218  -0.12326648 -0.02905415\n",
            " -0.08855569 -0.22886708 -0.04901999 -0.0613611  -0.05305632 -0.11874834\n",
            " -0.1231942  -0.08590357 -0.14443548 -0.19139752 -0.08379376 -0.09941465\n",
            " -0.10671371]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 100 reward=-3 new_state=[1 1 1 1 1 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.11614904 -0.30853513 -0.17077926 -0.11111423 -0.11560307 -0.07672893\n",
            " -0.08520685 -0.20030531 -0.05354702 -0.06601484 -0.04920326 -0.12827723\n",
            " -0.16024762 -0.07020503 -0.12188937 -0.21142071 -0.12551776 -0.11097683\n",
            " -0.12305751]\n",
            "Epsilon reduced to 0.09000000000000001\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.12751843 -0.31410098 -0.19206014 -0.09487699 -0.08498362 -0.08216216\n",
            " -0.06896679 -0.20804685 -0.04994401 -0.09708603 -0.06964768 -0.14537226\n",
            " -0.13158938 -0.0974661  -0.14994156 -0.20282154 -0.1406747  -0.09280415\n",
            " -0.07852377]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.13742873 -0.3966049  -0.21003906 -0.12030365 -0.08664617 -0.09955671\n",
            " -0.07557574 -0.2257474  -0.07761934 -0.13346449 -0.0786829  -0.15313374\n",
            " -0.16029944 -0.12445964 -0.1692825  -0.25684735 -0.1820256  -0.11269757\n",
            " -0.08371984]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.13921191 -0.44386202 -0.22796813 -0.13048431 -0.09328744 -0.14334285\n",
            " -0.10229518 -0.22456595 -0.11774582 -0.18075413 -0.10373154 -0.17268485\n",
            " -0.1068804  -0.1597903  -0.1928768  -0.28730014 -0.20672719 -0.11653563\n",
            " -0.08369957]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1087625  -0.34566104 -0.21509074 -0.13821663 -0.07364221 -0.15964809\n",
            " -0.12477478 -0.20047949 -0.17236623 -0.20204279 -0.12564328 -0.17304373\n",
            " -0.1251672  -0.13831192 -0.14189693 -0.316989   -0.25437406 -0.11291985\n",
            " -0.11652328]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1383664  -0.34251973 -0.24299945 -0.15784316 -0.06488156 -0.16343462\n",
            " -0.17260091 -0.23546216 -0.16114078 -0.14709085 -0.1315495  -0.18286946\n",
            " -0.1323351  -0.1406727  -0.16040236 -0.29511017 -0.26073635 -0.08386427\n",
            " -0.09955295]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.13143815 -0.32091317 -0.21490839 -0.14468244 -0.07601732 -0.1698936\n",
            " -0.18468755 -0.22376794 -0.1711497  -0.15782464 -0.1508462  -0.19654892\n",
            " -0.15811762 -0.14455613 -0.16839841 -0.30893692 -0.27213106 -0.09425398\n",
            " -0.10437515]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.12469666 -0.3700479  -0.2024362  -0.14672005 -0.10217609 -0.19512303\n",
            " -0.21241371 -0.21603906 -0.20149018 -0.18942167 -0.16862355 -0.18875949\n",
            " -0.14315127 -0.15748659 -0.15281984 -0.34233773 -0.28684556 -0.11351107\n",
            " -0.13750672]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.13469362 -0.3969122  -0.20240508 -0.14452243 -0.12828904 -0.19849709\n",
            " -0.25663978 -0.24170011 -0.21014087 -0.19962862 -0.19561899 -0.2053447\n",
            " -0.16287637 -0.1882779  -0.1710252  -0.38491824 -0.31076503 -0.11206998\n",
            " -0.12453775]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 9 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.13023914 -0.33958375 -0.19934535 -0.11022823 -0.13842267 -0.18304697\n",
            " -0.20731553 -0.19812936 -0.22649945 -0.2289617  -0.2117796  -0.20973897\n",
            " -0.08367779 -0.20080405 -0.15194012 -0.33492804 -0.2705246  -0.12302839\n",
            " -0.14287661]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 10 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.12587845 -0.36140627 -0.20155373 -0.12391528 -0.14700922 -0.19968836\n",
            " -0.22425452 -0.19476113 -0.24669585 -0.2319562  -0.23069865 -0.21223566\n",
            " -0.08912551 -0.20362963 -0.14637679 -0.36065817 -0.2898568  -0.13555004\n",
            " -0.16434306]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 11 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.14291418 -0.3153197  -0.20793507 -0.10379487 -0.16706222 -0.17592293\n",
            " -0.24125832 -0.2047353  -0.24402902 -0.21224684 -0.23518637 -0.22869049\n",
            " -0.06203975 -0.22869277 -0.16472521 -0.3467838  -0.27488184 -0.10850321\n",
            " -0.13794734]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 12 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.15603954 -0.31342292 -0.2178593  -0.12906653 -0.18204677 -0.19242378\n",
            " -0.31584972 -0.22490469 -0.25608194 -0.17238906 -0.23270258 -0.22959542\n",
            " -0.05702273 -0.24069078 -0.1733875  -0.36501247 -0.29891962 -0.083479\n",
            " -0.15479346]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 13 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.13938466 -0.3161181  -0.20576932 -0.11281982 -0.18936808 -0.18942064\n",
            " -0.27052748 -0.19865948 -0.27299628 -0.2107459  -0.25704178 -0.23581202\n",
            " -0.01239503 -0.24266696 -0.16335814 -0.36284453 -0.28759176 -0.11722797\n",
            " -0.16853395]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 14 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.15443501 -0.35068032 -0.20242445 -0.11613552 -0.21149671 -0.2175222\n",
            " -0.33429244 -0.21326135 -0.2738087  -0.1807549  -0.2630903  -0.24094266\n",
            " -0.00564453 -0.25256094 -0.16690254 -0.38831824 -0.30982476 -0.10173976\n",
            " -0.16639237]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 15 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.14307651 -0.33804786 -0.2026982  -0.09326915 -0.21060102 -0.20946048\n",
            " -0.27874687 -0.19618633 -0.2845859  -0.21603985 -0.2870217  -0.25004897\n",
            "  0.05262182 -0.25902554 -0.1599411  -0.3734703  -0.2914185  -0.12470499\n",
            " -0.16174906]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 16 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.14937593 -0.3319607  -0.22172073 -0.0929948  -0.22753534 -0.20489419\n",
            " -0.2962017  -0.1956537  -0.30439466 -0.2177329  -0.30441386 -0.26378408\n",
            "  0.12675099 -0.27584422 -0.16081926 -0.37318856 -0.28880328 -0.12479077\n",
            " -0.18674561]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 17 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.14733088 -0.3508678  -0.20856632 -0.09439009 -0.22857693 -0.22026634\n",
            " -0.29848492 -0.18890761 -0.29832083 -0.2225911  -0.31526953 -0.26213956\n",
            "  0.14285135 -0.27845478 -0.15897004 -0.3828817  -0.2967323  -0.13099022\n",
            " -0.17569514]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 18 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.15068294 -0.37320462 -0.21913643 -0.08752941 -0.2453402  -0.22707893\n",
            " -0.3103425  -0.18050563 -0.32222053 -0.2394273  -0.33734092 -0.27420348\n",
            "  0.22125424 -0.2905177  -0.15419406 -0.3900407  -0.29969692 -0.12285945\n",
            " -0.20088756]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 19 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.16380934 -0.32762292 -0.22919837 -0.08147031 -0.25550777 -0.20461401\n",
            " -0.31850943 -0.18531933 -0.3133067  -0.21919599 -0.33240503 -0.28232008\n",
            "  0.2624553  -0.3056296  -0.1620906  -0.3702302  -0.28457555 -0.08707207\n",
            " -0.189227  ]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 20 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.17182496 -0.34340417 -0.2509553  -0.07690803 -0.26557583 -0.21071011\n",
            " -0.32874304 -0.18479571 -0.33147705 -0.23229982 -0.355073   -0.2942861\n",
            "  0.34702227 -0.32218897 -0.16199884 -0.37437746 -0.28583527 -0.07057719\n",
            " -0.20177276]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.18150944 -0.32954818 -0.2551391  -0.07784683 -0.26960212 -0.21869972\n",
            " -0.3452862  -0.19449757 -0.3161627  -0.20858213 -0.3546     -0.29644942\n",
            "  0.37519538 -0.3345169  -0.1664221  -0.36674833 -0.28467536 -0.04380136\n",
            " -0.17869842]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.18609875 -0.3467856  -0.270378   -0.05418699 -0.2992666  -0.21078631\n",
            " -0.33903775 -0.1738565  -0.35039648 -0.2520898  -0.38873607 -0.3181795\n",
            "  0.51185274 -0.356569   -0.15937929 -0.36720788 -0.27343276 -0.04316301\n",
            " -0.21851057]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.20591137 -0.3573819  -0.2959527  -0.07935862 -0.28660426 -0.21594785\n",
            " -0.3835536  -0.21010864 -0.33778656 -0.20726305 -0.38463566 -0.32239467\n",
            "  0.51063824 -0.36442587 -0.17852879 -0.37579638 -0.29189098 -0.00863416\n",
            " -0.18326974]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.20741147 -0.38270694 -0.3241591  -0.04277904 -0.32146367 -0.21733554\n",
            " -0.35873365 -0.16995677 -0.38686857 -0.29122692 -0.45018604 -0.35001087\n",
            "  0.7429594  -0.4071307  -0.16311714 -0.36580303 -0.27309823 -0.01373561\n",
            " -0.24427766]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.19269903 -0.49977097 -0.32897568 -0.05033957 -0.32757    -0.2774312\n",
            " -0.3773803  -0.18979791 -0.40907288 -0.33040178 -0.51539326 -0.36245993\n",
            "  0.81333816 -0.4281926  -0.1607233  -0.4396566  -0.3171365  -0.03550733\n",
            " -0.23562896]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.24214968 -0.44878986 -0.39205533 -0.0079358  -0.37399858 -0.21406877\n",
            " -0.37395477 -0.16259062 -0.44798988 -0.3578199  -0.5394652  -0.4077877\n",
            "  1.0676703  -0.48150295 -0.17275965 -0.37109917 -0.26967946  0.00798848\n",
            " -0.27821308]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.2939543  -0.43540144 -0.41187838 -0.02924378 -0.3907219  -0.23025176\n",
            " -0.5134824  -0.2691427  -0.37785614 -0.1834979  -0.4917776  -0.41842237\n",
            "  0.9568971  -0.4944923  -0.21349987 -0.38958058 -0.29012963  0.07978927\n",
            " -0.18550763]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.24944335 -0.55698436 -0.49539724 -0.01021611 -0.3809707  -0.24978516\n",
            " -0.38049072 -0.14704168 -0.5420546  -0.48727912 -0.6806182  -0.4382542\n",
            "  1.5300236  -0.5864554  -0.17071663 -0.38563052 -0.2893539   0.01531177\n",
            " -0.35454446]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3357733  -0.7018598  -0.6534476   0.00781406 -0.43356568 -0.23286074\n",
            " -0.43192193 -0.16850352 -0.6093158  -0.5737082  -0.8329238  -0.5422517\n",
            "  1.9972943  -0.73390913 -0.2409089  -0.40019807 -0.28892568  0.07318045\n",
            " -0.3291828 ]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3041535  -0.7056272  -0.57090867  0.07070579 -0.48632944 -0.26859018\n",
            " -0.39033815 -0.16290666 -0.596204   -0.6049161  -0.8460817  -0.55508316\n",
            "  2.0418415  -0.74637103 -0.2330254  -0.40668455 -0.26322493  0.02087731\n",
            " -0.29274118]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.43290314 -0.65723157 -0.65914464  0.05778108 -0.55374146 -0.2674973\n",
            " -0.67669296 -0.36255568 -0.49696332 -0.2585314  -0.7594113  -0.59714115\n",
            "  1.9219626  -0.78177154 -0.2942111  -0.42935383 -0.29567724  0.17529234\n",
            " -0.18645465]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.39944255 -0.8333912  -0.695813    0.23578031 -0.66716385 -0.2756093\n",
            " -0.40783867 -0.15687041 -0.69510293 -0.7490947  -1.0561346  -0.70784074\n",
            "  2.892641   -0.96605873 -0.27454793 -0.37653148 -0.19120911  0.05995271\n",
            " -0.3029211 ]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.52340335 -1.2046883  -1.0924245   0.17741777 -0.7036971  -0.29579598\n",
            " -0.5214573  -0.15138435 -0.956059   -1.0408646  -1.4510562  -0.85108125\n",
            "  4.2193065  -1.3043034  -0.3762375  -0.4531631  -0.2758798   0.16517621\n",
            " -0.44089437]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 34 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3024625  -0.7216213  -0.66318065  0.1006787  -0.3945924  -0.38670105\n",
            " -0.29902267 -0.2685364  -0.59591967 -0.56271756 -0.9308651  -0.62072754\n",
            "  2.2697299  -0.70868695 -0.19920006 -0.38320568 -0.3378756  -0.03210488\n",
            " -0.14427713]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 35 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.24281569 -0.51587516 -0.54903567  0.00929521 -0.23773733 -0.41322297\n",
            " -0.31928036 -0.26898634 -0.4292289  -0.39704046 -0.77811426 -0.47209623\n",
            "  1.7530593  -0.58134186 -0.18292409 -0.3446164  -0.353246   -0.00191104\n",
            " -0.11657495]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 36 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.41529197 -1.2006884  -0.98382384  0.25038403 -0.77133507 -0.5983912\n",
            " -0.52960384 -0.20167321 -0.97279304 -1.1590779  -1.6749302  -0.91034484\n",
            "  4.6004443  -1.345067   -0.28925997 -0.7196628  -0.42753637  0.00713644\n",
            " -0.3790288 ]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 37 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.46706098 -0.5897877  -0.93002605  0.46666446 -0.9390092  -0.4461214\n",
            " -0.3032442  -0.0810662  -0.90197235 -1.1755873  -1.5905882  -0.9594141\n",
            "  5.1210732  -1.5308847  -0.38352183 -0.28823495 -0.07422283  0.05779544\n",
            " -0.25895664]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 38 reward=0 new_state=[1 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.2612396  -0.4207463  -0.49812582  0.05563537 -0.29521614 -0.3596176\n",
            " -0.26463616 -0.2193982  -0.42749405 -0.45471942 -0.7972428  -0.49926278\n",
            "  1.7989122  -0.5889466  -0.15094936 -0.3380735  -0.34121934 -0.03566687\n",
            " -0.090673  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 39 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.34324825 -0.04533192 -0.6294831   0.25359583 -0.6282729  -0.26747978\n",
            " -0.32932353 -0.250574   -0.55026513 -0.535458   -0.8716912  -0.63893443\n",
            "  2.8134909  -0.9374882  -0.29009244 -0.26081154 -0.08286296  0.1067658\n",
            " -0.12796953]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 40 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3399557   0.23306768 -0.6068245   0.45265365 -0.9711263  -0.2510599\n",
            " -0.2673912   0.09595755 -0.6606028  -1.0052351  -1.0947896  -0.68826497\n",
            "  4.5814834  -1.3147451  -0.29361933 -0.12332191  0.17787665  0.16283818\n",
            " -0.55816835]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 41 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.21283218 -0.10096756 -0.30353174  0.04772587 -0.41748625 -0.2681245\n",
            " -0.34155768 -0.13923576 -0.3110699  -0.36425745 -0.5502593  -0.3973122\n",
            "  1.671154   -0.5309367  -0.17871898 -0.31037968 -0.20878023  0.02270963\n",
            " -0.26550737]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 42 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2072092  -0.13465245 -0.23728463  0.01944648 -0.45399678 -0.21436127\n",
            " -0.4330257  -0.17696619 -0.29260346 -0.26324445 -0.47857562 -0.41969556\n",
            "  1.2728012  -0.43850547 -0.17635092 -0.40887117 -0.27850416  0.02366303\n",
            " -0.25609577]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 43 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.240022   -0.18002304 -0.38040793  0.05213806 -0.5068272  -0.38531756\n",
            " -0.54165524 -0.19110233 -0.22806329 -0.32988167 -0.65848565 -0.43979895\n",
            "  1.894944   -0.64169943 -0.17239046 -0.4405067  -0.25917384  0.1080924\n",
            " -0.28326136]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 44 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.21794564  0.02662545 -0.30563492  0.08133323 -0.41331097 -0.21394688\n",
            " -0.31055474 -0.11309159 -0.15084487 -0.30910057 -0.48441392 -0.3721941\n",
            "  1.4780438  -0.5074183  -0.14313239 -0.2625041  -0.13702747  0.07549778\n",
            " -0.23101363]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 45 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.21906917  0.03944029 -0.28960404  0.08986746 -0.46099684 -0.20605648\n",
            " -0.34119934 -0.07259705 -0.21610607 -0.3858118  -0.4986224  -0.4007787\n",
            "  1.517957   -0.51414376 -0.13612881 -0.3432312  -0.19712956  0.08704878\n",
            " -0.270441  ]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 46 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2227078   0.07284033 -0.25681216  0.07991263 -0.40200245 -0.20328498\n",
            " -0.33347622 -0.12169465 -0.07835133 -0.2345981  -0.41253605 -0.3362376\n",
            "  1.2530471  -0.4767332  -0.14752668 -0.24878488 -0.12147723  0.09439404\n",
            " -0.21870297]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 47 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.1950831   0.05628724 -0.22264485  0.04994098 -0.3511599  -0.20051807\n",
            " -0.29553732 -0.11043087 -0.06741691 -0.23191988 -0.37910643 -0.3047401\n",
            "  1.0760208  -0.41719133 -0.13402873 -0.25505114 -0.13715088  0.07345358\n",
            " -0.2120044 ]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 48 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.19004446  0.07149879 -0.20275879  0.05227762 -0.3418691  -0.19727108\n",
            " -0.28945997 -0.10749453 -0.04361504 -0.2142349  -0.35633987 -0.2909728\n",
            "  0.9938749  -0.4002906  -0.12944993 -0.2472753  -0.12909569  0.0764856\n",
            " -0.2060124 ]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 49 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18500416  0.08451755 -0.18373701  0.05370395 -0.33226922 -0.19401053\n",
            " -0.2835647  -0.10472207 -0.02299643 -0.19645174 -0.33345243 -0.277166\n",
            "  0.9139932  -0.38312852 -0.12489071 -0.23973474 -0.12154478  0.07946765\n",
            " -0.19988075]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 50 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.17998613  0.09559647 -0.16551615  0.05434014 -0.32244623 -0.190748\n",
            " -0.27785283 -0.1020963  -0.00519791 -0.17865837 -0.31054643 -0.26337814\n",
            "  0.83657163 -0.3658229  -0.12038399 -0.23241536 -0.11445902  0.08240925\n",
            " -0.19363412]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 51 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.17501059  0.10496822 -0.1480346   0.05429371 -0.31247494 -0.18749371\n",
            " -0.27232626 -0.09960245  0.01011449 -0.1609292  -0.28770858 -0.24965936\n",
            "  0.7617228  -0.34847432 -0.11595799 -0.22530505 -0.10780347  0.08531902\n",
            " -0.18729466]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 52 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.17009437  0.11284529 -0.13123325  0.05366043 -0.30242056 -0.18425635\n",
            " -0.26698703 -0.09722759  0.02324501 -0.14332627 -0.26501173 -0.2360524\n",
            "  0.6894915  -0.33116773 -0.11163668 -0.21839336 -0.10154669  0.08820468\n",
            " -0.18088236]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 53 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.16525123  0.11942036 -0.11505598  0.05252526 -0.29233947 -0.1810432\n",
            " -0.2618374  -0.0949605   0.03446881 -0.12590072 -0.24251631 -0.22259326\n",
            "  0.6198677  -0.31397444 -0.10744005 -0.21167104 -0.09566015  0.09107348\n",
            " -0.17441519]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 54 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.16049233  0.12486728 -0.09944995  0.05096326 -0.28228047 -0.17786035\n",
            " -0.25687957 -0.0927913   0.04403313 -0.10869398 -0.2202717  -0.20931181\n",
            "  0.55279845 -0.2969535  -0.10338459 -0.20512977 -0.09011764  0.09393185\n",
            " -0.16790935]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 55 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.15582648  0.12934214 -0.0843656   0.04904062 -0.27228534 -0.17471261\n",
            " -0.25211552 -0.09071131  0.05215884 -0.09173887 -0.1983174  -0.19623251\n",
            "  0.48819715 -0.2801534  -0.09948353 -0.19876195 -0.08489497  0.09678604\n",
            " -0.16137955]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 56 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.15126035  0.13298467 -0.06975669  0.0468154  -0.2623898  -0.17160368\n",
            " -0.24754716 -0.08871282  0.05904231 -0.07506063 -0.17668414 -0.18337491\n",
            "  0.42595112 -0.26361322 -0.09574719 -0.19256042 -0.07996958  0.09964177\n",
            " -0.15483898]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 57 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.14679888  0.13591947 -0.05558021  0.04433845 -0.25262406 -0.16853622\n",
            " -0.24317598 -0.08678896  0.06485742 -0.05867772 -0.15539497 -0.17075416\n",
            "  0.3659287  -0.24736366 -0.09218332 -0.18651837 -0.07532033  0.10250454\n",
            " -0.14829965]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 58 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.14335434  0.13743228 -0.04874132  0.04136473 -0.2430015  -0.16535018\n",
            " -0.23956132 -0.08585361  0.06907165 -0.04374596 -0.13695636 -0.1597917\n",
            "  0.31372324 -0.23262371 -0.0882487  -0.18181275 -0.07238682  0.10562302\n",
            " -0.1422202 ]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 59 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.13992836  0.1385485  -0.04148819  0.03825568 -0.23354995 -0.1622228\n",
            " -0.2360876  -0.08489092  0.07261646 -0.02901585 -0.11864002 -0.14894336\n",
            "  0.2627276  -0.218093   -0.0845492  -0.17713739 -0.06955007  0.10873581\n",
            " -0.1361135 ]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 60 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.13653034  0.13933891 -0.03386836  0.03503972 -0.22428523 -0.15915273\n",
            " -0.23275946 -0.08390457  0.07559024 -0.01450049 -0.1004739  -0.13822643\n",
            "  0.21287127 -0.2037935  -0.08108012 -0.1724968  -0.06680568  0.11184935\n",
            " -0.12999222]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 61 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.3316797e-01  1.3986705e-01 -2.5923632e-02  3.1741496e-02\n",
            " -2.1522024e-01 -1.5613803e-01 -2.2958107e-01 -8.2897797e-02\n",
            "  7.8079946e-02 -2.0825118e-04 -8.2479432e-02 -1.2765449e-01\n",
            "  1.6407171e-01 -1.8974169e-01 -7.7835813e-02 -1.6789459e-01\n",
            " -6.4148806e-02  1.1497003e-01 -1.2386785e-01]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 62 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1300377   0.14003268 -0.01880132  0.02865066 -0.20674509 -0.15334114\n",
            " -0.22634408 -0.08190304  0.07967834  0.01291006 -0.06572074 -0.11803158\n",
            "  0.12043415 -0.17297332 -0.07433102 -0.16365944 -0.0618785   0.11776476\n",
            " -0.11855278]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 63 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.12722608  0.13980445 -0.01241831  0.02580476 -0.19907936 -0.1508281\n",
            " -0.2234542  -0.08103804  0.0808474   0.02474696 -0.05063587 -0.10937392\n",
            "  0.0816689  -0.15783364 -0.07116523 -0.15989485 -0.05989354  0.12025913\n",
            " -0.11375119]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 64 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.12470228  0.13924643 -0.00670024  0.02318717 -0.19214474 -0.1485715\n",
            " -0.22087829 -0.08029026  0.08167455  0.03542677 -0.03706325 -0.10158822\n",
            "  0.047187   -0.1441628  -0.0683056  -0.15655568 -0.05816673  0.12248349\n",
            " -0.1094134 ]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 65 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.12243836  0.1384118  -0.00158044  0.02078161 -0.18587044 -0.14654662\n",
            " -0.21858624 -0.0796483   0.08223034  0.04506176 -0.02485674 -0.09459011\n",
            "  0.01647799 -0.13181677 -0.06572239 -0.1536009  -0.0566733   0.12446497\n",
            " -0.10549457]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 66 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.12040913  0.13734493  0.00300098  0.01857233 -0.18019255 -0.14473103\n",
            " -0.21655063 -0.07910182  0.08257163  0.05375335 -0.01388413 -0.08830322\n",
            " -0.01090161 -0.12066579 -0.06338884 -0.15099327 -0.05539064  0.12622799\n",
            " -0.10195416]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 67 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1185867   0.13607943  0.00710922  0.01653861 -0.17503911 -0.14309992\n",
            " -0.2147418  -0.07864013  0.08274446  0.06161474 -0.00399859 -0.08264296\n",
            " -0.03540521 -0.11056504 -0.06127489 -0.14869271 -0.05429519  0.12779877\n",
            " -0.09874674]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 68 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11696053  0.13465357  0.01076997  0.01467736 -0.17038648 -0.14164394\n",
            " -0.21314716 -0.0782572   0.08278472  0.0686854   0.00485351 -0.07757816\n",
            " -0.05723496 -0.10146476 -0.05937032 -0.14668131 -0.05337431  0.12918843\n",
            " -0.0958569 ]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 69 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1155069   0.13309029  0.01403812  0.0129695  -0.16617337 -0.14034198\n",
            " -0.21174136 -0.07794403  0.08272251  0.07506164  0.0127972  -0.07303679\n",
            " -0.07675378 -0.09324209 -0.05764958 -0.14492495 -0.05260798  0.13041905\n",
            " -0.09324594]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 70 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11420895  0.131412    0.01695281  0.01140237 -0.16235727 -0.13917904\n",
            " -0.21050593 -0.07769353  0.0825814   0.08081099  0.01992084 -0.06896786\n",
            " -0.09422027 -0.08581133 -0.0560948  -0.1433985  -0.05198073  0.13150659\n",
            " -0.09088685]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 71 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11299948  0.12984446  0.01960554  0.0099988  -0.15891689 -0.13814512\n",
            " -0.20933829 -0.07748108  0.08239929  0.08597833  0.02633317 -0.06530373\n",
            " -0.10985404 -0.07910606 -0.05469249 -0.14201018 -0.05139029  0.13207673\n",
            " -0.08876853]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 72 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1118696   0.12838243  0.02202237  0.00874449 -0.15581584 -0.1372279\n",
            " -0.2082316  -0.0773031   0.08218904  0.09061997  0.03210298 -0.06200526\n",
            " -0.12385599 -0.07305545 -0.0534278  -0.1407466  -0.05083296  0.13217026\n",
            " -0.08686773]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 73 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11081144  0.12702021  0.02422665  0.00762618 -0.15302122 -0.13641612\n",
            " -0.2071798  -0.07715628  0.08196062  0.09478689  0.03729243 -0.059037\n",
            " -0.13640325 -0.06759559 -0.05228746 -0.13959564 -0.05030552  0.13182639\n",
            " -0.0851634 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 74 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10981804  0.12575193  0.02623935  0.00663162 -0.15050334 -0.13569964\n",
            " -0.20617759 -0.0770376   0.08172173  0.09852514  0.04195782 -0.05636695\n",
            " -0.14765252 -0.06266883 -0.05125933 -0.13854638 -0.04980504  0.13108268\n",
            " -0.08363649]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 75 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10888319  0.12457162  0.02807923  0.0057495  -0.14823534 -0.13506918\n",
            " -0.20522034 -0.07694431  0.08147823  0.10187641  0.04615001 -0.05396613\n",
            " -0.15774234 -0.05822311 -0.05033253 -0.13758901 -0.04932901  0.12997513\n",
            " -0.08226977]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 76 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10800144  0.1234735   0.02976323  0.00496935 -0.14619297 -0.13451636\n",
            " -0.20430404 -0.07687382  0.08123455  0.10487851  0.04991514 -0.05180833\n",
            " -0.1667956  -0.05421145 -0.04949719 -0.13671467 -0.04887519  0.12853807\n",
            " -0.08104762]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 77 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10716791  0.12245183  0.03130643  0.00428161 -0.14435427 -0.13403356\n",
            " -0.20342518 -0.07682385  0.08099395  0.10756553  0.05329477 -0.04986991\n",
            " -0.17492145 -0.05059151 -0.04874438 -0.13591535 -0.04844162  0.1268042\n",
            " -0.07995596]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 78 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1063783   0.12150119  0.03272244  0.00367747 -0.14269945 -0.13361387\n",
            " -0.20258069 -0.0767922   0.0807588   0.10996832  0.05632664 -0.04812943\n",
            " -0.18221693 -0.04732502 -0.04806604 -0.13518387 -0.04802661  0.12480471\n",
            " -0.07898198]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 79 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10567014  0.12065651  0.03401516  0.00314742 -0.14119625 -0.13325551\n",
            " -0.20179623 -0.07676079  0.08089548  0.11215475  0.05905128 -0.04653587\n",
            " -0.18878008 -0.04440051 -0.04745186 -0.13449574 -0.04761148  0.12299466\n",
            " -0.07808287]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 80 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1049946   0.11986881  0.03520469  0.00268571 -0.13984424 -0.13294792\n",
            " -0.20103829 -0.0767456   0.0810068   0.11410629  0.06149232 -0.04510651\n",
            " -0.19467519 -0.04176144 -0.0468986  -0.1338649  -0.0472137   0.12093496\n",
            " -0.07728249]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 81 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10434901  0.11913387  0.03630088  0.00228559 -0.13862869 -0.13268594\n",
            " -0.20030501 -0.07674484  0.08109624  0.11584615  0.06367774 -0.04382519\n",
            " -0.19997112 -0.03937994 -0.04640029 -0.13328585 -0.04683184  0.11865421\n",
            " -0.07657112]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 82 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10377967  0.11824881  0.03725979  0.00190884 -0.13752018 -0.13246116\n",
            " -0.19967504 -0.07677409  0.08114887  0.11741001  0.06560519 -0.04269776\n",
            " -0.20473573 -0.0372208  -0.04594931 -0.13281861 -0.04654756  0.11656792\n",
            " -0.07592762]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 83 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10327911  0.11722939  0.03809498  0.00155341 -0.13650836 -0.1322699\n",
            " -0.19913816 -0.07683027  0.08116903  0.11881506  0.06729989 -0.04170916\n",
            " -0.2090231  -0.03526241 -0.04554102 -0.13245204 -0.0463511   0.11465905\n",
            " -0.07534541]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 84 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10274562  0.11657953  0.03857412  0.00115789 -0.13547127 -0.13240203\n",
            " -0.19970727 -0.07777187  0.08207366  0.1348046   0.07022446 -0.03991155\n",
            " -0.21391079 -0.03278115 -0.04448473 -0.13175055 -0.04544089  0.11380808\n",
            " -0.07475736]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 85 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10218038  0.116282    0.03875004  0.00072833 -0.13442455 -0.13282867\n",
            " -0.20128578 -0.07951472  0.08378521  0.16215703  0.07427626 -0.03738033\n",
            " -0.21936637 -0.02982187 -0.04283978 -0.13074522 -0.0438737   0.11391714\n",
            " -0.07415836]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 86 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10158303  0.11633801  0.0386819   0.00027004 -0.13338742 -0.13352348\n",
            " -0.20379192 -0.08198026  0.08623873  0.19909805  0.07937979 -0.0341782\n",
            " -0.22538207 -0.0264158  -0.04065405 -0.12946063 -0.04169263  0.11490852\n",
            " -0.07354172]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 87 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.0095314e-01  1.1676090e-01  3.8431883e-02 -2.1260255e-04\n",
            " -1.3238052e-01 -1.3446175e-01 -2.0715486e-01 -8.5093610e-02\n",
            "  8.9378230e-02  2.4447578e-01  8.5479997e-02 -3.0357786e-02\n",
            " -2.3196672e-01 -2.2583403e-02 -3.7966691e-02 -1.2791586e-01\n",
            " -3.8931288e-02  1.1672002e-01 -7.2900370e-02]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 88 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10029089  0.11757189  0.03806263 -0.00071607 -0.13142468 -0.13561988\n",
            " -0.21131238 -0.0887829   0.09315422  0.29751694  0.0925371  -0.02596342\n",
            " -0.23913972 -0.01833703 -0.03481015 -0.1261254  -0.03561654  0.11930145\n",
            " -0.07222743]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 89 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.09959748  0.11879653  0.03763533 -0.00123756 -0.13054007 -0.13697499\n",
            " -0.2162089  -0.09297922  0.09752201  0.35771018  0.10052265 -0.0210329\n",
            " -0.24692646 -0.013683   -0.03121181 -0.12410003 -0.03177053  0.12261165\n",
            " -0.07151647]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 90 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.09887513  0.1204621   0.03720815 -0.00177477 -0.12974551 -0.13850471\n",
            " -0.22179413 -0.09761667  0.10244074  0.42473832  0.10941635 -0.0155987\n",
            " -0.25535494 -0.00862365 -0.02719544 -0.1218477  -0.02741233  0.12661657\n",
            " -0.07076167]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 91 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.09812727  0.12259571  0.03683536 -0.0023258  -0.12905812 -0.14018701\n",
            " -0.22802189 -0.10263263  0.10787237  0.4984329   0.11920274 -0.00968912\n",
            " -0.26445308 -0.00315867 -0.02278204 -0.1193742  -0.02255901  0.13128708\n",
            " -0.06995793]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 92 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.09735836  0.12522247  0.03656632 -0.00288892 -0.12849292 -0.14200017\n",
            " -0.2348493  -0.10796794  0.11378088  0.5787416   0.12986937 -0.0033293\n",
            " -0.2742463   0.00271332 -0.01799092 -0.11668381 -0.01722678  0.1365979\n",
            " -0.06910095]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 93 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.09869061  0.12858468  0.03507476 -0.00434038 -0.127776   -0.14275573\n",
            " -0.24377081 -0.11206694  0.11943152  0.65508443  0.14038113  0.00281223\n",
            " -0.2835429   0.00794042 -0.0136939  -0.11482771 -0.01373664  0.16408448\n",
            " -0.06771909]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 94 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.09980074  0.13245006  0.03390408 -0.00571221 -0.12723693 -0.14371894\n",
            " -0.25306326 -0.11653327  0.12555866  0.7389642   0.15183838  0.0094151\n",
            " -0.29368895  0.01367648 -0.00897575 -0.11266067 -0.00957524  0.19075842\n",
            " -0.06632851]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 95 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10071576  0.13682787  0.03306838 -0.00701138 -0.12687907 -0.14485903\n",
            " -0.26270282 -0.12130676  0.13212249  0.8303853   0.16420986  0.01645096\n",
            " -0.3046736   0.01990552 -0.00386366 -0.11019772 -0.00478488  0.21678825\n",
            " -0.0649226 ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 96 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-1.0146132e-01  1.4172450e-01  3.2578528e-02 -8.2438942e-03\n",
            " -1.2670442e-01 -1.4614677e-01 -2.7266675e-01 -1.2633210e-01\n",
            "  1.3908534e-01  9.2941433e-01  1.7746523e-01  2.3893684e-02\n",
            " -3.1648389e-01  2.6611954e-02  1.6162768e-03 -1.0745263e-01\n",
            "  5.9425086e-04  2.4231696e-01 -6.3495800e-02]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 97 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10206182  0.14714263  0.03244152 -0.009415   -0.12671362 -0.14755426\n",
            " -0.2829328  -0.1315587   0.1464109   1.0361555   0.1915732   0.03171846\n",
            " -0.32910275  0.03377965  0.00743879 -0.10443837  0.00652322  0.2674647\n",
            " -0.0620436 ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 98 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10254042  0.15308051  0.03266034 -0.01052915 -0.12690574 -0.14905515\n",
            " -0.2934788  -0.13694039  0.15406412  1.1507279   0.20650008  0.03990106\n",
            " -0.3425082   0.04139083  0.01357851 -0.10116786  0.01296353  0.29233077\n",
            " -0.06056264]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 99 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10291892  0.15953144  0.03323371 -0.0115902  -0.12727842 -0.1506245\n",
            " -0.30428225 -0.14243525  0.16201048  1.273248    0.2222085   0.04841733\n",
            " -0.35667223  0.04942566  0.02000979 -0.09765434  0.01987655  0.31699517\n",
            " -0.05905068]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 100 reward=1 new_state=[1 0 0 0 0 0 1 1 1]\n",
            "Predicted scores for each action in next step: [-0.10321768  0.16648345  0.0341561  -0.01260137 -0.12782755 -0.1522391\n",
            " -0.31532004 -0.14800552  0.17021596  1.4038103   0.23865661  0.05724236\n",
            " -0.3715601   0.05786144  0.02670616 -0.09391174  0.0272232   0.34151995\n",
            " -0.05750662]\n",
            "Epsilon reduced to 0.08100000000000002\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.1035271   0.17206009  0.03473669 -0.01348424 -0.12820311 -0.1535737\n",
            " -0.3245418  -0.15268385  0.17700484  1.5145742   0.25215125  0.06453158\n",
            " -0.38373023  0.06476308  0.03221768 -0.09087185  0.03318395  0.36300874\n",
            " -0.05622143]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10383245  0.17654537  0.03505763 -0.01425884 -0.12844467 -0.15469004\n",
            " -0.33229443 -0.15664697  0.18266043  1.6085705   0.2632824   0.07058874\n",
            " -0.39372876  0.07043966  0.03678302 -0.08839105  0.03804754  0.38187027\n",
            " -0.05514589]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 3 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10413377  0.17998832  0.03511849 -0.01493399 -0.1285532  -0.15560618\n",
            " -0.33867162 -0.15995002  0.18725473  1.6856015   0.2721802   0.07548684\n",
            " -0.40167284  0.07495687  0.0404588  -0.08644004  0.04187436  0.3981784\n",
            " -0.05426784]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 4 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10431124  0.1853492   0.03539564 -0.01543559 -0.12873758 -0.1563511\n",
            " -0.34419674 -0.16271137  0.19157417  1.7566035   0.28053394  0.08011378\n",
            " -0.40885547  0.07895352  0.04375712 -0.08427466  0.04576526  0.41324547\n",
            " -0.05348509]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 5 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10457269  0.18749072  0.03459929 -0.0157789  -0.12841302 -0.15665033\n",
            " -0.34648618 -0.16406408  0.19326417  1.7783797   0.28299972  0.08179098\n",
            " -0.4105712   0.07987256  0.04478358 -0.08359437  0.04681052  0.42263353\n",
            " -0.05318914]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 6 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10484692  0.18677634  0.03284688 -0.01596829 -0.12761785 -0.15660726\n",
            " -0.34588718 -0.16421309  0.19268157  1.7595865   0.28026992  0.08087676\n",
            " -0.40747046  0.07809602  0.04382486 -0.08423061  0.04541354  0.42686343\n",
            " -0.05331884]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 7 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10508651  0.18357497  0.03025999 -0.01600747 -0.12640032 -0.15630148\n",
            " -0.3427537  -0.16334449  0.19016308  1.7081811   0.27303255  0.07772361\n",
            " -0.40019643  0.07399499  0.04115395 -0.08601373  0.04195389  0.42649928\n",
            " -0.05381406]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 8 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10526536  0.17824298  0.02695709 -0.01590084 -0.12481393 -0.15579134\n",
            " -0.33743477 -0.1616238   0.18601692  1.6313848   0.26194477  0.07266657\n",
            " -0.38935953  0.06791663  0.03702269 -0.08878042  0.03677528  0.42211553\n",
            " -0.05461839]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 9 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10537493  0.17111512  0.02304925 -0.01565331 -0.12291389 -0.15511805\n",
            " -0.33026564 -0.15919639  0.18052025  1.535626    0.24761742  0.06601654\n",
            " -0.37552533  0.06017812  0.03165874 -0.09237757  0.03018144  0.41427502\n",
            " -0.05568048]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 10 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10541946  0.16249922  0.01863813 -0.01527047 -0.12075481 -0.15430987\n",
            " -0.32156235 -0.1561893   0.17391971  1.4265139   0.23060904  0.05805779\n",
            " -0.3592089   0.05106514  0.02526544 -0.09666444  0.02243759  0.40351498\n",
            " -0.05695426]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 11 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10541221  0.15267484  0.01381522 -0.01475827 -0.11838879 -0.15338553\n",
            " -0.31161794 -0.15271294  0.16643327  1.308844    0.21142277  0.04904708\n",
            " -0.34087357  0.04083183  0.01802284 -0.10151354  0.01377315  0.3903383\n",
            " -0.05839875]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 12 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10537205  0.14189275  0.00866187 -0.0141226  -0.11586437 -0.15235712\n",
            " -0.30070028 -0.14886324  0.15825269  1.1866304   0.19050804  0.03921406\n",
            " -0.32093212  0.02970299  0.01008939 -0.10681058  0.00438555  0.37520912\n",
            " -0.0599778 ]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 13 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10532089  0.13037616  0.00324985 -0.01336943 -0.11322588 -0.15123186\n",
            " -0.28905153 -0.14472312  0.14954604  1.0631559   0.16826245  0.02876242\n",
            " -0.29974967  0.01787573  0.0016041  -0.11245415 -0.00555598  0.35855013\n",
            " -0.06165958]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 14 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10528176  0.11832181 -0.00235821 -0.01250441 -0.11051309 -0.15001383\n",
            " -0.27688795 -0.14036414  0.14046007  0.94103265  0.14503533  0.01787159\n",
            " -0.27764696  0.00552193 -0.00731215 -0.11835517 -0.01590692  0.34074205\n",
            " -0.06341609]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 15 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10527746  0.10590173 -0.00810783 -0.01153299 -0.10776106 -0.1487052\n",
            " -0.26440084 -0.13584788  0.13112241  0.8222685   0.12113085  0.00669839\n",
            " -0.25490323 -0.00720949 -0.01655428 -0.12443602 -0.0265443   0.32212394\n",
            " -0.06522271]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 16 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10532954  0.09326477 -0.01395211 -0.01046026 -0.1050003  -0.14730671\n",
            " -0.25175744 -0.13122714  0.12164371  0.708339    0.0968121  -0.0046213\n",
            " -0.2317605  -0.02019057 -0.02603181 -0.13062988 -0.03736401  0.30299532\n",
            " -0.06705773]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 17 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10545781  0.08053865 -0.01985085 -0.00929106 -0.10225691 -0.14581865\n",
            " -0.23910268 -0.12654684  0.11211952  0.6002566   0.07230435 -0.01597074\n",
            " -0.20842654 -0.03331229 -0.03566718 -0.13687982 -0.04827853  0.28361744\n",
            " -0.068902  ]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 18 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10567993  0.06783137 -0.02576999 -0.00802981 -0.09955281 -0.14424096\n",
            " -0.22656065 -0.12184516  0.1026319   0.49863985  0.04779857 -0.02725073\n",
            " -0.18507813 -0.04648264 -0.04539446 -0.14313804 -0.05921481  0.2642161\n",
            " -0.07073851]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 19 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10601124  0.05523316 -0.03168086 -0.00668065 -0.09690611 -0.14257371\n",
            " -0.21423656 -0.11715418  0.09325108  0.40377787  0.02345467 -0.03837781\n",
            " -0.16186452 -0.05962521 -0.05515819 -0.14936504 -0.07011276  0.24498416\n",
            " -0.07255213]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 20 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10646479  0.04281798 -0.03755954 -0.00524737 -0.09433136 -0.1408173\n",
            " -0.20221826 -0.11250074  0.08403684  0.31569135 -0.00059555 -0.04928295\n",
            " -0.13890986 -0.0726774  -0.06491219 -0.15552896 -0.08092361  0.2260842\n",
            " -0.07432929]\n",
            "\n",
            "Taking action 13 from 9\n",
            "\n",
            "Step 21 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.10705133  0.03064509 -0.04338643 -0.00373346 -0.09183994 -0.13897245\n",
            " -0.19057825 -0.10790701  0.0750398   0.23418677 -0.02424545 -0.05990991\n",
            " -0.11631621 -0.08558916 -0.07461873 -0.1616048  -0.09160867  0.20765133\n",
            " -0.07605771]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10777949  0.01876054 -0.04914567 -0.00214208 -0.08944035 -0.13704045\n",
            " -0.17937525 -0.10339113  0.0663025   0.15890507 -0.04741099 -0.07021398\n",
            " -0.0941658  -0.09832166 -0.08424748 -0.16757378 -0.1021382   0.18979591\n",
            " -0.07772629]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10837805  0.00798414 -0.05431599 -0.00067461 -0.08727643 -0.1353216\n",
            " -0.16918239 -0.09933759  0.0584079   0.09498738 -0.06834908 -0.07952781\n",
            " -0.07415669 -0.10981307 -0.09293415 -0.17294882 -0.11160395  0.17315392\n",
            " -0.07924595]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10886467 -0.00178667 -0.05895718  0.00067835 -0.08532507 -0.13379301\n",
            " -0.15990824 -0.09569946  0.05127465  0.04058594 -0.08727415 -0.08794669\n",
            " -0.05608165 -0.12018431 -0.10077085 -0.17778887 -0.12011277  0.15767519\n",
            " -0.08063003]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10925494 -0.01064509 -0.06312303  0.00192549 -0.0835654  -0.13243413\n",
            " -0.15146986 -0.09243453  0.04482929 -0.00583073 -0.10437994 -0.0955567\n",
            " -0.03975356 -0.12954454 -0.10784064 -0.18214704 -0.1277609   0.14329892\n",
            " -0.08189061]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 26 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10956264 -0.01867574 -0.06686191  0.00307483 -0.08197863 -0.13122657\n",
            " -0.14379194 -0.08950478  0.03900547 -0.04553364 -0.11984145 -0.1024356\n",
            " -0.02500352 -0.1379923  -0.11421856 -0.18607119 -0.134635    0.12995955\n",
            " -0.0830387 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 27 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10979986 -0.02595547 -0.07021737  0.00413375 -0.08054778 -0.13015385\n",
            " -0.13680609 -0.08687603  0.03374328 -0.07957928 -0.1338168  -0.10865363\n",
            " -0.01167894 -0.14561647 -0.11997223 -0.18960446 -0.1408131   0.11759066\n",
            " -0.0840843 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 28 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10997726 -0.03255402 -0.07322855  0.00510907 -0.07925755 -0.12920122\n",
            " -0.13045013 -0.08451756  0.02898858 -0.1088472  -0.14644884 -0.1142743\n",
            "  0.00035791 -0.15249735 -0.12516272 -0.19278571 -0.1463654   0.10612726\n",
            " -0.08503649]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 29 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11010418 -0.03853469 -0.07593066  0.00600708 -0.07809411 -0.1283555\n",
            " -0.12466761 -0.08240169  0.0246925  -0.13407089 -0.15786663 -0.11935496\n",
            "  0.01123146 -0.1587074  -0.12984511 -0.19565    -0.15135513  0.0955073\n",
            " -0.08590356]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 30 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11018881 -0.0439549  -0.07835536  0.00683359 -0.07704502 -0.12760489\n",
            " -0.11940726 -0.08050358  0.02081086 -0.1558631  -0.16818681 -0.12394741\n",
            "  0.02105405 -0.16431193 -0.13406911 -0.19822887 -0.1558392   0.08567233\n",
            " -0.08669299]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 31 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11023831 -0.04886676 -0.08053111  0.00759396 -0.07609902 -0.12693882\n",
            " -0.1146224  -0.07880086  0.01730377 -0.17473672 -0.17751476 -0.12809853\n",
            "  0.02992716 -0.16937003 -0.13787958 -0.20055076 -0.1598688   0.07656784\n",
            " -0.08741166]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 32 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11025889 -0.05331752 -0.08248346  0.00829314 -0.07524601 -0.12634787\n",
            " -0.11027066 -0.07727343  0.01413516 -0.19112188 -0.18594566 -0.13185063\n",
            "  0.03794244 -0.1739349  -0.14131697 -0.20264131 -0.16349004  0.06814331\n",
            " -0.0880658 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 33 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.110256   -0.05735007 -0.08423543  0.00893572 -0.07447683 -0.12582363\n",
            " -0.10631347 -0.07590322  0.01127248 -0.20537998 -0.1935656  -0.13524194\n",
            "  0.04518265 -0.17805462 -0.1444178  -0.20452358 -0.16674435  0.06035199\n",
            " -0.08866107]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 34 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11023434 -0.06100323 -0.08580767  0.00952591 -0.07378326 -0.12535854\n",
            " -0.10271578 -0.07467401  0.00868629 -0.21781546 -0.20045233 -0.13830706\n",
            "  0.05172256 -0.18177259 -0.14721498 -0.20621839 -0.16966903  0.05315079\n",
            " -0.08920266]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 35 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.110198   -0.06431227 -0.08721875  0.01006762 -0.07315782 -0.12494591\n",
            " -0.09944571 -0.0735712   0.00634999 -0.2286852  -0.20667607 -0.14107719\n",
            "  0.05762972 -0.18512794 -0.14973824 -0.20774445 -0.17229766  0.0464999\n",
            " -0.08969526]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 36 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11015712 -0.06733914 -0.08873671  0.01057445 -0.07246059 -0.12444449\n",
            " -0.09618004 -0.07250294  0.00425415 -0.23806617 -0.21217716 -0.14344624\n",
            "  0.06298278 -0.18776995 -0.14908664 -0.2091504  -0.17479299  0.04048526\n",
            " -0.0903221 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 37 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11011708 -0.07006976 -0.09009094  0.01102759 -0.07182427 -0.12398781\n",
            " -0.09323265 -0.07153729  0.00236351 -0.24628432 -0.21712154 -0.14557368\n",
            "  0.06775439 -0.19014163 -0.14848441 -0.21041381 -0.1770432   0.03505936\n",
            " -0.09087992]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 38 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11007753 -0.07253292 -0.09129623  0.01143197 -0.07124233 -0.12357115\n",
            " -0.09057233 -0.07066408  0.00065803 -0.25349602 -0.2215622  -0.14748232\n",
            "  0.07199638 -0.19226852 -0.14792988 -0.21154828 -0.17907217  0.0301669\n",
            " -0.09137487]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 39 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.11003821 -0.07475466 -0.09236609  0.01179208 -0.07070893 -0.12319031\n",
            " -0.08817092 -0.06987412 -0.00088041 -0.2598344  -0.22554712 -0.1491929\n",
            "  0.07575601 -0.19417372 -0.14742099 -0.21256605 -0.18090147  0.02575759\n",
            " -0.09181254]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 40 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1099989  -0.07675838 -0.09331283  0.01211198 -0.07021888 -0.12284151\n",
            " -0.08600299 -0.06915917 -0.00226813 -0.26541305 -0.22911967 -0.15072419\n",
            "  0.07907642 -0.19587818 -0.14695531 -0.21347821 -0.18255056  0.02178575\n",
            " -0.09219809]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 41 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10995941 -0.07856538 -0.09414762  0.01239538 -0.0697675  -0.12252137\n",
            " -0.08404568 -0.06851177 -0.00351986 -0.27032918 -0.2323192  -0.15209313\n",
            "  0.08199693 -0.19740081 -0.14653036 -0.21429482 -0.18403703  0.01820994\n",
            " -0.09253617]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 42 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10991962 -0.08019478 -0.09488067  0.01264565 -0.06935069 -0.12222689\n",
            " -0.08227828 -0.06792521 -0.00464892 -0.2746662  -0.23518127 -0.15331517\n",
            "  0.0845534  -0.19875889 -0.14614357 -0.21502501 -0.1853767   0.01499256\n",
            " -0.09283102]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 43 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10987941 -0.08166387 -0.09552126  0.01286584 -0.0689647  -0.12195536\n",
            " -0.08068219 -0.06739348 -0.00566729 -0.27849582 -0.23773807 -0.15440421\n",
            "  0.08677851 -0.19996798 -0.14579235 -0.21567698 -0.18658395  0.01209948\n",
            " -0.09308655]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 44 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10983869 -0.08298829 -0.09607783  0.01305874 -0.06860626 -0.12170436\n",
            " -0.0792406  -0.06691113 -0.00658579 -0.28187996 -0.24001877 -0.15537289\n",
            "  0.08870219 -0.20104215 -0.1454742  -0.21625823 -0.18767166  0.00949974\n",
            " -0.0933063 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 45 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10979742 -0.08418217 -0.09655809  0.01322687 -0.06827243 -0.12147175\n",
            " -0.07793832 -0.06647325 -0.0074142  -0.28487217 -0.24204975 -0.15623266\n",
            "  0.09035162 -0.20199424 -0.14518657 -0.21677548 -0.1886515   0.00716531\n",
            " -0.09349351]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 46 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10975555 -0.08525819 -0.09696907  0.01337255 -0.06796053 -0.12125558\n",
            " -0.07676171 -0.06607544 -0.00816132 -0.2875188  -0.2438549  -0.1569939\n",
            "  0.09175171 -0.20283586 -0.1449272  -0.21723484 -0.18953401  0.00507081\n",
            " -0.09365112]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 47 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10971305 -0.08622788 -0.09731713  0.01349783 -0.06766827 -0.12105417\n",
            " -0.07569843 -0.06571373 -0.00883512 -0.28986028 -0.24545583 -0.157666\n",
            "  0.0929251  -0.20357749 -0.14469382 -0.21764189 -0.19032869  0.0031932\n",
            " -0.09378183]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 48 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10966992 -0.08710162 -0.0976081   0.01360467 -0.06739356 -0.12086596\n",
            " -0.07473735 -0.06538454 -0.00944275 -0.29193172 -0.24687217 -0.15825741\n",
            "  0.09389254 -0.20422867 -0.14448428 -0.21800159 -0.19104412  0.00151168\n",
            " -0.0938881 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 49 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0962616e-01 -8.7888747e-02 -9.7847268e-02  1.3694747e-02\n",
            " -6.7134514e-02 -1.2068956e-01 -7.3868461e-02 -6.5084651e-02\n",
            " -9.9907052e-03 -2.9376388e-01 -2.4812156e-01 -1.5877588e-01\n",
            "  9.4672948e-02 -2.0479801e-01 -1.4429659e-01 -2.1831854e-01\n",
            " -1.9168806e-01  7.3388219e-06 -9.3972176e-02]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 50 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1095818  -0.0885978  -0.09803947  0.01376966 -0.06688951 -0.12052377\n",
            " -0.0730827  -0.06481116 -0.01048481 -0.29538363 -0.24922016 -0.15922841\n",
            "  0.09528353 -0.20529339 -0.14412893 -0.21859679 -0.19226748 -0.00133688\n",
            " -0.0940361 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 51 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10953683 -0.08923633 -0.0981891   0.01383084 -0.06665707 -0.1203675\n",
            " -0.07237193 -0.06456147 -0.01093034 -0.29681465 -0.2501824  -0.15962131\n",
            "  0.09574004 -0.20572181 -0.14397956 -0.2188401  -0.19278866 -0.00253644\n",
            " -0.09408173]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 52 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1094913  -0.08981129 -0.09830017  0.01387959 -0.06643587 -0.12021976\n",
            " -0.0717288  -0.06433322 -0.01133204 -0.2980777  -0.25102147 -0.15996033\n",
            "  0.09605685 -0.2060898  -0.1438469  -0.21905184 -0.19325736 -0.00360531\n",
            " -0.09411079]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 53 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10944522 -0.09032888 -0.0983763   0.01391707 -0.06622477 -0.12007968\n",
            " -0.07114665 -0.06412429 -0.01169422 -0.29919106 -0.25174934 -0.16025066\n",
            "  0.09624708 -0.20640314 -0.1437294  -0.21923505 -0.19367868 -0.00455617\n",
            " -0.09412481]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 54 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10939865 -0.0907947  -0.09842084  0.01394437 -0.0660227  -0.11994649\n",
            " -0.07061952 -0.06393278 -0.01202072 -0.30017096 -0.25237668 -0.16049698\n",
            "  0.0963227  -0.20666711 -0.1436258  -0.21939248 -0.19405727 -0.00540046\n",
            " -0.09412523]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 55 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10935161 -0.09121387 -0.0984368   0.01396249 -0.06582877 -0.11981949\n",
            " -0.07014201 -0.06375697 -0.01231507 -0.30103168 -0.2529133  -0.16070361\n",
            "  0.09629461 -0.20688647 -0.14353472 -0.21952666 -0.1943973  -0.00614852\n",
            " -0.09411331]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 56 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10930414 -0.09159092 -0.09842693  0.01397227 -0.06564215 -0.11969808\n",
            " -0.06970925 -0.0635953  -0.01258037 -0.3017859  -0.25336814 -0.16087437\n",
            "  0.09617278 -0.20706561 -0.14345512 -0.21963985 -0.19470257 -0.00680975\n",
            " -0.09409026]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 57 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10925627 -0.09193    -0.09839375  0.01397454 -0.0654621  -0.11958171\n",
            " -0.06931688 -0.06344642 -0.01281952 -0.3024449  -0.25374907 -0.16101278\n",
            "  0.09596631 -0.20720848 -0.14338592 -0.2197341  -0.1949765  -0.00739259\n",
            " -0.09405714]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 58 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10920806 -0.09223483 -0.09833954  0.01397005 -0.06528801 -0.11946986\n",
            " -0.06896092 -0.06330904 -0.01303503 -0.3030187  -0.2540634  -0.16112202\n",
            "  0.09568349 -0.20731857 -0.1433261  -0.21981128 -0.19522214 -0.00790471\n",
            " -0.09401494]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 59 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10915956 -0.09250878 -0.09826638  0.01395946 -0.06511927 -0.11936214\n",
            " -0.0686378  -0.06318206 -0.01322924 -0.3035162  -0.25431767 -0.16120496\n",
            "  0.09533186 -0.20739914 -0.14327484 -0.21987306 -0.19544229 -0.00835306\n",
            " -0.09396455]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 60 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10911077 -0.09275489 -0.09817617  0.01394336 -0.0649554  -0.11925814\n",
            " -0.06834432 -0.06306446 -0.01340423 -0.30394542 -0.25451785 -0.16126421\n",
            "  0.09491835 -0.20745316 -0.1432313  -0.21992095 -0.19563945 -0.00874385\n",
            " -0.09390679]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 61 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10906177 -0.09297588 -0.09807064  0.0139223  -0.06479593 -0.11915751\n",
            " -0.06807758 -0.06295531 -0.01356189 -0.30431336 -0.2546693  -0.16130212\n",
            "  0.09444925 -0.20748323 -0.14319476 -0.21995637 -0.19581589 -0.0090828\n",
            " -0.09384241]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 62 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10901257 -0.09317426 -0.09795136  0.01389684 -0.06464046 -0.11905994\n",
            " -0.06783496 -0.0628538  -0.01370391 -0.30462644 -0.25477684 -0.16132084\n",
            "  0.09393028 -0.20749179 -0.14316459 -0.21998054 -0.19597363 -0.00937499\n",
            " -0.09377208]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 63 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10896315 -0.09335253 -0.09781954  0.01386732 -0.0644884  -0.11896501\n",
            " -0.06761375 -0.06275903 -0.01383204 -0.30489072 -0.25484508 -0.16132233\n",
            "  0.09336571 -0.207481   -0.14314003 -0.2199946  -0.19611481 -0.00962549\n",
            " -0.0936963 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 64 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10891369 -0.09351213 -0.09767692  0.01383429 -0.06433994 -0.11887279\n",
            " -0.06741253 -0.06267066 -0.01394724 -0.30511045 -0.25487778 -0.16130832\n",
            "  0.09276226 -0.20745286 -0.1431207  -0.21999958 -0.19624054 -0.00983761\n",
            " -0.09361589]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 65 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10886415 -0.09365514 -0.09752447  0.01379807 -0.06419453 -0.1187829\n",
            " -0.06722903 -0.06258792 -0.01405096 -0.30529064 -0.25487864 -0.16128038\n",
            "  0.09212327 -0.20740917 -0.14310606 -0.2199964  -0.19635263 -0.01001559\n",
            " -0.09353122]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 66 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10881459 -0.09378321 -0.09736326  0.01375899 -0.06405196 -0.11869515\n",
            " -0.06706153 -0.06251028 -0.01414433 -0.30543536 -0.25485092 -0.16124001\n",
            "  0.09145273 -0.20735149 -0.14309561 -0.21998589 -0.19645241 -0.01016284\n",
            " -0.09344278]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 67 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10876501 -0.09389786 -0.09719429  0.01371738 -0.06391197 -0.11860939\n",
            " -0.06690846 -0.06243726 -0.01422837 -0.30554828 -0.25479758 -0.16118847\n",
            "  0.09075425 -0.20728141 -0.14308894 -0.21996883 -0.19654107 -0.01028242\n",
            " -0.09335101]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 68 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10871544 -0.09400038 -0.09701845  0.01367353 -0.0637744  -0.11852545\n",
            " -0.06676842 -0.0623684  -0.01430399 -0.30563265 -0.25472134 -0.16112696\n",
            "  0.0900311  -0.20720015 -0.14308569 -0.21994591 -0.19661975 -0.01037716\n",
            " -0.09325626]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 69 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10866591 -0.094092   -0.09683654  0.01362769 -0.06363907 -0.11844316\n",
            " -0.06664015 -0.06230333 -0.01437201 -0.30569154 -0.25462466 -0.16105658\n",
            "  0.08928631 -0.20710899 -0.14308552 -0.21991776 -0.19668946 -0.01044954\n",
            " -0.09315892]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 70 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10861648 -0.0941738  -0.09664932  0.01358007 -0.06350583 -0.11836246\n",
            " -0.0665225  -0.06224169 -0.0144332  -0.30572754 -0.25450963 -0.16097826\n",
            "  0.08852258 -0.20700897 -0.1430881  -0.21988492 -0.19675107 -0.01050184\n",
            " -0.0930593 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 71 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10856716 -0.09424675 -0.09645746  0.01353091 -0.06337454 -0.11828321\n",
            " -0.06641445 -0.06218316 -0.01448821 -0.3057431  -0.25437826 -0.16089287\n",
            "  0.0877424  -0.20690113 -0.14309318 -0.21984792 -0.19680539 -0.0105362\n",
            " -0.09295771]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 72 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10851795 -0.09431174 -0.09626156  0.01348038 -0.06324511 -0.11820532\n",
            " -0.06631505 -0.06212743 -0.01453763 -0.3057403  -0.25423247 -0.16080126\n",
            "  0.08694801 -0.20678633 -0.1431005  -0.21980721 -0.19685313 -0.01055438\n",
            " -0.0928544 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 73 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10846891 -0.09436958 -0.0960622   0.01342869 -0.06311739 -0.11812872\n",
            " -0.06622347 -0.06207429 -0.01458205 -0.30572113 -0.25407374 -0.16070414\n",
            "  0.08614149 -0.20666543 -0.14310977 -0.21976322 -0.19689503 -0.01055811\n",
            " -0.09274962]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 74 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10842004 -0.09442096 -0.09585986  0.01337598 -0.06299134 -0.11805332\n",
            " -0.06613898 -0.06202347 -0.01462195 -0.30568737 -0.2539037  -0.16060212\n",
            "  0.08532473 -0.2065391  -0.14312088 -0.21971633 -0.19693163 -0.01054889\n",
            " -0.0926436 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 75 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10837136 -0.09446655 -0.09565502  0.0133224  -0.06286684 -0.11797904\n",
            " -0.06606087 -0.06197479 -0.01465774 -0.3056406  -0.2537235  -0.16049582\n",
            "  0.0844994  -0.20640807 -0.14313355 -0.21966687 -0.19696344 -0.01052811\n",
            " -0.09253655]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 76 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1083229  -0.09450697 -0.09544811  0.01326806 -0.06274384 -0.11790586\n",
            " -0.06598856 -0.06192803 -0.01468986 -0.30558217 -0.25353447 -0.16038579\n",
            "  0.08366704 -0.20627293 -0.14314765 -0.21961518 -0.19699101 -0.01049699\n",
            " -0.09242864]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 77 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10827467 -0.09454269 -0.09523948  0.01321312 -0.06262226 -0.11783367\n",
            " -0.06592146 -0.06188305 -0.01471866 -0.30551338 -0.25333768 -0.16027251\n",
            "  0.0828291  -0.20613421 -0.14316303 -0.21956149 -0.19701472 -0.01045669\n",
            " -0.09232004]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 78 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10822668 -0.09457421 -0.0950295   0.01315764 -0.06250207 -0.11776248\n",
            " -0.06585908 -0.06183968 -0.01474447 -0.30543545 -0.25313413 -0.16015637\n",
            "  0.08198681 -0.2059924  -0.14317954 -0.21950611 -0.197035   -0.01040816\n",
            " -0.0922109 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 79 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10817894 -0.09460194 -0.09481847  0.01310176 -0.0623832  -0.11769222\n",
            " -0.06580099 -0.06179779 -0.01476759 -0.30534944 -0.25292468 -0.16003783\n",
            "  0.08114135 -0.20584798 -0.14319703 -0.21944922 -0.19705221 -0.01035236\n",
            " -0.0921014 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 80 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10812663 -0.09467848 -0.09491598  0.01307141 -0.06211959 -0.11747766\n",
            " -0.06538098 -0.06167359 -0.01477434 -0.30512464 -0.25260776 -0.15977977\n",
            "  0.08041238 -0.20527111 -0.13956939 -0.21943541 -0.19721696 -0.01030932\n",
            " -0.09221625]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 81 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10807499 -0.09474698 -0.09498271  0.0130382  -0.06187116 -0.11727791\n",
            " -0.0649998  -0.06155866 -0.01478028 -0.3049061  -0.25229603 -0.15953305\n",
            "  0.07966942 -0.20473376 -0.13629615 -0.21941608 -0.1973647  -0.01025856\n",
            " -0.09230905]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 82 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10802414 -0.09480799 -0.09502164  0.01300259 -0.06163722 -0.11709212\n",
            " -0.06465477 -0.06145246 -0.01478547 -0.30469394 -0.25199008 -0.15929739\n",
            "  0.07891669 -0.20423374 -0.13335219 -0.21939194 -0.19749671 -0.01020111\n",
            " -0.09238175]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 83 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10797401 -0.09486244 -0.09503573  0.01296475 -0.06141608 -0.11691872\n",
            " -0.0643418  -0.06135398 -0.01478999 -0.3044873  -0.25168914 -0.15907149\n",
            "  0.07815472 -0.20376682 -0.1307005  -0.21936348 -0.1976147  -0.01013763\n",
            " -0.09243649]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 84 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10792449 -0.0949109  -0.0950276   0.012925   -0.06120656 -0.11675652\n",
            " -0.06405777 -0.06126248 -0.0147939  -0.30428582 -0.25139284 -0.15885448\n",
            "  0.07738513 -0.20332983 -0.12831238 -0.21933115 -0.19772013 -0.01006883\n",
            " -0.09247519]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 85 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10787562 -0.09495402 -0.0949996   0.01288355 -0.06100762 -0.11660449\n",
            " -0.06379987 -0.06117731 -0.01479727 -0.30408913 -0.25110102 -0.15864563\n",
            "  0.07660928 -0.20291999 -0.12616192 -0.21929538 -0.1978142  -0.00999533\n",
            " -0.09249943]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 86 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10777701 -0.09506281 -0.09514448  0.01261156 -0.0599558  -0.11656423\n",
            " -0.06347285 -0.06112266 -0.01472628 -0.30391496 -0.25083113 -0.15825191\n",
            "  0.07583053 -0.20241827 -0.12412955 -0.21916679 -0.19806425 -0.00992559\n",
            " -0.09254891]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 87 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10763416 -0.09523004 -0.09544326  0.01213442 -0.05814527 -0.11662363\n",
            " -0.06308462 -0.06109527 -0.01458909 -0.30376112 -0.25058103 -0.15769395\n",
            "  0.07505006 -0.20183536 -0.12220454 -0.21895559 -0.19845307 -0.00985935\n",
            " -0.0926208 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 88 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10750122 -0.09538031 -0.09569255  0.01169866 -0.05650278 -0.11667158\n",
            " -0.0627328  -0.06106797 -0.01446523 -0.3036078  -0.25033152 -0.15717795\n",
            "  0.07426645 -0.20129433 -0.12047143 -0.21875873 -0.19880277 -0.00978844\n",
            " -0.09267526]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 89 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10737723 -0.09551526 -0.09589735  0.01130026 -0.05501194 -0.11670926\n",
            " -0.0624138  -0.06104077 -0.01435337 -0.30345494 -0.25008276 -0.1566999\n",
            "  0.07348069 -0.2007912  -0.11891136 -0.21857484 -0.19911718 -0.00971342\n",
            " -0.09271406]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 90 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10726137 -0.09563645 -0.09606221  0.01093563 -0.05365796 -0.11673773\n",
            " -0.06212444 -0.06101368 -0.01425236 -0.30330276 -0.24983479 -0.15625617\n",
            "  0.07269372 -0.20032239 -0.11750729 -0.21840268 -0.19939978 -0.00963484\n",
            " -0.09273885]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 91 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10715289 -0.09574522 -0.09619123  0.01060147 -0.05242749 -0.11675793\n",
            " -0.06186181 -0.06098672 -0.0141611  -0.30315125 -0.24958782 -0.15584353\n",
            "  0.07190634 -0.19988471 -0.11624396 -0.21824118 -0.19965371 -0.0095531\n",
            " -0.09275107]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 92 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10705109 -0.09584282 -0.09628811  0.01029485 -0.05130851 -0.11677074\n",
            " -0.0616233  -0.06095989 -0.01407868 -0.3030005  -0.24934196 -0.15545905\n",
            "  0.07111931 -0.19947518 -0.11510754 -0.21808937 -0.19988182 -0.0094686\n",
            " -0.09275207]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 93 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10695533 -0.09593035 -0.09635617  0.01001312 -0.05029012 -0.11677694\n",
            " -0.06140656 -0.0609332  -0.01400419 -0.30285054 -0.24909735 -0.15510005\n",
            "  0.07033334 -0.19909126 -0.11408549 -0.21794632 -0.20008665 -0.00938178\n",
            " -0.09274301]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 94 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10686509 -0.09600878 -0.09639841  0.00975388 -0.04936258 -0.11677722\n",
            " -0.06120946 -0.06090666 -0.01393688 -0.30270153 -0.24885409 -0.15476418\n",
            "  0.06954899 -0.1987305  -0.1131666  -0.21781126 -0.20027049 -0.00929294\n",
            " -0.09272498]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 95 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10677986 -0.09607907 -0.09641756  0.00951495 -0.04851703 -0.1167722\n",
            " -0.06103009 -0.06088027 -0.01387603 -0.3025534  -0.24861218 -0.15444925\n",
            "  0.06876685 -0.19839081 -0.11234074 -0.21768343 -0.2004354  -0.00920236\n",
            " -0.09269894]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 96 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10665585 -0.09620312 -0.09658191  0.00909685 -0.0470014  -0.11685134\n",
            " -0.06078688 -0.06087564 -0.01375723 -0.30242288 -0.2483887  -0.15398817\n",
            "  0.06799348 -0.19797076 -0.1115151  -0.2174853  -0.20072718 -0.00911799\n",
            " -0.09269927]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 97 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10654023 -0.09631453 -0.09671152  0.00871477 -0.04562571 -0.11691755\n",
            " -0.06056597 -0.06086905 -0.01364995 -0.30229175 -0.24816515 -0.15356055\n",
            "  0.06722261 -0.1975779  -0.11077322 -0.21730071 -0.20098947 -0.00903168\n",
            " -0.09268997]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 98 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10643222 -0.09641459 -0.09680998  0.00836524 -0.04437631 -0.11697215\n",
            " -0.06036517 -0.06086072 -0.01355307 -0.3021602  -0.2479417  -0.15316327\n",
            "  0.06645473 -0.19720969 -0.11010689 -0.21712843 -0.20122518 -0.0089437\n",
            " -0.09267206]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 99 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.1063311  -0.09650438 -0.09688054  0.0080451  -0.04324092 -0.11701634\n",
            " -0.06018252 -0.06085084 -0.01346557 -0.30202836 -0.2477186  -0.15279344\n",
            "  0.06569028 -0.1968638  -0.10950866 -0.21696728 -0.20143692 -0.00885434\n",
            " -0.09264649]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 100 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.10623625 -0.09658492 -0.09692609  0.00775151 -0.04220838 -0.11705117\n",
            " -0.06001627 -0.0608396  -0.01338653 -0.30189645 -0.24749602 -0.15244852\n",
            "  0.06492962 -0.19653821 -0.10897177 -0.21681625 -0.20162708 -0.00876381\n",
            " -0.0926141 ]\n",
            "Epsilon reduced to 0.07290000000000002\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10613254 -0.09665562 -0.09687774  0.00746249 -0.04123427 -0.11705782\n",
            " -0.05985864 -0.06081786 -0.01331458 -0.30171275 -0.24718937 -0.15208074\n",
            "  0.06389038 -0.1961769  -0.10850039 -0.21665174 -0.20179468 -0.008633\n",
            " -0.09253864]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10575647 -0.09668832 -0.09543952  0.00682343 -0.03968488 -0.11667806\n",
            " -0.05959705 -0.06061674 -0.01323821 -0.30053815 -0.24526063 -0.15086469\n",
            "  0.05745532 -0.19479498 -0.1082724  -0.2160613  -0.20188573 -0.00774727\n",
            " -0.09174946]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10551523 -0.0966448  -0.09418469  0.00619226 -0.03828677 -0.11629543\n",
            " -0.05950302 -0.06039907 -0.01315195 -0.29948467 -0.24345683 -0.14973913\n",
            "  0.05161759 -0.19354102 -0.10807076 -0.2155453  -0.2020253  -0.00601745\n",
            " -0.09101498]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10529484 -0.09660535 -0.09303893  0.00561924 -0.03701947 -0.11594648\n",
            " -0.05941699 -0.06020094 -0.01307406 -0.2985245  -0.2418135  -0.14871517\n",
            "  0.04630212 -0.19239959 -0.10789144 -0.21507551 -0.20215058 -0.00445143\n",
            " -0.09034528]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10509355 -0.09656961 -0.09199297  0.00509903 -0.03587077 -0.11562832\n",
            " -0.0593383  -0.06002059 -0.01300371 -0.2976495  -0.24031661 -0.14778377\n",
            "  0.04146264 -0.19136068 -0.10773197 -0.21464787 -0.20226315 -0.00303369\n",
            " -0.0897347 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10490972 -0.09653727 -0.09103833  0.00462682 -0.03482961 -0.1153383\n",
            " -0.05926633 -0.05985644 -0.01294019 -0.29685235 -0.2389534  -0.1469367\n",
            "  0.03705692 -0.1904153  -0.10759014 -0.21425867 -0.20236424 -0.00175012\n",
            " -0.08917812]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10474192 -0.09650804 -0.09016724  0.00419821 -0.03388597 -0.11507396\n",
            " -0.05920053 -0.05970706 -0.01288281 -0.29612625 -0.23771216 -0.14616641\n",
            "  0.03304658 -0.18955514 -0.10746402 -0.21390451 -0.20245512 -0.00058806\n",
            " -0.08867086]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10458878 -0.09648164 -0.08937261  0.00380923 -0.03303077 -0.11483312\n",
            " -0.05914038 -0.05957115 -0.01283101 -0.295465   -0.23658223 -0.14546609\n",
            "  0.02939666 -0.18877271 -0.10735181 -0.2135823  -0.20253679  0.00046399\n",
            " -0.08820865]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10444909 -0.09645782 -0.08864792  0.00345623 -0.03225579 -0.11461374\n",
            " -0.05908542 -0.0594475  -0.01278421 -0.294863   -0.23555382 -0.1448295\n",
            "  0.02607532 -0.18806112 -0.10725205 -0.21328923 -0.20261021  0.00141639\n",
            " -0.08778755]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10432167 -0.09643633 -0.08798719  0.00313594 -0.03155354 -0.11441395\n",
            " -0.05903522 -0.05933503 -0.01274196 -0.29431498 -0.23461807 -0.14425091\n",
            "  0.02305355 -0.18741408 -0.10716332 -0.21302271 -0.20267624  0.00227859\n",
            " -0.08740404]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10420552 -0.09641695 -0.08738499  0.00284538 -0.03091727 -0.11423206\n",
            " -0.05898938 -0.05923276 -0.01270382 -0.29381633 -0.23376688 -0.14372519\n",
            "  0.02030489 -0.18682584 -0.10708439 -0.21278039 -0.20273563  0.00305906\n",
            " -0.08705481]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10409966 -0.09639952 -0.08683633  0.00258182 -0.03034086 -0.11406653\n",
            " -0.05894753 -0.05913978 -0.01266937 -0.29336268 -0.23299274 -0.14324762\n",
            "  0.01780533 -0.18629126 -0.10701418 -0.21256012 -0.20278904  0.00376544\n",
            " -0.08673693]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10400323 -0.0963838  -0.08633663  0.0023428  -0.02981876 -0.11391591\n",
            " -0.05890935 -0.05905528 -0.01263826 -0.29295015 -0.23228899 -0.14281388\n",
            "  0.01553286 -0.18580556 -0.10695167 -0.21236    -0.20283708  0.00440478\n",
            " -0.08644766]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10391541 -0.09636965 -0.0858817   0.0021261  -0.0293459  -0.11377893\n",
            " -0.05887454 -0.05897849 -0.01261018 -0.29257506 -0.2316494  -0.14242008\n",
            "  0.01346751 -0.18536437 -0.10689604 -0.21217817 -0.20288035  0.00498335\n",
            " -0.08618452]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10383538 -0.0963569  -0.08546717  0.00192938 -0.02891715 -0.11365424\n",
            " -0.05884275 -0.05890866 -0.01258479 -0.29223377 -0.2310675  -0.14206216\n",
            "  0.01158851 -0.18496326 -0.10684648 -0.21201286 -0.2029193   0.00550754\n",
            " -0.08594494]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10413296 -0.09666191 -0.08540945  0.00199668 -0.02850728 -0.11322017\n",
            " -0.05891819 -0.05868354 -0.01263507 -0.2963504  -0.2312972  -0.14184523\n",
            "  0.0105874  -0.18521717 -0.10730514 -0.21212144 -0.2034779   0.00606266\n",
            " -0.08569416]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.1041256  -0.0969475  -0.08394944  0.00166947 -0.02745165 -0.1124506\n",
            " -0.05885982 -0.05828932 -0.01266746 -0.29907942 -0.22987862 -0.14077272\n",
            "  0.00418802 -0.18440813 -0.1079341  -0.21178481 -0.20395645  0.00730462\n",
            " -0.08471957]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 18 reward=-2 new_state=[0 1 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10224715 -0.09858377 -0.0818267   0.00241561 -0.02652546 -0.11252445\n",
            " -0.05609823 -0.05862525 -0.01303402 -0.30144417 -0.22982028 -0.14035368\n",
            " -0.00084662 -0.18381867 -0.10842935 -0.21115953 -0.20328309 -0.00808487\n",
            " -0.08428402]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 19 reward=-2 new_state=[0 1 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.10009994 -0.10148151 -0.08231606 -0.011254   -0.02163064 -0.11506595\n",
            " -0.05655266 -0.05911571 -0.01566059 -0.30289817 -0.22982551 -0.13807917\n",
            " -0.00616503 -0.18249011 -0.1090726  -0.212206   -0.20748568 -0.01989648\n",
            " -0.08521393]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 20 reward=-2 new_state=[0 1 0 0 1 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.09763344 -0.10414217 -0.07997657 -0.02432269 -0.01586235 -0.11662281\n",
            " -0.05671654 -0.05917642 -0.01799032 -0.30228266 -0.22664434 -0.13429907\n",
            " -0.02177262 -0.17926687 -0.11007775 -0.21229929 -0.21124178 -0.02904339\n",
            " -0.08455923]\n",
            "\n",
            "Taking action 8 from 8\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.09636413 -0.10531466 -0.07496069 -0.03205238 -0.02538615 -0.11634866\n",
            " -0.05848158 -0.05885521 -0.02133675 -0.3016256  -0.22387907 -0.13441509\n",
            " -0.03444844 -0.17861754 -0.11259054 -0.21403147 -0.21178125 -0.03723016\n",
            " -0.08349315]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.09470981 -0.10754326 -0.07125317 -0.04015543 -0.03456272 -0.11466639\n",
            " -0.06147641 -0.05878863 -0.04504285 -0.30114412 -0.22040156 -0.13589704\n",
            " -0.04585258 -0.17637196 -0.11493451 -0.2174047  -0.21463238 -0.0448451\n",
            " -0.08393557]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.09369895 -0.1089425  -0.06648496 -0.04546598 -0.05000334 -0.11231445\n",
            " -0.06496647 -0.0585517  -0.06691127 -0.30064952 -0.21732898 -0.13896474\n",
            " -0.05543691 -0.17547366 -0.11783265 -0.22125578 -0.21581909 -0.05164414\n",
            " -0.08410271]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.09256198 -0.11094964 -0.06339732 -0.05727109 -0.06191998 -0.11139929\n",
            " -0.06956613 -0.05844227 -0.08766891 -0.29987335 -0.21463178 -0.14081787\n",
            " -0.06444961 -0.17430104 -0.12053713 -0.22554596 -0.21927238 -0.05668808\n",
            " -0.08488669]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 1 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.08973762 -0.11419266 -0.05989719 -0.06687032 -0.07263698 -0.11125273\n",
            " -0.07105265 -0.05903815 -0.10667153 -0.29924852 -0.21355417 -0.14305787\n",
            " -0.07184866 -0.17351691 -0.12292919 -0.22915035 -0.22142762 -0.07649553\n",
            " -0.08598875]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.08653663 -0.11705726 -0.05658495 -0.07613102 -0.08208047 -0.11017968\n",
            " -0.07291746 -0.05927272 -0.12507418 -0.2986173  -0.21181253 -0.14427055\n",
            " -0.07777683 -0.17188765 -0.12403866 -0.23259851 -0.2236824  -0.09476517\n",
            " -0.09361938]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.08363713 -0.11962222 -0.05344739 -0.0844633  -0.09061469 -0.10922265\n",
            " -0.07459035 -0.05946591 -0.14162761 -0.29804003 -0.2102242  -0.14535348\n",
            " -0.0831392  -0.17041951 -0.1250558  -0.2356905  -0.22568713 -0.11125858\n",
            " -0.10050502]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.08101016 -0.12191802 -0.05047445 -0.0919597  -0.0983275  -0.10836969\n",
            " -0.07609062 -0.05962276 -0.15651989 -0.2975123  -0.20877501 -0.14631996\n",
            " -0.08799024 -0.16909659 -0.12598881 -0.23846227 -0.2274679  -0.12614724\n",
            " -0.10671806]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.07862964 -0.12397201 -0.04765683 -0.09870352 -0.10529827 -0.10761\n",
            " -0.07743568 -0.05974778 -0.16991961 -0.2970299  -0.20745221 -0.14718199\n",
            " -0.09237914 -0.16790457 -0.12684508 -0.24094614 -0.22904822 -0.1395864\n",
            " -0.11232392]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.07647197 -0.12580885 -0.04498592 -0.10476981 -0.11159872 -0.10693394\n",
            " -0.07864122 -0.05984497 -0.18197778 -0.29658914 -0.20624426 -0.14795035\n",
            " -0.09635037 -0.16683054 -0.12763135 -0.2431713  -0.23044914 -0.15171647\n",
            " -0.11738171]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.07451585 -0.12745075 -0.04245376 -0.11022614 -0.11729367 -0.10633276\n",
            " -0.07972135 -0.05991795 -0.19282983 -0.29618633 -0.20514071 -0.14863475\n",
            " -0.09994407 -0.16586287 -0.12835374 -0.24516393 -0.2316896  -0.16266447\n",
            " -0.12194487]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.07274204 -0.12891762 -0.0400529  -0.11513335 -0.12244166 -0.10579867\n",
            " -0.08068877 -0.05996986 -0.20259728 -0.29581824 -0.20413208 -0.14924386\n",
            " -0.10319658 -0.16499102 -0.12901777 -0.24694769 -0.23278669 -0.1725452\n",
            " -0.12606163]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.07113318 -0.13022745 -0.03777646 -0.11954626 -0.1270956  -0.10532462\n",
            " -0.08155492 -0.06000356 -0.21138921 -0.29548192 -0.20320979 -0.14978558\n",
            " -0.10614067 -0.16420561 -0.12962851 -0.2485438  -0.23375568 -0.18146238\n",
            " -0.12977555]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.06967355 -0.13139635 -0.03561798 -0.12351422 -0.13130322 -0.10490428\n",
            " -0.08233011 -0.06002155 -0.21930356 -0.29517463 -0.20236608 -0.15026692\n",
            " -0.10880597 -0.16349804 -0.13019054 -0.24997142 -0.23461023 -0.18950985\n",
            " -0.133126  ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.068349   -0.13243893 -0.03357145 -0.12708169 -0.1351077  -0.10453199\n",
            " -0.08302359 -0.06002604 -0.2264283  -0.29489383 -0.20159385 -0.15069424\n",
            " -0.11121924 -0.16286066 -0.13070798 -0.25124776 -0.23536271 -0.1967722\n",
            " -0.13614848]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.06714672 -0.1333682  -0.03163119 -0.13028869 -0.13854796 -0.10420265\n",
            " -0.08364372 -0.06001905 -0.23284249 -0.29463717 -0.20088674 -0.15107319\n",
            " -0.11340465 -0.16228653 -0.13118465 -0.2523883  -0.2360242  -0.20332603\n",
            " -0.13887507]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.06605512 -0.13419594 -0.0297919  -0.13317126 -0.14165919 -0.10391168\n",
            " -0.08419799 -0.06000229 -0.23861718 -0.29440257 -0.200239   -0.15140891\n",
            " -0.11538404 -0.16176939 -0.13162398 -0.253407   -0.23660457 -0.20924038\n",
            " -0.1413347 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.06506374 -0.13493268 -0.02804857 -0.13576192 -0.14447317 -0.10365497\n",
            " -0.08469315 -0.05997733 -0.24381623 -0.29418808 -0.1996453  -0.151706\n",
            " -0.11717714 -0.1613036  -0.1320291  -0.25431636 -0.23711276 -0.21457767\n",
            " -0.1435535 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.06416316 -0.13558793 -0.02639645 -0.13808985 -0.14701858 -0.1034288\n",
            " -0.0851353  -0.05994552 -0.24849716 -0.29399198 -0.19910097 -0.15196854\n",
            " -0.11880177 -0.16088408 -0.13240285 -0.2551277  -0.23755667 -0.21939415\n",
            " -0.14555502]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.0633448  -0.13617024 -0.02483108 -0.14018139 -0.14932133 -0.1032299\n",
            " -0.08552988 -0.05990809 -0.25271165 -0.2938126  -0.19860157 -0.15220025\n",
            " -0.12027401 -0.16050631 -0.13274787 -0.25585112 -0.23794353 -0.22374071\n",
            " -0.14736056]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.06260096 -0.13668726 -0.02334822 -0.1420603  -0.15140483 -0.10305528\n",
            " -0.0858818  -0.05986607 -0.25650612 -0.29364854 -0.19814327 -0.15240447\n",
            " -0.12160842 -0.1601661  -0.13306648 -0.25649577 -0.23827967 -0.22766326\n",
            " -0.1489893 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.06192465 -0.13714594 -0.02194383 -0.14374785 -0.15329017 -0.10290228\n",
            " -0.0861955  -0.05982039 -0.25992247 -0.2934984  -0.19772246 -0.1525842\n",
            " -0.12281814 -0.15985976 -0.13336083 -0.2570698  -0.23857084 -0.23120314\n",
            " -0.15045854]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.06130957 -0.13755238 -0.0206141  -0.14526333 -0.15499642 -0.10276851\n",
            " -0.08647494 -0.0597719  -0.2629984  -0.29336095 -0.19733588 -0.15274204\n",
            " -0.12391503 -0.15958391 -0.13363294 -0.25758064 -0.23882216 -0.23439783\n",
            " -0.15178394]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.06075    -0.13791218 -0.01935542 -0.146624   -0.15654083 -0.10265183\n",
            " -0.08672369 -0.05972126 -0.2657677  -0.29323515 -0.19698066 -0.15288043\n",
            " -0.12490983 -0.15933558 -0.13388455 -0.25803488 -0.23903826 -0.23728096\n",
            " -0.15297958]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.06024078 -0.13823035 -0.0181643  -0.14784548 -0.15793894 -0.10255034\n",
            " -0.08694496 -0.05966911 -0.26826096 -0.29311994 -0.19665399 -0.15300153\n",
            " -0.1258122  -0.159112   -0.13411734 -0.25843847 -0.23922315 -0.2398831\n",
            " -0.15405816]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05977723 -0.13851131 -0.01703746 -0.14894179 -0.15920478 -0.10246231\n",
            " -0.08714165 -0.05961598 -0.2705056  -0.2930144  -0.19635355 -0.15310726\n",
            " -0.12663093 -0.15891072 -0.13433278 -0.25879675 -0.23938063 -0.24223155\n",
            " -0.15503116]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05935514 -0.13875912 -0.01597175 -0.14992556 -0.16035107 -0.10238619\n",
            " -0.08731632 -0.05956236 -0.27252644 -0.29291767 -0.19607714 -0.15319932\n",
            " -0.12737392 -0.15872955 -0.13453223 -0.25911453 -0.23951387 -0.24435122\n",
            " -0.15590893]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05897067 -0.13897736 -0.01496419 -0.15080813 -0.1613892  -0.10232064\n",
            " -0.08747132 -0.0595086  -0.2743456  -0.29282904 -0.19582266 -0.15327924\n",
            " -0.12804832 -0.15856647 -0.13471697 -0.2593961  -0.23962578 -0.24626437\n",
            " -0.15670079]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05862037 -0.13916926 -0.01401194 -0.1515998  -0.16232955 -0.10226442\n",
            " -0.08760873 -0.05945509 -0.2759833  -0.29274777 -0.19558828 -0.15334846\n",
            " -0.1286606  -0.15841971 -0.13488813 -0.25964534 -0.23971903 -0.24799128\n",
            " -0.15741518]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05830111 -0.13933775 -0.01311227 -0.1523097  -0.16318148 -0.10221642\n",
            " -0.08773043 -0.05940208 -0.27745745 -0.29267323 -0.19537237 -0.15340817\n",
            " -0.12921663 -0.15828761 -0.13504678 -0.25986576 -0.2397959  -0.24955004\n",
            " -0.15805969]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 51 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05801004 -0.13948537 -0.01226254 -0.15294617 -0.16395344 -0.10217567\n",
            " -0.08783811 -0.05934987 -0.2787844  -0.29260486 -0.19517338 -0.15345952\n",
            " -0.12972166 -0.15816878 -0.13519385 -0.26006037 -0.23985845 -0.2509571\n",
            " -0.15864113]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 52 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05774458 -0.13961448 -0.01146034 -0.15351665 -0.16465303 -0.1021413\n",
            " -0.08793327 -0.05929861 -0.2799788  -0.29254213 -0.19498995 -0.15350342\n",
            " -0.13018048 -0.15806183 -0.13533026 -0.26023206 -0.23990855 -0.25222737\n",
            " -0.15916573]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 53 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05750244 -0.13972716 -0.01070326 -0.15402788 -0.16528712 -0.10211253\n",
            " -0.08801727 -0.05924849 -0.2810538  -0.29248452 -0.19482078 -0.15354082\n",
            " -0.13059741 -0.15796563 -0.13545679 -0.2603833  -0.2399478  -0.25337407\n",
            " -0.15963902]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 54 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05728147 -0.13982522 -0.00998906 -0.15448585 -0.16586196 -0.10208867\n",
            " -0.08809131 -0.05919965 -0.2820213  -0.29243165 -0.19466472 -0.15357243\n",
            " -0.13097638 -0.15787907 -0.13557416 -0.26051635 -0.23997769 -0.25440934\n",
            " -0.16006604]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 55 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05707978 -0.13991041 -0.00931558 -0.15489605 -0.16638319 -0.10206909\n",
            " -0.08815651 -0.0591522  -0.28289208 -0.29238307 -0.19452071 -0.15359902\n",
            " -0.1313209  -0.1578012  -0.13568309 -0.26063317 -0.23999944 -0.25534397\n",
            " -0.16045132]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 56 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05689563 -0.13998412 -0.00868078 -0.15526333 -0.16685584 -0.10205324\n",
            " -0.08821382 -0.05910623 -0.28367564 -0.2923385  -0.19438776 -0.15362121\n",
            " -0.13163418 -0.15773118 -0.13578422 -0.2607356  -0.24001423 -0.25618786\n",
            " -0.16079895]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 57 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05672744 -0.14004777 -0.00808267 -0.15559208 -0.16728456 -0.10204066\n",
            " -0.08826412 -0.05906181 -0.2843808  -0.29229748 -0.19426504 -0.15363953\n",
            " -0.13191913 -0.15766817 -0.13587809 -0.26082525 -0.2400231  -0.25694984\n",
            " -0.16111264]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 58 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.0565738  -0.14010248 -0.00751939 -0.15588626 -0.16767347 -0.10203089\n",
            " -0.08830819 -0.05901897 -0.28501523 -0.2922598  -0.1941517  -0.15365446\n",
            " -0.13217834 -0.15761153 -0.13596524 -0.26090354 -0.24002686 -0.2576378\n",
            " -0.16139565]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 59 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.0564334  -0.14014934 -0.00698914 -0.15614945 -0.16802633 -0.10202354\n",
            " -0.08834673 -0.05897777 -0.28558612 -0.29222518 -0.194047   -0.15366647\n",
            " -0.1324142  -0.1575606  -0.13604616 -0.26097178 -0.24002638 -0.25825912\n",
            " -0.16165106]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 60 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05630508 -0.14018929 -0.00649026 -0.15638483 -0.16834654 -0.10201828\n",
            " -0.08838038 -0.05893821 -0.28609973 -0.29219332 -0.1939503  -0.15367594\n",
            " -0.13262887 -0.15751481 -0.13612132 -0.26103115 -0.24002233 -0.25882018\n",
            " -0.1618815 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 61 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05618777 -0.14022316 -0.00602107 -0.15659523 -0.16863716 -0.10201482\n",
            " -0.08840968 -0.0589003  -0.2865618  -0.29216403 -0.1938609  -0.15368322\n",
            " -0.13282429 -0.15747364 -0.13619108 -0.26108262 -0.24001527 -0.25932688\n",
            " -0.16208947]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 62 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05608049 -0.14025176 -0.00558008 -0.15678331 -0.16890097 -0.10201287\n",
            " -0.08843514 -0.05886403 -0.28697747 -0.2921371  -0.19377834 -0.15368861\n",
            " -0.1330022  -0.15743664 -0.13625589 -0.26112714 -0.24000579 -0.25978452\n",
            " -0.16227715]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 63 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05598237 -0.14027564 -0.00516576 -0.15695131 -0.16914047 -0.1020122\n",
            " -0.08845721 -0.05882941 -0.2873514  -0.2921123  -0.19370204 -0.15369236\n",
            " -0.13316423 -0.15740338 -0.13631608 -0.2611655  -0.23999429 -0.26019782\n",
            " -0.16244651]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 64 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05589262 -0.14029549 -0.00477672 -0.1571014  -0.16935794 -0.10201263\n",
            " -0.08847629 -0.05879639 -0.28768766 -0.2920895  -0.19363149 -0.15369475\n",
            " -0.1333118  -0.15737349 -0.13637194 -0.2611985  -0.2399813  -0.26057112\n",
            " -0.16259935]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 65 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05581049 -0.14031178 -0.00441162 -0.15723538 -0.16955543 -0.10201396\n",
            " -0.08849272 -0.05876496 -0.28799024 -0.29206854 -0.19356628 -0.15369596\n",
            " -0.13344622 -0.1573466  -0.13642383 -0.2612267  -0.23996705 -0.2609083\n",
            " -0.1627373 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 66 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05573534 -0.14032504 -0.00406917 -0.15735497 -0.16973482 -0.10201602\n",
            " -0.08850685 -0.05873509 -0.28826225 -0.2920492  -0.193506   -0.15369621\n",
            " -0.1335687  -0.15732248 -0.13647199 -0.2612508  -0.23995188 -0.26121294\n",
            " -0.16286178]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 8\n",
            "\n",
            "Step 67 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05566655 -0.14033562 -0.00374818 -0.15746167 -0.16989775 -0.10201868\n",
            " -0.08851891 -0.05870673 -0.2885069  -0.29203147 -0.19345029 -0.15369561\n",
            " -0.13368031 -0.15730077 -0.13651672 -0.2612712  -0.23993605 -0.26148808\n",
            " -0.16297413]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 68 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05572315 -0.13990262 -0.00325292 -0.1572034  -0.16988142 -0.10245675\n",
            " -0.0881586  -0.05866867 -0.28181267 -0.2918642  -0.19360009 -0.15322252\n",
            " -0.13381457 -0.15773141 -0.13648167 -0.26069975 -0.23910517 -0.2614944\n",
            " -0.1626888 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 69 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05577333 -0.13951121 -0.00279689 -0.1569697  -0.16986771 -0.10285229\n",
            " -0.08783328 -0.05863321 -0.27578956 -0.29171312 -0.19373403 -0.15279539\n",
            " -0.13393655 -0.15811965 -0.1364511  -0.26018357 -0.23835438 -0.2615004\n",
            " -0.16243169]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 70 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05581785 -0.13915743 -0.00237752 -0.15675823 -0.16985616 -0.10320941\n",
            " -0.08753955 -0.05860025 -0.27036917 -0.2915767  -0.1938538  -0.15240982\n",
            " -0.13404733 -0.1584697  -0.13642444 -0.25971738 -0.23767614 -0.26150617\n",
            " -0.16219997]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 71 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05585739 -0.13883771 -0.00199248 -0.15656693 -0.1698465  -0.10353182\n",
            " -0.08727439 -0.05856972 -0.2654905  -0.2914535  -0.19396096 -0.15206178\n",
            " -0.13414788 -0.15878528 -0.13640115 -0.25929636 -0.23706344 -0.26151156\n",
            " -0.16199118]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 72 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.0558925  -0.13854887 -0.00163948 -0.15639393 -0.16983837 -0.10382284\n",
            " -0.08703505 -0.05854149 -0.26109868 -0.29134226 -0.19405691 -0.15174761\n",
            " -0.1342391  -0.1590698  -0.13638084 -0.2589162  -0.2365101  -0.2615167\n",
            " -0.16180299]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 73 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05592374 -0.1382879  -0.00131643 -0.15623751 -0.16983154 -0.10408553\n",
            " -0.08681901 -0.05851545 -0.2571446  -0.29124182 -0.19414282 -0.1514641\n",
            " -0.13432181 -0.15932631 -0.13636301 -0.258573   -0.2360104  -0.2615215\n",
            " -0.16163342]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 74 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05595157 -0.13805223 -0.00102133 -0.15609615 -0.16982579 -0.10432258\n",
            " -0.08662406 -0.05849154 -0.25358424 -0.2911512  -0.19421983 -0.15120825\n",
            " -0.13439675 -0.1595576  -0.13634735 -0.25826317 -0.23555928 -0.26152596\n",
            " -0.16148058]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 75 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05597639 -0.1378394  -0.00075229 -0.15596843 -0.16982093 -0.10453647\n",
            " -0.08644816 -0.0584696  -0.25037804 -0.29106936 -0.19428888 -0.15097743\n",
            " -0.1344646  -0.15976606 -0.13633358 -0.2579836  -0.23515207 -0.26153013\n",
            " -0.16134283]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 76 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05599857 -0.13764736 -0.00050753 -0.1558531  -0.16981676 -0.10472943\n",
            " -0.08628948 -0.05844957 -0.2474905  -0.29099557 -0.19435088 -0.1507692\n",
            " -0.134526   -0.15995404 -0.13632143 -0.25773132 -0.23478457 -0.2615339\n",
            " -0.16121869]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 77 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.6018434e-02 -1.3747403e-01 -2.8543174e-04 -1.5574896e-01\n",
            " -1.6981317e-01 -1.0490348e-01 -8.6146355e-02 -5.8431327e-02\n",
            " -2.4488974e-01 -2.9092896e-01 -1.9440657e-01 -1.5058140e-01\n",
            " -1.3458148e-01 -1.6012345e-01 -1.3631064e-01 -2.5750375e-01\n",
            " -2.3445305e-01 -2.6153743e-01 -1.6110680e-01]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 78 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.6036264e-02 -1.3731772e-01 -8.4400177e-05 -1.5565501e-01\n",
            " -1.6981000e-01 -1.0506043e-01 -8.6017281e-02 -5.8414780e-02\n",
            " -2.4254715e-01 -2.9086891e-01 -1.9445670e-01 -1.5041205e-01\n",
            " -1.3463157e-01 -1.6027620e-01 -1.3630098e-01 -2.5729850e-01\n",
            " -2.3415405e-01 -2.6154053e-01 -1.6100593e-01]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 79 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.6052312e-02 -1.3717678e-01  9.7036362e-05 -1.5557030e-01\n",
            " -1.6980718e-01 -1.0520193e-01 -8.5900933e-02 -5.8399819e-02\n",
            " -2.4043688e-01 -2.9081479e-01 -1.9450185e-01 -1.5025938e-01\n",
            " -1.3467675e-01 -1.6041389e-01 -1.3629232e-01 -2.5711346e-01\n",
            " -2.3388445e-01 -2.6154342e-01 -1.6091502e-01]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 80 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-5.60668074e-02 -1.37049749e-01  2.60271132e-04 -1.55493975e-01\n",
            " -1.69804603e-01 -1.05329454e-01 -8.57960582e-02 -5.83863817e-02\n",
            " -2.38535926e-01 -2.90766001e-01 -1.94542587e-01 -1.50121763e-01\n",
            " -1.34717464e-01 -1.60538003e-01 -1.36284471e-01 -2.56946683e-01\n",
            " -2.33641505e-01 -2.61545986e-01 -1.60833061e-01]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 81 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05607994 -0.13693535  0.00040659 -0.15542524 -0.16980219 -0.10544437\n",
            " -0.08570157 -0.05837437 -0.2368233  -0.290722   -0.19457941 -0.14999777\n",
            " -0.13475406 -0.16064988 -0.13627732 -0.25679645 -0.23342264 -0.26154822\n",
            " -0.16075917]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 82 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05609189 -0.13683234  0.0005372  -0.15536338 -0.1697999  -0.10554787\n",
            " -0.08561645 -0.05836367 -0.23528042 -0.29068244 -0.19461271 -0.1498861\n",
            " -0.13478693 -0.16075072 -0.13627073 -0.25666115 -0.23322555 -0.26155022\n",
            " -0.16069254]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 83 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05610279 -0.1367397   0.00065329 -0.15530777 -0.16979766 -0.10564105\n",
            " -0.08553982 -0.05835423 -0.23389037 -0.2906468  -0.19464295 -0.14978552\n",
            " -0.13481641 -0.16084161 -0.13626462 -0.25653937 -0.2330482  -0.26155192\n",
            " -0.16063246]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 84 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.0561128  -0.13665642  0.00075592 -0.15525782 -0.16979545 -0.10572492\n",
            " -0.08547086 -0.05834596 -0.23263796 -0.29061472 -0.19467041 -0.14969501\n",
            " -0.13484277 -0.16092354 -0.1362589  -0.2564298  -0.2328887  -0.26155332\n",
            " -0.1605783 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 85 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05612201 -0.13658157  0.00084609 -0.15521301 -0.1697932  -0.10580036\n",
            " -0.08540879 -0.05833877 -0.2315096  -0.2905859  -0.19469544 -0.14961357\n",
            " -0.13486631 -0.16099735 -0.1362535  -0.25633126 -0.23274532 -0.26155448\n",
            " -0.16052943]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 86 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05613054 -0.1365144   0.00092476 -0.15517285 -0.16979092 -0.10586819\n",
            " -0.08535301 -0.05833259 -0.23049296 -0.29055998 -0.1947183  -0.14954033\n",
            " -0.13488728 -0.1610639  -0.13624837 -0.25624272 -0.23261651 -0.2615554\n",
            " -0.16048537]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 87 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05613847 -0.13645416  0.00099283 -0.15513691 -0.16978857 -0.10592914\n",
            " -0.08530287 -0.05832736 -0.22957699 -0.2905367  -0.19473921 -0.1494745\n",
            " -0.13490589 -0.16112384 -0.13624343 -0.2561632  -0.23250091 -0.26155606\n",
            " -0.16044562]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 88 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05614588 -0.13640024  0.00105111 -0.15510479 -0.16978614 -0.1059839\n",
            " -0.08525784 -0.05832301 -0.22875172 -0.29051584 -0.19475845 -0.14941534\n",
            " -0.13492236 -0.16117787 -0.13623865 -0.25609186 -0.23239726 -0.26155657\n",
            " -0.16040975]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 89 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05615284 -0.13635196  0.00110039 -0.15507613 -0.16978362 -0.10603303\n",
            " -0.08521742 -0.05831946 -0.22800821 -0.29049706 -0.1947761  -0.14936227\n",
            " -0.1349369  -0.16122656 -0.136234   -0.2560279  -0.23230436 -0.26155683\n",
            " -0.16037737]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 90 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05615939 -0.1363088   0.00114139 -0.1550506  -0.169781   -0.10607709\n",
            " -0.08518117 -0.05831666 -0.2273383  -0.2904803  -0.19479245 -0.14931461\n",
            " -0.13494965 -0.16127041 -0.13622941 -0.2559706  -0.23222128 -0.26155695\n",
            " -0.16034815]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 91 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.0561656  -0.13627033  0.00117484 -0.15502791 -0.16977829 -0.10611656\n",
            " -0.08514868 -0.05831456 -0.22673477 -0.2904653  -0.19480756 -0.1492719\n",
            " -0.13496083 -0.16130991 -0.13622494 -0.2559193  -0.23214695 -0.2615568\n",
            " -0.16032179]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 92 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05617151 -0.13623606  0.00120132 -0.1550078  -0.16977544 -0.10615191\n",
            " -0.08511959 -0.05831309 -0.22619106 -0.29045182 -0.19482157 -0.14923364\n",
            " -0.13497052 -0.16134551 -0.13622051 -0.25587347 -0.23208061 -0.2615566\n",
            " -0.16029796]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 93 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05617716 -0.13620557  0.00122146 -0.15499    -0.16977252 -0.10618353\n",
            " -0.08509357 -0.0583122  -0.22570124 -0.29043984 -0.19483465 -0.14919941\n",
            " -0.13497889 -0.16137758 -0.13621613 -0.25583255 -0.23202148 -0.26155615\n",
            " -0.16027646]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 94 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05618258 -0.1361785   0.00123581 -0.15497431 -0.16976948 -0.10621177\n",
            " -0.08507031 -0.05831185 -0.22525997 -0.29042912 -0.19484688 -0.14916879\n",
            " -0.13498607 -0.16140647 -0.13621178 -0.25579607 -0.23196882 -0.26155564\n",
            " -0.16025703]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 95 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05618778 -0.13615452  0.0012449  -0.15496051 -0.16976637 -0.10623698\n",
            " -0.08504957 -0.05831199 -0.22486252 -0.29041958 -0.19485831 -0.14914146\n",
            " -0.13499215 -0.16143247 -0.13620745 -0.25576356 -0.23192206 -0.2615549\n",
            " -0.16023947]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 96 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05619281 -0.13613339  0.00124919 -0.15494841 -0.16976316 -0.10625944\n",
            " -0.08503106 -0.05831256 -0.2245045  -0.2904111  -0.19486907 -0.14911708\n",
            " -0.13499725 -0.16145588 -0.13620313 -0.25573468 -0.23188058 -0.26155412\n",
            " -0.1602236 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 97 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05619767 -0.13611473  0.00124916 -0.1549379  -0.16975987 -0.10627943\n",
            " -0.0850146  -0.05831354 -0.22418202 -0.29040357 -0.19487923 -0.14909536\n",
            " -0.13500147 -0.16147698 -0.1361988  -0.25570908 -0.23184389 -0.2615532\n",
            " -0.16020927]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 98 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05620239 -0.13609834  0.00124522 -0.15492871 -0.1697565  -0.10629718\n",
            " -0.08499998 -0.05831489 -0.22389156 -0.29039693 -0.1948888  -0.14907601\n",
            " -0.13500488 -0.16149595 -0.1361945  -0.2556864  -0.2318115  -0.2615522\n",
            " -0.16019629]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 99 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05620697 -0.13608404  0.00123778 -0.15492083 -0.16975306 -0.10631293\n",
            " -0.084987   -0.05831657 -0.22363004 -0.290391   -0.19489789 -0.14905886\n",
            " -0.13500758 -0.16151303 -0.1361902  -0.25566638 -0.23178294 -0.26155114\n",
            " -0.16018455]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 100 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.05621144 -0.13607153  0.00122716 -0.15491408 -0.16974957 -0.10632687\n",
            " -0.0849755  -0.05831852 -0.22339445 -0.2903858  -0.1949065  -0.14904365\n",
            " -0.13500962 -0.16152841 -0.13618591 -0.25564873 -0.23175794 -0.26155\n",
            " -0.16017394]\n",
            "Epsilon reduced to 0.06561000000000002\n",
            "Total reward: -100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEz_2JAate0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6500c4a0-c08f-45b8-f7e8-ef6b5660b1d4"
      },
      "source": [
        "env = GemelEnv(interval=10, max_steps=50, actions=GemelEnv.ActionSpace.DOUBLE_BUTTON)\n",
        "env.reset()\n",
        "agent = DQNAgent(env, max_eps=10, period=5, state_mode=DQNAgent.StateModel.IDS, gamma=0.8, model=model_conv_27(env), max_epsilon=0.1, epsilon_decay=0.9)\n",
        "hist = agent.train()\n",
        "flat_hist = [x for h in hist for x in h]\n",
        "ticks = [idx for idx, x in enumerate(flat_hist) if x[\"random\"]]\n",
        "for xc in ticks: plt.axvline(x=xc, color='y')\n",
        "plt.plot([x['reward'] for x in flat_hist])\n",
        "agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_9 (Reshape)          (None, 189, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 187, 16)           64        \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 2992)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 24)                71832     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 19)                475       \n",
            "=================================================================\n",
            "Total params: 72,371\n",
            "Trainable params: 72,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\r |██████████------------------------------------------------------------------------------------------| 10.0% \r\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.07631966 -0.04679336  0.06042093  0.06511348  0.03564708 -0.0852726\n",
            " -0.00834369 -0.15929882  0.12203674 -0.10070121 -0.01516002 -0.07825407\n",
            " -0.12234782  0.23591337 -0.03371555 -0.13318712  0.17057656  0.04394283\n",
            "  0.08444654]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.5859683  -0.8066599   0.52476454  0.06703179 -0.44230524 -0.1451096\n",
            " -0.38168025  0.2546343  -0.78369695  0.21788482  0.6998096  -0.02655577\n",
            " -0.16758783  0.02865432  0.15800825 -0.2922127  -0.22771384  0.32658663\n",
            " -0.16213475]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.9512904  -0.7466583   0.8050387   0.29206628 -0.61881524 -0.40228465\n",
            "  0.11859128  0.64229    -0.3852902   0.6679757  -1.3265878   0.7484964\n",
            " -0.90274686  0.12781624 -0.24966842 -0.7605361   0.15570323  0.61597025\n",
            "  0.588669  ]\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 4 reward=-2 new_state=[0 0 0 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.881358   -0.6028958  -0.18166238  0.19658141 -0.8745383  -0.4217844\n",
            "  0.24577403  0.8108028  -0.613733    0.35325122 -1.0992827   0.868904\n",
            " -0.6647242   0.2044763  -0.01651476 -0.31013393  0.12517829  0.36411607\n",
            "  0.54050595]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 5 reward=-3 new_state=[0 0 1 0 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-2.8902056  -1.2322309  -0.264212    0.25657314 -1.0088738  -0.54400635\n",
            "  0.2632898   1.020902   -1.0252283   0.26667342 -0.74598014  0.27446786\n",
            " -0.90281224  0.2586272   0.5617688  -0.11769614  0.07653023  0.49677446\n",
            "  0.22258657]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 7\n",
            "\n",
            "Step 6 reward=-4 new_state=[0 0 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.539299   -0.2339599  -0.21541815  0.10678543 -0.16172765 -0.23649018\n",
            "  0.14152047  0.24810195 -0.19808476  0.08158699 -0.36644286 -0.0149257\n",
            " -0.33268163  0.05028537  0.2108983  -0.01295851  0.17435485  0.11133161\n",
            " -0.01266797]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 9 from 11\n",
            "\n",
            "Step 7 reward=-4 new_state=[0 0 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7382728  -0.49464673 -0.63468075  0.16137871 -0.45602024 -0.6610418\n",
            "  0.15560606  0.01467677 -0.3078879  -0.12238535 -0.3990465  -0.39986265\n",
            " -0.5579448   0.38312978  0.4841332  -0.1012964   0.31822768 -0.1533614\n",
            " -0.1512289 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 8 reward=-3 new_state=[0 0 0 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.6873422  -0.98098546 -0.69165665  0.3397574  -0.9046428  -1.2212179\n",
            "  0.0843285  -0.6628572  -0.6888205  -0.15443374 -0.44680625 -1.0867743\n",
            " -0.9926581   1.2678838   0.731089   -0.3858331   0.12016329 -0.28135785\n",
            " -0.22715063]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 9 reward=-2 new_state=[0 0 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.95614046 -0.56155276 -0.45508906  0.20426963 -0.52494925 -0.68461794\n",
            "  0.12069016 -0.39418483 -0.46544844  0.01751794 -0.3583673  -0.8593241\n",
            " -0.645897    0.7057404   0.53836745 -0.10972529  0.20856385 -0.21681738\n",
            " -0.26581457]\n",
            "\n",
            "Taking action 10 from 14\n",
            "\n",
            "Step 10 reward=-2 new_state=[0 0 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6397265  -0.409946   -0.20883739  0.22732142 -0.44897234 -0.5380339\n",
            "  0.21677715 -0.19860046 -0.31836948  0.02447838 -0.33844003 -0.7951402\n",
            " -0.6916195   0.22578055  0.464938   -0.1818394   0.11208623 -0.33848944\n",
            " -0.21271557]\n",
            "\n",
            "Taking action 10 from 14\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.35594857 -0.29949817 -0.29670367  0.35546818 -0.5109671  -0.6593734\n",
            "  0.19310433 -0.26789895 -0.29326755  0.03564373 -0.47951627 -0.7826468\n",
            " -0.64454466  0.18431053  0.17033987 -0.27125788  0.3011531  -0.38414818\n",
            " -0.06175298]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 12 reward=-3 new_state=[0 1 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.33296126 -0.2980051  -0.29269832  0.40214908 -0.6564798  -0.77291536\n",
            "  0.09707421 -0.40180105 -0.30741876  0.05981965 -0.48499757 -0.80793566\n",
            " -0.5958215   0.19727717 -0.05936741 -0.38074735  0.33005294 -0.4540748\n",
            "  0.00330409]\n",
            "\n",
            "Taking action 12 from 16\n",
            "\n",
            "Step 13 reward=-3 new_state=[0 1 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5303244  -0.54021955 -0.48323727  0.24315755 -0.87595886 -0.9282754\n",
            "  0.09697758 -0.38672286 -0.40367258 -0.03474866 -0.263505   -1.0476154\n",
            " -0.6244669   0.12011451  0.14667574 -0.28338253  0.28795394 -0.6941821\n",
            " -0.26266474]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 14 reward=-3 new_state=[0 1 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8226478  -0.5506822  -0.21004266  0.246607   -1.0693942  -0.96367776\n",
            "  0.05487382 -0.697251   -0.46790135 -0.00486819 -0.35379016 -1.2953002\n",
            " -0.9063746   0.10593492  0.01016641 -0.5982908   0.02270336 -0.77423376\n",
            " -0.14511481]\n",
            "\n",
            "Taking action 15 from 13\n",
            "\n",
            "Step 15 reward=-3 new_state=[0 1 0 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.9667771  -0.6329547  -0.22097175 -0.01348364 -1.0298942  -0.9036661\n",
            " -0.00951917 -0.85619336 -0.55384666 -0.06827537 -0.07177365 -1.2498717\n",
            " -0.5707477   0.1640114  -0.20066966 -0.47829512 -0.19124304 -0.7305685\n",
            " -0.16561067]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 16 reward=-2 new_state=[0 1 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.6968411  -0.42707115  0.02568075 -0.0283989  -0.7775101  -0.52777237\n",
            "  0.08703046 -0.9176782  -0.41111803 -0.06112641 -0.16643016 -1.4887686\n",
            " -0.71800834  0.0906744  -0.20038253 -0.4713785  -0.20697947 -0.7401925\n",
            " -0.16081625]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.38090897 -0.43938375 -0.32482395 -0.25121716 -0.9579435  -0.8627332\n",
            " -0.02944236 -0.5549851  -0.27385688 -0.15873091 -0.12804812 -1.049508\n",
            " -0.4734432  -0.08885404 -0.23793282 -0.45869052 -0.23475644 -0.9251417\n",
            " -0.12504971]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.1090184  -0.6167807   0.11741247 -0.27350834 -0.7708506  -0.5134845\n",
            " -0.17350182 -0.6326638  -0.51499397  0.06910954  0.00869367 -1.0725608\n",
            " -0.441318   -0.16624182 -0.33977157 -0.41302025 -0.42591178 -0.50356615\n",
            " -0.17483415]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.8000619  -0.7304238  -0.6121878  -0.4639513  -0.80309206 -0.6089758\n",
            " -0.04174551 -0.5040528  -0.6165237  -0.28839448  0.45675257 -1.3132576\n",
            " -0.15617944 -0.11936615 -0.15083706  0.03098702 -0.34743667 -0.7823083\n",
            " -0.4748989 ]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.1664628  -0.56101847  0.08598937 -0.38063878 -0.85537565 -0.6138198\n",
            " -0.12655844 -0.44395635 -0.39454395  0.17850235 -0.5289928  -0.8581914\n",
            " -0.552922   -0.18717791 -0.5407476  -0.5018079  -0.38758546 -0.45816714\n",
            "  0.04463893]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 21 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4375563  -0.45794606  0.08144106 -0.31147143 -0.5682233  -0.3250246\n",
            "  0.00344601 -0.5940192  -0.28259957 -0.07568127  0.02105847 -1.5640512\n",
            " -0.6279162  -0.3936165  -0.23789357 -0.34641528 -0.32912844 -0.7904227\n",
            " -0.29842496]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 22 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.44498608 -0.47772872  0.00855826 -0.42453343 -0.45733643 -0.2916559\n",
            "  0.10169916 -0.42759013 -0.186192   -0.02900542 -0.05278547 -1.4036024\n",
            " -0.52481574 -0.30808333 -0.24032141 -0.21450719 -0.32029766 -0.6771234\n",
            " -0.29534268]\n",
            "\n",
            "Taking action 6 from 6\n",
            "\n",
            "Step 23 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.52022123 -0.50641906 -0.5271662  -0.58658874 -0.5858708  -0.47181818\n",
            " -0.14969186 -0.25782254 -0.37529126 -0.16577497  0.2411362  -0.7987878\n",
            " -0.09012312 -0.21329135 -0.1588439  -0.00755174 -0.28140587 -0.6077994\n",
            " -0.4083453 ]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 24 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5012288  -0.44381273  0.08805826 -0.47157365 -0.41637796 -0.1884861\n",
            "  0.02014627 -0.37322    -0.2103205   0.01833133  0.07398675 -1.0984411\n",
            " -0.37662593 -0.3486242  -0.22554523 -0.19442907 -0.41884175 -0.58880085\n",
            " -0.28250137]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4700329  -0.5229825   0.17099488 -0.48036414 -0.3941079  -0.14998761\n",
            "  0.12133379 -0.42766985 -0.20721932  0.01395676  0.15790752 -1.4758608\n",
            " -0.4735837  -0.43687454 -0.1607497  -0.15119442 -0.46777526 -0.70018715\n",
            " -0.37147766]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4686445  -0.4040186  -0.02777936 -0.5762586  -0.67985207 -0.41665754\n",
            " -0.18406174 -0.3081575  -0.25804213  0.01171991  0.10187858 -0.8422888\n",
            " -0.3880377  -0.49182543 -0.23700143 -0.37843442 -0.46223366 -0.75898975\n",
            " -0.27289906]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.46291417 -0.31461018  0.108935   -0.45112923 -0.6784467  -0.39986116\n",
            " -0.25974965 -0.37797067 -0.25297552  0.11369832  0.01360988 -0.8391072\n",
            " -0.43253887 -0.57244533 -0.3080506  -0.45925248 -0.4251684  -0.6852452\n",
            " -0.19043952]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.39714077 -0.30395287  0.15015413 -0.41520026 -0.5784137  -0.3124157\n",
            " -0.07930532 -0.5503996  -0.23036389  0.16207607 -0.06778514 -1.2242604\n",
            " -0.48469126 -0.3848023  -0.40031037 -0.34267426 -0.37932345 -0.70374554\n",
            " -0.19213906]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.60338247 -0.40029782  0.14628175 -0.51310015 -0.8128828  -0.4410544\n",
            " -0.20783001 -0.38511875 -0.31991863  0.2095117  -0.05166581 -0.9330689\n",
            " -0.5168579  -0.6574734  -0.44891807 -0.5209782  -0.47339547 -0.7870998\n",
            " -0.1753051 ]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.42061418 -0.3124948   0.14152533 -0.49220163 -0.8824332  -0.5395183\n",
            " -0.3110773  -0.4105064  -0.28449404  0.21285501 -0.06906134 -0.83671767\n",
            " -0.51337516 -0.6672173  -0.49068272 -0.6193405  -0.42486238 -0.85851675\n",
            " -0.11684158]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3928638  -0.3457071  -0.05558182 -0.5684629  -0.5767027  -0.33527163\n",
            " -0.18691577 -0.46156433 -0.2700033   0.0594695   0.24407986 -0.88840747\n",
            " -0.15975557 -0.35376003 -0.4079651  -0.19835523 -0.39887926 -0.64760625\n",
            " -0.2239005 ]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.47736782 -0.44182104  0.15915139 -0.55554855 -0.53392285 -0.27622774\n",
            "  0.04665497 -0.17860763 -0.18681997  0.17543727  0.0493783  -0.85783654\n",
            " -0.31799072 -0.5304214  -0.31779724 -0.19811541 -0.41209525 -0.6487926\n",
            " -0.22987173]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.20622785 -0.19183081  0.22033116 -0.4994807  -0.7521871  -0.38403392\n",
            " -0.3250476  -0.6726987  -0.25994694  0.18497162  0.09049396 -1.0896723\n",
            " -0.41374946 -0.5668768  -0.67045534 -0.60158336 -0.4609044  -0.8898073\n",
            " -0.10474942]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3456565  -0.42938742  0.18788365 -0.56319875 -0.5698475  -0.198035\n",
            " -0.0970057  -0.60474604 -0.3240109   0.07359424  0.42600244 -1.3402718\n",
            " -0.27424484 -0.5505507  -0.424227   -0.21956919 -0.47251406 -0.8236018\n",
            " -0.3178571 ]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4357641  -0.48644453  0.30179214 -0.6878366  -0.44933453 -0.09789087\n",
            "  0.04932232 -0.3723861  -0.20782082  0.06795117  0.42455    -1.1601398\n",
            " -0.2399499  -0.56589615 -0.32507604 -0.17468092 -0.6176499  -0.76927865\n",
            " -0.33510268]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.24666637 -0.23740453  0.4965146  -0.46717823 -0.41889146 -0.07077538\n",
            "  0.04305604 -0.4018461  -0.06938091  0.2354578  -0.00299227 -1.0908769\n",
            " -0.4790758  -0.5801678  -0.38972628 -0.37775242 -0.47471446 -0.68409586\n",
            " -0.13943543]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4321568  -0.30679578  0.662392   -0.41952336 -0.6716289  -0.17985423\n",
            " -0.05078461 -0.55672693 -0.22953446  0.382617    0.02379453 -1.2541418\n",
            " -0.5814458  -0.8821571  -0.63680184 -0.5691355  -0.58152413 -0.8606771\n",
            " -0.08826715]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.3699571  -0.28109375  0.6734721  -0.48417133 -0.63695574 -0.11458032\n",
            " -0.11442731 -0.5976211  -0.2255648   0.34250718  0.09978995 -1.2964948\n",
            " -0.5386519  -0.79615706 -0.6268632  -0.5730307  -0.6171008  -0.86976504\n",
            " -0.12110923]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.32890898 -0.44128937  0.57133085 -0.51506835 -0.2972583   0.0835426\n",
            "  0.26106423 -0.47366095 -0.08954047  0.13778548  0.31317514 -1.4556724\n",
            " -0.3992469  -0.5467258  -0.2238773  -0.12242375 -0.5939852  -0.71800613\n",
            " -0.31445014]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.41105318 -0.43781936  0.22731441 -0.5879302  -0.5598291  -0.21837096\n",
            " -0.06109893 -0.40071157 -0.23441336  0.09799063  0.2974984  -1.0740204\n",
            " -0.3236473  -0.60482895 -0.2896979  -0.2504348  -0.50532335 -0.74621123\n",
            " -0.31725624]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.4135953  -0.32083297  0.16728404 -0.47839177 -0.7526964  -0.39197826\n",
            " -0.32481086 -0.4778688  -0.34674168  0.23698288  0.10928914 -0.9291165\n",
            " -0.35880646 -0.73096734 -0.5231159  -0.43872398 -0.3488364  -0.75879765\n",
            " -0.23760065]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.33871293 -0.38356087  0.42544752 -0.47419032 -0.53049356 -0.0886906\n",
            "  0.06693622 -0.514805   -0.23382701  0.16759378  0.26864856 -1.349087\n",
            " -0.44008958 -0.6955234  -0.33338875 -0.28223684 -0.5445981  -0.807294\n",
            " -0.26735145]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.19831608 -0.15971787  0.4147082  -0.5298722  -0.34870043 -0.01488759\n",
            "  0.08016945 -0.420973   -0.0206226   0.2466138  -0.06278846 -0.9563038\n",
            " -0.3471247  -0.3681357  -0.5152967  -0.32724857 -0.47404137 -0.61173373\n",
            " -0.05175651]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.321045   -0.46661472  0.4297751  -0.58045655 -0.39321935  0.03324253\n",
            "  0.2311504  -0.39748392 -0.1424485   0.14638059  0.353518   -1.3469762\n",
            " -0.34758604 -0.6285786  -0.266227   -0.1358762  -0.56816775 -0.7725563\n",
            " -0.31230754]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.24563381 -0.22211778  0.15542787 -0.5148529  -0.6851658  -0.30330133\n",
            " -0.39891016 -0.56415117 -0.31644812  0.15461774  0.29758915 -0.8973799\n",
            " -0.2801586  -0.6853007  -0.47950065 -0.4768489  -0.44931984 -0.77949053\n",
            " -0.23772812]\n",
            "\n",
            "Taking action 8 from 10\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.21446317 -0.32348013  0.50961715 -0.4464648  -0.35101134  0.02647829\n",
            "  0.16456582 -0.42433304 -0.10614581  0.2526072   0.15034789 -1.3485209\n",
            " -0.4546848  -0.6869332  -0.33206016 -0.24235258 -0.4804152  -0.71808267\n",
            " -0.23688331]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5026615  -0.35105002  0.35652903 -0.53321826 -0.43041766 -0.09404103\n",
            "  0.03365233 -0.2171715  -0.14736494  0.2969543   0.0390804  -0.79283464\n",
            " -0.35682875 -0.6005829  -0.33470055 -0.28025582 -0.48942992 -0.5579092\n",
            " -0.18065402]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.235913   -0.22601962  0.03203821 -0.43078342 -0.5898499  -0.26654983\n",
            " -0.11869945 -0.41273305 -0.26149774  0.21036488  0.10530899 -0.9294531\n",
            " -0.28624105 -0.5079861  -0.43256968 -0.2700199  -0.32703173 -0.67809063\n",
            " -0.18886764]\n",
            "\n",
            "Taking action 17 from 9\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.32473415 -0.22324172  0.40342456 -0.41924587 -0.5220436  -0.11094184\n",
            "  0.02891424 -0.52023196 -0.17013659  0.34181717  0.006943   -1.0933361\n",
            " -0.4900317  -0.62306714 -0.5104681  -0.43185917 -0.49810895 -0.6958575\n",
            " -0.07992561]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 1 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.32041657 -0.2228362   0.24960676 -0.54321164 -0.39602655 -0.07501191\n",
            " -0.00711613 -0.29086128 -0.11922667  0.24977142  0.04902551 -0.7574652\n",
            " -0.27600095 -0.45051685 -0.43696144 -0.29539552 -0.48546052 -0.56099135\n",
            " -0.10252856]\n",
            "Epsilon reduced to 0.09000000000000001\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5691654  -0.3870092  -0.19231303 -0.60575974 -0.5458077  -0.3104012\n",
            " -0.09329568 -0.164342   -0.32790032  0.2218176   0.06636225 -0.6500155\n",
            " -0.1815126  -0.37179124 -0.35035655 -0.11164849 -0.34777737 -0.45496246\n",
            " -0.20068271]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3018701  -0.37001663  0.09924373 -0.6510397  -0.3239539  -0.12076752\n",
            "  0.12383648 -0.30276906 -0.15941039  0.24092776  0.15591536 -1.1580725\n",
            " -0.28105175 -0.48336998 -0.38507771 -0.1136585  -0.45609015 -0.60831404\n",
            " -0.26933193]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.29466248 -0.19130479  0.09734669 -0.50466883 -0.33229035 -0.08341796\n",
            " -0.0127891  -0.25171724 -0.10349903  0.19914377 -0.00702177 -0.69400096\n",
            " -0.22870131 -0.345952   -0.33988592 -0.20666488 -0.42987752 -0.4447105\n",
            " -0.074237  ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.22810926 -0.24750191  0.07483325 -0.5159525  -0.2993218  -0.04647418\n",
            "  0.18588804 -0.2510781  -0.10636411  0.20568529  0.02364135 -0.9957083\n",
            " -0.3224838  -0.40218213 -0.33608487 -0.13281532 -0.42349356 -0.54202294\n",
            " -0.14148805]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.35066128 -0.2691946  -0.23792975 -0.5897711  -0.441837   -0.23692247\n",
            " -0.17563483 -0.23315021 -0.22296567  0.06013771  0.27028525 -0.5025132\n",
            " -0.0216334  -0.26762986 -0.27538007 -0.09417112 -0.3894453  -0.4589836\n",
            " -0.20487061]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.23390505 -0.23556626  0.04121928 -0.4537336  -0.28703627 -0.02342425\n",
            "  0.15979207 -0.2826725  -0.11907956  0.18952763  0.10014921 -0.94759613\n",
            " -0.26664186 -0.37122387 -0.30187172 -0.09141117 -0.4090568  -0.49154878\n",
            " -0.15422513]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.17697738 -0.19164568  0.08028053 -0.47387543 -0.26685226 -0.02180203\n",
            "  0.16646144 -0.27883697 -0.04907347  0.19297227  0.03179703 -0.91976523\n",
            " -0.26849854 -0.34808227 -0.34429443 -0.12871404 -0.42100033 -0.510267\n",
            " -0.10478333]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.49368355 -0.29277733 -0.09209766 -0.5527471  -0.42048186 -0.15717429\n",
            " -0.15525423 -0.26502493 -0.23361138  0.16202787  0.11836471 -0.57065076\n",
            " -0.13430944 -0.27403533 -0.27734405 -0.15043582 -0.43418193 -0.36161393\n",
            " -0.14153436]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.15571839 -0.20117897  0.07600269 -0.48243147 -0.25316352 -0.02270158\n",
            "  0.16397214 -0.24898756 -0.05227746  0.18550345  0.05031816 -0.92767954\n",
            " -0.26654303 -0.40774256 -0.34051526 -0.13054144 -0.4156837  -0.5237422\n",
            " -0.1255619 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.3494659  -0.25922415 -0.09114052 -0.6408462  -0.33564162 -0.12998076\n",
            " -0.00716082 -0.2490579  -0.15130985  0.18118213  0.09641441 -0.6845509\n",
            " -0.14423457 -0.27202016 -0.3774678  -0.13185701 -0.43688312 -0.45284307\n",
            " -0.1586442 ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.21297419 -0.21531582 -0.25941646 -0.5186586  -0.30688623 -0.1540711\n",
            " -0.00607691 -0.16941708 -0.17810799  0.04812568  0.22193283 -0.592\n",
            " -0.04824968 -0.28248486 -0.2565291  -0.00728471 -0.3221273  -0.42789763\n",
            " -0.21351993]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.20005813 -0.15378243 -0.11680605 -0.4796271  -0.31202716 -0.12218844\n",
            "  0.03238546 -0.21147922 -0.10312749  0.11774924 -0.01176668 -0.6311761\n",
            " -0.19271697 -0.2944446  -0.3354786  -0.14503624 -0.34869668 -0.43207663\n",
            " -0.0628694 ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.20058411 -0.20973234  0.09181571 -0.529396   -0.19614053  0.02557685\n",
            "  0.15302007 -0.21300273 -0.0299027   0.17009938  0.08575785 -0.8170979\n",
            " -0.19556071 -0.36174116 -0.29347172 -0.09457013 -0.44137305 -0.46204126\n",
            " -0.13315347]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.23447853 -0.1210103  -0.13039553 -0.48681468 -0.31778222 -0.14400443\n",
            " -0.08140351 -0.2230396  -0.10471343  0.07192466  0.01322883 -0.48599166\n",
            " -0.14266919 -0.26291648 -0.31514314 -0.1903296  -0.39577258 -0.39844614\n",
            " -0.07820058]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.33204848 -0.19481587 -0.04560379 -0.5055687  -0.40166906 -0.15034837\n",
            " -0.18347105 -0.1692193  -0.19353972  0.13956659  0.1540961  -0.4077471\n",
            " -0.12450169 -0.4500658  -0.27444917 -0.2270411  -0.40060827 -0.44270653\n",
            " -0.1565418 ]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.25764486 -0.21021664 -0.09196664 -0.56359076 -0.2981267  -0.09036091\n",
            " -0.08197924 -0.2538933  -0.12217327  0.08750803  0.16993046 -0.58497286\n",
            " -0.074251   -0.2959013  -0.32030714 -0.12104082 -0.38765875 -0.4174769\n",
            " -0.15084149]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.28459537 -0.2503605  -0.18687496 -0.6201527  -0.33179295 -0.14156786\n",
            " -0.02936074 -0.1632697  -0.1918289   0.09576684  0.08266906 -0.663772\n",
            " -0.14743192 -0.3817905  -0.3086063  -0.09638712 -0.32839608 -0.47119856\n",
            " -0.22757503]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.23050736 -0.09773901 -0.1198176  -0.50937927 -0.27867275 -0.11331741\n",
            " -0.07086389 -0.15052663 -0.08017987 -0.02229239 -0.01599894 -0.36522353\n",
            " -0.10438381 -0.26874983 -0.25821617 -0.17899337 -0.43457294 -0.37084633\n",
            " -0.05299111]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.2544654  -0.13775527 -0.03315584 -0.55146176 -0.30830765 -0.09451319\n",
            " -0.12053663 -0.21135798 -0.10267174  0.04924459  0.00611137 -0.44409204\n",
            " -0.09866202 -0.34131026 -0.34937158 -0.21402067 -0.42128873 -0.41873834\n",
            " -0.0951402 ]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4612895  -0.21139273 -0.12486301 -0.61293733 -0.39313844 -0.17877409\n",
            " -0.2717637  -0.27719557 -0.22695994  0.04850534 -0.01658143 -0.42030776\n",
            " -0.05113323 -0.2952996  -0.35349077 -0.20146067 -0.44066143 -0.3502449\n",
            " -0.11752521]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-1.5073615e-01 -4.7089323e-02 -1.8579283e-01 -4.5601153e-01\n",
            " -2.2479767e-01 -7.4727118e-02 -3.8178302e-03 -1.7417911e-01\n",
            " -5.5640541e-02 -1.7353237e-01 -1.3734445e-01 -4.3465790e-01\n",
            " -1.3926302e-01 -2.2356403e-01 -2.6642960e-01 -1.4720757e-01\n",
            " -3.9428523e-01 -3.4861451e-01  3.1907111e-04]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-2.1212871e-01 -2.0977759e-01  1.1069700e-02 -4.9565622e-01\n",
            " -1.9890890e-01  4.2901933e-04  1.3198142e-01 -1.6825648e-01\n",
            " -5.9553765e-02 -3.2561772e-02 -1.2740967e-01 -7.4632150e-01\n",
            " -2.3108329e-01 -3.6776161e-01 -2.6008624e-01 -9.9553823e-02\n",
            " -3.4908545e-01 -4.1627324e-01 -1.5854773e-01]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.20037913 -0.12185399 -0.23967126 -0.5552176  -0.26790836 -0.13550974\n",
            "  0.06752463 -0.13761216 -0.05583996 -0.238825   -0.26255468 -0.53761464\n",
            " -0.19207439 -0.23686862 -0.26194915 -0.12058403 -0.3899429  -0.41841602\n",
            " -0.10072855]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.23377879 -0.10402954 -0.20950031 -0.5252354  -0.29117084 -0.15870024\n",
            " -0.16368277 -0.18176007 -0.11410818 -0.19224302 -0.17818244 -0.3732579\n",
            " -0.08272834 -0.24262668 -0.26615822 -0.15577245 -0.32271916 -0.34786868\n",
            " -0.18374921]\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 25 reward=-1 new_state=[0 1 0 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.38953775 -0.23836663 -0.11924785 -0.61048937 -0.22594604 -0.05483711\n",
            " -0.09193176 -0.17331925 -0.11921054 -0.18874356 -0.13589832 -0.4992158\n",
            " -0.09209281 -0.2333262  -0.2388371  -0.08618888 -0.39978775 -0.30927783\n",
            " -0.26312202]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 26 reward=-2 new_state=[0 1 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.20293373 -0.07082319 -0.17468688 -0.47898233 -0.2802673  -0.13068867\n",
            " -0.09923201 -0.18791841 -0.05919506 -0.27055547 -0.25949335 -0.39034352\n",
            " -0.19579859 -0.22143158 -0.25876343 -0.1949758  -0.36246213 -0.36236954\n",
            " -0.17708246]\n",
            "\n",
            "Taking action 14 from 8\n",
            "\n",
            "Step 27 reward=-3 new_state=[0 1 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.35048795 -0.16093823 -0.03867958 -0.42339325 -0.36875334 -0.16896935\n",
            " -0.24267867 -0.28979078 -0.17633614 -0.1250458  -0.16118464 -0.4294455\n",
            " -0.20295048 -0.3355722  -0.32494348 -0.24727127 -0.35651028 -0.33713853\n",
            " -0.21783143]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 28 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.23868185 -0.20339921 -0.26265913 -0.59196573 -0.30237752 -0.20320176\n",
            " -0.08773044 -0.25849202 -0.1455894  -0.39291745 -0.15974556 -0.6851048\n",
            " -0.2728355  -0.22638    -0.23771796 -0.09398232 -0.37241113 -0.44153535\n",
            " -0.36973062]\n",
            "\n",
            "Taking action 7 from 15\n",
            "\n",
            "Step 29 reward=-3 new_state=[0 0 1 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.25806865 -0.07254989 -0.15899083 -0.43028218 -0.43764618 -0.27085397\n",
            " -0.27225664 -0.28249922 -0.2525764  -0.16904204 -0.34092805 -0.3811548\n",
            " -0.32684785 -0.2858301  -0.3819059  -0.32287064 -0.29428342 -0.36194625\n",
            " -0.21352148]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 30 reward=-4 new_state=[1 0 1 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.3054871  -0.14147744 -0.34727556 -0.5603286  -0.502542   -0.35868922\n",
            " -0.20594165 -0.23495942 -0.33579102 -0.29766738 -0.33554137 -0.4320966\n",
            " -0.3621868  -0.26199275 -0.3461601  -0.28336808 -0.34200433 -0.44615304\n",
            " -0.3018476 ]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 31 reward=-4 new_state=[1 0 1 1 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.29113424 -0.13895261 -0.12232627 -0.4838589  -0.26669282 -0.21479774\n",
            " -0.1977503  -0.12550321 -0.17242196 -0.12322921 -0.33899096 -0.19235203\n",
            " -0.2334848  -0.14954588 -0.33452836 -0.2511844  -0.27065465 -0.22336128\n",
            " -0.20558296]\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 32 reward=-5 new_state=[1 0 1 1 1 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5647346  -0.42132023 -0.23540214 -0.6150795  -0.47856864 -0.40225044\n",
            " -0.26959914 -0.14806928 -0.41339657 -0.1925447  -0.33157563 -0.47819036\n",
            " -0.45651022 -0.30650827 -0.28205222 -0.35127774 -0.37447223 -0.3379312\n",
            " -0.39848754]\n",
            "\n",
            "Taking action 15 from 9\n",
            "\n",
            "Step 33 reward=-4 new_state=[1 0 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.25805157 -0.28730398 -0.16107652 -0.48896587 -0.3909881  -0.3067878\n",
            " -0.20563197 -0.16822177 -0.31836015 -0.2618573  -0.38098976 -0.4003749\n",
            " -0.47935507 -0.37019363 -0.39794123 -0.4318829  -0.2760684  -0.411603\n",
            " -0.30409434]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 34 reward=-4 new_state=[1 0 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.42547145 -0.5369327  -0.35407522 -0.6748314  -0.46587485 -0.40030584\n",
            " -0.3119949  -0.39678526 -0.47234866 -0.46491876 -0.29612267 -0.60660976\n",
            " -0.45879874 -0.29414487 -0.5251996  -0.47004938 -0.37228218 -0.46640697\n",
            " -0.46667978]\n",
            "\n",
            "Taking action 10 from 10\n",
            "\n",
            "Step 35 reward=-4 new_state=[1 0 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.52390176 -0.67727673 -0.4301725  -0.96016705 -0.5057176  -0.46716765\n",
            " -0.28464484 -0.23488668 -0.50803894 -0.6945683  -0.38279802 -0.5340653\n",
            " -0.6055418  -0.39255482 -0.40443373 -0.5426272  -0.5919506  -0.5981295\n",
            " -0.5998719 ]\n",
            "\n",
            "Taking action 9 from 7\n",
            "\n",
            "Step 36 reward=-4 new_state=[1 0 1 1 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.5280607  -0.94092846 -0.63757235 -0.9837761  -0.91073877 -0.8447992\n",
            " -0.48647824 -0.42094398 -0.76952285 -0.7966091  -0.5163537  -0.81513494\n",
            " -0.902638   -0.5912818  -0.5144961  -0.82774764 -0.53456765 -0.8907013\n",
            " -0.7923875 ]\n",
            "\n",
            "Taking action 8 from 6\n",
            "\n",
            "Step 37 reward=-3 new_state=[1 0 1 1 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.49684674 -0.93322575 -0.7106435  -1.0650064  -0.7915696  -0.67851925\n",
            " -0.43382406 -0.63258386 -0.80324244 -1.0458719  -0.49254057 -0.9512089\n",
            " -0.9528543  -0.40969354 -0.65109587 -0.9357073  -0.7079593  -0.909729\n",
            " -0.76231545]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 38 reward=-2 new_state=[1 0 1 1 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.55429304 -1.0515292  -0.8081535  -0.95643604 -1.0068226  -0.8806445\n",
            " -0.75054836 -0.9226775  -0.94087905 -1.0199143  -0.7335836  -1.0084858\n",
            " -1.0934845  -0.45795938 -0.76907766 -1.192325   -0.5608834  -0.91313803\n",
            " -0.7632638 ]\n",
            "\n",
            "Taking action 16 from 16\n",
            "\n",
            "Step 39 reward=-2 new_state=[1 0 1 1 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.8397894  -1.4447767  -0.7710602  -1.1608194  -1.1808376  -0.93417656\n",
            " -1.039567   -1.115381   -1.1970822  -1.1908033  -0.7029532  -1.3049266\n",
            " -1.3516532  -0.8563433  -0.6867224  -1.435696   -0.7825949  -1.1342459\n",
            " -1.1665286 ]\n",
            "\n",
            "Taking action 6 from 14\n",
            "\n",
            "Step 40 reward=-1 new_state=[1 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.1898915 -2.2636096 -2.1974068 -1.7789301 -1.8856639 -1.8663596\n",
            " -1.4216478 -1.5892375 -1.999899  -2.0023663 -1.6567792 -1.7255095\n",
            " -2.0396376 -0.4189413 -1.0198445 -1.984617  -1.0024006 -1.4553038\n",
            " -1.5804812]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 41 reward=-1 new_state=[1 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.095136  -1.6864603 -1.5699987 -1.6312575 -1.5863569 -1.3685182\n",
            " -1.0973701 -1.4573972 -1.7313683 -1.7961235 -2.1613994 -1.3823268\n",
            " -2.497516  -0.5590006 -1.3137208 -2.3018925 -1.3777521 -1.3881352\n",
            " -1.0645795]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 42 reward=-1 new_state=[1 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.5204144 -2.2442117 -1.1981329 -1.5968218 -2.1545832 -1.5678164\n",
            " -2.1601303 -2.4209776 -1.9723276 -1.6378633 -1.9962366 -2.047848\n",
            " -2.5516188 -1.2120485 -1.5904093 -2.897893  -1.702431  -1.6701542\n",
            " -1.4894183]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 43 reward=-1 new_state=[1 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.3909596 -3.0414062 -2.596995  -2.4870825 -2.653179  -2.2571883\n",
            " -2.824132  -3.2601316 -2.6307113 -3.6202972 -2.2200274 -2.611032\n",
            " -3.3809464 -1.3688289 -1.8246542 -3.8909364 -2.9162855 -2.4882176\n",
            " -2.133355 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.543627  -2.7379923 -2.0241268 -2.239494  -2.2310233 -1.7982295\n",
            " -2.427189  -2.0308204 -2.374164  -2.5228367 -1.8002577 -1.6759844\n",
            " -2.4372835 -1.8378055 -1.5559478 -2.6914914 -2.0971656 -1.9545923\n",
            " -1.9046242]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.3968444 -3.9918437 -4.1222615 -2.7132409 -3.1194413 -2.7495072\n",
            " -2.9398448 -2.4380095 -3.853687  -4.507574  -2.3321044 -2.6201916\n",
            " -3.674554  -2.7276769 -1.7353534 -3.4634495 -2.8599098 -2.892475\n",
            " -2.7445757]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-1.2727473 -3.6873603 -3.2204986 -2.5748842 -3.6817534 -2.8003654\n",
            " -3.2011409 -2.7140324 -4.048942  -3.7649786 -3.7705114 -2.70593\n",
            " -4.929779  -3.4753745 -2.2307005 -4.662766  -3.3952866 -3.2419734\n",
            " -2.4527998]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [-0.909751  -4.8475666 -3.5623357 -3.3144011 -3.354402  -3.3492813\n",
            " -2.8482661 -2.5306034 -3.525748  -4.0572376 -3.9359267 -3.225048\n",
            " -4.773171  -3.702684  -2.282881  -4.211178  -3.2162197 -3.3380167\n",
            " -2.9826338]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [ 0.16177332 -4.8897943  -4.1743426  -2.8191807  -3.6466458  -3.130763\n",
            " -3.350465   -3.6361604  -4.3829904  -4.913196   -4.409969   -4.96849\n",
            " -6.3666215  -4.739383   -2.591076   -5.580578   -3.7654505  -4.1439686\n",
            " -3.8512118 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 1 0 0 0 1 1 0]\n",
            "Predicted scores for each action in next step: [ 0.41130775 -6.202058   -5.2504673  -3.1199372  -4.541931   -4.069299\n",
            " -4.976261   -4.2160187  -5.04559    -5.5644546  -3.7930214  -4.573028\n",
            " -5.476056   -6.0092506  -3.244785   -5.5751896  -4.0433908  -4.3450046\n",
            " -4.1823406 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12 from 12\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [ 0.54021204 -5.8400984  -5.78901    -3.4360123  -3.9643805  -3.681252\n",
            " -4.141545   -3.439194   -5.289998   -6.8357563  -3.8628063  -4.5967712\n",
            " -6.2771406  -5.688031   -2.4608219  -5.5771823  -4.7792134  -4.482408\n",
            " -4.1740136 ]\n",
            "Epsilon reduced to 0.08100000000000002\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 1 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [ 0.19306532 -4.639057   -4.4613338  -2.170443   -3.6126485  -3.130194\n",
            " -4.135286   -3.4199634  -4.4666357  -4.607329   -3.4670153  -3.4365382\n",
            " -4.020485   -4.895824   -2.9378374  -4.5797715  -3.529614   -3.381898\n",
            " -2.9045556 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 2 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.50198394 -5.563026   -5.879182   -2.388548   -3.5302079  -3.5480196\n",
            " -3.952969   -2.870676   -4.8110185  -5.243361   -2.4105973  -3.5397182\n",
            " -2.2463498  -4.5572886  -2.3982527  -2.8934984  -3.1836748  -3.085557\n",
            " -3.3750377 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.555275  -4.706533  -3.021774  -1.8461288 -2.9286425 -2.454787\n",
            " -3.1240633 -2.7610114 -3.8626027 -3.3265767 -3.2203293 -3.5785964\n",
            " -2.7114465 -4.7500443 -2.475259  -3.3669794 -3.3541048 -2.5148878\n",
            " -2.5305774]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.2761574  -3.6453235  -3.176301   -2.3621218  -2.061228   -2.154988\n",
            " -2.8782716  -2.5141473  -2.5643532  -3.3025022  -1.5100784  -1.7881613\n",
            " -0.43678492 -2.1306934  -1.7978373  -1.9001102  -3.1541495  -1.80904\n",
            " -1.962336  ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.9180324  -3.0377111  -2.7443292  -2.3712733  -1.5034041  -1.7436328\n",
            " -2.647914   -1.9848397  -1.9471905  -2.921856   -0.9001296  -1.0707684\n",
            "  0.46865442 -2.074266   -1.685334   -1.0112954  -2.4798532  -1.5291826\n",
            " -1.7750701 ]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-1.0750934  -2.9994597  -3.1405234  -2.3994339  -1.7079315  -2.0217907\n",
            " -2.8550925  -2.2278423  -2.2264538  -2.9891078  -1.5985568  -1.3146875\n",
            " -0.20101264 -1.7142214  -1.7126892  -1.7331232  -2.658621   -1.4712946\n",
            " -1.7707593 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.3019084 -2.9955065 -2.4519284 -2.0898626 -1.6386317 -1.831571\n",
            " -2.5170746 -1.8404628 -2.0791001 -2.2581556 -1.7085314 -1.5045358\n",
            " -0.3308193 -2.149013  -1.6870611 -1.4971149 -2.1397765 -1.3666196\n",
            " -1.7343568]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.9678364 -3.347937  -2.8696332 -2.1189196 -1.8751553 -1.8542035\n",
            " -2.893289  -1.9068323 -2.783589  -2.7811666 -1.6926612 -1.2510952\n",
            " -0.7422248 -2.4104092 -1.4079893 -1.827633  -2.603271  -1.1041356\n",
            " -1.9764726]\n",
            "\n",
            "Taking action 6 from 12\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7379199  -2.6210155  -2.2091398  -1.6432321  -1.5861753  -1.4907405\n",
            " -2.1264002  -1.4086382  -2.0591404  -1.7589128  -1.4139705  -0.95997274\n",
            " -0.56404114 -1.5660263  -1.1510514  -1.2719859  -1.7195635  -0.5727781\n",
            " -1.3652216 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.0413187 -2.9866426 -2.3984728 -1.7316964 -1.6504257 -1.518355\n",
            " -2.3279321 -1.6080776 -2.4887154 -2.3548572 -1.6877873 -1.0866046\n",
            " -1.0302131 -2.198266  -1.2956903 -1.7596225 -2.375427  -0.605256\n",
            " -1.6134523]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.472614   -2.247633   -1.9962213  -1.4063572  -1.2077339  -1.2892165\n",
            " -1.7851496  -1.1694502  -1.8369306  -1.6331375  -1.5443678  -0.83276427\n",
            " -0.60726035 -1.6469858  -1.0673629  -1.1822133  -1.573812   -0.26199958\n",
            " -1.1734998 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8433213  -2.7059286  -2.2224877  -1.5177606  -1.4809592  -1.4026967\n",
            " -2.1889217  -1.4895421  -2.2991593  -2.120578   -1.5288601  -1.064776\n",
            " -0.8462548  -2.1394997  -1.2108971  -1.5822624  -2.1337285  -0.31287622\n",
            " -1.503832  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-2.0968227  -2.5408075  -2.073358   -1.3947119  -1.5036875  -1.361121\n",
            " -2.0491467  -1.2425113  -2.3203344  -1.5714469  -1.7421229  -0.9004761\n",
            " -0.9488596  -1.8120122  -0.9640138  -1.464211   -1.8200094   0.10201923\n",
            " -1.2257856 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7448082  -2.2952569  -1.865754   -1.1487893  -1.4534127  -1.2797598\n",
            " -1.9159662  -1.2486475  -2.187702   -1.6332078  -1.5474699  -0.86009055\n",
            " -0.8819909  -1.8637812  -1.0516182  -1.5097739  -1.8844105   0.01695513\n",
            " -1.0573322 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.5596819  -1.9437894  -1.6222633  -1.1759957  -1.0582035  -1.0928268\n",
            " -1.4693587  -0.9850948  -1.5915041  -1.2506615  -1.3032049  -0.4891156\n",
            " -0.63280034 -1.2163244  -0.8369511  -1.0518824  -1.503502    0.29053771\n",
            " -0.8004747 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.3389558  -1.8430679  -1.5635164  -1.0224199  -1.017942   -0.9670744\n",
            " -1.4829152  -0.94251794 -1.7030452  -1.4212583  -1.2571574  -0.6994178\n",
            " -0.5819665  -1.6790856  -0.86574405 -1.0385816  -1.4882064   0.1302996\n",
            " -0.9108941 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8829627  -2.2010949  -1.7824299  -1.127894   -1.2260085  -1.1082911\n",
            " -1.4494616  -0.7586403  -2.0267584  -1.3819985  -1.4434777  -0.62298363\n",
            " -1.0077858  -1.6567292  -0.68092656 -1.0931427  -1.5100117   0.34174067\n",
            " -1.028335  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.8790731  -2.1893232  -1.773056   -1.1641334  -1.3717306  -1.1620191\n",
            " -1.8812295  -1.3114212  -2.085346   -1.4311903  -1.617234   -0.7520476\n",
            " -0.7183753  -1.6420637  -1.1399444  -1.3958545  -1.9157145   0.40579388\n",
            " -0.8701065 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.7504107  -2.0626385  -1.7587594  -1.1152091  -1.2526882  -1.092705\n",
            " -1.5462973  -1.0033952  -1.9374996  -1.4324043  -1.5792196  -0.68548685\n",
            " -0.98122275 -1.6996037  -0.91054475 -1.3004143  -1.6735326   0.31763166\n",
            " -0.9299117 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.279088   -1.8246781  -1.6041268  -0.9322345  -1.0404215  -1.0278543\n",
            " -1.5196681  -1.0328146  -1.6475039  -1.4300613  -1.2868724  -0.74474317\n",
            " -0.5481543  -1.7544914  -0.94242704 -1.1375357  -1.5703095   0.24642028\n",
            " -0.81814814]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.3653183  -1.6341108  -1.512138   -1.0214422  -0.7485054  -0.91450375\n",
            " -1.457257   -0.9405607  -1.3152574  -1.1572052  -0.9719751  -0.33617908\n",
            " -0.36518756 -0.8995411  -0.6663147  -0.9012285  -1.3026277   0.66232884\n",
            " -0.6804428 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.1414262  -1.5686791  -1.2512497  -0.7983453  -0.82750726 -0.8118362\n",
            " -1.235092   -0.88183707 -1.317324   -1.161673   -1.2949282  -0.7405698\n",
            " -0.61066186 -1.6456357  -0.85988194 -1.0476589  -1.252777    0.15953708\n",
            " -0.7962682 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.9009002  -2.0231566  -1.6321858  -1.114704   -0.9973705  -1.0043879\n",
            " -1.5207019  -0.8832154  -1.7959362  -1.1771231  -1.2191229  -0.62042266\n",
            " -0.75158155 -1.2230711  -0.5165107  -1.027458   -1.4197835   0.6494141\n",
            " -0.9804157 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.194991   -1.6392615  -1.2899176  -1.0254033  -0.85582554 -0.8498479\n",
            " -1.358861   -1.0396471  -1.2080334  -1.1737928  -1.0786413  -0.58675885\n",
            " -0.29348513 -1.2428402  -0.92867863 -0.869356   -1.3023075   0.18054144\n",
            " -0.7841688 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.223485   -1.4600434  -1.3432518  -0.9313136  -0.6858453  -0.82100475\n",
            " -1.3007481  -0.80697775 -1.1903968  -1.0007405  -0.88385135 -0.36656043\n",
            " -0.3388845  -0.83863616 -0.5641007  -0.7777976  -1.0727024   0.49142107\n",
            " -0.68006593]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0747088  -1.4683156  -1.2093624  -0.93643826 -0.7760616  -0.7746094\n",
            " -1.1695063  -0.96536404 -1.0973438  -1.152854   -1.0883162  -0.61056423\n",
            " -0.48300788 -1.1992856  -0.79193497 -0.9662258  -1.2662896   0.11134262\n",
            " -0.7357672 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.4201515  -1.6561772  -1.3476242  -0.98955554 -0.8081926  -0.79747146\n",
            " -1.2105207  -0.75378764 -1.3839983  -1.1488509  -0.90566266 -0.41180795\n",
            " -0.49376744 -1.0237639  -0.51891357 -0.73234254 -1.207537    0.32991952\n",
            " -0.83998436]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.3009218  -1.6447146  -1.196247   -1.0158798  -0.8431168  -0.8168785\n",
            " -1.239585   -0.87576175 -1.2283018  -1.0299345  -0.9491391  -0.48825806\n",
            " -0.27806884 -1.020269   -0.76603585 -0.6793302  -1.1730899   0.2029309\n",
            " -0.7668968 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.3067838  -1.4917383  -1.1735569  -0.95058465 -0.75091827 -0.7574047\n",
            " -1.2152103  -0.7810539  -1.2843325  -1.0451765  -1.0019027  -0.41894382\n",
            " -0.52923113 -1.0440986  -0.5920003  -0.8395596  -1.1740129   0.3033051\n",
            " -0.75773025]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.2318816  -1.5442511  -1.1590387  -0.81702745 -0.9239671  -0.8167286\n",
            " -1.1955168  -0.8349721  -1.2344022  -1.0157261  -0.97666633 -0.57625043\n",
            " -0.4859381  -1.1479218  -0.6329948  -0.8353157  -1.0486181   0.08735307\n",
            " -0.779923  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0938712  -1.3411716  -1.0113403  -0.8162611  -0.6455322  -0.6857518\n",
            " -1.1619228  -0.7887832  -1.0499043  -0.95375323 -0.78222036 -0.41780937\n",
            " -0.32017586 -0.8924537  -0.5733475  -0.72677284 -0.98890233  0.24109802\n",
            " -0.701507  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9491199  -1.2413298  -0.85235775 -0.7059457  -0.6361429  -0.6272127\n",
            " -0.9053112  -0.66874576 -0.9292003  -0.85304666 -0.75219834 -0.48316315\n",
            " -0.3778946  -0.91194516 -0.5110508  -0.65717    -0.8924409   0.08624357\n",
            " -0.6366749 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0944201  -1.2878253  -0.9143263  -0.8171834  -0.69050026 -0.6762478\n",
            " -1.2525209  -0.98553324 -0.9903269  -0.9606153  -0.8432559  -0.3939963\n",
            " -0.35123765 -0.7934632  -0.69594675 -0.9366127  -1.1635227   0.23785609\n",
            " -0.5757122 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.012043   -1.2902389  -0.82819325 -0.7221381  -0.6376696  -0.6376531\n",
            " -0.92659104 -0.61325794 -0.96763164 -0.7805165  -0.7423936  -0.42558712\n",
            " -0.3249831  -0.8744417  -0.47512883 -0.5628762  -0.80372447  0.11200202\n",
            " -0.63695836]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.95352197 -1.1867516  -0.84165144 -0.72808    -0.5794666  -0.60266846\n",
            " -1.0470757  -0.7644267  -0.9079913  -0.87424725 -0.72030675 -0.40328002\n",
            " -0.28459662 -0.82543683 -0.57293636 -0.6893251  -0.9011195   0.1537559\n",
            " -0.6285065 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.95106214 -1.1614897  -0.752491   -0.6437937  -0.65405566 -0.5874528\n",
            " -0.8302311  -0.5780649  -0.93554336 -0.7759111  -0.7076236  -0.43104658\n",
            " -0.45406532 -0.8332786  -0.3938349  -0.6390157  -0.82167464  0.03394242\n",
            " -0.59557444]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7234391  -0.94011855 -0.6734649  -0.6156641  -0.44545943 -0.52172905\n",
            " -0.9118258  -0.71112704 -0.7087239  -0.7796984  -0.6178632  -0.33507845\n",
            " -0.2393153  -0.66980904 -0.48994005 -0.6820735  -0.84770703  0.15661679\n",
            " -0.47828305]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.89887005 -1.1567205  -0.7621712  -0.63864887 -0.62807655 -0.5893214\n",
            " -0.8498908  -0.6178366  -0.9442046  -0.8530663  -0.78581583 -0.48094016\n",
            " -0.5141533  -0.96534616 -0.4703403  -0.74258745 -0.8917267   0.01911858\n",
            " -0.6108354 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8882036  -1.1680489  -0.77132106 -0.6881861  -0.5923428  -0.60119164\n",
            " -0.99161613 -0.7565658  -0.9044038  -0.89919025 -0.7237616  -0.46767795\n",
            " -0.32119653 -0.92356086 -0.588238   -0.719843   -0.9637215   0.06348683\n",
            " -0.6255847 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.98460096 -1.1851041  -0.79913723 -0.67723113 -0.69596475 -0.6334119\n",
            " -0.91839737 -0.60283434 -1.0114179  -0.85799646 -0.80352587 -0.40146476\n",
            " -0.54416054 -0.9335712  -0.42903113 -0.7491096  -0.90360886  0.01985632\n",
            " -0.6225353 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7822728  -1.0193532  -0.72931296 -0.5685573  -0.59074664 -0.57306206\n",
            " -0.9416056  -0.6736162  -0.8739424  -0.81381917 -0.68813455 -0.38527343\n",
            " -0.27231976 -0.87079704 -0.51076055 -0.6617802  -0.85436606  0.05976067\n",
            " -0.51351523]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7895256  -1.0093603  -0.72265786 -0.5979408  -0.56501985 -0.53925884\n",
            " -0.81335604 -0.59349555 -0.80575156 -0.77925324 -0.6495859  -0.38513076\n",
            " -0.3434521  -0.80005765 -0.429354   -0.5888966  -0.7510426  -0.00271575\n",
            " -0.56990856]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.79132855 -0.99299777 -0.6531402  -0.52130264 -0.53035426 -0.52714086\n",
            " -0.83543587 -0.63277465 -0.82030153 -0.75091445 -0.75871956 -0.45213437\n",
            " -0.32506543 -0.93090296 -0.490854   -0.6553972  -0.8048345   0.06259488\n",
            " -0.50491965]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7509663  -0.9520117  -0.5443685  -0.5119606  -0.47454548 -0.47750828\n",
            " -0.7748399  -0.55076474 -0.7221399  -0.619604   -0.5697673  -0.35817552\n",
            " -0.28120255 -0.7271986  -0.38230717 -0.54385674 -0.66641265  0.04367369\n",
            " -0.49515125]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.72780204 -0.9669337  -0.59958756 -0.51190996 -0.4841713  -0.5014588\n",
            " -0.7798906  -0.59720945 -0.7573113  -0.71545327 -0.8004043  -0.4317463\n",
            " -0.3333851  -0.9760217  -0.5531317  -0.63625336 -0.76216674  0.03446518\n",
            " -0.49019516]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.76277983 -0.950472   -0.5808609  -0.5624497  -0.46258825 -0.48160413\n",
            " -0.7833601  -0.584728   -0.6961745  -0.64893776 -0.56463873 -0.32178727\n",
            " -0.23531821 -0.636178   -0.39757073 -0.517527   -0.68294656  0.05329832\n",
            " -0.49199912]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6797813  -0.9601102  -0.6119977  -0.62571746 -0.4530236  -0.5003123\n",
            " -0.8084276  -0.6479409  -0.6650731  -0.77114654 -0.60981536 -0.37379098\n",
            " -0.19375113 -0.75043404 -0.5113327  -0.5485883  -0.7791871   0.01344206\n",
            " -0.5120058 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 13\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.78833234 -0.9610203  -0.5566218  -0.6294408  -0.5095273  -0.4999793\n",
            " -0.90251434 -0.6523736  -0.76611686 -0.7262683  -0.6661376  -0.27604535\n",
            " -0.34033132 -0.72069633 -0.49301586 -0.68726933 -0.83600986  0.03615483\n",
            " -0.4771128 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8234787  -1.1068755  -0.563251   -0.7277773  -0.48405164 -0.53358555\n",
            " -0.94430494 -0.7522507  -0.7545767  -0.76042    -0.71701115 -0.41544583\n",
            " -0.18345039 -0.8573261  -0.63991606 -0.6148688  -0.9101945   0.03220795\n",
            " -0.5485556 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 50 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.8607834  -1.2983465  -0.7138764  -0.7812792  -0.6075403  -0.686462\n",
            " -1.1699423  -0.76193696 -0.86239094 -0.9111042  -0.57607    -0.38035196\n",
            " -0.0119723  -0.9828516  -0.6409402  -0.52789783 -0.9202444   0.00773805\n",
            " -0.6473217 ]\n",
            "Epsilon reduced to 0.07290000000000002\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8208044  -1.1675701  -0.6772704  -0.6600368  -0.48739555 -0.60343003\n",
            " -0.8605866  -0.56310105 -0.7601238  -0.75371027 -0.5768605  -0.38255286\n",
            " -0.06611499 -0.80278087 -0.45089114 -0.3702299  -0.7272832   0.01684906\n",
            " -0.59274334]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.72189933 -0.9745312  -0.69607955 -0.6192287  -0.47151935 -0.52313083\n",
            " -0.87587523 -0.65445864 -0.7153929  -0.8219986  -0.5619956  -0.3493521\n",
            " -0.16947441 -0.7317284  -0.5195604  -0.5276161  -0.7624439  -0.04548422\n",
            " -0.53775066]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.77100766 -1.0499029  -0.69006366 -0.58233446 -0.56359076 -0.5863359\n",
            " -0.88546526 -0.6234397  -0.79013157 -0.76482    -0.6646298  -0.36377934\n",
            " -0.22582127 -0.83245957 -0.49446326 -0.56531525 -0.7640236  -0.09831203\n",
            " -0.5407805 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6460014  -0.9007404  -0.6943497  -0.5769566  -0.45882228 -0.50882673\n",
            " -0.8267261  -0.61026376 -0.6810165  -0.8192564  -0.5490533  -0.35049453\n",
            " -0.2064644  -0.73889    -0.45521381 -0.53888077 -0.7239806  -0.10768725\n",
            " -0.53514683]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6343632  -0.90636814 -0.67030334 -0.54772407 -0.4660754  -0.4964502\n",
            " -0.7975147  -0.59334457 -0.6900328  -0.7518853  -0.60740983 -0.38539714\n",
            " -0.21828212 -0.79280907 -0.46991655 -0.53967834 -0.6781234  -0.13691212\n",
            " -0.53292274]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.74594074 -0.9812508  -0.6582869  -0.6211118  -0.4834375  -0.5005149\n",
            " -0.843168   -0.64880055 -0.73041004 -0.811779   -0.55906403 -0.40126708\n",
            " -0.2437734  -0.7201054  -0.45174432 -0.5646397  -0.77637625 -0.17906336\n",
            " -0.58219445]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7158537  -0.9884075  -0.6460094  -0.5624899  -0.5076931  -0.5308092\n",
            " -0.8373235  -0.6607937  -0.7399011  -0.72492194 -0.7131213  -0.4302885\n",
            " -0.24254018 -0.84991246 -0.55805457 -0.60396355 -0.75421363 -0.18665849\n",
            " -0.5292831 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.717763   -0.98156506 -0.6872934  -0.5818838  -0.5066413  -0.51480436\n",
            " -0.8339679  -0.63652074 -0.7682065  -0.83072615 -0.5185947  -0.43160066\n",
            " -0.2061869  -0.7575959  -0.4442269  -0.5334823  -0.7708538  -0.21420844\n",
            " -0.571546  ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6058677  -0.8664766  -0.54850215 -0.4940502  -0.4630556  -0.44111115\n",
            " -0.72584814 -0.5717507  -0.6576607  -0.69771564 -0.55637157 -0.41632897\n",
            " -0.2406832  -0.7981975  -0.44649744 -0.52808887 -0.68013483 -0.2512214\n",
            " -0.516975  ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.85832644 -1.0669929  -0.67508703 -0.59613216 -0.6197666  -0.5591525\n",
            " -0.85677516 -0.6132612  -0.89525867 -0.8190918  -0.63828003 -0.3887778\n",
            " -0.35175428 -0.80187404 -0.4311218  -0.6136861  -0.85703325 -0.28011933\n",
            " -0.5564684 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.650739   -0.93399906 -0.67392904 -0.6017158  -0.5060025  -0.5089457\n",
            " -0.8135816  -0.612455   -0.71758074 -0.80790174 -0.58806264 -0.36692163\n",
            " -0.21032274 -0.7880992  -0.48338977 -0.5267016  -0.73267186 -0.26729038\n",
            " -0.5472375 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.95593613 -1.1177071  -0.73405975 -0.6333476  -0.6829924  -0.59887964\n",
            " -0.9506361  -0.7008481  -0.945045   -0.8519465  -0.6402511  -0.42185146\n",
            " -0.33127028 -0.76542044 -0.41920698 -0.656669   -0.9208059  -0.29295224\n",
            " -0.56727946]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.71451926 -0.9296086  -0.5771699  -0.6054803  -0.4891235  -0.4750394\n",
            " -0.7940462  -0.6201921  -0.6991011  -0.720435   -0.54409003 -0.32974148\n",
            " -0.19645171 -0.66060114 -0.45068306 -0.5086874  -0.7450037  -0.27190858\n",
            " -0.5002786 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7163974  -0.97998106 -0.6675864  -0.6318257  -0.5401267  -0.53065336\n",
            " -0.8389194  -0.68357235 -0.7431011  -0.8427367  -0.71998465 -0.4272384\n",
            " -0.32008755 -0.8500274  -0.5056371  -0.671635   -0.8618227  -0.2848448\n",
            " -0.5369734 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.59935737 -0.851279   -0.59117293 -0.60589623 -0.4170934  -0.4554314\n",
            " -0.8339422  -0.6691575  -0.622165   -0.7586359  -0.5495499  -0.31418276\n",
            " -0.11811112 -0.6730807  -0.54906255 -0.5247991  -0.7520335  -0.23868978\n",
            " -0.4700162 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.8231497  -1.0974948  -0.66264445 -0.6295698  -0.5397392  -0.53520346\n",
            " -0.8234196  -0.6283358  -0.7972686  -0.7584066  -0.6523235  -0.44694227\n",
            " -0.22554785 -0.8142717  -0.48845825 -0.51326823 -0.78457797 -0.2743175\n",
            " -0.5847645 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7786656  -1.0083759  -0.66736674 -0.6734358  -0.55674165 -0.5126517\n",
            " -0.9397451  -0.7713397  -0.7692181  -0.85181534 -0.64021593 -0.42081252\n",
            " -0.25345448 -0.7693987  -0.55285394 -0.6676351  -0.8796537  -0.3225495\n",
            " -0.58184767]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.7354968  -0.9805516  -0.61119986 -0.5825309  -0.48039326 -0.4883989\n",
            " -0.8052237  -0.6284357  -0.69630283 -0.72689474 -0.5879985  -0.40268475\n",
            " -0.1683243  -0.74598724 -0.47066942 -0.48954993 -0.7327902  -0.24817502\n",
            " -0.5448007 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6543341  -0.9278789  -0.6833923  -0.5909225  -0.4911688  -0.5234765\n",
            " -0.87925804 -0.6481596  -0.69602454 -0.84000885 -0.54370743 -0.3270082\n",
            " -0.15059888 -0.76530945 -0.5206163  -0.5246165  -0.7634145  -0.25138187\n",
            " -0.5285466 ]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6816029  -0.9399072  -0.60916674 -0.56759626 -0.4630613  -0.4985687\n",
            " -0.7610508  -0.57508993 -0.6638906  -0.72280145 -0.5769974  -0.37409294\n",
            " -0.18963304 -0.7394175  -0.4393274  -0.47943607 -0.6866121  -0.23844963\n",
            " -0.5278143 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 10 from 16\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.78581566 -1.0103319  -0.7022925  -0.6114533  -0.5933098  -0.58180225\n",
            " -0.99698377 -0.740656   -0.81141657 -0.8512318  -0.5389115  -0.33351636\n",
            " -0.13662009 -0.73847616 -0.52271014 -0.5853911  -0.87014806 -0.26355833\n",
            " -0.49843103]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 0 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6819539  -1.0195273  -0.74934393 -0.58384526 -0.56053466 -0.56578994\n",
            " -0.8368824  -0.6182259  -0.7816863  -0.8724656  -0.6576583  -0.41417888\n",
            " -0.2892726  -0.9243123  -0.50307673 -0.600451   -0.78098094 -0.2791538\n",
            " -0.5726407 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.80544883 -1.0100907  -0.62410843 -0.5582699  -0.5488302  -0.51458335\n",
            " -0.7927833  -0.6124425  -0.78671265 -0.76247734 -0.6759729  -0.4567052\n",
            " -0.3456108  -0.8567963  -0.42770568 -0.6132347  -0.80580103 -0.26710635\n",
            " -0.5522203 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5959461  -0.8221028  -0.57186025 -0.60223776 -0.4151057  -0.44254512\n",
            " -0.78562087 -0.64779496 -0.6140195  -0.7358332  -0.55254245 -0.2942418\n",
            " -0.15057343 -0.62509334 -0.50695616 -0.5143272  -0.76285    -0.23899469\n",
            " -0.45108914]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.68265927 -0.94596505 -0.6238671  -0.6254298  -0.46955034 -0.49030656\n",
            " -0.7697318  -0.6407892  -0.68952763 -0.79777277 -0.6264021  -0.40051898\n",
            " -0.26565343 -0.7542413  -0.47180843 -0.57597536 -0.8341152  -0.24765721\n",
            " -0.5102608 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 26 reward=-1 new_state=[1 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6346048  -0.89043117 -0.6071059  -0.53688365 -0.4608001  -0.44767958\n",
            " -0.7461149  -0.579015   -0.6882979  -0.77758443 -0.55527854 -0.39716578\n",
            " -0.3052723  -0.7790253  -0.41743922 -0.5659256  -0.74807864 -0.25722945\n",
            " -0.5513356 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 27 reward=-1 new_state=[1 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5615944  -0.84848684 -0.6010858  -0.5035169  -0.41466016 -0.44646648\n",
            " -0.7202332  -0.59076947 -0.61318964 -0.72508466 -0.5855061  -0.38121477\n",
            " -0.17197987 -0.77382207 -0.5123162  -0.48056182 -0.7054753  -0.21134533\n",
            " -0.48173973]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 28 reward=-1 new_state=[1 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.65370435 -0.88130367 -0.59801525 -0.51032466 -0.44468284 -0.47347194\n",
            " -0.73771334 -0.60152036 -0.6247299  -0.7043344  -0.4914475  -0.34941292\n",
            " -0.14594679 -0.6322122  -0.43110755 -0.43982756 -0.7168857  -0.21082383\n",
            " -0.46852854]\n",
            "\n",
            "Taking action 16 from 12\n",
            "\n",
            "Step 29 reward=-1 new_state=[1 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.62410754 -0.88147694 -0.568622   -0.512146   -0.45429397 -0.44959456\n",
            " -0.717406   -0.56469095 -0.65145546 -0.6870593  -0.57875824 -0.36578006\n",
            " -0.25933737 -0.7244384  -0.45962518 -0.5066378  -0.6784954  -0.2713291\n",
            " -0.49557948]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 30 reward=-1 new_state=[1 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.5908562  -0.8252686  -0.55186814 -0.449344   -0.4235642  -0.43142164\n",
            " -0.6796203  -0.53165305 -0.62682575 -0.6869885  -0.6576831  -0.39907756\n",
            " -0.3399151  -0.84627026 -0.43228215 -0.5649716  -0.69330174 -0.2302701\n",
            " -0.47082716]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 31 reward=-1 new_state=[1 0 0 0 0 1 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.6373056  -0.886085   -0.60795766 -0.5560263  -0.485061   -0.4572304\n",
            " -0.7786464  -0.64417547 -0.65747464 -0.7520035  -0.6040849  -0.39432716\n",
            " -0.3150308  -0.7288319  -0.47679234 -0.5986884  -0.77578056 -0.31406364\n",
            " -0.5141448 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 32 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6970742  -0.9027697  -0.6293468  -0.5260259  -0.52722037 -0.47944844\n",
            " -0.7666166  -0.64846647 -0.69970846 -0.6958779  -0.6994035  -0.42274943\n",
            " -0.3920129  -0.73500854 -0.4640393  -0.6458145  -0.7487017  -0.34273267\n",
            " -0.5197178 ]\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 33 reward=0 new_state=[1 0 0 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.7153842  -0.9792266  -0.5954723  -0.60558397 -0.59981066 -0.4891675\n",
            " -0.8032969  -0.6472529  -0.75492465 -0.8247787  -0.65012276 -0.40136743\n",
            " -0.4949568  -0.81287056 -0.44049895 -0.7032325  -0.8823036  -0.5003403\n",
            " -0.5964418 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 34 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.70798504 -0.94913983 -0.65536296 -0.5449793  -0.60099053 -0.50182253\n",
            " -0.7372094  -0.6225482  -0.75963444 -0.7744409  -0.6870055  -0.42225873\n",
            " -0.49486655 -0.7643083  -0.44345033 -0.67354465 -0.8318658  -0.45844075\n",
            " -0.53496945]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 35 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5799095  -0.86326075 -0.60439473 -0.526701   -0.5181221  -0.466708\n",
            " -0.7280843  -0.61404026 -0.659644   -0.7602161  -0.63557416 -0.3607015\n",
            " -0.41400802 -0.75989676 -0.4922001  -0.6360941  -0.7864635  -0.4516102\n",
            " -0.49661952]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 36 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.72361505 -0.9338072  -0.6050264  -0.55121475 -0.59129673 -0.4847306\n",
            " -0.73496693 -0.602049   -0.72805387 -0.733306   -0.6472916  -0.32625943\n",
            " -0.5095976  -0.69164413 -0.40121907 -0.6550185  -0.80213743 -0.5048146\n",
            " -0.53207207]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 37 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6055963  -0.8857946  -0.5733157  -0.53182936 -0.54881865 -0.4709109\n",
            " -0.73508275 -0.5728716  -0.67651755 -0.7399416  -0.586245   -0.25616223\n",
            " -0.41463172 -0.75517297 -0.43429083 -0.5833194  -0.76300037 -0.52004695\n",
            " -0.5188939 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 38 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6173181  -0.9052523  -0.62931436 -0.558417   -0.5935753  -0.53858703\n",
            " -0.8615161  -0.6003679  -0.6890749  -0.76731914 -0.57614124 -0.13448088\n",
            " -0.37563908 -0.7383897  -0.44639105 -0.606351   -0.7699077  -0.5262759\n",
            " -0.5227674 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 39 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5474712  -0.73198605 -0.545864   -0.4752252  -0.49592152 -0.42476785\n",
            " -0.68303716 -0.5622274  -0.5913109  -0.6490439  -0.6183745  -0.11834565\n",
            " -0.43721622 -0.6090057  -0.41104138 -0.61160886 -0.6834668  -0.47681642\n",
            " -0.4419918 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 40 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5425182  -0.76203036 -0.55011153 -0.5051307  -0.50035375 -0.43354866\n",
            " -0.6900379  -0.53620195 -0.60056967 -0.6868216  -0.58455884 -0.0320124\n",
            " -0.43977675 -0.6298164  -0.40946302 -0.5858551  -0.6948152  -0.5077885\n",
            " -0.4555658 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 41 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6018875  -0.812996   -0.5879218  -0.52259827 -0.5751186  -0.48402262\n",
            " -0.76379627 -0.5502878  -0.6654633  -0.71566546 -0.59065574  0.07067343\n",
            " -0.47553393 -0.6448319  -0.38675892 -0.6233459  -0.7480448  -0.5434833\n",
            " -0.46925172]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 42 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.60084885 -0.8899641  -0.597989   -0.61488074 -0.5523136  -0.52986294\n",
            " -0.8448076  -0.56169635 -0.64138436 -0.7298734  -0.55952764  0.20463978\n",
            " -0.3461815  -0.6428821  -0.450045   -0.5330998  -0.7334364  -0.55959964\n",
            " -0.50161695]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 12 from 6\n",
            "\n",
            "Step 43 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.66820467 -0.83396924 -0.5797729  -0.58970946 -0.5831831  -0.49612993\n",
            " -0.7788836  -0.58654857 -0.66185737 -0.682868   -0.69633704  0.23897247\n",
            " -0.50185835 -0.5856981  -0.42823744 -0.6419671  -0.7651571  -0.5730485\n",
            " -0.4634471 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 44 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.49721235 -0.672174   -0.4752893  -0.46619856 -0.4897547  -0.41009942\n",
            " -0.6377667  -0.47486544 -0.5531877  -0.5966342  -0.55433595  0.274068\n",
            " -0.42740187 -0.55969447 -0.3770211  -0.531011   -0.63023835 -0.5352869\n",
            " -0.40423244]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 45 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5247784  -0.7376803  -0.5545073  -0.50582695 -0.54017955 -0.47438422\n",
            " -0.69679576 -0.50195485 -0.6025495  -0.65453964 -0.5604273   0.37769523\n",
            " -0.41652554 -0.5935774  -0.4051963  -0.5449782  -0.6600393  -0.55602133\n",
            " -0.42860094]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 46 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.57170236 -0.7707319  -0.55498195 -0.5142106  -0.563854   -0.49498373\n",
            " -0.69191283 -0.49298662 -0.63274086 -0.63736486 -0.5913853   0.47720945\n",
            " -0.4220466  -0.5974932  -0.40766853 -0.5305626  -0.6754084  -0.57048666\n",
            " -0.43085217]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 47 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5059633  -0.68139964 -0.5157794  -0.50430524 -0.50059706 -0.43333372\n",
            " -0.61019236 -0.4806804  -0.56221944 -0.61936086 -0.57809067  0.5052901\n",
            " -0.43303335 -0.53192157 -0.3990977  -0.5297023  -0.640598   -0.5463847\n",
            " -0.393722  ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 48 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.5356509  -0.6936244  -0.49696982 -0.5265472  -0.51386744 -0.44100913\n",
            " -0.59507936 -0.4616935  -0.5884298  -0.61848    -0.62895656  0.6108947\n",
            " -0.48203316 -0.5554142  -0.39857608 -0.5488294  -0.6801735  -0.56140167\n",
            " -0.37945238]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 49 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.51534975 -0.69751817 -0.5123107  -0.52205026 -0.5348933  -0.47148407\n",
            " -0.63964844 -0.45025373 -0.58807755 -0.6054969  -0.55335444  0.74579436\n",
            " -0.3843217  -0.5495905  -0.40435335 -0.475757   -0.6347978  -0.5731462\n",
            " -0.3925859 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 50 reward=1 new_state=[1 0 0 0 0 1 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.52060914 -0.6896515  -0.517919   -0.51166844 -0.53195333 -0.47092235\n",
            " -0.6466045  -0.48610127 -0.5826136  -0.6006755  -0.57402617  0.8115504\n",
            " -0.36830422 -0.5331126  -0.42863515 -0.49338597 -0.6512182  -0.5657973\n",
            " -0.38241076]\n",
            "Epsilon reduced to 0.06561000000000002\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5365844  -0.6748734  -0.45271266 -0.5081236  -0.56503725 -0.44067574\n",
            " -0.6146672  -0.47486445 -0.61195225 -0.6009823  -0.59544927  0.87626433\n",
            " -0.4928868  -0.5431456  -0.3978554  -0.5823147  -0.7024729  -0.6294784\n",
            " -0.373125  ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5897029  -0.7493991  -0.5181711  -0.555839   -0.58305126 -0.472713\n",
            " -0.5651758  -0.42507982 -0.6658784  -0.6509411  -0.62507784  0.93427837\n",
            " -0.5735861  -0.58047694 -0.36081958 -0.5765042  -0.71619695 -0.6347124\n",
            " -0.40655956]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5483704  -0.7021288  -0.4948895  -0.53313196 -0.58236015 -0.4698726\n",
            " -0.6332308  -0.49024647 -0.6148837  -0.61669266 -0.5715571   1.0152478\n",
            " -0.4372607  -0.53972495 -0.41247886 -0.5409483  -0.69084543 -0.63653666\n",
            " -0.3898263 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5489869  -0.67692614 -0.4754758  -0.53240395 -0.5862085  -0.4562913\n",
            " -0.55012035 -0.42640078 -0.639496   -0.6130754  -0.64226913  1.0402592\n",
            " -0.5728587  -0.5669843  -0.38598043 -0.5906371  -0.70894223 -0.6486423\n",
            " -0.369666  ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4989255  -0.63690543 -0.4697779  -0.50375783 -0.5375148  -0.4435786\n",
            " -0.54509526 -0.4336254  -0.57547784 -0.5894536  -0.5823551   1.046629\n",
            " -0.44949412 -0.52801186 -0.39929402 -0.50979745 -0.64778906 -0.60508263\n",
            " -0.3515037 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6161056  -0.754389   -0.54223615 -0.5654018  -0.64912796 -0.5065851\n",
            " -0.5776041  -0.42903718 -0.7214849  -0.6894492  -0.642537    1.1790253\n",
            " -0.642066   -0.6010805  -0.3566532  -0.63548595 -0.7797128  -0.6853945\n",
            " -0.40391716]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.49837163 -0.6519507  -0.49217343 -0.508282   -0.556343   -0.4560262\n",
            " -0.5371169  -0.42379308 -0.5939211  -0.59904176 -0.59172726  1.1264466\n",
            " -0.46083558 -0.55016476 -0.41092962 -0.50733864 -0.6369924  -0.6226809\n",
            " -0.36125642]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.61333376 -0.7082466  -0.49300233 -0.5582197  -0.6026933  -0.47582844\n",
            " -0.5327823  -0.41435122 -0.66851556 -0.6182872  -0.6225924   1.186821\n",
            " -0.59978646 -0.514022   -0.33567697 -0.58273745 -0.722739   -0.6539465\n",
            " -0.37947822]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 9 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4803634  -0.61298937 -0.4778344  -0.48877108 -0.568244   -0.44694763\n",
            " -0.5128606  -0.41697857 -0.5985529  -0.5888925  -0.6182746   1.1587542\n",
            " -0.504686   -0.5606705  -0.41983122 -0.5403835  -0.6497119  -0.63384795\n",
            " -0.336001  ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 10 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4722504  -0.56912    -0.46428448 -0.49122435 -0.53850555 -0.42798838\n",
            " -0.5032524  -0.41301668 -0.55914074 -0.5729613  -0.5656883   1.1419258\n",
            " -0.4718306  -0.4821217  -0.37219542 -0.5118276  -0.6201123  -0.6047288\n",
            " -0.32937396]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 11 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.53171116 -0.60387963 -0.4786458  -0.51950216 -0.5669665  -0.4457875\n",
            " -0.50317544 -0.4068886  -0.6059982  -0.59407175 -0.584468    1.1747099\n",
            " -0.5270062  -0.48474905 -0.3521902  -0.53358716 -0.6648482  -0.627987\n",
            " -0.34838665]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 12 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.45743856 -0.5714971  -0.4635821  -0.5164897  -0.50694114 -0.4360526\n",
            " -0.5015765  -0.40931436 -0.5433043  -0.5785054  -0.5610538   1.1760359\n",
            " -0.3963795  -0.5002901  -0.40536124 -0.4496076  -0.62983805 -0.58570945\n",
            " -0.32617489]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 13 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.44068992 -0.511899   -0.49121687 -0.47674727 -0.4989342  -0.42161158\n",
            " -0.47229016 -0.3960606  -0.5418564  -0.5649085  -0.58039737  1.071163\n",
            " -0.44027072 -0.4665234  -0.37653592 -0.48053336 -0.5957124  -0.5489192\n",
            " -0.3048767 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 14 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.45346165 -0.4898842  -0.48178568 -0.45758456 -0.50947565 -0.4149481\n",
            " -0.45897138 -0.3944838  -0.5439271  -0.5556941  -0.57462204  1.0136538\n",
            " -0.47242224 -0.4440787  -0.3504923  -0.5012436  -0.59807986 -0.5536331\n",
            " -0.30098554]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 15 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.41658926 -0.45222956 -0.43327105 -0.42753547 -0.46579754 -0.38283223\n",
            " -0.4053278  -0.3616387  -0.49308133 -0.5140862  -0.53825784  0.9017662\n",
            " -0.45620432 -0.42099437 -0.31849468 -0.46530852 -0.5415958  -0.5326193\n",
            " -0.2921951 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 16 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39182776 -0.418858   -0.43364003 -0.400016   -0.4396952  -0.3645965\n",
            " -0.37812513 -0.35900784 -0.46729437 -0.50291324 -0.52866024  0.81276464\n",
            " -0.4404657  -0.39932132 -0.3122017  -0.45278946 -0.51536614 -0.503213\n",
            " -0.27635485]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 17 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.38435227 -0.40646723 -0.4267047  -0.3865633  -0.42681295 -0.35567045\n",
            " -0.35902753 -0.35236344 -0.454893   -0.49329078 -0.522486    0.7451103\n",
            " -0.43709674 -0.39263898 -0.30225652 -0.44408163 -0.50262237 -0.49170196\n",
            " -0.27255428]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 18 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.37926996 -0.39689285 -0.41977978 -0.37380075 -0.41570607 -0.34758767\n",
            " -0.34137094 -0.34710932 -0.4439314  -0.48367324 -0.5175482   0.6782455\n",
            " -0.4340477  -0.38747138 -0.29317263 -0.43616283 -0.49127224 -0.48186004\n",
            " -0.26979482]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 19 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.37413317 -0.38815227 -0.41269884 -0.36064428 -0.40415242 -0.33920228\n",
            " -0.32324582 -0.34176305 -0.4326653  -0.47400594 -0.5124737   0.61028856\n",
            " -0.4310729  -0.38236478 -0.2837342  -0.4281442  -0.4797273  -0.47166416\n",
            " -0.26710555]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 20 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.36898577 -0.38019562 -0.40553552 -0.34725487 -0.39227596 -0.33059368\n",
            " -0.30481228 -0.33637637 -0.42121965 -0.4644095  -0.50734025  0.5422193\n",
            " -0.42823267 -0.37735766 -0.27405047 -0.42013523 -0.4681479  -0.46123797\n",
            " -0.26448673]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 21 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.36386836 -0.37297356 -0.3983572  -0.3337773  -0.38018748 -0.32183272\n",
            " -0.2862121  -0.33099836 -0.40970632 -0.45499343 -0.5022212   0.4748087\n",
            " -0.4255833  -0.37248614 -0.26422086 -0.41223785 -0.45667964 -0.45069253\n",
            " -0.26194036]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 22 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.35881788 -0.36643875 -0.39122528 -0.32034108 -0.36798584 -0.31298235\n",
            " -0.26757017 -0.3256747  -0.3982253  -0.4458565  -0.4971851   0.40863892\n",
            " -0.42317635 -0.36778373 -0.25433534 -0.40454572 -0.4454538  -0.44012642\n",
            " -0.25946957]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 23 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.35386792 -0.3605463  -0.38419506 -0.30706084 -0.35575864 -0.30409813\n",
            " -0.24899562 -0.32044768 -0.38686526 -0.43708706 -0.49229565  0.34412456\n",
            " -0.4210593  -0.3632814  -0.24447447 -0.39714405 -0.4345879  -0.42962712\n",
            " -0.25707847]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 24 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.34904864 -0.35525456 -0.3773162  -0.29403752 -0.34358352 -0.2952291\n",
            " -0.23058319 -0.31535584 -0.37570453 -0.42876366 -0.4876108   0.28153342\n",
            " -0.41927546 -0.35900742 -0.23470977 -0.39011025 -0.424186   -0.419272\n",
            " -0.25477207]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 25 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.344387   -0.35052526 -0.3706331  -0.28135896 -0.3315286  -0.286418\n",
            " -0.21241425 -0.3104341  -0.36481178 -0.42095536 -0.48318338  0.22100686\n",
            " -0.41786408 -0.35498756 -0.22510412 -0.3835136  -0.4143394  -0.4091287\n",
            " -0.2525559 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 26 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.33990723 -0.3463236  -0.36418495 -0.26910114 -0.31965357 -0.27770182\n",
            " -0.19455819 -0.30571383 -0.35424685 -0.41372257 -0.47906113  0.16257867\n",
            " -0.41686082 -0.351245   -0.21571252 -0.37741598 -0.40512758 -0.39925617\n",
            " -0.25043613]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 27 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3356306  -0.3426184  -0.35800624 -0.2573288  -0.3080105  -0.26911274\n",
            " -0.17707323 -0.30122304 -0.34406167 -0.40711766 -0.4752869   0.10619283\n",
            " -0.41629803 -0.3478006  -0.20658232 -0.37187234 -0.39661902 -0.38970554\n",
            " -0.2484194 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 28 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.33157587 -0.33938164 -0.35212722 -0.24609663 -0.29664427 -0.26067793\n",
            " -0.1600076  -0.29698658 -0.33430064 -0.40118542 -0.47189894  0.05171971\n",
            " -0.416205   -0.34467328 -0.19775411 -0.36693102 -0.38887188 -0.3805207\n",
            " -0.24651273]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 29 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.32775947 -0.3365886  -0.3465739  -0.23544984 -0.28559336 -0.25242046\n",
            " -0.1434005  -0.29302618 -0.32500142 -0.3959638  -0.46893144 -0.00102981\n",
            " -0.41660792 -0.34187967 -0.18926206 -0.3626344  -0.38193497 -0.37173885\n",
            " -0.24472353]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 30 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.32419562 -0.33421737 -0.34136862 -0.22542532 -0.27489054 -0.2443594\n",
            " -0.12728298 -0.28936106 -0.31619558 -0.39148456 -0.46641463 -0.05229143\n",
            " -0.41753054 -0.33943483 -0.18113464 -0.35901937 -0.37584868 -0.36339122\n",
            " -0.2430594 ]\n",
            "\n",
            "Taking action 17 from 11\n",
            "\n",
            "Step 31 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3208967  -0.33224887 -0.33653045 -0.21605223 -0.26456335 -0.23651043\n",
            " -0.11167873 -0.28600788 -0.30790913 -0.38777378 -0.46437532 -0.10233656\n",
            " -0.41899437 -0.33735222 -0.17339502 -0.356118   -0.37064552 -0.35550368\n",
            " -0.2415283 ]\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 32 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31787306 -0.33066612 -0.3320752  -0.20735288 -0.25463447 -0.22888595\n",
            " -0.096605   -0.28298104 -0.30016294 -0.38485238 -0.46283716 -0.15146291\n",
            " -0.42101857 -0.33564353 -0.1660619  -0.35395792 -0.3663512  -0.34809706\n",
            " -0.24013828]\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 33 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31567353 -0.32956854 -0.32922548 -0.2005549  -0.24682307 -0.22306998\n",
            " -0.09272072 -0.28215304 -0.2939691  -0.38311046 -0.46206665 -0.19415453\n",
            " -0.42285317 -0.33474118 -0.16047367 -0.35459316 -0.36340675 -0.34182605\n",
            " -0.23966676]\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 34 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31421903 -0.32889307 -0.32782102 -0.19547041 -0.24092008 -0.21887736\n",
            " -0.09890522 -0.2833116  -0.28917044 -0.3824432  -0.46199632 -0.23132518\n",
            " -0.4245353  -0.3345629  -0.15645552 -0.35776302 -0.3616906  -0.33658314\n",
            " -0.2400221 ]\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 35 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31344366 -0.32858974 -0.32772374 -0.1919333  -0.23673886 -0.21613777\n",
            " -0.1141765  -0.286272   -0.28562737 -0.38277042 -0.46257478 -0.26375896\n",
            " -0.4261127  -0.33503997 -0.15385035 -0.36324492 -0.3611056  -0.33227456\n",
            " -0.24112564]\n",
            "\n",
            "Taking action 14 from 6\n",
            "\n",
            "Step 36 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31329212 -0.3286194  -0.32881498 -0.1897963  -0.23411287 -0.21469477\n",
            " -0.13766761 -0.2908735  -0.2832154  -0.38403195 -0.46376318 -0.29213387\n",
            " -0.4276411  -0.33611545 -0.15251641 -0.37084934 -0.36157447 -0.32881945\n",
            " -0.24291058]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 37 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31371853 -0.32895184 -0.33099282 -0.18892825 -0.23289368 -0.21440531\n",
            " -0.16861388 -0.2969765  -0.28182358 -0.38618392 -0.46553278 -0.31703985\n",
            " -0.42918104 -0.33774227 -0.15232545 -0.38041446 -0.3630361  -0.32614768\n",
            " -0.24532068]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 38 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31385684 -0.32968864 -0.3335856  -0.18859982 -0.23209442 -0.21427263\n",
            " -0.19798532 -0.30427137 -0.28099927 -0.38901818 -0.46885473 -0.33943057\n",
            " -0.43053055 -0.3402127  -0.16446592 -0.39051548 -0.36530662 -0.3246429\n",
            " -0.24686834]\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 39 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.313747   -0.33079392 -0.3365537  -0.18876097 -0.2316792  -0.21427663\n",
            " -0.22587183 -0.31264207 -0.28068537 -0.3924847  -0.47357517 -0.35963917\n",
            " -0.43173492 -0.34345767 -0.18726966 -0.4011165  -0.36831892 -0.32419813\n",
            " -0.24765377]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 40 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31343523 -0.33224466 -0.3398711  -0.18937147 -0.23162605 -0.21439797\n",
            " -0.25237525 -0.32199368 -0.28083703 -0.39655942 -0.47956318 -0.3779676\n",
            " -0.43285686 -0.3474298  -0.21936138 -0.41220412 -0.37203136 -0.32472834\n",
            " -0.24777903]\n",
            "\n",
            "Taking action 3 from 3\n",
            "\n",
            "Step 41 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31496868 -0.33686745 -0.3451513  -0.21227016 -0.23030587 -0.21506321\n",
            " -0.27874884 -0.3318166  -0.2813011  -0.404067   -0.48665804 -0.3951911\n",
            " -0.43495157 -0.3503233  -0.24952053 -0.4229595  -0.37762678 -0.32660973\n",
            " -0.25251794]\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 42 reward=-3 new_state=[1 1 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.31811878 -0.34421343 -0.35213083 -0.25276148 -0.22787167 -0.21613315\n",
            " -0.3050052  -0.34204137 -0.28204542 -0.41461048 -0.49478042 -0.4115979\n",
            " -0.43799508 -0.35233518 -0.27799755 -0.4334839  -0.3848903  -0.32970917\n",
            " -0.2613122 ]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 43 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.32403642 -0.35572058 -0.36215252 -0.29029855 -0.23106144 -0.25076142\n",
            " -0.3317736  -0.35174984 -0.28366303 -0.42568606 -0.5039383  -0.42554426\n",
            " -0.44242138 -0.3563894  -0.30420947 -0.44573584 -0.39243895 -0.3344631\n",
            " -0.27129778]\n",
            "\n",
            "Taking action 4 from 4\n",
            "\n",
            "Step 44 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.33235484 -0.36781248 -0.37377405 -0.32338002 -0.27071205 -0.286125\n",
            " -0.358481   -0.3627876  -0.2895108  -0.4368862  -0.514106   -0.43791762\n",
            " -0.44968098 -0.3626169  -0.32894632 -0.46055016 -0.40252143 -0.3440166\n",
            " -0.2797153 ]\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 45 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3428684  -0.38051504 -0.38685066 -0.35269064 -0.33411232 -0.32216758\n",
            " -0.38519636 -0.37507865 -0.29911822 -0.4483724  -0.5252263  -0.44905764\n",
            " -0.4596136  -0.37089986 -0.35249153 -0.47777754 -0.4149673  -0.35792375\n",
            " -0.28683472]\n",
            "\n",
            "Taking action 6 from 8\n",
            "\n",
            "Step 46 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3545426  -0.39710227 -0.4011129  -0.38422674 -0.3925764  -0.3567953\n",
            " -0.41120943 -0.38612226 -0.30884236 -0.46184766 -0.53591657 -0.45966965\n",
            " -0.47060615 -0.3811452  -0.37316123 -0.4938276  -0.42644036 -0.37257805\n",
            " -0.34355175]\n",
            "\n",
            "Taking action 6 from 8\n",
            "\n",
            "Step 47 reward=-2 new_state=[1 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.36810976 -0.4143498  -0.41750923 -0.41378072 -0.4512865  -0.3894027\n",
            " -0.43647602 -0.39683726 -0.35438463 -0.47656262 -0.54785097 -0.46998507\n",
            " -0.4839536  -0.39460063 -0.39305773 -0.51103926 -0.44046292 -0.38848042\n",
            " -0.39771846]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 48 reward=-1 new_state=[0 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3835061  -0.4322756  -0.43583086 -0.44171345 -0.5102575  -0.42031282\n",
            " -0.46115065 -0.4073042  -0.42588052 -0.49242914 -0.56097317 -0.48020756\n",
            " -0.4996064  -0.41093096 -0.41222283 -0.52944225 -0.45680103 -0.40560478\n",
            " -0.44967526]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 49 reward=-1 new_state=[0 1 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40612102 -0.45028132 -0.45324892 -0.46840367 -0.5664068  -0.44978637\n",
            " -0.48422748 -0.41737854 -0.4941895  -0.5072514  -0.5739221  -0.48989213\n",
            " -0.51506996 -0.42640543 -0.42971385 -0.54683346 -0.47270155 -0.42194593\n",
            " -0.49941579]\n",
            "\n",
            "Taking action 15 from 7\n",
            "\n",
            "Step 50 reward=0 new_state=[0 1 0 0 0 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.43504316 -0.46835303 -0.46985078 -0.49394768 -0.61992484 -0.47791272\n",
            " -0.5058578  -0.42710102 -0.5595122  -0.52113557 -0.5867102  -0.4990675\n",
            " -0.5303608  -0.4411058  -0.44568264 -0.56331605 -0.48820737 -0.4375798\n",
            " -0.54708284]\n",
            "Epsilon reduced to 0.05904900000000002\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.46098787 -0.48443055 -0.48440868 -0.5168952  -0.66840714 -0.5034678\n",
            " -0.5244791  -0.43238515 -0.61981344 -0.53306144 -0.59796035 -0.5073434\n",
            " -0.54418284 -0.45452914 -0.45936647 -0.57718503 -0.50129604 -0.45127\n",
            " -0.5911154 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.48445842 -0.4988852  -0.4974564  -0.5377738  -0.7127192  -0.52677345\n",
            " -0.541154   -0.4363705  -0.67547476 -0.54369843 -0.60805094 -0.5149077\n",
            " -0.55665755 -0.46667078 -0.47166806 -0.58949375 -0.5129214  -0.46353698\n",
            " -0.63174564]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5056699  -0.5118765  -0.5091411  -0.5567316  -0.753121   -0.5479904\n",
            " -0.5560541  -0.439164   -0.72669077 -0.55317175 -0.6170952  -0.5218041\n",
            " -0.567917   -0.47765726 -0.48269194 -0.6003933  -0.5232247  -0.4745195\n",
            " -0.6691204 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5248219  -0.52354836 -0.51959574 -0.57391226 -0.7898757  -0.5672747\n",
            " -0.56933796 -0.44086576 -0.77368563 -0.5615949  -0.62519515 -0.52807677\n",
            " -0.5780811  -0.48760226 -0.49253857 -0.61002076 -0.53233445 -0.48434305\n",
            " -0.70340735]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.54209834 -0.5340305  -0.5289407  -0.58945405 -0.8232434  -0.58477676\n",
            " -0.5811518  -0.44156957 -0.8167021  -0.56907046 -0.6324433  -0.53377\n",
            " -0.5872575  -0.49660832 -0.5013033  -0.6185005  -0.54036665 -0.49312118\n",
            " -0.7347864 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5576691  -0.5434404  -0.5372845  -0.6034887  -0.85347795 -0.6006395\n",
            " -0.59163034 -0.44136328 -0.85599124 -0.57569134 -0.63892317 -0.53892684\n",
            " -0.5955437  -0.5047683  -0.5090768  -0.6259446  -0.5474266  -0.5009566\n",
            " -0.76344323]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5716899  -0.5518836  -0.5447254  -0.61614084 -0.8808233  -0.6149983\n",
            " -0.6008973  -0.44032925 -0.891806   -0.58154166 -0.64471024 -0.5435891\n",
            " -0.6030277  -0.5121655  -0.5159437  -0.6324556  -0.5536097  -0.50794214\n",
            " -0.78956413]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5843038  -0.5594557  -0.5513524  -0.6275273  -0.90551275 -0.62797993\n",
            " -0.60906607 -0.4385445  -0.9243966  -0.5866978  -0.64987266 -0.5477966\n",
            " -0.6097887  -0.51887536 -0.52198344 -0.63812566 -0.55900246 -0.5141617\n",
            " -0.8133328 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.5956416  -0.5662429  -0.5572457  -0.6377576  -0.9277663  -0.63970315\n",
            " -0.61624086 -0.43608117 -0.95400566 -0.59122837 -0.65447205 -0.5515872\n",
            " -0.61589813 -0.5249655  -0.5272701  -0.64303887 -0.563683   -0.51969135\n",
            " -0.83492756]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6058232  -0.5723228  -0.5624778  -0.64693344 -0.9477912  -0.6502784\n",
            " -0.62251645 -0.43300653 -0.98086715 -0.595196   -0.65856415 -0.5549965\n",
            " -0.62142086 -0.5304973  -0.53187215 -0.64727104 -0.5677223  -0.5245998\n",
            " -0.8545193 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6149577  -0.5777658  -0.5671146  -0.6551492  -0.9657812  -0.659808\n",
            " -0.6279798  -0.42938346 -1.0052029  -0.5986572  -0.6621994  -0.5580579\n",
            " -0.626415   -0.53552586 -0.53585327 -0.65089107 -0.57118475 -0.528949\n",
            " -0.8722709 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.62314475 -0.5826354  -0.5712152  -0.66249204 -0.9819162  -0.6683863\n",
            " -0.6327102  -0.42527044 -1.0272232  -0.601663   -0.6654233  -0.5608023\n",
            " -0.63093317 -0.54010105 -0.53927195 -0.65396154 -0.57412857 -0.53279525\n",
            " -0.8883356 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6304752  -0.58698857 -0.5748332  -0.6690421  -0.99636376 -0.67610043\n",
            " -0.63677996 -0.42072177 -1.0471251  -0.6042598  -0.6682768  -0.56325877\n",
            " -0.6350227  -0.54426754 -0.5421822  -0.65653884 -0.5766064  -0.5361893\n",
            " -0.9028577 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6370314  -0.5908771  -0.57801723 -0.67487377 -1.0092779  -0.6830305\n",
            " -0.6402547  -0.41578788 -1.0650929  -0.6064893  -0.6707972  -0.5654539\n",
            " -0.6387264  -0.54806596 -0.5446335  -0.65867424 -0.5786661  -0.5391769\n",
            " -0.91597116]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6428885  -0.5943476  -0.580811   -0.68005425 -1.0208013  -0.6892496\n",
            " -0.64319444 -0.4105155  -1.0812974  -0.6083896  -0.6730182  -0.56741226\n",
            " -0.6420831  -0.5515326  -0.54667145 -0.6604141  -0.5803509  -0.54179966\n",
            " -0.9278012 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.64811444 -0.5974421  -0.5832541  -0.68464595 -1.0310644  -0.69482505\n",
            " -0.64565367 -0.4049477  -1.0958972  -0.60999495 -0.67497003 -0.56915647\n",
            " -0.6451273  -0.5547003  -0.5483375  -0.6618006  -0.5817     -0.54409504\n",
            " -0.9384631 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.65277135 -0.6001986  -0.5853822  -0.68870544 -1.0401874  -0.699818\n",
            " -0.64768183 -0.39912432 -1.1090388  -0.61133635 -0.67667985 -0.5707074\n",
            " -0.64789045 -0.5575987  -0.5496697  -0.66287184 -0.582749   -0.5460969\n",
            " -0.9480637 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 6 from 8\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6569153  -0.60265136 -0.5872278  -0.69228446 -1.0482799  -0.70428455\n",
            " -0.6493238  -0.39308205 -1.1208565  -0.6124417  -0.6781727  -0.5720838\n",
            " -0.6504009  -0.56025434 -0.55070275 -0.6636617  -0.58352983 -0.5478359\n",
            " -0.9567007 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6580919  -0.6030489  -0.5866851  -0.69392544 -1.051175   -0.7068267\n",
            " -0.64912224 -0.3869192  -1.1065172  -0.6114969  -0.67747015 -0.5718282\n",
            " -0.64949703 -0.56010854 -0.5506091  -0.6617377  -0.58171654 -0.5472374\n",
            " -0.96162003]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.65909404 -0.6033765  -0.5861248  -0.6953141  -1.0536292  -0.70907253\n",
            " -0.648756   -0.38062614 -1.093544   -0.61054474 -0.6767892  -0.5715722\n",
            " -0.6487013  -0.5600189  -0.5503563  -0.6598339  -0.5799218  -0.54663485\n",
            " -0.9660009 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.65994054 -0.603642   -0.58555007 -0.6964778  -1.0556908  -0.7110528\n",
            " -0.6482451  -0.37422884 -1.0818025  -0.6095883  -0.676129   -0.5713172\n",
            " -0.6480039  -0.55998003 -0.5499635  -0.65795183 -0.5781465  -0.5460306\n",
            " -0.9698988 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6606482  -0.60385257 -0.5849639  -0.6974406  -1.0574027  -0.71279526\n",
            " -0.64760745 -0.36775088 -1.0711715  -0.60863006 -0.6754885  -0.5710642\n",
            " -0.64739555 -0.559987   -0.5494479  -0.656093   -0.57639164 -0.54542637\n",
            " -0.9733639 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6612323  -0.6040145  -0.584369   -0.6982249  -1.0588042  -0.71432483\n",
            " -0.64685905 -0.36121356 -1.0615425  -0.6076724  -0.6748668  -0.57081395\n",
            " -0.64686817 -0.56003535 -0.54882514 -0.6542587  -0.57465816 -0.544824\n",
            " -0.9764404 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6617063  -0.604134   -0.58376753 -0.69885033 -1.0599298  -0.715664\n",
            " -0.64601463 -0.35463595 -1.0528172  -0.60671747 -0.67426324 -0.5705674\n",
            " -0.64641416 -0.5601211  -0.5481093  -0.6524501  -0.5729467  -0.5442252\n",
            " -0.9791695 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66208273 -0.60421586 -0.58316183 -0.6993346  -1.0608113  -0.7168329\n",
            " -0.64508736 -0.34803528 -1.0449078  -0.60576725 -0.67367697 -0.5703254\n",
            " -0.64602697 -0.56024057 -0.5473129  -0.65066826 -0.5712583  -0.54363126\n",
            " -0.98158705]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66237247 -0.604265   -0.5825536  -0.6996938  -1.061477   -0.7178497\n",
            " -0.64408886 -0.34142697 -1.037735   -0.6048236  -0.6731074  -0.5700883\n",
            " -0.64570045 -0.5603905  -0.54644734 -0.64891434 -0.5695934  -0.5430435\n",
            " -0.9837258 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6625854  -0.60428554 -0.58194494 -0.6999421  -1.0619518  -0.71873075\n",
            " -0.64302987 -0.33482486 -1.0312274  -0.60388786 -0.672554   -0.56985676\n",
            " -0.6454291  -0.5605678  -0.5455228  -0.647189   -0.56795263 -0.54246294\n",
            " -0.9856152 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6627307  -0.6042813  -0.58133715 -0.7000927  -1.062259   -0.71949065\n",
            " -0.64192003 -0.32824132 -1.0253206  -0.60296154 -0.67201614 -0.5696313\n",
            " -0.6452079  -0.56076956 -0.5445485  -0.64549303 -0.56633675 -0.54189074\n",
            " -0.98728174]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66281635 -0.60425574 -0.5807317  -0.7001569  -1.0624189  -0.7201427\n",
            " -0.6407677  -0.32168734 -1.0199571  -0.60204583 -0.6714934  -0.5694121\n",
            " -0.6450323  -0.5609935  -0.5435326  -0.6438271  -0.5647459  -0.54132766\n",
            " -0.98874915]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66284955 -0.6042119  -0.5801297  -0.7001453  -1.0624503  -0.7206988\n",
            " -0.6395807  -0.31517276 -1.0150845  -0.6011418  -0.67098546 -0.56919956\n",
            " -0.6448981  -0.56123734 -0.54248255 -0.64219177 -0.56318074 -0.54077435\n",
            " -0.99003863]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66283685 -0.6041524  -0.5795324  -0.7000671  -1.062369   -0.7211694\n",
            " -0.63836604 -0.30870616 -1.010656   -0.60025036 -0.6704917  -0.56899405\n",
            " -0.6448016  -0.5614988  -0.5414047  -0.6405872  -0.5616416  -0.5402316\n",
            " -0.9911697 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66278386 -0.6040796  -0.57894063 -0.6999308  -1.06219    -0.7215642\n",
            " -0.63712966 -0.30229524 -1.0066295  -0.5993724  -0.6700116  -0.5687955\n",
            " -0.6447395  -0.56177616 -0.5403053  -0.6390139  -0.5601285  -0.5396998\n",
            " -0.9921595 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66269594 -0.603996   -0.57835543 -0.6997438  -1.0619266  -0.7218918\n",
            " -0.6358771  -0.29594672 -1.0029662  -0.5985086  -0.66954494 -0.56860423\n",
            " -0.6447085  -0.56206757 -0.5391894  -0.63747203 -0.55864185 -0.53917944\n",
            " -0.9930234 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6625779  -0.6039032  -0.5777773  -0.69951284 -1.0615903  -0.72215986\n",
            " -0.6346133  -0.28966647 -0.99963224 -0.59765947 -0.66909146 -0.5684203\n",
            " -0.6447059  -0.56237143 -0.53806174 -0.6359619  -0.5571817  -0.53867096\n",
            " -0.99377537]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6624336  -0.60380304 -0.5772071  -0.69924414 -1.061192   -0.7223754\n",
            " -0.6333426  -0.2834596  -0.99659634 -0.5968256  -0.66865057 -0.5682437\n",
            " -0.6447289  -0.56268644 -0.53692645 -0.6344836  -0.5557484  -0.5381745\n",
            " -0.9944278 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 5 from 5\n",
            "\n",
            "Step 36 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6622671  -0.60369706 -0.5766454  -0.6989429  -1.0607407  -0.72254443\n",
            " -0.6320689  -0.27733052 -0.99383056 -0.5960072  -0.6682222  -0.5680745\n",
            " -0.6447754  -0.56301117 -0.5357872  -0.63303685 -0.55434185 -0.53769046\n",
            " -0.99499196]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 37 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6629061  -0.6047246  -0.5770132  -0.6990895  -1.0620252  -0.7331352\n",
            " -0.63173187 -0.27201992 -0.99217474 -0.59593505 -0.6684454  -0.56818724\n",
            " -0.64548695 -0.56408197 -0.53515553 -0.63268864 -0.5535743  -0.5378438\n",
            " -0.99672216]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 38 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66407496 -0.6058975  -0.5781284  -0.70018363 -1.064654   -0.7431023\n",
            " -0.6335406  -0.27621654 -0.991152   -0.59696025 -0.6691543  -0.56843704\n",
            " -0.6456799  -0.5643502  -0.53648937 -0.63424706 -0.5547776  -0.5385704\n",
            " -0.9986413 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 39 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.66572523 -0.607206   -0.57991785 -0.7021369  -1.0685077  -0.75252587\n",
            " -0.63728786 -0.28889325 -0.9907075  -0.59897745 -0.6703035  -0.5688156\n",
            " -0.6454139  -0.56390256 -0.5395974  -0.63753057 -0.5577579  -0.53981996\n",
            " -1.0007423 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 40 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6678217  -0.6086512  -0.5823231  -0.7048776  -1.0735044  -0.76148623\n",
            " -0.64279646 -0.30916262 -0.9908115  -0.60190177 -0.6718583  -0.5693259\n",
            " -0.6447587  -0.56283146 -0.5443168  -0.64238966 -0.5623461  -0.5415615\n",
            " -1.0030369 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 41 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.67033935 -0.61024386 -0.58529806 -0.7083484  -1.0795935  -0.7700646\n",
            " -0.6499149  -0.33625692 -0.9914544  -0.60566676 -0.67379254 -0.56998\n",
            " -0.6437899  -0.5612323  -0.55050915 -0.648703   -0.5683944  -0.5437779\n",
            " -1.0055507 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 42 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.67326134 -0.6120012  -0.5888071  -0.7125032  -1.0867472  -0.7783417\n",
            " -0.6585131  -0.3695147  -0.9926417  -0.6102201  -0.6760875  -0.5707964\n",
            " -0.6425873  -0.5592022  -0.5580564  -0.6563709  -0.5757737  -0.5464639\n",
            " -1.0083215 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 43 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6765777  -0.6139454  -0.59282315 -0.71730596 -1.0949574  -0.7863982\n",
            " -0.66848    -0.40837014 -0.99438983 -0.61552167 -0.67873037 -0.5717979\n",
            " -0.6412324  -0.5568372  -0.56685793 -0.6653129  -0.58437115 -0.54962283\n",
            " -1.0113941 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 44 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.68028337 -0.616102   -0.5973257  -0.72272813 -1.1042286  -0.79431224\n",
            " -0.6797196  -0.45234513 -0.9967236  -0.6215408  -0.68171275 -0.5730106\n",
            " -0.6398064  -0.55423176 -0.5768272  -0.6754631  -0.59408796 -0.55326384\n",
            " -1.0148191 ]\n",
            "\n",
            "Taking action 17 from 7\n",
            "\n",
            "Step 45 reward=-1 new_state=[0 0 1 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6843771  -0.61849797 -0.60230005 -0.7287472  -1.114576   -0.80216044\n",
            " -0.6921496  -0.5010418  -0.9996726  -0.62825453 -0.68503034 -0.57446194\n",
            " -0.6383886  -0.5514769  -0.58788985 -0.6867677  -0.60483766 -0.5574006\n",
            " -1.0186498 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6888599  -0.62116134 -0.6077353  -0.73534584 -1.1260207  -0.8100158\n",
            " -0.70569766 -0.5541361  -1.0032693  -0.6356455  -0.688681   -0.57617974\n",
            " -0.6370558  -0.54865974 -0.5999811  -0.6991825  -0.61654395 -0.5620489\n",
            " -1.0229402 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6927944  -0.62322426 -0.61255956 -0.7412439  -1.136026   -0.81694424\n",
            " -0.7178017  -0.6025166  -1.0061843  -0.64209527 -0.69178534 -0.57764924\n",
            " -0.6355938  -0.54449046 -0.6106982  -0.7101984  -0.6269546  -0.56606346\n",
            " -1.0265262 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.69623476 -0.62474763 -0.6168332  -0.7465106  -1.1447346  -0.82302815\n",
            " -0.7286039  -0.64653605 -1.0084888  -0.6476977  -0.69439787 -0.5788954\n",
            " -0.63401705 -0.5390943  -0.62017715 -0.7199538  -0.63619775 -0.5695076\n",
            " -1.029478  ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.6992307  -0.6257877  -0.6206113  -0.7512082  -1.1522762  -0.82834405\n",
            " -0.7382335  -0.68653417 -1.0102471  -0.65253806 -0.69656885 -0.5799409\n",
            " -0.6323394  -0.53259194 -0.628541   -0.7285745  -0.64438957 -0.57243913\n",
            " -1.0318601 ]\n",
            "\n",
            "Taking action 11 from 13\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 1 0 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.70182717 -0.62639546 -0.6239437  -0.7553933  -1.1587688  -0.8329636\n",
            " -0.74680716 -0.7228349  -1.0115182  -0.65669405 -0.6983439  -0.5808066\n",
            " -0.6305741  -0.52509856 -0.63590145 -0.7361747  -0.6516353  -0.57491046\n",
            " -1.0337316 ]\n",
            "Epsilon reduced to 0.05314410000000002\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.70365137 -0.62532    -0.6265848  -0.7589174  -1.1631049  -0.8361983\n",
            " -0.75399834 -0.75574803 -1.0111289  -0.6594162  -0.6990465  -0.58120936\n",
            " -0.627741   -0.51036906 -0.64164364 -0.74217534 -0.6574741  -0.57629377\n",
            " -1.0340546 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.70519686 -0.6240348  -0.6288964  -0.7620479  -1.1667225  -0.8389368\n",
            " -0.7603801  -0.7855244  -1.0104809  -0.6616726  -0.699506   -0.5814998\n",
            " -0.62494594 -0.49550268 -0.64664865 -0.7474226  -0.66260546 -0.57737774\n",
            " -1.0340828 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.70649475 -0.6225703  -0.6309135  -0.76482594 -1.1697023  -0.8412322\n",
            " -0.7660361  -0.812444   -1.0096099  -0.66351587 -0.6997521  -0.58169234\n",
            " -0.6221935  -0.48055828 -0.6509944  -0.75199676 -0.6671039  -0.57819724\n",
            " -1.033854  ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7075733  -0.62095386 -0.632668   -0.76728874 -1.1721176  -0.8431321\n",
            " -0.7710419  -0.83676535 -1.0085481  -0.6649934  -0.6998112  -0.58179945\n",
            " -0.6194876  -0.4655893  -0.6547515  -0.75597036 -0.67103654 -0.57878387\n",
            " -1.0334028 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7084577  -0.61921006 -0.63418865 -0.7694697  -1.1740346  -0.8446808\n",
            " -0.77546597 -0.85872734 -1.0073243  -0.66614825 -0.6997075  -0.5818325\n",
            " -0.6168318  -0.4506445  -0.6579833  -0.7594087  -0.674464   -0.5791662\n",
            " -1.0327603 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.70917076 -0.61736107 -0.63550127 -0.7713987  -1.1755122  -0.8459174\n",
            " -0.7793694  -0.8785488  -1.0059644  -0.66701865 -0.6994626  -0.5818019\n",
            " -0.6142292  -0.43576765 -0.66074735 -0.7623706  -0.6774406  -0.57936966\n",
            " -1.0319542 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.709733   -0.6154268  -0.6366289  -0.77310306 -1.1766042  -0.8468785\n",
            " -0.78280777 -0.896431   -1.0044918  -0.66763943 -0.69909585 -0.5817167\n",
            " -0.61168253 -0.42099822 -0.6630949  -0.764909   -0.6800155  -0.5794174\n",
            " -1.0310094 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.71016276 -0.61342514 -0.63759243 -0.7746072  -1.1773583  -0.84759647\n",
            " -0.78583074 -0.91255754 -1.002927   -0.6680416  -0.69862485 -0.5815852\n",
            " -0.6091941  -0.40637165 -0.6650727  -0.76707137 -0.682233   -0.57932997\n",
            " -1.0299487 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.71047646 -0.6113721  -0.6384107  -0.77593267 -1.1778177  -0.84810066\n",
            " -0.788483   -0.9270962  -1.0012884  -0.6682531  -0.69806504 -0.58141464\n",
            " -0.60676605 -0.39191902 -0.6667225  -0.7689005  -0.6841327  -0.579126\n",
            " -1.028792  ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7106892  -0.6092819  -0.6391005  -0.77709955 -1.1780208  -0.84841794\n",
            " -0.7908049  -0.9401998  -0.9995929  -0.6682992  -0.6974306  -0.5812116\n",
            " -0.6043998  -0.37766832 -0.6680818  -0.770435   -0.68575054 -0.578822\n",
            " -1.0275574 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.71081406 -0.6071674  -0.6396768  -0.77812505 -1.1780024  -0.848572\n",
            " -0.7928327  -0.9520072  -0.9978547  -0.6682022  -0.6967342  -0.58098185\n",
            " -0.6020966  -0.36364385 -0.6691846  -0.7717091  -0.6871183  -0.578433\n",
            " -1.0262609 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.71086276 -0.6050398  -0.6401533  -0.7790254  -1.1777936  -0.84858453\n",
            " -0.79459864 -0.9626448  -0.99608713 -0.6679823  -0.69598675 -0.58073056\n",
            " -0.5998577  -0.34986687 -0.67006105 -0.77275395 -0.68826514 -0.57797205\n",
            " -1.0249162 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.710846   -0.6029089  -0.6405419  -0.7798144  -1.1774216  -0.84847474\n",
            " -0.79613227 -0.97222686 -0.9943013  -0.6676573  -0.69519836 -0.58046216\n",
            " -0.59768355 -0.3363557  -0.6707386  -0.7735971  -0.6892166  -0.5774513\n",
            " -1.0235366 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7107731  -0.6007837  -0.6408535  -0.78050494 -1.1769114  -0.8482598\n",
            " -0.7974593  -0.9808575  -0.99250746 -0.66724324 -0.69437766 -0.58018076\n",
            " -0.59557456 -0.32312593 -0.67124134 -0.7742634  -0.68999606 -0.5768808\n",
            " -1.0221323 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7106527  -0.598672   -0.6410979  -0.7811084  -1.1762844  -0.8479553\n",
            " -0.7986036  -0.9886303  -0.99071455 -0.6667545  -0.693533   -0.5798899\n",
            " -0.5935312  -0.31019074 -0.6715915  -0.77477527 -0.6906243  -0.5762704\n",
            " -1.0207137 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.71049184 -0.5965803  -0.6412835  -0.7816347  -1.1755607  -0.8475752\n",
            " -0.7995859  -0.99563056 -0.9889301  -0.6662036  -0.6926708  -0.57959247\n",
            " -0.59155315 -0.29756108 -0.6718085  -0.7751522  -0.6911199  -0.57562816\n",
            " -1.0192891 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 14\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7102977  -0.5945148  -0.6414181  -0.7820929  -1.1747572  -0.84713143\n",
            " -0.8004252  -1.001935   -0.98716104 -0.665602   -0.6917975  -0.5792912\n",
            " -0.5896402  -0.2852457  -0.6719101  -0.7754121  -0.69149935 -0.57496136\n",
            " -1.0178659 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7100263  -0.592272   -0.64104974 -0.781892   -1.1730833  -0.8462719\n",
            " -0.8003659  -1.0061696  -0.9850108  -0.6642133  -0.6899758  -0.5786077\n",
            " -0.5875597  -0.27344838 -0.661003   -0.7745285  -0.69104445 -0.57368875\n",
            " -1.0163293 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7097371  -0.59008515 -0.64068836 -0.7816973  -1.1714374  -0.84540457\n",
            " -0.80027133 -1.0099761  -0.9829259  -0.66286576 -0.6882457  -0.577962\n",
            " -0.5855661  -0.26196483 -0.6510949  -0.77365994 -0.6905737  -0.5724615\n",
            " -1.0148181 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.70943415 -0.58795565 -0.6403349  -0.7815091  -1.1698236  -0.84453464\n",
            " -0.8001472  -1.0133988  -0.9809067  -0.66155994 -0.6866021  -0.5773522\n",
            " -0.5836562  -0.25079894 -0.64209086 -0.7728088  -0.69009155 -0.57127905\n",
            " -1.0133356 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.709121   -0.5858848  -0.6399901  -0.7813281  -1.1682451  -0.84366655\n",
            " -0.7999988  -1.0164773  -0.9789531  -0.6602961  -0.68504    -0.5767764\n",
            " -0.581827   -0.23995343 -0.6339048  -0.77197695 -0.68960214 -0.5701404\n",
            " -1.0118848 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7088008  -0.58387345 -0.6396545  -0.78115445 -1.1667044  -0.8428041\n",
            " -0.7998306  -1.019247   -0.97706515 -0.6590741  -0.68355536 -0.5762327\n",
            " -0.58007586 -0.22942966 -0.6264595  -0.77116585 -0.68910897 -0.56904477\n",
            " -1.0104685 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.70847607 -0.58192205 -0.6393286  -0.7809884  -1.165204   -0.8419508\n",
            " -0.79964674 -1.0217398  -0.9752419  -0.6578941  -0.68214405 -0.5757194\n",
            " -0.57839966 -0.21922784 -0.6196847  -0.7703769  -0.68861526 -0.56799114\n",
            " -1.0090882 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.70814914 -0.58003104 -0.6390129  -0.7808302  -1.1637456  -0.8411093\n",
            " -0.79945064 -1.0239843  -0.97348285 -0.65675557 -0.6808023  -0.575235\n",
            " -0.5767959  -0.20934719 -0.6135172  -0.769611   -0.68812364 -0.56697863\n",
            " -1.0077455 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11 from 17\n",
            "\n",
            "Step 25 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7078223  -0.57820034 -0.6387075  -0.78067976 -1.1623304  -0.8402821\n",
            " -0.7992455  -1.0260059  -0.9717874  -0.6556583  -0.6795263  -0.57477784\n",
            " -0.57526183 -0.19978607 -0.6079001  -0.76886904 -0.68763626 -0.56600624\n",
            " -1.0064418 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 26 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.70576864 -0.57450944 -0.63812697 -0.77849376 -1.154726   -0.83738947\n",
            " -0.7980786  -1.0250069  -0.96669865 -0.6528698  -0.67724633 -0.57385087\n",
            " -0.5710736  -0.18920761 -0.6008488  -0.7658386  -0.6849544  -0.5338801\n",
            " -1.001849  ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 27 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7031433  -0.56811404 -0.63710463 -0.776365   -1.1454699  -0.83314717\n",
            " -0.7963741  -1.024199   -0.95943797 -0.6486466  -0.67358947 -0.572532\n",
            " -0.56516534 -0.16373733 -0.59302807 -0.76189244 -0.68147904 -0.50359756\n",
            " -0.9954671 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 28 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.7000308  -0.5593365  -0.63571066 -0.7743215  -1.1348156  -0.8277403\n",
            " -0.7942467  -1.0236146  -0.9502846  -0.64318734 -0.6687306  -0.57088786\n",
            " -0.55776244 -0.1253131  -0.584562   -0.75719213 -0.67733383 -0.47502124\n",
            " -0.9875443 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 29 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.69656456 -0.5485098  -0.63405675 -0.7724616  -1.123124   -0.82139754\n",
            " -0.79188883 -1.0233953  -0.93957555 -0.636745   -0.6628715  -0.56903976\n",
            " -0.5491453  -0.07566753 -0.5756039  -0.7519872  -0.67270553 -0.44806337\n",
            " -0.97840744]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 30 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6927873  -0.5358832  -0.63217723 -0.7707711  -1.1105375  -0.81424004\n",
            " -0.7893462  -1.0235103  -0.9275033  -0.6294467  -0.6561361  -0.5670126\n",
            " -0.5394635  -0.01631037 -0.5662308  -0.74635625 -0.66765934 -0.422578\n",
            " -0.9682107 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 31 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6887902  -0.52171886 -0.63014776 -0.7693051  -1.0973065  -0.8064341\n",
            " -0.7867428  -1.0240359  -0.91431844 -0.62147474 -0.6486772  -0.564886\n",
            " -0.5289232   0.05144067 -0.5565506  -0.74046576 -0.6623232  -0.39846933\n",
            " -0.95718783]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 32 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6846622  -0.50625813 -0.6280418  -0.7681215  -1.0836701  -0.79813594\n",
            " -0.7841983  -1.0250525  -0.9002573  -0.61300087 -0.64063704 -0.5627381\n",
            " -0.51771873  0.1264649  -0.5466634  -0.7344774  -0.6568205  -0.3756495\n",
            " -0.9455601 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 33 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.68048835 -0.48972207 -0.6259295  -0.76727813 -1.0698546  -0.78949094\n",
            " -0.7818266  -1.0266405  -0.88553894 -0.6041844  -0.63214827 -0.5606451\n",
            " -0.50603163  0.20783071 -0.53666097 -0.7285434  -0.6512685  -0.35403746\n",
            " -0.93353444]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 34 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.676349   -0.4723112  -0.6238762  -0.76683164 -1.0560685  -0.78063273\n",
            " -0.7797339  -1.0288782  -0.870366   -0.59517217 -0.6233326  -0.55867845\n",
            " -0.4940294   0.29478496 -0.52662665 -0.7228061  -0.6457765  -0.33355808\n",
            " -0.92130244]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 35 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.67231834 -0.45420688 -0.62194216 -0.766835   -1.0425031  -0.7716831\n",
            " -0.77801716 -1.0318385  -0.8549224  -0.5860974  -0.6143014  -0.55690473\n",
            " -0.48186523  0.38673994 -0.516635   -0.7173954  -0.6404451  -0.31414163\n",
            " -0.90903854]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 36 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6684645  -0.4355721  -0.6201815  -0.76733714 -1.0293306  -0.7627517\n",
            " -0.77676374 -1.0355878  -0.8393742  -0.57707936 -0.6051555  -0.5553847\n",
            " -0.46967757  0.48325813 -0.5067523  -0.71242976 -0.6353651  -0.29572338\n",
            " -0.8969001 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 37 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6648441  -0.41652822 -0.6186397  -0.76838315 -1.0166879  -0.7539255\n",
            " -0.776049   -1.04019    -0.8238502  -0.56821245 -0.59597397 -0.5541706\n",
            " -0.4575749   0.5841615  -0.49702412 -0.708007   -0.6306116  -0.2782213\n",
            " -0.8850122 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 38 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6615205  -0.39725178 -0.617362   -0.77000815 -1.0047402  -0.7453126\n",
            " -0.77594066 -1.0456824  -0.8085198  -0.5596108  -0.58686006 -0.55331314\n",
            " -0.44569713  0.68900895 -0.48752466 -0.70422924 -0.6262672  -0.26162347\n",
            " -0.8735273 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 39 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6585361  -0.37783226 -0.6163806  -0.7722449  -0.9935857  -0.7369767\n",
            " -0.7764924  -1.0521097  -0.793477   -0.551342   -0.5778731  -0.5528505\n",
            " -0.4341243   0.79783314 -0.47828388 -0.7011672  -0.62238634 -0.24585631\n",
            " -0.8625358 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 40 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.65593046 -0.35836923 -0.61572325 -0.7751177  -0.98331887 -0.7289817\n",
            " -0.77774733 -1.0595003  -0.7788186  -0.5434719  -0.5690746  -0.5528157\n",
            " -0.4229383   0.9106189  -0.4693368  -0.69888437 -0.6190189  -0.23087204\n",
            " -0.85212684]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 41 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.65373623 -0.33894897 -0.6154108  -0.77864426 -0.97401595 -0.7213808\n",
            " -0.77973825 -1.0678718  -0.7646264  -0.5360542  -0.5605172  -0.5532341\n",
            " -0.4122079   1.0274087  -0.46071154 -0.6974299  -0.6162052  -0.21662638\n",
            " -0.8423733 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 42 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.6519788  -0.31964615 -0.61545813 -0.78283477 -0.96573555 -0.7142174\n",
            " -0.78248656 -1.0772305  -0.7509672  -0.5291312  -0.5522453  -0.5541244\n",
            " -0.40198982  1.1482862  -0.45243007 -0.6968404  -0.6139756  -0.20307836\n",
            " -0.8333332 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 43 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.61551595 -0.29617637 -0.6123107  -0.78220963 -0.9504773  -0.7030661\n",
            " -0.78208965 -1.0817323  -0.73087776 -0.52091    -0.541087   -0.5535616\n",
            " -0.3881582   1.2576138  -0.4448319  -0.6931671  -0.6078749  -0.18872362\n",
            " -0.81798923]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 44 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5823537  -0.27336583 -0.6098741  -0.7827857  -0.93709934 -0.69285274\n",
            " -0.7828432  -1.0877708  -0.71210086 -0.5134126  -0.53059316 -0.5536778\n",
            " -0.37532958  1.3718002  -0.43757018 -0.690779   -0.60282797 -0.17518237\n",
            " -0.8041286 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 45 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5521695  -0.2512142  -0.60811126 -0.78449464 -0.92552483 -0.68354553\n",
            " -0.7846981  -1.0952609  -0.6945913  -0.50663024 -0.5207509  -0.55444926\n",
            " -0.36348078  1.4908934  -0.4306512  -0.68963206 -0.5987907  -0.1623916\n",
            " -0.79169285]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 46 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.5246794  -0.2297188  -0.6069839  -0.7872681  -0.91567427 -0.6751114\n",
            " -0.78760195 -1.1041157  -0.6783023  -0.50054944 -0.51154554 -0.5558497\n",
            " -0.3525854   1.6149423  -0.42407766 -0.68967855 -0.5957175  -0.15029573\n",
            " -0.7806213 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 47 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.49963275 -0.20887515 -0.60645294 -0.79103816 -0.9074649  -0.66751564\n",
            " -0.7914989  -1.1142454  -0.6631848  -0.4951521  -0.5029603  -0.5578501\n",
            " -0.3426143   1.7439778  -0.41784948 -0.690866   -0.59356064 -0.13884562\n",
            " -0.7708508 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 48 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.47680646 -0.18867749 -0.6064786  -0.7957358  -0.9008104  -0.66072136\n",
            " -0.7963295  -1.1255574  -0.6491885  -0.4904156  -0.49497703 -0.5604189\n",
            " -0.33353508  1.8779987  -0.41196352 -0.6931367  -0.59227026 -0.12799773\n",
            " -0.762316  ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 49 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.45600113 -0.16911933 -0.6070193  -0.80129087 -0.89562124 -0.6546903\n",
            " -0.8020305  -1.1379557  -0.63626146 -0.48631355 -0.48757556 -0.5635214\n",
            " -0.3253128   2.0169587  -0.40641463 -0.69642866 -0.5917943  -0.11771341\n",
            " -0.75494915]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 50 reward=1 new_state=[0 0 0 0 0 1 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.4370379  -0.15019406 -0.6080333  -0.8076314  -0.8918043  -0.64938253\n",
            " -0.8085346  -1.1513393  -0.62435037 -0.4828162  -0.4807343  -0.5671201\n",
            " -0.31790978  2.1607542  -0.401195   -0.7006743  -0.59207815 -0.1079582\n",
            " -0.74868023]\n",
            "Epsilon reduced to 0.04782969000000002\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.41913193 -0.13582008 -0.60790426 -0.81050074 -0.88646096 -0.64426506\n",
            " -0.8115851  -1.1587906  -0.6141655  -0.47922343 -0.47490028 -0.56859136\n",
            " -0.31111717  2.2581356  -0.39696097 -0.7018148  -0.5908324  -0.10001221\n",
            " -0.74209756]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.40261364 -0.12387262 -0.607355   -0.811969   -0.8808705  -0.63950086\n",
            " -0.8132001  -1.1636833  -0.6051496  -0.4757863  -0.46974412 -0.5692278\n",
            " -0.3049273   2.3336341  -0.39330074 -0.70175755 -0.5890967  -0.09317394\n",
            " -0.7357895 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.3874184  -0.1141409  -0.60639524 -0.81210804 -0.874991   -0.6350299\n",
            " -0.8134474  -1.1661646  -0.5971691  -0.47246253 -0.46518832 -0.56906927\n",
            " -0.29925746  2.3881583  -0.39015356 -0.70054483 -0.58687264 -0.08735223\n",
            " -0.7296883 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.37347507 -0.10642262 -0.6050393  -0.811      -0.8687978  -0.6308018\n",
            " -0.8124055  -1.1663935  -0.59010243 -0.46921828 -0.46116322 -0.56816286\n",
            " -0.29403692  2.4229481  -0.38746315 -0.69823277 -0.5841718  -0.08245985\n",
            " -0.7237412 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.36070865 -0.10052425 -0.603306   -0.80873597 -0.8622825  -0.626775\n",
            " -0.8101615  -1.1645391  -0.5838403  -0.46602762 -0.45760673 -0.56656206\n",
            " -0.28920597  2.439501   -0.3851776  -0.6948894  -0.58101463 -0.07841336\n",
            " -0.7179082 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.349042   -0.09626116 -0.601218   -0.8054138  -0.8554511  -0.6229166\n",
            " -0.8068107  -1.1607792  -0.5782845  -0.46287155 -0.45446402 -0.5643252\n",
            " -0.2847147   2.4395008  -0.3832496  -0.6905925  -0.57742923 -0.07513332\n",
            " -0.7121608 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.33839792 -0.09345801 -0.5988012  -0.80113596 -0.84832036 -0.61920047\n",
            " -0.802454   -1.1552951  -0.5733475  -0.4597373  -0.4516865  -0.56151426\n",
            " -0.2805215   2.4247458  -0.3816361  -0.68542826 -0.5734498  -0.07254411\n",
            " -0.7064798 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.32870018 -0.09194921 -0.596084   -0.7960077  -0.8409169  -0.6156068\n",
            " -0.79719585 -1.1482714  -0.5689517  -0.45661715 -0.44923156 -0.5581931\n",
            " -0.27659237  2.3970823  -0.38029778 -0.6794878  -0.56911534 -0.07057431\n",
            " -0.70085394]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.31987453 -0.09157929 -0.5930967  -0.79013425 -0.8332737  -0.6121211\n",
            " -0.791142   -1.1398909  -0.56502837 -0.4535078  -0.44706187 -0.5544259\n",
            " -0.2728993   2.3583512  -0.3791995  -0.6728655  -0.56446785 -0.06915651\n",
            " -0.69527835]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.3118496  -0.09220316 -0.5898708  -0.78361976 -0.82542825 -0.60873306\n",
            " -0.7843982  -1.1303326  -0.5615174  -0.45040917 -0.44514453 -0.5502762\n",
            " -0.26941937  2.3103385  -0.37830973 -0.6656562  -0.5595514  -0.06822758\n",
            " -0.6897528 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.30455732 -0.09368618 -0.5864382  -0.77656555 -0.81742156 -0.6054358\n",
            " -0.7770677  -1.1197703  -0.5583661  -0.44732368 -0.4434511  -0.5458058\n",
            " -0.26613393  2.2547398  -0.37760007 -0.6579542  -0.5544107  -0.06772866\n",
            " -0.68428093]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.29793343 -0.09590435 -0.5828303  -0.76906794 -0.80929565 -0.6022253\n",
            " -0.76925004 -1.108369   -0.5555286  -0.44425598 -0.4419567  -0.5410738\n",
            " -0.26302773  2.193134   -0.3770455  -0.64985025 -0.5490898  -0.0676052\n",
            " -0.6788688 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 13 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.2919177  -0.09874418 -0.57907844 -0.76121837 -0.8010924  -0.59909964\n",
            " -0.76104015 -1.0962849  -0.5529649  -0.4412117  -0.44063944 -0.53613555\n",
            " -0.2600881   2.1269624  -0.3766238  -0.64143133 -0.5436317  -0.06780688\n",
            " -0.6735239 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 14 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.2845492  -0.06335378 -0.57423306 -0.7523067  -0.79200286 -0.59419954\n",
            " -0.7532693  -1.084335   -0.54894453 -0.43641257 -0.4388217  -0.5312613\n",
            " -0.256473    2.0697293  -0.37570077 -0.6333989  -0.53779787 -0.06711359\n",
            " -0.6652293 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 15 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.27764332 -0.03535212 -0.56826264 -0.74096125 -0.781112   -0.5890708\n",
            " -0.7426608  -1.0679995  -0.54524106 -0.43122593 -0.4372671  -0.5248479\n",
            " -0.25281408  1.9855213  -0.3750765  -0.6227417  -0.53049004 -0.06745246\n",
            " -0.6563581 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 16 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.27118936 -0.01434839 -0.56130683 -0.72759485 -0.76871955 -0.5837688\n",
            " -0.72963893 -1.0479546  -0.54178655 -0.42572322 -0.43594295 -0.5171746\n",
            " -0.2491526   1.8806005  -0.37469304 -0.6098776  -0.52193594 -0.06869078\n",
            " -0.64704865]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 17 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.6516849e-01  1.7621368e-04 -5.5350649e-01 -7.1259010e-01\n",
            " -7.5510156e-01 -5.7834840e-01 -7.1460950e-01 -1.0248339e+00\n",
            " -5.3853154e-01 -4.1997311e-01 -4.3482515e-01 -5.0849265e-01\n",
            " -2.4552396e-01  1.7606817e+00 -3.7450272e-01 -5.9520376e-01\n",
            " -5.1235312e-01 -7.0700780e-02 -6.3741940e-01]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 18 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.25955635  0.00881802 -0.545      -0.69629675 -0.74050933 -0.572863\n",
            " -0.6979542  -0.999222   -0.53544104 -0.41404015 -0.4338963  -0.49902633\n",
            " -0.24195765  1.6308186  -0.37446633 -0.5790907  -0.50194615 -0.07336232\n",
            " -0.6275704 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 19 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.25432557  0.01221194 -0.5359208  -0.67903125 -0.7251693  -0.5673636\n",
            " -0.68002594 -0.9716518  -0.53249186 -0.407984   -0.4331442  -0.48897308\n",
            " -0.23847784  1.495357   -0.3745522  -0.5618783  -0.49090338 -0.07656421\n",
            " -0.6175863 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 20 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.24944697  0.01100164 -0.52639425 -0.6610765  -0.7092838  -0.5618979\n",
            " -0.6611464  -0.94260347 -0.5296695  -0.4018588  -0.43256056 -0.47850603\n",
            " -0.23510379  1.3579308  -0.3747347  -0.5438748  -0.47939658 -0.08020501\n",
            " -0.6075386 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 21 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.24489142  0.00581788 -0.5165371  -0.64268345 -0.69303226 -0.55651027\n",
            " -0.641606   -0.9125059  -0.5269662  -0.3957135  -0.4321403  -0.46777552\n",
            " -0.23185085  1.2214907  -0.37499335 -0.52535635 -0.46758014 -0.08419357\n",
            " -0.59748775]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 22 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-2.4031088e-01  7.9545379e-04 -4.9827170e-01 -6.2519598e-01\n",
            " -6.7716372e-01 -5.5059868e-01 -6.2310570e-01 -8.8448632e-01\n",
            " -5.2319455e-01 -3.8872263e-01 -4.3133056e-01 -4.5787776e-01\n",
            " -2.2864743e-01  1.1037753e+00 -3.7473261e-01 -5.0829309e-01\n",
            " -4.5635152e-01 -8.7871723e-02 -5.8720845e-01]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 23 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.23601797 -0.0073028  -0.48124576 -0.60753924 -0.6611384  -0.54489815\n",
            " -0.60426146 -0.8558072  -0.5196558  -0.38187134 -0.43073422 -0.4478364\n",
            " -0.22558576  0.9878734  -0.37458014 -0.4909731  -0.44497868 -0.09180717\n",
            " -0.5770288 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 24 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.23198634 -0.01797192 -0.46538645 -0.58988065 -0.6450757  -0.539431\n",
            " -0.585272   -0.8267671  -0.51633734 -0.37518162 -0.43034303 -0.43775326\n",
            " -0.22267154  0.87539756 -0.3745194  -0.4735866  -0.4335672  -0.09593555\n",
            " -0.56698257]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 25 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.22769481 -0.02729533 -0.45085934 -0.5727297  -0.62967944 -0.53372574\n",
            " -0.56752956 -0.80000496 -0.5123933  -0.3683375  -0.4294901  -0.42819226\n",
            " -0.21923593  0.7788223  -0.3743989  -0.45735982 -0.42283788 -0.09928796\n",
            " -0.5409825 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 26 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.2236596  -0.03859831 -0.43728244 -0.5557922  -0.6143637  -0.52833754\n",
            " -0.5498313  -0.7731043  -0.5087348  -0.36171806 -0.42889708 -0.4186898\n",
            " -0.21601662  0.6853712  -0.374362   -0.4412557  -0.4121693  -0.10282277\n",
            " -0.51826644]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 27 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.2198567  -0.05150018 -0.4245929  -0.53917    -0.5992057  -0.5232745\n",
            " -0.5323086  -0.74626935 -0.50534767 -0.355332   -0.4285519  -0.40931046\n",
            " -0.21300696  0.59582317 -0.37439856 -0.4253977  -0.40163139 -0.10649078\n",
            " -0.49846876]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 28 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.21626455 -0.06565954 -0.41273028 -0.52295065 -0.58427215 -0.51854503\n",
            " -0.515076   -0.71967673 -0.50222117 -0.34918672 -0.4284451  -0.4001093\n",
            " -0.21019973  0.5106884  -0.37450114 -0.40989268 -0.39128488 -0.11024937\n",
            " -0.4812467 ]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 29 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.21286377 -0.08077354 -0.40163794 -0.5072082  -0.5696201  -0.5141574\n",
            " -0.49823153 -0.6934782  -0.49934778 -0.34328866 -0.42856926 -0.39113277\n",
            " -0.20758703  0.43024987 -0.37466472 -0.39483172 -0.38118213 -0.114062\n",
            " -0.46628338]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 30 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.2096374  -0.09657642 -0.39126277 -0.49200436 -0.5552975  -0.51012\n",
            " -0.48185822 -0.6678017  -0.49672228 -0.33764303 -0.4289186  -0.38241935\n",
            " -0.20516048  0.35460204 -0.3748862  -0.3802915  -0.37136745 -0.11789769\n",
            " -0.45328927]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 31 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.2065704  -0.11283764 -0.38155615 -0.4773895  -0.54134375 -0.5064404\n",
            " -0.4660253  -0.64275384 -0.49434158 -0.3322539  -0.429489   -0.37400052\n",
            " -0.20291108  0.28368706 -0.37516433 -0.36633497 -0.3618777  -0.12173046\n",
            " -0.44200307]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 32 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.20364955 -0.12935936 -0.37247354 -0.46340367 -0.527791   -0.50312614\n",
            " -0.45078915 -0.61842203 -0.49220437 -0.32712436 -0.43027726 -0.3659014\n",
            " -0.20082939  0.21732739 -0.37549943 -0.3530134  -0.35274324 -0.12553886\n",
            " -0.43219042]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 33 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.20086353 -0.14597413 -0.36397457 -0.45007777 -0.51466435 -0.50018436\n",
            " -0.43619484 -0.5948763  -0.49031082 -0.3222564  -0.43128154 -0.3581414\n",
            " -0.19890553  0.15525495 -0.375893   -0.34036666 -0.34398818 -0.12930539\n",
            " -0.42364365]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 34 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.19820249 -0.16254209 -0.3560229  -0.43743438 -0.5019828  -0.4976215\n",
            " -0.42227703 -0.572171   -0.48866218 -0.31765115 -0.43250075 -0.3507347\n",
            " -0.19712918  0.09713569 -0.37634778 -0.32842517 -0.33563134 -0.13301623\n",
            " -0.41617942]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 35 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.19565806 -0.17894852 -0.34858596 -0.4254887  -0.48975983 -0.495444\n",
            " -0.40906137 -0.5503465  -0.48726112 -0.31330895 -0.43393463 -0.34369114\n",
            " -0.1954897   0.04259101 -0.37686747 -0.31721014 -0.32768664 -0.13666068\n",
            " -0.40963757]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 36 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.19322318 -0.1951012  -0.3416347  -0.41424945 -0.4780038  -0.49365747\n",
            " -0.39656532 -0.5294309  -0.4861107  -0.30922937 -0.43558344 -0.33701643\n",
            " -0.19397607 -0.0087847  -0.37745667 -0.30673525 -0.32016352 -0.14023083\n",
            " -0.40387872]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 37 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.19089177 -0.21092811 -0.33514333 -0.40371966 -0.46671867 -0.4922672\n",
            " -0.38479903 -0.5094416  -0.48521525 -0.3054114  -0.4374481  -0.33071277\n",
            " -0.19257702 -0.05740985 -0.3781205  -0.29700702 -0.31306773 -0.14372128\n",
            " -0.39878252]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 38 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.18865895 -0.22637501 -0.329089   -0.39389735 -0.45590466 -0.4912781\n",
            " -0.37376642 -0.49038607 -0.48457935 -0.3018533  -0.4395299  -0.3247792\n",
            " -0.19128105 -0.10370427 -0.37886488 -0.28802615 -0.3064016  -0.14712867\n",
            " -0.39424574]\n",
            "\n",
            "Taking action 13 from 13\n",
            "\n",
            "Step 39 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.18652064 -0.24140345 -0.32345137 -0.38477612 -0.4455583  -0.49069464\n",
            " -0.36346588 -0.47226357 -0.48420826 -0.29855293 -0.4418304  -0.31921217\n",
            " -0.19007644 -0.14808065 -0.379696   -0.27978778 -0.30016446 -0.15045154\n",
            " -0.39018032]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 18 from 18\n",
            "\n",
            "Step 40 reward=-1 new_state=[1 0 0 0 0 0 1 0 0]\n",
            "Predicted scores for each action in next step: [-0.18447337 -0.2559887  -0.3182126  -0.37634587 -0.43567312 -0.49052075\n",
            " -0.35389084 -0.45506603 -0.48410738 -0.29550773 -0.44435155 -0.31400555\n",
            " -0.1889513  -0.19093809 -0.38062048 -0.27228248 -0.29435298 -0.15368995\n",
            " -0.3865116 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 41 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18457934 -0.27249232 -0.3152234  -0.3719355  -0.42906326 -0.4923572\n",
            " -0.3468143  -0.440974   -0.48658985 -0.2949043  -0.44796306 -0.3105795\n",
            " -0.19009584 -0.22847727 -0.38162336 -0.2668715  -0.2902645  -0.15753226\n",
            " -0.40508574]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 42 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18465656 -0.2874021  -0.31256706 -0.36793524 -0.42299777 -0.4939831\n",
            " -0.34043616 -0.4282356  -0.48876902 -0.2943391  -0.45120886 -0.30750066\n",
            " -0.19108768 -0.2615548  -0.3824972  -0.2619709  -0.28654492 -0.16059345\n",
            " -0.42227393]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 43 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18470663 -0.30086103 -0.31020433 -0.36430427 -0.41742077 -0.49541852\n",
            " -0.33468756 -0.41671664 -0.49067363 -0.29380786 -0.45412493 -0.30473506\n",
            " -0.19194098 -0.2907518  -0.38325405 -0.25752982 -0.28315717 -0.16293317\n",
            " -0.43812102]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 44 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18473127 -0.3130008  -0.30810094 -0.3610057  -0.41228232 -0.4966817\n",
            " -0.32950646 -0.40629637 -0.49232987 -0.29330695 -0.45674363 -0.30225194\n",
            " -0.19266877 -0.31656563 -0.38390502 -0.25350267 -0.28006804 -0.16460675\n",
            " -0.45268458]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 45 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18473221 -0.32394218 -0.30622697 -0.35800648 -0.40753788 -0.49778953\n",
            " -0.3248369  -0.39686596 -0.49376142 -0.2928331  -0.45909426 -0.3000236\n",
            " -0.1932829  -0.33942315 -0.3844601  -0.24984828 -0.2772476  -0.16566586\n",
            " -0.46603066]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 46 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18471122 -0.33379596 -0.3045565  -0.35527688 -0.4031477  -0.498757\n",
            " -0.3206285  -0.3883276  -0.49499017 -0.2923835  -0.4612031  -0.29802495\n",
            " -0.19379428 -0.35969162 -0.3849285  -0.24652973 -0.27466917 -0.16615912\n",
            " -0.47823012]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 47 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18467006 -0.34266347 -0.3030665  -0.3527903  -0.3990764  -0.4995979\n",
            " -0.3168357  -0.38059306 -0.49603587 -0.2919556  -0.46309406 -0.29623345\n",
            " -0.19421285 -0.37768802 -0.3853187  -0.24351385 -0.27230865 -0.16613242\n",
            " -0.48935628]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 48 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18461043 -0.35063723 -0.3017369  -0.35052264 -0.39529228 -0.5003247\n",
            " -0.31341755 -0.37358302 -0.4969165  -0.2915473  -0.46478868 -0.29462868\n",
            " -0.19454768 -0.3936864  -0.38563836 -0.24077076 -0.27014458 -0.16562927\n",
            " -0.49948275]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 49 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18453398 -0.3578017  -0.3005501  -0.3484524  -0.39176732 -0.5009489\n",
            " -0.31033707 -0.3672261  -0.49764866 -0.29115668 -0.4663064  -0.29319224\n",
            " -0.19480705 -0.4079247  -0.38589448 -0.23827356 -0.26815763 -0.164691\n",
            " -0.508682  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 50 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-0.18444234 -0.3642339  -0.29949045 -0.3465603  -0.38847643 -0.5014808\n",
            " -0.30756098 -0.361458   -0.4982473  -0.29078212 -0.4676648  -0.29190746\n",
            " -0.19499862 -0.42060953 -0.38609362 -0.23599815 -0.26633048 -0.1633569\n",
            " -0.51702464]\n",
            "Epsilon reduced to 0.043046721000000024\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 1 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.1842438  -0.3698492  -0.2985586  -0.34472528 -0.38494578 -0.50180805\n",
            " -0.30506116 -0.3560691  -0.49848315 -0.29033297 -0.4688418  -0.2907992\n",
            " -0.19496149 -0.43189448 -0.38611525 -0.23382534 -0.26450408 -0.15978548\n",
            " -0.524413  ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 2 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18404257 -0.37488174 -0.29772657 -0.34304583 -0.38165283 -0.5020734\n",
            " -0.30280825 -0.3511744  -0.49863654 -0.28990626 -0.46989357 -0.28980997\n",
            " -0.19488686 -0.4419668  -0.38610393 -0.23184292 -0.26282263 -0.15606835\n",
            " -0.53109336]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 3 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18383935 -0.37938833 -0.29698363 -0.34150726 -0.37857652 -0.5022838\n",
            " -0.30077788 -0.34672588 -0.49871707 -0.2895003  -0.47083285 -0.28892785\n",
            " -0.19477934 -0.4509639  -0.38606358 -0.23003261 -0.2612726  -0.15222958\n",
            " -0.53712523]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 4 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18363486 -0.3834206  -0.29632038 -0.34009618 -0.37569797 -0.50244564\n",
            " -0.29894796 -0.34268028 -0.49873337 -0.28911352 -0.47167096 -0.288142\n",
            " -0.19464311 -0.45900673 -0.38599777 -0.22837791 -0.2598418  -0.14829138\n",
            " -0.5425644 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 5 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18342972 -0.38702536 -0.2957284  -0.3388006  -0.37300026 -0.5025642\n",
            " -0.29729864 -0.33899862 -0.4986935  -0.28874457 -0.47241825 -0.28744274\n",
            " -0.19448191 -0.4662011  -0.38590991 -0.22686401 -0.25851935 -0.14427434\n",
            " -0.5474628 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 6 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18322454 -0.39024496 -0.29520017 -0.3376098  -0.3704682  -0.50264496\n",
            " -0.295812   -0.33564577 -0.4986045  -0.2883922  -0.4730841  -0.28682125\n",
            " -0.19429915 -0.47264063 -0.3858028  -0.22547755 -0.2572953  -0.1401974\n",
            " -0.5518689 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 7 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.1830199  -0.39311773 -0.29472914 -0.33651403 -0.36808813 -0.5026923\n",
            " -0.29447195 -0.33259007 -0.49847296 -0.28805533 -0.4736768  -0.28626966\n",
            " -0.19409797 -0.47840768 -0.3856793  -0.22420648 -0.25616083 -0.13607806\n",
            " -0.5558274 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 8 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18281609 -0.39568102 -0.29430884 -0.33550352 -0.36584553 -0.5027105\n",
            " -0.29326272 -0.3298003  -0.4983044  -0.28773266 -0.47420442 -0.28578028\n",
            " -0.19388098 -0.48358023 -0.38554156 -0.22303884 -0.25510693 -0.1319283\n",
            " -0.55938303]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 9 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18261397 -0.39796114 -0.29393482 -0.3345724  -0.36373383 -0.50270325\n",
            " -0.29217362 -0.32725635 -0.49810445 -0.28742394 -0.47467273 -0.28534767\n",
            " -0.19365114 -0.48821253 -0.3853921  -0.22196718 -0.25412852 -0.12777106\n",
            " -0.56256676]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 10 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18241385 -0.39998898 -0.293602   -0.33371258 -0.36174092 -0.5026738\n",
            " -0.2911916  -0.3249323  -0.49787775 -0.28712806 -0.4750883  -0.28496557\n",
            " -0.19341055 -0.49236685 -0.38523275 -0.22098154 -0.25321802 -0.12361576\n",
            " -0.5654169 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 11 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18221603 -0.40179026 -0.29330605 -0.3329177  -0.3598578  -0.50262535\n",
            " -0.2903061  -0.32280734 -0.4976285  -0.2868443  -0.47545668 -0.2846287\n",
            " -0.19316128 -0.49609402 -0.38506532 -0.220074   -0.25236964 -0.1194746\n",
            " -0.5679652 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 12 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.1820209  -0.40338814 -0.29304314 -0.33218187 -0.35807645 -0.50256056\n",
            " -0.28950745 -0.32086265 -0.4973606  -0.28657204 -0.47578287 -0.28433233\n",
            " -0.19290522 -0.499439   -0.38489133 -0.21923742 -0.25157812 -0.11535879\n",
            " -0.5702409 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 13 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18182883 -0.4048035  -0.2928099  -0.33150002 -0.35638958 -0.50248176\n",
            " -0.28878707 -0.3190813  -0.4970774  -0.28631067 -0.4760713  -0.28407228\n",
            " -0.19264406 -0.5024421  -0.3847123  -0.21846536 -0.25083873 -0.11127834\n",
            " -0.5722705 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 14 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18164    -0.40605527 -0.29260325 -0.33086735 -0.35479063 -0.5023913\n",
            " -0.28813714 -0.31744808 -0.49678195 -0.28605968 -0.47632587 -0.28384465\n",
            " -0.19237933 -0.5051389  -0.38452947 -0.21775202 -0.25014716 -0.10724232\n",
            " -0.5740783 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 15 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18145475 -0.40716046 -0.29242048 -0.3302797  -0.35327357 -0.50229096\n",
            " -0.28755072 -0.3159492  -0.49647704 -0.28581855 -0.47655037 -0.283646\n",
            " -0.19211236 -0.5075613  -0.38434398 -0.21709219 -0.24949959 -0.10325879\n",
            " -0.5756862 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 7 from 5\n",
            "\n",
            "Step 16 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18127325 -0.40813437 -0.29225907 -0.32973322 -0.35183316 -0.50218254\n",
            " -0.28702146 -0.31457233 -0.49616513 -0.2855869  -0.4767479  -0.28347325\n",
            " -0.19184439 -0.50973785 -0.3841569  -0.21648112 -0.24889252 -0.09933494\n",
            " -0.5771143 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 17 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18212706 -0.41043752 -0.29338485 -0.32995847 -0.3530728  -0.5115244\n",
            " -0.28770706 -0.3144473  -0.49727133 -0.28641033 -0.4779     -0.28378597\n",
            " -0.19272134 -0.5121993  -0.38450745 -0.2173585  -0.24917209 -0.09628935\n",
            " -0.5802715 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 18 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18358904 -0.41359723 -0.29425347 -0.330961   -0.35766396 -0.5208293\n",
            " -0.2883541  -0.31556103 -0.5000384  -0.28783527 -0.47917017 -0.28376985\n",
            " -0.19481061 -0.51451516 -0.38577121 -0.21892196 -0.2505362  -0.10867108\n",
            " -0.5842993 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 19 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18559659 -0.41752338 -0.29489106 -0.33266336 -0.36526436 -0.5300996\n",
            " -0.28896984 -0.31778967 -0.5042962  -0.28980243 -0.4805483  -0.28346226\n",
            " -0.19798827 -0.51669794 -0.3878574  -0.22110417 -0.25287795 -0.13405995\n",
            " -0.5891065 ]\n",
            "\n",
            "Taking action 17 from 17\n",
            "\n",
            "Step 20 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.18810004 -0.42218065 -0.2953381  -0.33499122 -0.37555093 -0.539363\n",
            " -0.2895372  -0.32098657 -0.50992453 -0.29225916 -0.4820487  -0.28289425\n",
            " -0.2021367  -0.51879525 -0.3906889  -0.22382659 -0.25609264 -0.1705367\n",
            " -0.5946452 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 21 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.1910569  -0.42755565 -0.29563937 -0.33787644 -0.3882246  -0.5486573\n",
            " -0.29003102 -0.32500488 -0.5168284  -0.2951596  -0.483695   -0.28209472\n",
            " -0.20714797 -0.5208656  -0.39419794 -0.22701156 -0.26008314 -0.2165592\n",
            " -0.60088754]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 22 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.20297074 -0.43503347 -0.29702824 -0.34194654 -0.40218753 -0.55874467\n",
            " -0.29115766 -0.32958797 -0.52587163 -0.298396   -0.48660934 -0.28164548\n",
            " -0.2133089  -0.52265406 -0.39728427 -0.23060298 -0.26513407 -0.2589879\n",
            " -0.60913086]\n",
            "\n",
            "Taking action 4 from 12\n",
            "\n",
            "Step 23 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.22272483 -0.4444015  -0.29939404 -0.3470776  -0.417298   -0.5695355\n",
            " -0.29284337 -0.33466876 -0.5368305  -0.30193442 -0.49066168 -0.28150916\n",
            " -0.22049442 -0.5241934  -0.39998958 -0.23454978 -0.27113378 -0.29817134\n",
            " -0.61916894]\n",
            "\n",
            "Taking action 4 from 12\n",
            "\n",
            "Step 24 reward=-1 new_state=[0 0 0 1 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.24218029 -0.45370397 -0.30205375 -0.35319287 -0.43475598 -0.5810889\n",
            " -0.2954164  -0.3400671  -0.5505152  -0.30600333 -0.49690226 -0.2831787\n",
            " -0.23680478 -0.525496   -0.40291113 -0.24147348 -0.27814564 -0.33493346\n",
            " -0.6309539 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2613518  -0.46295342 -0.30497545 -0.36018655 -0.4543074  -0.59331167\n",
            " -0.29877508 -0.34574562 -0.56663764 -0.31054646 -0.50510395 -0.2864644\n",
            " -0.26118213 -0.52658844 -0.40602905 -0.2510574  -0.2860588  -0.36947483\n",
            " -0.6442976 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 26 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.27861488 -0.47131035 -0.30757758 -0.36641622 -0.47169852 -0.60420704\n",
            " -0.30159488 -0.35066652 -0.5810025  -0.31457224 -0.5123599  -0.2893367\n",
            " -0.28300083 -0.527544   -0.40872782 -0.25731665 -0.2930471  -0.40070245\n",
            " -0.6562197 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 27 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.2941541  -0.4788645  -0.30989128 -0.37195542 -0.48713732 -0.6139029\n",
            " -0.30392343 -0.3548993  -0.59377927 -0.31813025 -0.5187604  -0.29183424\n",
            " -0.30250543 -0.52837586 -0.4110464  -0.260424   -0.29919776 -0.42892134\n",
            " -0.6668587 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 28 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3081372  -0.48569736 -0.31194532 -0.3768715  -0.50081515 -0.62251693\n",
            " -0.3058064  -0.35850877 -0.6051236  -0.32126608 -0.5243887  -0.29399297\n",
            " -0.31991997 -0.5290959  -0.41302165 -0.2605926  -0.3045911  -0.45441172\n",
            " -0.6763414 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 29 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.32071638 -0.49188316 -0.3137663  -0.3812263  -0.512908   -0.63015705\n",
            " -0.3072873  -0.3615557  -0.61517835 -0.324022   -0.529322   -0.29584652\n",
            " -0.3354495  -0.5297154  -0.4146887  -0.2580614  -0.3093012  -0.47743058\n",
            " -0.68478405]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 30 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.33202955 -0.4974887  -0.31537858 -0.3850763  -0.52357745 -0.63692236\n",
            " -0.30840763 -0.36409694 -0.6240742  -0.32643685 -0.5336315  -0.2974261\n",
            " -0.34928164 -0.53024507 -0.41608062 -0.25308433 -0.31339633 -0.49821305\n",
            " -0.6922929 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.34220225 -0.50257486 -0.31680465 -0.38847324 -0.5329717  -0.6429036\n",
            " -0.30920687 -0.36618555 -0.6319312  -0.32854643 -0.53738296 -0.2987609\n",
            " -0.3615878  -0.53069484 -0.41722828 -0.24592239 -0.31693935 -0.5169739\n",
            " -0.69896483]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3513479  -0.5071961  -0.31806505 -0.39146444 -0.5412267  -0.64818406\n",
            " -0.30972245 -0.36787108 -0.6388589  -0.33038372 -0.5406368  -0.29987776\n",
            " -0.37252453 -0.53107405 -0.41816074 -0.23683791 -0.31998837 -0.5339091\n",
            " -0.7048886 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.35956928 -0.51140136 -0.31917843 -0.39409298 -0.5484666  -0.6528394\n",
            " -0.30998972 -0.36919978 -0.6449577  -0.33197892 -0.5434486  -0.30080175\n",
            " -0.3822345  -0.5313914  -0.41890478 -0.22609033 -0.32259688 -0.5491967\n",
            " -0.7101446 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.36695945 -0.51523453 -0.32016176 -0.39639843 -0.55480486 -0.6569389\n",
            " -0.310042   -0.37021464 -0.65031886 -0.33335963 -0.5458695  -0.30155596\n",
            " -0.39084774 -0.5316547  -0.41948533 -0.21393289 -0.32481402 -0.5629988\n",
            " -0.71480584]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.37360257 -0.5187344  -0.32103038 -0.39841667 -0.56034446 -0.6605452\n",
            " -0.30991045 -0.37095565 -0.6550254  -0.33455107 -0.54794586 -0.3021616\n",
            " -0.3984822  -0.5318714  -0.4199251  -0.20061049 -0.32668516 -0.57546186\n",
            " -0.71893865]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.37957448 -0.52193534 -0.321798   -0.40018025 -0.5651791  -0.6637151\n",
            " -0.30962425 -0.37145987 -0.6591526  -0.33557615 -0.54972017 -0.3026382\n",
            " -0.40524492 -0.532048   -0.42024508 -0.18635806 -0.32825184 -0.5867186\n",
            " -0.7226027 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.38494357 -0.5248678  -0.322477   -0.40171883 -0.5693933  -0.6665002\n",
            " -0.30921048 -0.37176156 -0.66276884 -0.33645567 -0.55123085 -0.30300367\n",
            " -0.41123265 -0.53219056 -0.4204641  -0.17139925 -0.3295522  -0.5968889\n",
            " -0.7258519 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 38 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.3897716  -0.52755815 -0.32307827 -0.40305918 -0.57306325 -0.6689464\n",
            " -0.3086941  -0.37189227 -0.6659353  -0.33720845 -0.5525125  -0.30327427\n",
            " -0.41653287 -0.5323044  -0.4205993  -0.1559459  -0.33062112 -0.6060804\n",
            " -0.7287342 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 39 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39411402 -0.5300296  -0.32361156 -0.40422565 -0.5762577  -0.67109525\n",
            " -0.3080981  -0.37188095 -0.66870713 -0.33785135 -0.5535967  -0.30346483\n",
            " -0.42122442 -0.5323944  -0.42066592 -0.14019734 -0.33149037 -0.61439025\n",
            " -0.7312929 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 40 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.39802057 -0.53230214 -0.32408538 -0.40523988 -0.5790378  -0.67298365\n",
            " -0.3074435  -0.37175414 -0.67113394 -0.3383994  -0.5545111  -0.30358884\n",
            " -0.4253779  -0.5324646  -0.42067757 -0.12434012 -0.33218887 -0.62190527\n",
            " -0.73356605]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 41 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.401536   -0.5343931  -0.32450718 -0.4061216  -0.58145833 -0.6746446\n",
            " -0.30674925 -0.3715359  -0.67325974 -0.3388663  -0.5552809  -0.30365834\n",
            " -0.42905676 -0.53251886 -0.42064625 -0.10854785 -0.33274278 -0.6287035\n",
            " -0.73558784]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 42 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40146318 -0.5353507  -0.32450253 -0.40641615 -0.58279586 -0.67555046\n",
            " -0.30592832 -0.37104976 -0.674206   -0.33907264 -0.5554677  -0.30363283\n",
            " -0.4318086  -0.53256106 -0.4206406  -0.09411802 -0.33274978 -0.63451684\n",
            " -0.73649967]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 43 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40139163 -0.53625    -0.32449555 -0.40665758 -0.5839406  -0.6763382\n",
            " -0.30510947 -0.37053227 -0.6750152  -0.33923894 -0.55559653 -0.30357805\n",
            " -0.43424183 -0.53259283 -0.42060533 -0.07996278 -0.33269852 -0.639777\n",
            " -0.737304  ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 44 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40132242 -0.5370928  -0.3244874  -0.40685502 -0.5849221  -0.67702484\n",
            " -0.30430424 -0.3699981  -0.6757083  -0.33937243 -0.55567974 -0.30350167\n",
            " -0.43639606 -0.53261644 -0.42054856 -0.06621102 -0.33260322 -0.6445371\n",
            " -0.7380153 ]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 45 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40125674 -0.5378803  -0.3244788  -0.40701637 -0.5857662  -0.6776252\n",
            " -0.3035229  -0.3694603  -0.676304   -0.33947945 -0.55572796 -0.30341053\n",
            " -0.43830636 -0.5326338  -0.4204771  -0.05297845 -0.33247653 -0.6488446\n",
            " -0.73864603]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 46 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4011952  -0.5386135  -0.3244704  -0.40714872 -0.5864956  -0.67815197\n",
            " -0.3027741  -0.36893037 -0.6768184  -0.3395654  -0.5557504  -0.30331063\n",
            " -0.44000375 -0.5326464  -0.42039692 -0.04036728 -0.3323297  -0.65274185\n",
            " -0.73920697]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 47 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.40113807 -0.53929275 -0.32446253 -0.4072581  -0.5871296  -0.6786163\n",
            " -0.30206525 -0.36841816 -0.67726505 -0.33963507 -0.555755   -0.30320704\n",
            " -0.4415155  -0.5326556  -0.42031282 -0.02846622 -0.33217227 -0.6562668\n",
            " -0.73970723]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 48 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4010854  -0.5399183  -0.32445535 -0.40734977 -0.58768487 -0.6790278\n",
            " -0.30140245 -0.36793202 -0.6776563  -0.3396925  -0.55574816 -0.30310413\n",
            " -0.44286567 -0.5326624  -0.42022884 -0.01735018 -0.33201274 -0.65945315\n",
            " -0.74015474]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 49 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4010373  -0.5404907  -0.3244489  -0.40742815 -0.58817583 -0.6793947\n",
            " -0.3007904  -0.3674789  -0.6780021  -0.339741   -0.55573547 -0.3030056\n",
            " -0.44407523 -0.5326676  -0.4201483  -0.0070801  -0.33185813 -0.66233164\n",
            " -0.74055624]\n",
            "\n",
            "Taking action 11 from 15\n",
            "\n",
            "Step 50 reward=0 new_state=[0 0 0 1 0 1 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.4009934  -0.54101    -0.32444316 -0.4074971  -0.58861446 -0.67972374\n",
            " -0.30023286 -0.36706442 -0.678311   -0.33978337 -0.5557214  -0.30291444\n",
            " -0.44516265 -0.5326719  -0.4200738   0.00229677 -0.33171442 -0.6649293\n",
            " -0.74091744]\n",
            "Epsilon reduced to 0.03874204890000002\n",
            " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
            "\n",
            "Taking action 5 from 15\n",
            "\n",
            "Step 1 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.40095362 -0.54147214 -0.32443786 -0.40756205 -0.5890159  -0.6800229\n",
            " -0.29974008 -0.3667008  -0.67859375 -0.33982366 -0.55571294 -0.3028361\n",
            " -0.44614774 -0.53267634 -0.42000973  0.010633   -0.33159205 -0.6672682\n",
            " -0.74124384]\n",
            "\n",
            "Taking action 5 from 15\n",
            "\n",
            "Step 2 reward=-2 new_state=[0 0 1 0 0 0 0 0 0]\n",
            "Predicted scores for each action in next step: [-0.40091693 -0.53678966 -0.3243692  -0.40994585 -0.59456015 -0.68251956\n",
            " -0.30760154 -0.37496227 -0.682498   -0.3416325  -0.55928075 -0.30605426\n",
            " -0.45115793 -0.5330666  -0.42262602 -0.04239261 -0.33762607 -0.66635334\n",
            " -0.7423583 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 3 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.40098137 -0.52804846 -0.3244755  -0.4144219  -0.60495603 -0.6872585\n",
            " -0.3226773  -0.3906526  -0.68991435 -0.34510773 -0.56615263 -0.31214556\n",
            " -0.45997012 -0.53392243 -0.42771995 -0.13152708 -0.34905064 -0.66304576\n",
            " -0.74450755]\n",
            "\n",
            "Taking action 5 from 15\n",
            "\n",
            "Step 4 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.40127704 -0.5203509  -0.3248816  -0.41967744 -0.6156385  -0.692225\n",
            " -0.33660525 -0.40569383 -0.69834155 -0.3495856  -0.5734058  -0.3272297\n",
            " -0.46939796 -0.53477144 -0.4328459  -0.21244717 -0.36089662 -0.6598072\n",
            " -0.7480331 ]\n",
            "\n",
            "Taking action 5 from 15\n",
            "\n",
            "Step 5 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.40175468 -0.5116409  -0.3255338  -0.42567426 -0.62815094 -0.69824505\n",
            " -0.35303348 -0.4231631  -0.7082651  -0.35470575 -0.58198833 -0.3423046\n",
            " -0.4802523  -0.5359947  -0.43889922 -0.3052461  -0.37464958 -0.65609616\n",
            " -0.752151  ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 6 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.40247867 -0.5022768  -0.32651827 -0.43239036 -0.6424153  -0.705372\n",
            " -0.37165123 -0.4427137  -0.71973497 -0.3604924  -0.5918785  -0.35736334\n",
            " -0.4925045  -0.53765774 -0.44582605 -0.40788287 -0.39016205 -0.65220034\n",
            " -0.7569963 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 7 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.40427572 -0.49559405 -0.34012115 -0.44022238 -0.6576822  -0.71395016\n",
            " -0.39009303 -0.4620104  -0.7327657  -0.368662   -0.60161614 -0.37125352\n",
            " -0.5040485  -0.539243   -0.45305216 -0.5005711  -0.4052449  -0.648888\n",
            " -0.7639891 ]\n",
            "\n",
            "Taking action 2 from 2\n",
            "\n",
            "Step 8 reward=-1 new_state=[0 0 1 0 0 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.40704387 -0.49133995 -0.36493593 -0.44905332 -0.6738589  -0.72384495\n",
            " -0.40836418 -0.48106882 -0.74721724 -0.37896898 -0.611223   -0.38411647\n",
            " -0.5149688  -0.5407692  -0.46054488 -0.5846776  -0.4199486  -0.64613014\n",
            " -0.7729219 ]\n",
            "\n",
            "Taking action 9 from 9\n",
            "\n",
            "Step 9 reward=-2 new_state=[0 0 1 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.41069913 -0.48930076 -0.39980164 -0.45878208 -0.6908742  -0.73494804\n",
            " -0.42646933 -0.49990186 -0.7629837  -0.39119488 -0.6207291  -0.39608073\n",
            " -0.52535576 -0.5422635  -0.4682784  -0.6613718  -0.4343284  -0.64392054\n",
            " -0.7836233 ]\n",
            "\n",
            "Taking action 15 from 11\n",
            "\n",
            "Step 10 reward=-2 new_state=[0 0 1 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.41528735 -0.49251887 -0.437437   -0.47407365 -0.7111665  -0.74849695\n",
            " -0.4465127  -0.52172023 -0.78197646 -0.43948165 -0.6294966  -0.4100865\n",
            " -0.53637403 -0.54395914 -0.478553   -0.7340469  -0.45386466 -0.6435137\n",
            " -0.79983854]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 11 reward=-2 new_state=[0 0 1 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.4200394  -0.4960365  -0.47247326 -0.49082944 -0.7327332  -0.76252645\n",
            " -0.46527445 -0.54347885 -0.8034911  -0.4866177  -0.6400007  -0.44520786\n",
            " -0.550001   -0.54580426 -0.4890976  -0.8034824  -0.4751975  -0.64295447\n",
            " -0.8184793 ]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 12 reward=-2 new_state=[0 0 1 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.44352692 -0.5047556  -0.50682276 -0.50892603 -0.75721824 -0.7786768\n",
            " -0.48333442 -0.56481683 -0.82862353 -0.53089136 -0.6525162  -0.47784534\n",
            " -0.56561476 -0.5474978  -0.49845958 -0.86809385 -0.49735475 -0.6444142\n",
            " -0.84070194]\n",
            "\n",
            "Taking action 0 from 0\n",
            "\n",
            "Step 13 reward=-2 new_state=[0 0 1 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.48290944 -0.5181092  -0.5406171  -0.52819854 -0.78428817 -0.7967068\n",
            " -0.5007397  -0.58574843 -0.8569667  -0.5726806  -0.66680706 -0.5082621\n",
            " -0.5829882  -0.5490557  -0.5067616  -0.9285437  -0.5202349  -0.64769816\n",
            " -0.8660995 ]\n",
            "\n",
            "Taking action 4 from 14\n",
            "\n",
            "Step 14 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.53623575 -0.5356075  -0.5739753  -0.5485176  -0.81368375 -0.8164308\n",
            " -0.5175487  -0.60629964 -0.8882058  -0.6123313  -0.68268794 -0.5367068\n",
            " -0.601954   -0.55050653 -0.5141273  -0.9854357  -0.5437764  -0.6526639\n",
            " -0.8943377 ]\n",
            "\n",
            "Taking action 10 from 6\n",
            "\n",
            "Step 15 reward=-1 new_state=[0 0 0 0 1 0 0 1 0]\n",
            "Predicted scores for each action in next step: [-0.584932   -0.5519948  -0.60534954 -0.56783533 -0.84175265 -0.8350567\n",
            " -0.5340781  -0.62773633 -0.91725063 -0.65010756 -0.69898355 -0.56335354\n",
            " -0.6194898  -0.5521711  -0.5420693  -1.039228   -0.5664556  -0.6581614\n",
            " -0.920231  ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 16 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.6299703  -0.56647205 -0.6354923  -0.58738387 -0.8698753  -0.85361606\n",
            " -0.5660145  -0.65041363 -0.94516134 -0.68636435 -0.7149304  -0.5880269\n",
            " -0.6361458  -0.5540054  -0.56918406 -1.0921497  -0.5885993  -0.6627642\n",
            " -0.94564223]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 17 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.67103624 -0.5794494  -0.66284007 -0.6049603  -0.8952484  -0.87022275\n",
            " -0.59508884 -0.6708677  -0.9702441  -0.71928716 -0.7292135  -0.6104523\n",
            " -0.6511594  -0.55476886 -0.5940243  -1.1401616  -0.6085337  -0.66681385\n",
            " -0.96850604]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 18 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.70842564 -0.5910752  -0.68762386 -0.62075937 -0.91814387 -0.8850698\n",
            " -0.621517   -0.6893182  -0.9927784  -0.7491423  -0.7419971  -0.6308032\n",
            " -0.66469294 -0.55452245 -0.6167237  -1.1836435  -0.62647706 -0.6703647\n",
            " -0.9890743 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 19 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.74242365 -0.6014831  -0.71006054 -0.6349567  -0.938807   -0.89833117\n",
            " -0.6455063  -0.70596325 -1.0130165  -0.7761824  -0.7534296  -0.649245\n",
            " -0.676893   -0.5533292  -0.6374204  -1.2229582  -0.6426264  -0.6734662\n",
            " -1.0075741 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 20 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.77330244 -0.6107943  -0.73035294 -0.64771116 -0.95745873 -0.91016424\n",
            " -0.66725415 -0.7209816  -1.0311859  -0.8006453  -0.76364493 -0.6659353\n",
            " -0.68789166 -0.5512533  -0.6562531  -1.2584504  -0.6571595  -0.676163\n",
            " -1.0242105 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 21 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.8013191  -0.6191177  -0.7486898  -0.65916574 -0.97429883 -0.92071116\n",
            " -0.6869472  -0.73453426 -1.0474924  -0.8227532  -0.772764   -0.6810216\n",
            " -0.69780827 -0.5483589  -0.6733582  -1.290444   -0.6702367  -0.67849594\n",
            " -1.0391688 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 22 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.8267162  -0.6265521  -0.7652458  -0.6694493  -0.98950684 -0.9301003\n",
            " -0.70476055 -0.7467667  -1.0621214  -0.84271324 -0.7808963  -0.69464254\n",
            " -0.7067505  -0.54470944 -0.6888683  -1.3192425  -0.68200284 -0.68050194\n",
            " -1.0526159 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 23 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.84971905 -0.63318676 -0.7801827  -0.6786787  -1.0032455  -0.9384481\n",
            " -0.7208578  -0.75780964 -1.0752409  -0.86071795 -0.7881404  -0.7069273\n",
            " -0.7148153  -0.5403678  -0.7029104  -1.3451285  -0.69258827 -0.6822146\n",
            " -1.0647025 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 24 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.8705386  -0.639102   -0.79364884 -0.6869587  -1.015661   -0.94585896\n",
            " -0.73539096 -0.76778126 -1.0870016  -0.87694466 -0.7945854  -0.7179953\n",
            " -0.7220905  -0.5353946  -0.7156055  -1.3683639  -0.7021112  -0.68366444\n",
            " -1.0755651 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 25 reward=0 new_state=[0 0 0 0 1 0 0 1 1]\n",
            "Predicted scores for each action in next step: [-0.8893701  -0.64437073 -0.80578125 -0.69438434 -1.0268852  -0.95242786\n",
            " -0.74850094 -0.7767881  -1.0975403  -0.89155734 -0.8003119  -0.72795737\n",
            " -0.72865486 -0.52984893 -0.7270675  -1.3891917  -0.71067774 -0.68487895\n",
            " -1.0853262 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 14 from 10\n",
            "\n",
            "Step 26 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.906393   -0.6490586  -0.81670505 -0.70104134 -1.0370375  -0.95824045\n",
            " -0.7603177  -0.7849262  -1.1069803  -0.90470636 -0.8053927  -0.73691523\n",
            " -0.7345799  -0.52378714 -0.73740315 -1.4078349  -0.7183837  -0.6858835\n",
            " -1.0940968 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 27 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.92301357 -0.6537303  -0.827356   -0.70792747 -1.0477457  -0.964594\n",
            " -0.77201533 -0.79309046 -1.1171725  -0.9169185  -0.82686436 -0.74605453\n",
            " -0.74168307 -0.5187186  -0.7482883  -1.4271469  -0.72612214 -0.68729496\n",
            " -1.1033659 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 28 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.93816066 -0.65842676 -0.8372725  -0.7144141  -1.0570496  -0.9712013\n",
            " -0.7829491  -0.8002509  -1.1268072  -0.92839146 -0.8470903  -0.75469065\n",
            " -0.7479824  -0.52279866 -0.75860023 -1.4461205  -0.73319006 -0.6893087\n",
            " -1.1119057 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 29 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.95198756 -0.6631513  -0.8465396  -0.7205446  -1.0650991  -0.9780437\n",
            " -0.79320186 -0.80651224 -1.1359512  -0.93921745 -0.8661771  -0.7628871\n",
            " -0.7535642  -0.5352633  -0.76840305 -1.464819   -0.73966146 -0.6918689\n",
            " -1.1197976 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 30 reward=-1 new_state=[0 0 0 0 1 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.96464825 -0.66792053 -0.85524666 -0.7263683  -1.072051   -0.98512053\n",
            " -0.8028629  -0.8119817  -1.1446867  -0.94949675 -0.8842357  -0.77070886\n",
            " -0.75852114 -0.5554073  -0.77776635 -1.4833206  -0.7456195  -0.69493574\n",
            " -1.1271355 ]\n",
            "PERFORMING RANDOM ACTION\n",
            "\n",
            "Taking action 8 from 8\n",
            "\n",
            "Step 31 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.97629493 -0.67276126 -0.86348593 -0.7319387  -1.0780663  -0.9924439\n",
            " -0.8120264  -0.8167664  -1.1531094  -0.9593351  -0.90138185 -0.77822113\n",
            " -0.7629499  -0.58258206 -0.78676355 -1.5017157  -0.75115335 -0.6984819\n",
            " -1.1340239 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 32 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.98413205 -0.6757394  -0.8683293  -0.7355025  -1.0794145  -0.9972898\n",
            " -0.81861115 -0.81923974 -1.1272414  -0.9656078  -0.9148438  -0.7828745\n",
            " -0.76405346 -0.6067865  -0.7937908  -1.5145411  -0.7537125  -0.69979423\n",
            " -1.1373646 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 33 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9911728  -0.67833656 -0.8726436  -0.7386607  -1.080646   -1.0015134\n",
            " -0.8244858  -0.82147825 -1.1036549  -0.9711871  -0.92691433 -0.7870153\n",
            " -0.765041   -0.6274718  -0.8000614  -1.5258842  -0.7559827  -0.70086086\n",
            " -1.1403193 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 34 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-0.9974846  -0.68058246 -0.8764708  -0.7414478  -1.0817657  -1.0051653\n",
            " -0.8297092  -0.82350147 -1.0821581  -0.97612655 -0.93771046 -0.7906839\n",
            " -0.76591873 -0.6449011  -0.8056371  -1.5358633  -0.75798553 -0.70169765\n",
            " -1.142917  ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 35 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0031292  -0.6825047  -0.87984973 -0.74389577 -1.0827787  -1.0082924\n",
            " -0.834336   -0.8253275  -1.0625719  -0.980476   -0.94734037 -0.79391813\n",
            " -0.76669294 -0.6593254  -0.81057614 -1.5445884  -0.7597413  -0.7023206\n",
            " -1.1451844 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 36 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0081645  -0.6841295  -0.8828168  -0.74603367 -1.08369    -1.0109389\n",
            " -0.83841693 -0.8269726  -1.0447295  -0.9842831  -0.9559048  -0.7967535\n",
            " -0.7673695  -0.6709836  -0.8149321  -1.5521631  -0.76126915 -0.7027446\n",
            " -1.1471469 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 37 reward=0 new_state=[0 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0126438  -0.6854808  -0.8854065  -0.7478889  -1.0845054  -1.0131452\n",
            " -0.8419992  -0.82845235 -1.0284762  -0.98759204 -0.96349657 -0.79922307\n",
            " -0.7679548  -0.6801021  -0.8187552  -1.558684   -0.76258713 -0.7029844\n",
            " -1.148829  ]\n",
            "\n",
            "Taking action 1 from 1\n",
            "\n",
            "Step 38 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0166162  -0.68658197 -0.8876507  -0.7494863  -1.085231   -1.0149497\n",
            " -0.8451264  -0.82978106 -1.0136704  -0.9904448  -0.9702022  -0.8013582\n",
            " -0.76845527 -0.68689513 -0.82209206 -1.5642416  -0.76371247 -0.7030542\n",
            " -1.1502532 ]\n",
            "\n",
            "Taking action 17 from 13\n",
            "\n",
            "Step 39 reward=-1 new_state=[1 0 0 0 0 0 0 0 1]\n",
            "Predicted scores for each action in next step: [-1.0231438  -0.71662724 -0.8918489  -0.75270057 -1.0872121  -1.0188946\n",
            " -0.8479375  -0.8314235  -1.0016725  -0.99594456 -0.9771087  -0.8039113\n",
            " -0.7697913  -0.6937865  -0.82609224 -1.5699699  -0.7656916  -0.7050073\n",
            " -1.1560233 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 40 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0294921  -0.7446251  -0.8962447  -0.7560797  -1.0890882  -1.0236353\n",
            " -0.85115755 -0.8329519  -0.99148595 -1.0017784  -0.984333   -0.8068259\n",
            " -0.7711866  -0.70922506 -0.8304448  -1.5772612  -0.76789457 -0.70770967\n",
            " -1.1618298 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 41 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0349877  -0.76966316 -0.9000685  -0.7590009  -1.0902718  -1.0276706\n",
            " -0.8540828  -0.83423746 -0.9818696  -1.0067773  -0.9907247  -0.809363\n",
            " -0.77223253 -0.72305465 -0.83414567 -1.5835652  -0.7697136  -0.7060482\n",
            " -1.1667619 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 42 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0397124  -0.79199743 -0.90337634 -0.7615105  -1.0908337  -1.031072\n",
            " -0.8567413  -0.8353051  -0.97279346 -1.0110221  -0.996361   -0.81155866\n",
            " -0.7729654  -0.7354131  -0.83725727 -1.5889758  -0.771188   -0.7004317\n",
            " -1.1709077 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 43 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0437448  -0.811869   -0.9062207  -0.7636515  -1.0908417  -1.0339056\n",
            " -0.85915947 -0.83617866 -0.96422875 -1.0145903  -1.0013149  -0.81344724\n",
            " -0.7734196  -0.7464306  -0.839839   -1.5935825  -0.7723556  -0.6912626\n",
            " -1.1743509 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 44 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0471573  -0.8295046  -0.90865076 -0.76546395 -1.090361   -1.0362337\n",
            " -0.8613624  -0.83688056 -0.95614815 -1.0175533  -1.0056549  -0.81506157\n",
            " -0.77362895 -0.75622916 -0.8419464  -1.5974706  -0.773252   -0.6789328\n",
            " -1.1771679 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 45 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.050017   -0.84511536 -0.9107129  -0.76698494 -1.0894537  -1.038114\n",
            " -0.8633734  -0.8374319  -0.9485266  -1.0199796  -1.009445   -0.81643283\n",
            " -0.773625   -0.76492316 -0.84363174 -1.6007206  -0.77391124 -0.66382134\n",
            " -1.1794311 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 46 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0523872  -0.85889846 -0.91245013 -0.7682488  -1.0881778  -1.0395997\n",
            " -0.8652144  -0.8378527  -0.941341   -1.0219325  -1.0127449  -0.8175901\n",
            " -0.77343804 -0.7726189  -0.8449439  -1.6034075  -0.7743648  -0.6462904\n",
            " -1.1812077 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 47 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0543257  -0.871036   -0.91390204 -0.769287   -1.0865887  -1.0407405\n",
            " -0.86690533 -0.83816135 -0.9345696  -1.0234706  -1.01561    -0.8185609\n",
            " -0.77309597 -0.779415   -0.84592843 -1.6056015  -0.7746426  -0.6266844\n",
            " -1.1825585 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 48 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0558858  -0.88169676 -0.91510564 -0.7701285  -1.0847374  -1.0415815\n",
            " -0.8684647  -0.83837485 -0.92819226 -1.0246483  -1.018091   -0.81937027\n",
            " -0.7726252  -0.7854025  -0.84662724 -1.6073678  -0.7747716  -0.6053278\n",
            " -1.1835413 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 49 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0571163  -0.89103544 -0.9160942  -0.77079976 -1.0826715  -1.0421646\n",
            " -0.8699089  -0.8385086  -0.9221904  -1.0255153  -1.0202346  -0.8200415\n",
            " -0.7720499  -0.79066515 -0.84707874 -1.6087656  -0.77477694 -0.58252436\n",
            " -1.1842074 ]\n",
            "\n",
            "Taking action 13 from 17\n",
            "\n",
            "Step 50 reward=0 new_state=[1 0 0 0 0 0 1 0 1]\n",
            "Predicted scores for each action in next step: [-1.0580618  -0.8991945  -0.91689813 -0.77132475 -1.0804347  -1.042527\n",
            " -0.8712534  -0.83857703 -0.9165467  -1.0261171  -1.0220833  -0.82059556\n",
            " -0.77139187 -0.79528    -0.84731853 -1.6098492  -0.7746813  -0.55855644\n",
            " -1.1846043 ]\n",
            "Epsilon reduced to 0.03486784401000002\n",
            "Total reward: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-Wsi_VVI3y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}